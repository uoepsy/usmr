---
title: "<b>Week 7: The Linear Model</b>"
subtitle: "Univariate Statistics and Methodology using R"
author: ""
institute: "Department of Psychology<br/>The University of Edinburgh"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - xaringan-themer.css
      - mc_libs/tweaks.css
    nature:
      beforeInit: "mc_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
library(knitr)
library(tidyverse)
library(ggplot2)
source('R/pres_theme.R')
knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
source('R/myfuncs.R')

library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

class: inverse, center, middle

# Part 1:  Correlation++

```{r recreate,include=FALSE}
set.seed(993)
give_me_a_sample <- function(size = 50){
  dat = as.data.frame(MASS::mvrnorm(size, mu=c(90,13), matrix(c(13,7,7,8),nrow=2)) )
  names(dat) = c("brain_vol","weekly_actv")
  dat %>% 
    mutate(
      brain_vol = pmax(0,pmin(100,round(brain_vol, 2))), 
      weekly_actv = pmax(0, round(weekly_actv))
    )
}
dat <- give_me_a_sample(20)
```

---
# Exercising our brains

.pull-left[
```{r frecap,fig.asp=.6,echo=FALSE}
p1 <- dat %>% ggplot(aes(x=weekly_actv,y=brain_vol)) +
  xlab("Weekly Activity (hours)") + ylab("Brain Volume\n(% of intracranial space)") +
  geom_point(size=3)+
  ylim(80,100)
p1
rd <- cor.test(dat$brain_vol,dat$weekly_actv)
```

.center[
$r= `r rd$estimate`$, $p = `r rd$p.value`$
]
]
.pull-right[
![:scale 70%](lecture_7_files/img/playmo_brain.jpg)
]
???
- start by picking up where Monica left off in week 5.
- now if you remember, we were talking about how exercise and brain volume are associated
- so here we are, we've got data from 20 of our little playmo characters. 
- we've put little smartwatches on them, and we've measured their brain volume. 

- and we've established that there is a positive correlation. hooray. 

- is there anything more the data about exercise and brain volume can tell us?
- we know that exercise increases brain volume, it would be good if we could say something about _how good_ exercise is for the brain? 
  + how much better does exercise make things?  
  
---
# Exercising our brains (2)

.pull-left[
```{r frecap2,fig.asp=.6,echo=FALSE}
p1 + geom_smooth(method="lm",se=FALSE,col = '#FCBB06',lwd=2)
m1 <- lm(brain_vol ~ weekly_actv, data=dat)
```

.tc[
"for every extra 1 hour more weekly activity, brain volume increases by `r round(coef(m1)[2],2)` (% of intracranial space)"
]
]
.pull-right[
![:scale 70%](lecture_7_files/img/playmo_brain.jpg)

]
???
- so we saw this a lot in week 5 - the idea of the correlation being represented by the line of best fit. 
- but we can actually describe this line in a lot more detail! 
- we can use it to make statements like this one "for every extra 1 hour more weekly activity, brain volume increases by `r round(coef(m1)[2],2)` (% of intracranial space)"
 
- using this kind of line on a plot is only permissible if the relationship between activity and brain volume is _linear._  
- that is, that each additional minute of activity affects brain volume by the same amount


---
# The Only Equation You Will Ever Need

.br3.pa2.f2.white.bg-gray[
$$ \textrm{outcome}_i = (\textrm{model})_i + \textrm{error}_i $$
]

.center.pt2[
![:scale 25%](lecture_7_files/img/model_error.svg)
]

???
- in order to put this line on the graph, we've built a statistica model of the data
- that model is representable, at least in basi terms, by "the only equation you will ever need". 
- and that equation says that for a particular outcome, that outcome can be represented as whatever the model says for that outcome, plus an error term.  

- in other words, the model makes a prediction for the ith datapoint. 
- that's the red dot here.
- but that prediction won't be the same as the actual observed ith datapoint. 
- that's because of other factors that can affect things. 

- so in our exercise and brain volume example, there are other things which influence brain volume, that we haven't measured. 
- things like age, etc. 
- as our model doesn't include things like age, it's not going to capture that. 
- so there's going to be a rough approximation to brainvolumes, but not exact. 
- aim of statistics is often = maximise explanatory value of model. 
- minimise amount of variance attributable to error

---
count: false
# The Only Equation You Will Ever Need

.br3.pa2.f2.white.bg-gray[
$$ \textrm{outcome}_i = (\textrm{model})_i + \textrm{error}_i $$
]

- to get any further, we need to make _assumptions_

- nature of the **model** .tr[
(linear)
]

- nature of the **errors** .tr[
(normal)
]

???
- so let's start looking into how the linear world works

- before we do, we need to make a couple of assumptions
  - the first is about the nature of the model. it's linear.
  - the second, which we return to next week, is that the errors conform to the normal distribution - i.e. they're normally distributed
  - this will allow us to use all sorts of approximation functions to work out what the model might be - i.e. how it may generalise beyond our sample to the wider world

---
# A Linear Model

.flex.items-top[
.w-40.pa2[
$$ \color{red}{\textrm{outcome}_i} = \color{blue}{(\textrm{model})_i} + \textrm{error}_i $$
$$ \color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \epsilon_i $$
.center[
so the linear model itself is...
]

$$ \hat{y}_i = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} $$
.center[
![:scale 40%](lecture_7_files/img/formula.svg)
]
]
.w-60[
&nbsp;
]]

???
- linear model looks like this:
- [GENERAL MODEL FORMULA]
- [Y = b0 + b1 x + e]
- [MODEL part]



---
count: false
# A Linear Model

.flex.items-top[
.w-40.pa2[
$$ \color{red}{\textrm{outcome}_i} = \color{blue}{(\textrm{model})_i} + \textrm{error}_i $$
$$ \color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \epsilon_i $$
.center[
so the linear model itself is...
]

$$ \hat{y}_i = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} $$

.center[
![:scale 40%](lecture_7_files/img/formula.svg)
]]

.w-60.pa2[
```{r bb,echo=F, fig.asp=.6}
x <- tibble(x=c(-1,4))
f <- function(x) {5+2*x}
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=f,size=1) +
  geom_segment(aes(x=0,xend=0,y=0,yend=f(0)),arrow=arrow(ends="both",length=unit(.05,"native")),colour="blue") +
  geom_segment(aes(x=1,xend=2,y=f(1),yend=f(1)),linetype="dotted") +
  geom_segment(aes(x=2,y=f(1),xend=2,yend=f(2)),arrow=arrow(ends="both",length=unit(.05,"native")),colour="blue") +
  annotate("text",x=.6,y=2.5,label="b[0]~(intercept)",
           size=5,parse=TRUE) +
  annotate("text",x=2.6,y=7.5,label="b[1]~(slope)",
           size=5,parse=TRUE) +
    ggtitle(expression(paste(b[0]," = 5, ",b[1]," = 2")))

p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  theme(axis.title.y = element_text(colour = "blue"))
```

]]

???
- so here's an example graph
- arbitrarily set b0 and b1
- so where x = 0, then the b1 cancels out, and it's just b0
- where x = 0, estimated y is 5
- where x != 0, we need to multiply b1 time x and add it to b0
- where x = 2, y is going to be 5 + 2*2 = 9
- where x = 3, y is going to be 5 + 2*3 = 11
- [R syntax]

---
count: false
# A Linear Model

.flex.items-top[
.w-40.pa2[
$$ \color{red}{\textrm{outcome}_i} = \color{blue}{(\textrm{model})_i} + \textrm{error}_i $$
$$ \color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \epsilon_i $$
.center[
so the linear model itself is...
]

$$ \hat{y}_i = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} $$
.center[
![:scale 40%](lecture_7_files/img/formula.svg)
]

.br3.pa3.bg-light-yellow[
$$ \hat{y} = \color{blue}{b_0 + b_1 \cdot{} x_i} $$
.center[
![:scale 50%](lecture_7_files/img/formula2.svg)
]]
]
.w-60.pa2[
```{r bb2,fig.asp=.6,echo=F}
p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  theme(axis.title.y = element_text(colour = "blue"))
```

]]

???
- if you're astute
- b0 x 1 is a bit redundant
- b0 x 1 is just b0
- so we can simplify, and just write the model as b0 + b1 times x
- and in R, as just y ~ x


---
# Take An Observation

.flex.items-top[
.w-40.pa2[

.br3.bg-gray.white.f2.pa2.tc[
x<sub>i</sub> = 1.2, y<sub>i</sub> = 9.9
]
```{r vals,include=F}
xX <-1.2
yY <- 9.9
```
 
$$ \hat{y}_i = b_0 + b_1\cdot{}x_i = 7.4 $$ 
$$ y_i = \hat{y}_i + \epsilon_i = 7.4 + \color{red}{2.5} $$

]
.w-60.pa2[
```{r errplot,fig.asp=.6,echo=FALSE}
p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="red") +
  annotate("text",.8,8.6,label=expression(paste(epsilon[i]," (error)")),colour="red",size=5)
```
]
]
???
- lets take an observation
- $\hat{y}_i$ is what the model _predicts_ for $x_i$

- $y_i$ is the actual value that was observed for $x_i$

- when we have data, and fit a model, what it's doing is minimising these errors. 
  
  + for one thing, the model can predict $\hat{y}$ for values of $x$ that we have never observed


---
# More Brain Exercises 

.pull-left[

.center[
"for every extra 1 hour more weekly activity, brain volume increases by `r round(coef(m1)[2],2)` (% of intracranial space)"
]

]
.pull-right[
```{r eval=FALSE}
+ geom_smooth(method="lm")
```
```{r plot2, fig.asp=.6, echo=F}
dat %>% ggplot(aes(x=weekly_actv,y=brain_vol)) +
  xlab("Weekly Activity (hours)") + ylab("Brain Volume\n(% of intracranial space)") +
  geom_point(size=3) + 
  ylim(80,100) + 
  geom_smooth(method="lm",lwd=2)
```
]

???
- okay lets go back to our data
- we've fit our model, and it's saying that for every extra 1 hour more weekly activity, brain volume increases by `r round(coef(m1)[2],2)` (% of intracranial space)
- and we can actually visualise this simple model by adding a geom_smooth().  



--

.center.f3.pt3[
but how can we evaluate our model?
]

???
- but we're going to want to evaluate our model.
- so lets take a look at how we build this model in R

---
# Linear Models in R

```{r lm1, eval=F}
mod <- lm(brain_vol ~ weekly_actv, data=dat)
```

???
- here we are. 
- lm() function
- syntax like we've seen with t.test.. it's a formula syntax
- y predicted by x
- no output, we save it as an object


---
count: false
# Linear Models in R

```{r lm2, highlight.output=c(11,12)}
mod <- lm(brain_vol ~ weekly_actv, data=dat)
summary(mod)
```

???
- to look at the output, we use summary() on the model object
- whole load of useful information
- the main thing we're going to look at first, is these highlighted lines
- intercept and slope
- b0 and b1 we saw before
- [VALUES]
- what does it look like

---
# Intercept and Slope Again

$$ b_0= `r round(coef(mod)[1],1)`; \quad b_1= `r round(coef(mod)[2],2)` $$
.center[
```{r git,echo=F,fig.asp=.6}
p1 <- dat %>% ggplot(aes(x=weekly_actv, y = brain_vol)) +
  xlab("Weekly Activity (hours)") + ylab("Brain Volume\n(% of intracranial space)") +
  ylim(67.5,100) +
  scale_x_continuous(limits=c(-.5,18.5),breaks=seq(0,20,2)) +
  geom_segment(aes(x=0,y=70,xend=0,yend=coef(mod)[1]),colour="red") +
  geom_segment(x=0,y=67.5,xend=0,yend=70,col="red",lty="dashed")+
  annotate("text",x=.5,y=coef(mod)[1]-1,label=expression(b[0]),size=5,colour="red") + 
  NULL

p1
```
]
???
- $b_0$ is the the predicted value, $\bar{y}$, when $x=0$.  Here I've represented it as a line, so where the top is the intercept value of `r coef(mod)[1]`

---
count: false
# Intercept and Slope Again

$$ b_0= `r round(coef(mod)[1],1)`; \quad b_1= `r round(coef(mod)[2],2)` $$
.center[
```{r git2,echo=F,fig.asp=.6}
lmf <- function(x) {
  coef(mod)[1]+coef(mod)[2]*x
}
p2 <- p1 +
  geom_segment(aes(x=4,xend=5,y=lmf(4),yend=lmf(4)),
               arrow=arrow(ends = "both",length = unit(.015,"native"))) +
  annotate("text",x=4.5,y=lmf(3.2),label="1",size=5) +
  geom_segment(aes(x=5,xend=5,y=lmf(4),yend=lmf(5)),
               colour="red") +
  annotate("text",x=5.4,y=lmf(4.5),label=expression(b[1]),colour="red",size=5) +
  geom_segment(aes(x=0,xend=1,y=lmf(0),yend=lmf(0)),
                arrow=arrow(ends = "both",length = unit(.015,"native"))) +
  geom_segment(aes(x=1,xend=1,y=lmf(0),yend=lmf(1)),
                colour="red") +
  annotate("text",x=1.4,y=lmf(0.5),label=expression(b[1]),colour="red",size=5)
p2  
```
]

???
- the slope is how much y changes for every 1 unit of x
- how much brain volume changes for every hour increase in activity time

---
count: false
# Intercept and Slope Again

$$ b_0= `r round(coef(mod)[1],1)`; \quad b_1= `r round(coef(mod)[2],2)` $$
.center[
```{r git3, echo=FALSE, fig.asp=.6}
p2 + geom_abline(intercept=coef(mod)[1],slope=coef(mod)[2],colour="blue",size=.7)
```
]

???
- so we can draw the line across here

---
count: false
# Intercept and Slope Again

$$ b_0= `r round(coef(mod)[1],1)`; \quad b_1= `r round(coef(mod)[2],2)` $$
.center[
```{r git4, echo=FALSE, fig.asp=.6}
p2 +
  geom_abline(intercept=coef(mod)[1],slope=coef(mod)[2],colour="blue",size=.7) + 
  geom_point(size=3)
```
]
???
- note that the intercept is really very far from the data we're interested in.

- it may be pretty meaningless to talk about the brain volume of someone who does zero hours activity

- may recenter x
- or just focus on slope

- neither intercept or slope are interesting if they're not significant, so that's what we'll focus on next.  

---
class: inverse, center, middle, animated, fadeInDownBig
# End of Part 1

---
class: inverse, center, middle

# Part 2
## Significance

---
# Intercept and Slope

```{r lm5,highlight.output=c(11,12)}
<<lm2>>
```

???
- here we are
- fitted the model to some data using lm()
- summarise the model, using summary()

- we've talked about the intercept and slope estimates
- there are other numbers
- standard errors, t values, and something that looks like probability

---
# Are We Impressed?

```{r getvals,include=FALSE}
m.i <- round(coef(mod)[1],1)
m.s <- round(coef(mod)[2],2)
```

- we have an intercept of `r m.i` and a slope of `r m.s`

- in NHST world, our pressing question is

???
- so in broad terms our question is - are we impressed by our intercept and slope
- in the null hypothesis significance testing world, that question is...

--

.pt4[
&nbsp;
]

.br3.bg-gray.pa2.white.tc[
how likely would we have been to find these parameters under the null hypothesis?
]

???
- how likely would we have been to find these parameter estimates under the null hypothesis. 
- in other words:
- in a universe where there's nothing but chance, how likely would we be to see something like what we have seen

---
# Testing Chance

.center[
```{r chanceg,echo=F, fig.asp=.6, fig.width=5.5}
iS <- function(size) {
  tt <- give_me_a_sample(20)
  m <- lm(brain_vol ~ weekly_actv,data=tt)
  return(coef(m))
}

cS <- as_tibble(t(replicate(1e3,iS(20)))) 

#cS = MASS::mvrnorm(n=1e3,mu=coef(mod),Sigma=vcov(mod)) %>% as.data.frame

dat %>% ggplot(aes(x=weekly_actv,y=brain_vol)) +
  ylim(67.5,100) +
  scale_x_continuous(limits=c(-.5,18.5),breaks=seq(0,20,2)) +
  geom_abline(data=cS,aes(intercept=`(Intercept)`,slope=weekly_actv),alpha=.1,colour="blue")
```
]

- repeatedly sampling 20~datapoints from the population

  + variability in _height_ of line = variability in intercept ( $b_0$ )
  + variability in _angle_ of line = variability in slope ( $b_1$ )
  
???
- continuing with our trend, 
- use simulation to build the intuition behind testing chance
- in reality, we don't have to simulate, we can use math, but simulations are useful to kind of "see what is happening" 

- this data is fake
- i **know** the parameters defining what is happening here.
- this is a graph where i have taken 1000 samples of 20 people. and fitted a model to each.
- and we can see that the mass of the lines appear to be going up. 

- the variability in the height of the line is the SE of intercept
- variability in slope is SE of slope

- one important thing to remember is that in practice, we don't have access to the population.
- i.e. you don't know how i've faked the data, you just have 20 datapoints and a single line
  
---
# We've Seen This Before

.center[
```{r figfig,fig.asp=.6,fig.width=5.5,echo=F}
dat %>% ggplot(aes(x=weekly_actv,y=brain_vol)) +
  ylim(67.5,100) +
  scale_x_continuous(limits=c(-.5,18.5),breaks=seq(0,20,2)) +
  geom_point(size=3) +
  geom_smooth(method="lm", fullrange = TRUE)
```
]
- shaded area represents "95% confidence interval"

  + if we repeatedly sampled 20 items from the population...
  + assumes that the 20 we have are the _best estimate_ of the population

???
- so that single line is your best estimate -that's the blue line here
- and the grey areas are the uncertainty around that, calculated not by simulations, but using a formula for the standard error. 
- and we've seen this already
- this is what we got with geom_smooth() function

  + **just like means and standard errors** (here the edges of the grey regions are `r -qt(.025,18)` standard errors from the line)
  + what range of lines would we get 95% of the time


---

# The Good Old _t_-Test

```{r pp,echo=F}
.pp(summary(mod),l=list(c(10:12)))

```


- for each model parameter we are interested in whether it is _different from zero_

- **intercept**: just like a mean

- **slope**: does the best-fit line differ from horizontal?

???
- for each parameter, we are able to test whether it is different from zero

--

- these are just (two-tailed) one-sample $t$-tests

  + **standard error** is the standard deviation of doing these lots of times
  + **t value** is $\frac{\textrm{Estimate}}{\textrm{Std. Error}}$
  + to calculate $p$, we need to know the _degrees of freedom_
  
???
- note, this is two-tailed
  - not asking if it's larger/smaller than zero. just if it's different.
  
- the intercept is just like a mean 
- is that mean different from zero?
- askign if the slope is different from zero is asking if the line is different from horizontal

- signif positive then it's going up
- signif negative then it's going down

- these are just our standard t tests.
- standard error is just "the standard deviation of doing these lots of times"
- an estimate of what variation we would expect if we repeatedly sampled from the population
- and again, like we saw earlier, the test statistic is the thing we measured the difference from zero for our coef, in terms of standard errors.

- and we can get out our p-values, but to do so, we need to know the degrees of freedom 
  
---
# Degrees of Freedom

.center[
```{r plotdat,fig.asp=.6,echo=FALSE}
## function to work out the "last two" points on for a given best fit
## line, by working out the residuals for given x and y
## (maths courtesy of Umberto Noè)
library(patchwork)
plot_dat <- function() {
  df <- tibble(
      x = sample(2:14,8),
      y = 20 + runif(8,-15,15),
      resid = y - (5+2*x),
      colour="a"
  )

  df <- df %>% mutate(xprod=x*resid)  

  sr <- sum(df$resid)
  sp <- sum(df$xprod)

  df2 <- tibble(
    x= c(1,15),
    yhat = 5 + 2 * x
  )  

  r2 <- sr*df2$x[1] - sp
  r1 <- -sr - r2

  df2 <- df2 %>% mutate(
    resid=c(r1,r2),
    y=yhat+resid,
    colour="b"
  ) %>% relocate(y, .after=yhat)

  df %>% bind_rows(df2) %>% select(-xprod) %>% select(-yhat)
}

pd1 <- plot_dat()
pd2 <- plot_dat()
pd3 <- plot_dat()
pd4 <- plot_dat()

do_p <- function(x) {
x %>% filter(colour=="a") %>% ggplot(aes(x=x,y=y)) +
  theme_presentation(14) +
  geom_point(size=3) +
  xlim(c(0,15)) +
  geom_smooth(method="lm",se=FALSE)
}

p1 <- do_p(pd1)
p2 <- do_p(pd2)
p3 <- do_p(pd3)
p4 <- do_p(pd4)

( p1 + p2 ) / ( p3 + p4 )
```
]

???
- let's talk about DF
- here are random collections of 8 datapoints
- each graph has 8 points, each graph has a line of best fit

- now, for these collections of 8 points,
- we can always add in another two points to each plot and make the best fit line the same in each


---
count: false
# Degrees of Freedom

.center[
```{r dfplots2,fig.asp=.6,echo=F}
do_p <- function(x) {
x %>% ggplot(aes(x=x,y=y)) +
  theme_presentation(14) +
  geom_point(aes(colour=colour),size=3) +
  xlim(c(0,15)) +
  scale_colour_manual(values=c('a'='darkgrey','b'='red')) +
  theme(legend.position = "none") +
  geom_smooth(method="lm",se=FALSE)
}

p1 <- do_p(pd1)
p2 <- do_p(pd2)
p3 <- do_p(pd3)
p4 <- do_p(pd4)

( p1 + p2 ) / ( p3 + p4 )
```
]

???
- so here we've added two points to each, and the lines are now the same in each

- this is one way of showing that for $n$ data points, there are $n-2$ degrees of freedom

---
# Degrees of Freedom

- in fact we subtract 2 degrees of freedom because we "know" two things

  + intercept ( $b_0$ )
  
  + slope ( $b_1$ )

- the remaining degrees of freedom are the _residual_ degrees of freedom

???
- we subtract 2, because we know 2 things - the intercept and the slope. 
- the remaining degrees of freedom are residual degrees of freedom
- so in our activity brain volume example we have 20 people, so we have 18 residual degrees of freedom
- and our tests of intercept and slope will be calculated using t distribution with 18 df

--

- the _model_ also has associated degrees of freedom

  + 2 (intercept, slope) - 1 (knowing one affects the other)
  
.br3.bg-gray.pa2.white.tc[
the models we have been looking at have 20 observations and 1 predictor

**(1, 18)** degrees of freedom
]  

???
- as well as residual degrees of freedom, it is useful to think of the overall model degrees of freedom
- the model has two parameters. the intercept and the slope
- if you think about it, the intercept and the slope are related. 
- once we define one, it affects the possible values the other can take
- so one way to think about this is that the model has 1 predictor, so 1 degree of freedom. 
- and the residuals have 18 degrees of freedom

---
# Linear Models in R

```{r lm3, highlight.output=c(17,18)}
<<lm2>>
```

???
- why is this useful?
- well, we can calculate other things
- rather than effects of particular predictors
- we can ask questions about whether the model overall is of value
  - this will become a lot more relevant when we start building more complex models
  
- the crucial part of the summary output is at the bottom, highlighted here
- two things. R2 and F


---
# Total Sum of Squares

$$ \textrm{total SS}=\sum{(y - \bar{y})^2} $$

.pull-left[
- sum of squares between observed $y$ and mean $\bar{y}$

- represents the total amount of variance in the model

- how much does the observed data vary from a model which says "there is no effect of $x$" (**null model**)?
]
.pull-right[
```{r totss,echo=F,fig.asp=.6}
dat <- dat %>% mutate(pred=predict(m1),mWA=mean(brain_vol))
p1 <- dat %>% ggplot(aes(x=weekly_actv,y=brain_vol,yend=pred))

pTot <- p1 + geom_abline(intercept=mean(dat$brain_vol),slope=0,size=1) +
  geom_segment(aes(yend=mean(mWA),xend=weekly_actv),linetype="dotted",colour="red",size=1) +
  geom_point(size=3)
pTot
```

]


???
- to understand these, we need to think about a couple of calculations
- we'll start with total sum of squares
- these are the sum of squares between observed y and the mean

- sum of all those red dotted lines, squared. 

- represents total amount of variance in the data
- if we had no info about weekly activity, the best model we could make is one that has just an intercept, i.e. the mean. 
- that model would be called the "null model" 


---
# Residual Sum of Squares

$$ \textrm{residual SS} = \sum{(y - \hat{y})^2} $$
.pull-left[
- sum of squared differences between observed $y$ and predicted $\hat{y}$

- represents the unexplained variance in the model

- how much does the observed data vary from the existing model?
]
.pull-right[
```{r residp,echo=F,fig.asp=.6}
pRes<- p1 + geom_segment(aes(xend=weekly_actv),linetype="dotted",colour="red",size=1) +
  geom_smooth(method="lm",se=FALSE) +
  geom_point(size=3)
pRes
```
]

???
- contrast that with the residual Sums of squares from our model
- this is the sum of squared differences from observed y to the predicted y. 
- again, sum of squared red dotted lines here

- this is representing the variance that is left unexplained by our model, once we allow our model to have the predictor of weekly activity

- like asking how much does the data vary from the model that we're now testing? 

---
# Model Sum of Squares

$$ \textrm{model SS} = \sum{(\hat{y} - \bar{y})^2} $$

.pull-left[
- sum of squared differences between predicted $\hat{y}$ and mean $\bar{y}$

- represents the additional variance explained by the current model over the null model
]
.pull-right[
```{r modp,echo=F,fig.asp=.6}
pMod <- p1 + geom_segment(aes(y=mWA,xend=weekly_actv),colour="red",linetype="dotted",size=1) +
  geom_hline(yintercept = mean(dat$brain_vol),size=1) +
  geom_smooth(method="lm",se=FALSE) +
  geom_point(size=3)
pMod
```

]

???
- finally the model sums of squares is the third one we need to look at
- and this is the sum of squared differences between the predicted values and the mean. 

- so this is the additional variance that is explained by the current model over the null model

- i.e. the variance we explain by having weekly ativity as a predictor of brain volume, instead of just using the mean brain volume

---
# Testing the Model: _R_<sup>2</sup>

.pull-left[
$$ R^2 = \frac{\textrm{model SS}}{\textrm{total SS}} = \frac{\sum{(\hat{y}-\bar{y})^2}}{\sum{(y-\bar{y})^2}} $$



"how much the model improves over the null"


.pt3[
- $0 \le R^2 \le 1$

- we want $R^2$ to be large

- for a single predictor, $\sqrt{R^2} = |r|$

]]

.pull-right[
```{r echo=F,fig.asp=0.8}
library(patchwork)
pMod/pTot
```

]

???
- out of these three things, we can calculate a couple of useful statistics
- the first is R^2. 

- R squared is model sum of squares divided by total sum of squares

- again, here are the relevant graphs on the right.

- so we can see that Rsquared is telling us what proportion of the total variance is captured/explained by our model

- so a value of 0 is that our model with weekly activity explains nothing more than just using a mean
- a value of 1 would mean that our model explains literally all the variance

- a little aside, when we have a single predictor like this, Rsquared is going to be the value of r (correlation coefficient from week 5), squared. hence the name! 


---
# Testing the Model: _F_

.pull-left[
$F$ ratio depends on **mean squares** <br/>
.tc[
( $\textrm{MS}_x = \textrm{SS}_x/\textrm{df}_x$ )
]

$$F=\frac{\textrm{model MS}}{\textrm{residual MS}}=\frac{\sum{(\hat{y}-\bar{y})^2}/\textrm{df}_m}{\sum{(y-\hat{y})^2}/\textrm{df}_r}$$



"how much the model improves over chance"


.pt3[
- $0 < F$

- we want $F$ to be large

- significance of $F$ does not always equate to a large (or theoretically sensible) effect

]]

.pull-right[
```{r echo=F,fig.asp=0.8}
library(patchwork)
pMod/pRes
```

]

???
- another metric we can use to evaluate our model is the F ratio, or F statistic

- this is calculated using mean squares
- and mean squares are the sums of squares divided by the relevant degrees of freedom

- so you can sort of think of these as the sums of squares, but weighted by how many bits of information are contributing to those variances.
- the model explains some variance in our data, thats the model SS, and to do so it uses 1 thing - the predictor of weekly activity
- the residual sums of squares also accounts for variance in our data, and to do so it uses all the available datapoints - i.e. it can use 18, because 2 of them have to be fixed to get our intercept nad slope

- and the F ratio is, if you like, a measure of how much the model improves over chance. 
- So how much does the average amount of variance explained by the model improve over the average amount of variance caused by an observation.

- F is always going to be >0, and we want it to be large, but the significance does not always equate to a sensible effect. 
- it simply tells us that knowing weekly activity hours is better than not knowing about it. 
- so we are best to also look at the coefficients to determine if the effect is particularly impressive

---
# A Linear Model for 20 Brains

.center[
```{r mod,fig.asp=.55,echo=FALSE,fig.width=6}
pRes
```
]

- a linear model describes the **best-fit line** through the data

- minimises the error terms $\epsilon$ or **residuals**

???
- so lets return to our 20 brains
- here's a linear model for our 20 people 
- the model describes the best fit line through the data
- it minimises the error terms/epsilons/residuals

---
# Two Types of Significance

```{r lmlm,highlight.output=c(11,12,17,18)}
<<lm2>>
```

???
- and we've looked at 2 types of significance
- first is whether the coefficients b0 b1 are different from zero
- second is about variance explained. 
- that's R^2, so that tells us that knowing about weekly activity hours allows us to account for 56% of the variance in brain volumes
- and the F statistic, which is telling us that knowing about weekly activity hours improves the model over the null model, in which we know nothing other than what the brain volumes are

---
# The Good, the Bad, and the Ugly

.flex.items-center[
.w-50.br3.bg-washed-green.pa2[
- we can easily extend this approach

  + use more than one predictor
  
  + generalised linear model
]
.w-50.br3.bg-washed-red.pa2[
- not a panacea

  + depends on _assumptions_ about the data
  
  + depends on _decisions_ about analysis
]]

???
- okay so here's the good, the bad and the ugly about these models
- the good is that these can easily be extended to use more than one predictor (that's next week), and can be applied to different types of outcomes (that's week 10)

- but it's not a panacea. it depends on assumptions.
- throuhgout the lecture we've assuemd there's a linear relationship between activity hours and brain volume. and that may simply not be the case
- and it depends on decisions we make in the analysis. we'll see this a lot more in coming weeks

--

.flex.items-center[
.w-70.br3.pa2.bg-gray.white[
- like other statistics, linear models don't tell you "about" your data

- they simply assess what is (un)likely to be due to chance

- the key to good statistics is _common sense and good interpretation_
]
.w-30.center[
![:scale 70%](lecture_7_files/img/playmo_clown.jpg)
]
]


???
- the ugly, is that linear models don't really tell us about our data.
- they simply assess what is likely to be observed in a universe governed by chance.

- the key to good statistics is always going to be common sense and good interpretation


---
class: inverse, center, middle, animated, fadeInDownBig

# End

