---
title: "Measurement and Distributions"
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
source('_theme/theme_quarto.R')
```


::: {.content-visible when-profile="local"}

# A Quick Chat about AI

## ELM {.nostretch}

::: r-stack
![](img/elm1.png){fig-align="center" width=60% .fragment .fade-out fragment-index=1}

![](img/elm2.png){fig-align="center" width=60% .fragment fragment-index=1}

:::

[[elm.edina.ac.uk](https://elm.edina.ac.uk/)]{.center}


## Verifiable Results {.smaller}

:::: {.columns}

::: {.column width="50%"}
I have some R code

```r
dice <- function(num=1) {
  sum(sample(1:6,num,replace = TRUE))
}

throws <- table(replicate(1000,dice(2)))

barplot(throws)
```
I want to change the colours of the bars (each bar should be a different colour), add a title to the graph, and label the axes. Can you help me change the code?
:::

::: {.column width="50%" .fragment}
Certainly! You can modify your R code to change the colors of the bars, add a title, and label the axes. Here's how you can do it:

```r
...
barplot(throws, 
        col = bar_colors,          # Set different colors for each bar
        main = "Distribution of Dice Throws",  # Add a title
        xlab = "Sum of Two Dice",  # Label for the x-axis
        ylab = "Frequency")        # Label for the y-axis
```

```{r}
#| echo: false
# Define the dice function
dice <- function(num=1) {
  sum(sample(1:6, num, replace = TRUE))
}

# Simulate 1000 throws of two dice
throws <- table(replicate(1000, dice(2)))

# Create a vector of colors for the bars
bar_colors <- rainbow(length(throws))

# Plot the barplot with customizations
barplot(throws, 
        col = bar_colors,          # Set different colors for each bar
        main = "Distribution of Dice Throws",  # Add a title
        xlab = "Sum of Two Dice",  # Label for the x-axis
        ylab = "Frequency")        # Label for the y-axis

```


:::

::::

## Beware!

:::: {.columns}

::: {.column width="66%"}
![](img/AI_tripe.png)
:::

::: {.column width="34%"}
![](img/AI_atoms.png)
:::

::::

. . .

- AI is only useful you can _understand_ and _verify_ its response


:::

# Measurement

## The problem with measurement

- when we measure something, we want to identify its **true measurement** (the **ground truth**)

:::: {.columns}

::: {.column width="50%"}
- we don't have any way of measuring accurately enough

- our measurements are likely to be _close to_ the truth

- they might vary, if we measure more than once
:::

::: {.column width="50%"}
![](img/playmo2.jpg){width=80%}
:::

::::


## Measurement
- we might _expect_ values close to the "true" measurement to be more frequent

:::: {.columns}

::: {.column width="40%"}
![](img/playmo_proft.svg){width=80% .center-img}
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| fig-asp: .7

set.seed(8423398)
pm <- data.frame(x=rnorm(70,7.5,.05))
pm %>% ggplot(aes(x=x)) +
  geom_density(linewidth=1.5) + scale_y_continuous(breaks=NULL) +
  geom_vline(xintercept=7.5,colour="red",linewidth=2) +
  annotate(geom="text",colour="red",x=7.5,y=3,label="TRUE MEASURE",size=9) +
  xlab('height (cm)') + ylab("")
```

:::

::::

::: notes
- so let's do a thought experiment and imagine what things
would look like if lots and lots of people tried to measure
the "true" distance of the notch from the end of the stick.

- most of them would be quite competent, and we would expect
the majority of the measurements to be close to the "true" value.

- every now and again, someone would overshoot or undershoot by rather
more.

- _theoretically_ they might be completely off-beam, although the chances of being way off get vanishingly small quite quickly.
:::

## Something quite familiar


```{r}
#| fig-asp: .55
#| echo: false
#| label: normnorm

t <- data.frame(x=c(7.35,7.65))
t %>% ggplot(aes(x)) +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.05),size=2) +
  ylab("") +
  geom_vline(xintercept = 7.5,colour="red",size=2) +
  scale_y_continuous(breaks=NULL) +
  xlab("height (cm)")
```


::: notes
- if we think a bit more about our thought experiment, we've actually described
something quite familiar:  A bell curve

- but what is a "bell curve"?

- to answer that question, let's start back where we were last week, with dice.
:::

## Dice again


```{r dice, echo=FALSE,fig.asp=.6}
set.seed(13)
dice <- function(num=1) {
  sum(sample(1:6,num,replace = T))
}
t <- data.frame(x=replicate(1000,dice(2)))
t %>% ggplot(aes(x)) + geom_bar() +
  scale_x_continuous(breaks=2:12,name = 'sum of dice',limits = c(1,13)) +
  ggtitle('1000 throws of 2 dice')

```


- the heights of the bars represent the numbers of times we obtain each value

- but why are the bars not touching each other?


## Dice throws aren't really numbers


:::: {.columns}

::: {.column width="50%"}
- **A** = ![](img/A1.svg){width=20%}

- **B** = ![](img/B1.svg){width=20%} or ![](img/B2.svg){width=20%}

- **C** = ![](img/C1.svg){width=20%} or ![](img/C2.svg){width=20%} or ![](img/C3.svg){width=20%}
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-asp: .5
t %>% ggplot(aes(x)) + geom_bar() +
  scale_x_continuous(breaks=2:12,labels = c('A','B','C','D','E','F','G','H','I','J','K'),
  name = 'outcome type',limits = c(1,13)) +
  ggtitle('1000 throws of 2 dice')
```
:::

::::

- bar plot ("bar chart") always has gaps between bars
- represents frequencies of _discrete categories_ (`factors`)

::: notes
- I could just label the possible outcomes of throwing two dice arbitrarily

- if you think about it, there are only 11 possible values that the sum
  of two dice can take.

- and if the dice didn't have actual numbers on their faces, you could still
  enumerate the outcomes

- so the outcomes are _discrete_ (you can never throw a value between 3 and 4, or between "B and C")
- and the bars on a bar plot have gaps between them to show this.
:::

## Back to playmobil

:::: {.columns}

::: {.column width="40%"}
![](img/playmo2.jpg)
:::

::: {.column width="60%"}
- **height** is a Ratio variable
- there will be limits to our precision, coventionally indicated by number of digits

::: fragment

| written | ⊢ min | ⊢ max |
|----|---|---|
| 7.5 | ≥ 7.450 | < 7.550 |
| 7.50 | ≥ 7.495 | < 7.505 |

: **height of figure (cm)**
:::

:::
::::

## Histograms    

:::: {.columns}

::: {.column width="40%"}
- we can represent all the measurements with a **histogram**

- the bars are touching because this represents _continuous_ data

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| fig-asp: .6

minx <- round(min(pm$x - .02),2)
maxx <- round(max(pm$x + .02),2)
p <- pm %>% ggplot(aes(x=x)) + geom_histogram(binwidth=0.01) +
  scale_x_continuous(limits=c(minx,maxx)) + xlab('height (cm)')
layD <- layer_data(p) %>% filter(x>=7.495 & x<=7.505)
barHt <- layD$y
p
```

:::
::::

## Histograms {visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
- we can represent all the measurements with a **histogram**

- the bars are touching because this represents _continuous_ data

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| fig-asp: .6

p + geom_rect(xmin=layD$xmin,xmax=layD$xmax,ymin=0,ymax=barHt,fill="red")
```

:::

::::

- we know that there were `r barHt` measurements of about `r .rround(layD$x,2)` cm
  + strictly, ≥ `r .rround(layD$xmin,3)` and < `r .rround(layD$xmax,3)` cm

## Histograms (2)

:::: {.columns}

::: {.column width="40%"}
- note that the _bin width_ of the histogram matters

- these histograms all show the same data
:::

::: {.column width="60%"}

:::: r-stack

::: {#1}
```{r}
#| echo: false
#| fig-asp: .6
p
```
:::

::: {.fragment}

```{r}
#| echo: false
#| fig-asp: .6
pm %>% ggplot(aes(x=x)) + geom_histogram(binwidth=0.025) +
  scale_x_continuous(limits=c(minx,maxx)) + xlab('height (cm)')
```
:::

::: {.fragment}
```{r}
#| echo: false
#| fig-asp: .6
pm |> ggplot(aes(x=x)) + geom_histogram(binwidth=0.05) +
  scale_x_continuous(limits=c(minx,maxx)) + xlab('height (cm)')
```
:::

::::

:::

::::

## Histograms in R
```{r}
#| include: false

heights <- pm$x
```

:::: {.columns}

::: {.column width="30%"}
```{r}
head(heights)
```

```{r}
#| label: histy
#| output: false
#| fig-asp: .6

hist(heights)
```

:::

::: {.column width="70%"}

![](`r knitr::fig_chunk('histy','svg')`)

:::

::::




::: aside
you can make prettier graphs using `ggplot()`, but `hist()` is useful for exploring data
:::


## Histograms

:::: {.columns}

::: {.column width="48%"}
**the good**

- way to examine the _distribution_ of data

- easy to interpret ($y$ axis = counts)

- [sometimes helpful in spotting weird data]{.fragment .highlight-red}
:::

::: {.column width="4%"}
<!-- spacer -->
:::

::: {.column width="48%"}
**the bad**

- changing bin width can completely change graph

- only gives info about distribution and mode

  - not, e.g., mean or median

:::

::::

## Histograms

:::: {.columns}

::: {.column width="40%"}
```{r}
#| echo: false

pm2 <- rbind(pm,data.frame(x=c(4.196,4.199)))
sample(pm2$x)
```
:::

::: {.column width="60%" .fragment}
```{r}
#| echo: false
#| fig-asp: .6

p <- pm2 |> ggplot(aes(x)) + geom_histogram(binwidth=0.05) +
  scale_x_continuous() + xlab('height (cm)')
layD <- layer_data(p) |> filter(x<=4.201)
p + geom_rect(xmin=layD$xmin,xmax=layD$xmax,ymin=0,ymax=layD$y,fill="red")
```
:::
::::

## Density Plots

:::: {.columns}

::: {.column width="60%"}
:::: r-stack

::: {#1}
```{r}
#| echo: false
#| fig-asp: .6
p <- pm |> ggplot(aes(x=x)) + geom_density(size=2) +
  xlab("height (cm)") + ylim(0,10)
p
```
:::

::: {.fragment fragment-index=1}
```{r}
#| label: dg
#| echo: false
#| fig-asp: .6
p + geom_rug()
```
:::

::: {.fragment fragment-index=2}
```{r}
#| echo: false
#| fig-asp: .6

pm |> ggplot(aes(x=x)) +
  geom_density(bw=.006,size=2) +
  geom_rug() +
  xlab("height (cm)") +
  ylim(0,10)
```


:::
::::

:::

::: {.column width="40%" .fragment fragment-index=1}

- density plots depend on a _smoothing function_

- essentially, they're making guesses where there is no data

:::

::::


## Density Plots

:::: {.columns}

::: {.column width="60%"}
```{r}
#| echo: false
#| fig-asp: .6

<<dg>>
```

:::
::: {.column width="40%"}

- $y$ axis is no longer a count
- _total area_ under curve = "all possibilities" = **1**
:::
::::

## Density Plots

:::: {.columns}

::: {.column width="60%"}
```{r}
#| echo: false
#| fig-asp: .6

gp <- layer_data(p)
gp <- gp |> filter( x>=7.5 &  x<7.525)
p + geom_area(data=gp, aes(x=x,y=y), fill="red") +
  geom_density(size=2) + ylab("density")

dens <- density(pm$x)
prop <- sum(dens$y[dens$x>=7.5 & dens$x<7.525]) / sum(dens$y)
```

:::

::: {.column width="40%"}
- partial area under the curve gives _proportion_ of cases (here, `r prop` ≥ 7.500 and < 7.525)
:::

::::

::: {.myblock .fragment}
this is equivalent to saying that if I pick an observation $x_i$ from this sample at random, there is a probability of `r .pv(prop)` that $7.500 \le x_i < 7.525$
:::

# The Normal Distribution

## A Famous Density Plot

:::: {.columns}

::: {.column width="40%"}
- when we started thinking about measurement,
  we thought things might look a bit like this

- the so-called **normal curve**
:::

::: {.column width="60%"}
![](`r knitr::fig_chunk('normnorm','svg')`)
:::

::::
::: {.myblock .fragment}
the normal curve is a _hypothetical_, asymptotic, density plot, with an area under the curve of **1**
:::

::: notes
- in part 3, we'll look at where the normal curve comes from

- for now, let's look at some of its features
:::

## Normal Curves: Mean

:::: {.columns}

::: {.column width="40%"}
- normal curves can be defined in terms of _two parameters_

- one is the centre or **mean** of the distribution ($\bar{x}$, or sometimes $\mu$)

:::

::: {.column width="60%"}
```{r}
#| echo: false
#| label: norms
#| fig-asp: .6

t <- data.frame(x=c(6,9))
p <- t %>% ggplot(aes(x)) +
  stat_function(fun = dnorm, n =151, args=list(mean=6.9,sd=.25),size=2) +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.25),size=2) +
  stat_function(fun = dnorm, n =151, args=list(mean=8.1,sd=.25),size=2) +
  ylab("") +
#  scale_y_continuous(breaks=NULL) +
  xlab("height (cm)")
p + geom_vline(xintercept = 7.5,colour="red",size=1.5) +
  geom_vline(xintercept = 6.9,colour="red",size=1.5) +
  geom_vline(xintercept = 8.1,colour="red",size=1.5)
```

:::
::::

::: notes
- the y value here is just what is needed to ensure that the area under the is 1
:::

## Standard Deviation

:::: {.columns}

::: {.column width="40%"}


- the other is the **standard deviation** (sd, or sometimes $\sigma$)

$$\textrm{sd}=\sqrt{\frac{\sum{(x-\bar{x})^2}}{n-1}}$$

:::

::: {.column width="60%"}
```{r annotated, echo=FALSE, fig.asp=.6}
p <- t %>% ggplot(aes(x)) +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.1),size=2) +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.25),size=2) +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.5),size=2) +
  ylab("") +
#  scale_y_continuous(breaks=NULL) +
  xlab("height (cm)")
p +
  geom_vline(xintercept = 7.5, col=alpha("red",.5)) +
  geom_segment(aes(x=7.5, xend=7.5+.1, y=dnorm(7.5+.1,7.5,.1),yend=dnorm(7.5+.1,7.5,.1)), col="red", size=2) +
  geom_segment(aes(x=7.5, xend=7.5+.25, y=dnorm(7.5+.25,7.5,.25),yend=dnorm(7.5+.25,7.5,.25)), col="red", size=2) +
  geom_segment(aes(x=7.5, xend=7.5+.5, y=dnorm(7.5+.5,7.5,.5),yend=dnorm(7.5+.5,7.5,.5)), col="red", size=2) 
```
:::

::::


- standard deviation is the "average distance of observations from the mean"


## Why Does the Height Vary?

::: {layout="[1,1]"}
```{r}
#| echo: false
#| fig-asp: .5

p <- t |> ggplot(aes(x)) +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.1),size=0) +
  ylab("") +
#  scale_y_continuous(breaks=NULL) +
  xlab("height (cm)")
layD <- layer_data(p) |> filter(x>=7.4 & x<=7.6)
p + geom_area(data=layD,aes(x=x,y=y),fill="red") +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.1),size=2) +
  ylim(0,4)
```

```{r}
#| echo: false
#| fig-asp: .5

p <- t |> ggplot(aes(x)) +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.5),size=0) +
  ylab("") +
#  scale_y_continuous(breaks=NULL) +
  xlab("height (cm)")
layD <- layer_data(p) |> filter(x>=7.0 & x<=8.0)
p + geom_area(data=layD,aes(x=x,y=y),fill="red") +
  stat_function(fun = dnorm, n =151, args=list(mean=7.5,sd=.5),size=2) +
  ylim(0,4)
```
:::

- area under the curve is always the same by sd

- for ± 1 sd it's `r 1-2*pnorm(1,lower.tail=F)`

. . .

::: myblock
there is a 68% chance of obtaining data within 1 sd of the mean
:::

::: notes
- remember though that we're in _hypothetical_ world!
:::


## The Standard Normal Curve

:::: {.columns}
::: {.column width="50%"}
- we can **standardize** any value on any normal curve by

- subtracting the mean
  + the effective mean is now **zero**

- dividing by the standard deviation
  + the effective standard deviation is now **one**
:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-asp: .6

p <- ggplot(data=data.frame(x=c(-3.5,3.5)),aes(x=x)) +
  stat_function(fun = dnorm, n =151,size=2) + ylab("") +
  xlab("standard deviations (z)")
p
```

:::: {.fragment}

$$ z_i = \frac{x_i - \bar{x}}{\sigma} $$
::::
:::
::::

## The Standard Normal Curve

:::: {.columns}
::: {.column width="50%"}
- the area between 1 standard deviation below the mean and 1 standard deviation above the mean is, as we know, [`r pnorm(1)-pnorm(-1)`]{.fragment fragment-index=1}
:::

::: {.column width="50%"}

:::: r-stack

```{r}
#| echo: false
#| fig-asp: .6

d <- layer_data(p) |> filter(x >=-1 & x <=1)
p + geom_area(data=d, aes(x=x,y=y),fill="red") +
  stat_function(fun = dnorm, n =151,size=2)
```

::: {.fragment fragment-index=2}
```{r}
#| echo: false
#| fig-asp: .6
d <- layer_data(p) |> filter(x >=qnorm(.025) & x <=qnorm(.975))
p + geom_area(data=d, aes(x=x,y=y),fill="red") +
  stat_function(fun = dnorm, n =151,size=2) -> p2sd
p2sd
```
:::
::::
:::
::::

:::: {.fragment fragment-index=2}
- we can ask the question the other way around:  _an area of .95_ lies between `r qnorm(.025)` and `r qnorm(.975)` standard deviations from the mean

::: myblock
95% of the hypothetical observations (95% confidence interval)
:::

::::

# Samples

## {background-image="img/playmo_pop.jpg"}

## Samples vs Populations

:::: {.columns}

::: {.column width="40%"}

- **population**: all members of group you are hypothesizing about

:::: {.fragment fragment-index=1}

- **sample**: the subset of the population you're testing
::::

:::

::: {.column width="60%"}

:::: r-stack
![](img/playmo_pop.jpg)

![](img/playmo_samp.jpg){.fragment fragment-index=1}
::::
:::

::::

## Central Limit Theorem

- lay version: _sample means will be normally distributed about the true mean_


- if we repeatedly sample from a population, we'll get a _normal distribution_ of means

- the _mean of the distribution of means_ will be (close to) the population mean


::: myblock
the standard deviation ("width") of the distribution of sample means is referred to as the **standard error** of the distribution
:::

## Central Limit Theorem (2)

- if you look up CLT on Wikipedia you'll see it's defined in terms of _adding two numbers_
  + the sample mean is a sum of _many_ numbers, divided by $n$

  + adding many numbers is like adding two numbers:

  $$\color{red}{1 + 3 + 2} + 5 = \color{red}{(1 + 3 + 2)} + 5 = \color{red}{6} + 5$$


  + dividing by something doesn't make any difference

## $n-1$

- we've just shown how adding many numbers is equivalent to adding two numbers

- so _if we know the sum_ of a bunch of numbers, $n-1$ of those numbers can be anything


```{r}
#| echo: false

library(gt)
t <- tibble(`sum of n-1 numbers`=c(90,102,67),`nth number`=c(10,-2,33),sum=c(100,100,100))
t %>% gt() |> tab_options(table.font.size = pct(70))
```

- so if we know a summary statistic (e.g., mean, sd) we know about the data with $n-1$ **degrees of freedom**

<!-- HERE -->

## Putting it Together

- the _normal curve_ is a density plot with known properties

  + it can be defined in terms of two parameters: **mean**, and **standard deviation**

- if we repeatedly sample from a population and measure the mean, we'll get a normal distribution

  + the mean of means will be (close to) the population mean

  + the standard deviation of the means is called the **standard error**

. . .

- if we don't know the population, we'll have to _estimate_ the population mean

- we'll want some kind of way of assessing how good our estimation is


## Experiments (Unknown Population)

::: r-stack 

![](img/playmo_sample_e1.jpg){.fragment fragment-index=1 .fade-out width=80% fig-align="center"}

![](img/playmo_sample_e2.jpg){.fragment fragment-index=1 width=80% fig-align="center"}

:::

- in an experiment, we have access to a _sample_ but not the _population_

::: {.fragment fragment-index=1}
- we have to _estimate_ the population mean
::: 

## Sample Estimates

  + the _estimated mean_ is the sample mean (we have no other info)

+ the estimated _standard error_ of the mean is defined in terms of the _sample standard deviation_

::: r-stack

:::: {.fragment fragment-index=1 .fade-out}

  $$ \textrm{se} = \frac{\sigma}{\sqrt{n}} = \frac{\sqrt{\frac{\sum{(x-\bar{x})^2}}{n-1}}}{\sqrt{n}} $$

::::

:::: {.fragment fragment-index=1}
$$ \textrm{se} = \frac{\sigma}{\color{red}{\sqrt{n}}} = \frac{\sqrt{\frac{\sum{(x-\bar{x})^2}}{n-1}}}{\color{red}{\sqrt{n}}} $$
::::

:::

::: {.fragment fragment-index=1}

- the larger the sample size, the smaller the standard error

:::


## Confidence Intervals

:::: {.columns}

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-asp: .6
p2sd
```
:::

::: {.column width="60%"}
- there is only one **population mean**

- our _best estimate_ is the **sample mean**

- we know that 95% of our best estimates will fall within ±1.96 standard errors of the sample mean

::: {.fragment}

- we have **95% confidence** that our estimation technique will capture the true mean within ±1.96 standard errors

:::

:::

::::

## Can We Use This For Real?

```{r}
#| include: false

# library(googlesheets4)
# clData <- read_sheet("1JacU9_yb9lt9FaHeiblZTw4vvFwLVOaCOiUIVdzmoPQ")
# hData <- lapply(clData[,4],as.character,simplify=T)[[1]]
# hData[hData=="NULL"] <- NA
# hData <- hData[!is.na(hData)]
# hData <- sub(' *cm','',ignore.case=T,hData)
# hData <- sub(',','.',hData)
# hData <- as.numeric(hData)
# hData[hData <100] <- hData[hData <100] * 100
survey <- read_csv('https://uoepsy.github.io/data/surveydata_historical.csv') |> select(course,height) |> drop_na()
uheights <- survey |> filter(course=='usmr') |> pull(height)
```

- we have some survey data from the last couple of USMR classes, including _height_ in cm

- [perhaps we're interested in the "mean height of a young statistician" (!)]{.r-fit-text}

  + "young statisticians" are a **population**

  + the USMR classes are a **sample**

::: myblock
can we use the information from the sample of `r length(uheights)` responses we have to say anything about the population?
:::

## Looking at the Class Data

```{r}
#| output-location: column
#| tidy.opts: { width.cutoff: 24 }

# the class heights are in uheights
hist(uheights,xlab="height (cm)")
```

- histogram suggests that the heights are (approximately) normally distributed

::: aside
survey data from google forms can be read in directly with the `{googlesheets4}` library
:::

## Mean, Standard Deviation

:::: {.columns}

::: {.column width="40%"}
- information about the distribution of the sample

```{r}
mean(uheights)

sd(uheights)
```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| fig-asp: .6

t <- data.frame(x=c(min(uheights-15),max(uheights+15)))
p <- t %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(uheights), sd=sd(uheights)), size=1.5, colour="darkgrey") +
  xlab("height (cm)") + ylab("density")
p
```

:::

::::

## Standard Error

:::: {.columns}

::: {.column width="40%"}
- **standard error** is the "standard deviation of the mean"

- as we saw in the simulation

- can be _estimated_ as $\frac{\sigma}{\sqrt{n}}$

```{r}
n <- length(uheights)
# standard error
sd(uheights) / sqrt(n)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| fig-asp: .6

se=sd(uheights)/sqrt(n)
p2 <- p +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(uheights), sd=se), size=1.5) +
  xlim(min(uheights),max(uheights))
p2
```

:::

::::

## Statistically Useful Information


:::: {.columns}

::: {.column width="60%"}
```{r}
#| echo: false
#| fig-asp: .6

fillme <- layer_data(p2,2) %>% filter(x >= mean(uheights)-1.96*se & x <= mean(uheights)+1.96*se)

p2 + geom_area(data=fillme,aes(x=x,y=y),fill="red") +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(uheights), sd=se), size=1.5)
```

- we know that the area between $\bar{x}-1.96\sigma$ and $\bar{x}+1.96\sigma$ is 0.95

:::

::: {.column width="40%"}

:::: myblock
based our sample of `r length(uheights)` people from the USMR class, we are 95% confident that the population mean lies between `r .rround(mean(uheights)-1.96*se,1)`cm and `r .rround(mean(uheights)+1.96*se,1)`cm
::::

:::

::::

## Statistically Useful Information

- we also have information from 3 other statistics courses

```{r}
#| echo: false

survey |> group_by(course) |>
  summarise(mean=mean(height),se=sd(height)/sqrt(n()), n=n()) |>
  gt() |>
  tab_options(table.font.size=pct(70))
```

::: aside
```{r}
#| code-copy: false
#| eval: false
#| tidy: false
survey |> group_by(course) |> summarise(mean=mean(height),se=sd(height)/sqrt(n()), n=n())
```
:::

- are the young statisticians on those courses from different populations? (in terms of height)

## Standard Errors (again)

```{r}
#| include: false

h2 <- survey |> filter(course=='dapr2') |> pull(height)
se2 <- sd(h2)/sqrt(length(h2))
```


:::: {.columns}

::: {.column width="20%"}

[USMR]{.red}

$\bar{x}=`r .rround(mean(uheights),1)`$ 

$\text{se}=`r .rround(se,2)`$

::: {.fragment fragment-index=1}
[DAPR2]{.blue}

$\bar{x}=`r .rround(mean(h2),1)`$ 

$\text{se}=`r .rround(se2,2)`$
:::


:::

::: {.column width="80%"}

::: r-stack

```{r}
#| echo: false
#| fig-asp: .6

t <- survey |> group_by(course) |> summarise(min=mean(height)-sd(height)/sqrt(n()),
          max=mean(height)+sd(height)/sqrt(n()))
l <- min(t$min)
u <- max(t$max)
x <- .15*(u-l)
t <- data.frame(x=c(l-2*x,u+2*x))
p <- t |> ggplot(aes(x=x)) +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(uheights), sd=se),size=0) +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(h2),sd=se2),size=0) +
  xlab('height (cm)') + ylab('density')
f1 <- layer_data(p,1) %>% filter(x >= mean(uheights)-1.96*se & x <= mean(uheights)+1.96*se)
f2 <-  layer_data(p,2) %>% filter(x >= mean(h2)-1.96*se2 & x <= mean(h2)+1.96*se2)
p2 <- p + geom_area(data=f1,aes(x=x,y=y),fill=alpha("red",.5)) + 
  stat_function(fun=dnorm, n=151, args=list(mean=mean(uheights), sd=se),size=2,colour=alpha('red',.7)) +
  geom_vline(xintercept = mean(uheights),size=2,colour=alpha('red',.7))

p2
```
:::: {.fragment fragment-index=1}
```{r}
#| echo: false
#| fig-asp: .6

p2 +   geom_area(data=f2,aes(x=x,y=y),fill=alpha("blue",.5)) + 
  stat_function(fun=dnorm, n=151, args=list(mean=mean(h2), sd=se2),size=2,colour=alpha('blue',.7)) +
  geom_vline(xintercept=mean(h2),size=2,colour=alpha('blue',.7))


```


::::
:::
:::

::::

## Statistical Inference

- not much evidence that DAPR2 and USMR come from different populations

- inferring from **samples** to **populations** is a major goal of statistics

- more about this next time

# End
