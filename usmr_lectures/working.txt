
# Central Limit Theorem

- what we have just see is a demonstration of **Central Limit Theorem**

- lay version: _sample means will be normally distributed about the true mean_

.br3.center.pa2.pt2.bg-gray.white.f3[
the standard deviation ("width") of the distribution of sample means is referred to as the **standard error** of the distribution
]

---
# Central Limit Theorem (2)

- if you look up CLT on Wikipedia you'll see it's defined in terms of _adding two numbers_
  + the sample mean is a sum of _many_ numbers, divided by $n$

  + adding many numbers is like adding two numbers:
.pt0[
  $1 + 3 + 2 + 5 = (1 + 3 + 2) + 5 = 6 + 5$
]

  + dividing by something doesn't make any difference

---
# $n-1$

- we've just shown how adding many numbers is equivalent to adding two numbers

- so _if we know the sum_ of a bunch of numbers, $n-1$ of those numbers can be anything

.center[
```{r table,echo=FALSE}
library(gt)
t <- tibble(`sum of n-1 numbers`=c(90,102,67),`nth number`=c(10,-2,33),sum=c(100,100,100))
t %>% gt()
```
]

- so if we know a summary statistic (e.g., mean, sd) we know about the data with $n-1$ **degrees of freedom**

---
# Statistical Estimates

- so far, we've talked about sampling repeatedly from a population

- this might not be possible(!)

- if we only have one sample we can make _estimates_ of the mean and standard error

  + the estimated _mean_ is the sample mean (we have no other info)

  + the estimated _standard error_ of the mean is defined in terms of the sample standard deviation

  $$ \textrm{se} = \frac{\sigma}{\sqrt{n}} = \frac{\sqrt{\frac{\sum{(x-\bar{x})^2}}{n-1}}}{\sqrt{n}} $$

---
# Putting it Together

- the _normal curve_ is a density plot with known properties

  + it can be defined in terms of two parameters, mean, and standard deviation

- if we repeatedly sample from a population and measure the mean of a population, we'll get a normal distribution

  + the mean will be (close to) the population mean

- if we sample once from a population which is approximately normal

  + our estimated mean and sd for the population are the sample mean and sd

  + the _standard error_, or standard deviation of the sample means can be estimated as $\sigma/\sqrt{n}$

---
# Can We Use This For Real?

```{r get_data, include=FALSE}
library(googlesheets4)
clData <- read_sheet("1JacU9_yb9lt9FaHeiblZTw4vvFwLVOaCOiUIVdzmoPQ")
hData <- lapply(clData[,4],as.character,simplify=T)[[1]]
hData[hData=="NULL"] <- NA
hData <- hData[!is.na(hData)]
hData <- sub(' *cm','',hData)
hData <- sub(',','.',hData)
hData <- as.numeric(hData)
hData[hData <100] <- hData[hData <100] * 100
```

- we have some survey data from the USMR class, including _height_ in cm

- perhaps we're interested in the "average height of a young statistician" (!)

  + "young statisticians" are a **population**

  + the USMR class of 2020 is a **sample**

.pt2[
&nbsp;
]

.br3.center.pa2.pt2.bg-gray.white.f3[
can we use the information from the sample of `r length(hData)` responses we have to say anything about the population?
]

---
# Looking at the class data

.pull-left[
```{r doahist, fig.asp=.55, fig.show='hide'}
# the class heights in cm are in hData
hist(hData, xlab="height in cm")
```
]
.pull-right[
`r include_graphics("lecture_2_files/figure-html/doahist-1.svg")`
]


.flex.items-center[
.w-5.pa1[
![:scale 70%](lecture_1_files/img/danger.svg)
]
.w-95.pa1[
- data taken directly from the class survey responses
- uses the `googlesheets4` library
]]

---
# Mean, Standard Deviation

.pull-left[

- information about the distribution of the sample

```{r sd}
mean(hData)

sd(hData)
```

]
.pull-right[
```{r realnorm, echo=FALSE, fig.asp=.6}
t <- data.frame(x=c(min(hData-15),max(hData+15)))
p <- t %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(hData), sd=sd(hData)), size=1.5, colour="darkgrey") +
  xlab("height in cm") + ylab("density")
p
```
]

---
# Standard Error

.pull-left[

- **standard error** is the "standard deviation of the mean"

- as we saw in the simulation

- can be _estimated_ as $\frac{\sigma}{\sqrt{n}}$

```{r se}
n <- length(hData)
# standard error
sd(hData) / sqrt(n)
```
]
.pull-right[
```{r senorm,echo=FALSE, fig.asp=.6}
se=sd(hData)/sqrt(n)
p2 <- p +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(hData), sd=se), size=1.5)
p2

```

]

---
# Statistically Useful Information

.flex.items-center[.w-50.pa2[
```{r senorm2,echo=FALSE, fig.asp=.6}
fillme <- layer_data(p2,2) %>% filter(x >= mean(hData)-1.96*se & x <= mean(hData)+1.96*se)

p2 + geom_area(data=fillme,aes(x=x,y=y),fill="red") +
  stat_function(fun=dnorm, n=151, args=list(mean=mean(hData), sd=se), size=1.5)

```
- we know that the area between $\bar{x}-1.96\sigma$ and $\bar{x}+1.96\sigma$ is 0.95
]
.w-50.pa2[
.br3.center.pa2.pt2.bg-gray.white.f3[
if we measure the mean height of `r length(hData)` people from the same population as the USMR class, we estimate that the answer we obtain will lie between `r round(mean(hData)-1.96*se,1)`cm and `r round(mean(hData)+1.96*se,1)`cm 95% of the time
]

]]

---
# The Aim of the Game

- as statisticians, a major goal is to infer from **samples** to **populations**

- more about how we do this next time


---
class: inverse, center, middle, animated, swing

# End

---
# Acknowledgements

- icons by Diego Lavecchia from the [Noun Project](https://thenounproject.com/)
