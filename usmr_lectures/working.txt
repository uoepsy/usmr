
---
---

---

---

---


---
class: inverse, center, middle, animated, bounceInUp

# Intermission

---
class: inverse, center, middle

# Part 1a

## Correlations Contd.

---
# Significance of a Correlation

.pull-left[
![](lecture_5_files/figure-html/ba-1.svg)
$$ r = `r cor(dat$BloodAlc,dat$RT)` $$
]
.pull-right[
![](lecture_5_files/img/playmo_police.jpg)
]
???
- the police have stopped our friends and measured their blood alcohol

- is their evidence sufficient to conclude that there is likely to be a relationship between blood alcohol and reaction time?
---
# Significance of a Correlation

- we can measure a correlation using $r$

- we want to know whether that correlation is **significant**

  + i.e., whether the probability of finding it by chance is low enough

.pt2[
- cardinal rule in NHST:  compare everything to chance

- let's investigate...
]
---
# Random Correlations

- function to pick some pairs of numbers entirely at random, return correlation

- arbitrarily, I've picked numbers uniformly distributed between 0 and 100

```{r}
#| label: pcor0
x <- runif(5, min=0, max=100)
y <- runif(5, min=0, max=100)
cbind(x,y)
cor(x,y)
```

---
count: false
# Random Correlations

- function to pick some pairs of numbers entirely at random, return correlation

- arbitrarily, I've picked numbers uniformly distributed between 0 and 100

.flex.items-top[
.w-50.pa2[

```{r}
#| label: pcor
#| fig.asp: 0.6
#| fig.show: hide
randomCor <- function(size) {
  x <- runif(size, min=0, max=100)
  y <- runif(size, min=0, max=100)
  cor(x,y) # calculate r
}

# then we can use the usual trick:
rs <- replicate(1000, randomCor(5))
hist(rs)
```

]
.w-50.pa2[
![](lecture_5_files/figure-html/pcor-1.svg)
]]

---
# Random Correlations

.pull-left[

```{r}
#| label: pcor2
#| echo: false
#| fig.asp: 0.8
t <- tibble(r=replicate(1000,randomCor(15)))
t %>% ggplot(aes(x=r)) + geom_density(size=2) +
  ggtitle("1000 correlations of 15 random pairs") +
  ylim(0,2.5) + xlim(-.8,.8) +
  geom_segment(aes(x=quantile(r,.025),xend=quantile(r,.975),y=1.5,yend=1.5),colour="red",size=1,arrow=arrow(type="open",ends="both")) +
  annotate("text",x=0,y=1.65,label="95% of observations",size=6,colour="red")
```

]
.pull-right[
```{r}
#| label: pcor3
#| echo: false
#| fig.asp: 0.8
t <- tibble(r=replicate(1000,randomCor(30)))
t %>% ggplot(aes(x=r)) + geom_density(size=2) +
  ggtitle("1000 correlations of 30 random pairs") +
  ylim(0,2.5) + xlim(-.8,.8) +
  geom_segment(aes(x=quantile(r,.025),xend=quantile(r,.975),y=1.5,yend=1.5),colour="red",size=1,arrow=arrow(type="open",ends="both")) +
  annotate("text",x=0,y=1.65,label="95% of observations",size=6,colour="red")
```

]
---
# Larger Sample Size

.pull-left[
```{r}
#| label: pcor4
#| echo: false
#| fig.asp: 0.8
t <- tibble(r=replicate(10000,randomCor(30)))
rt <- function(r,n) {r*sqrt((n-2)/(1-r^2))}
dtr <- function(r,n) {dt(rt(r,n),n-2)*sqrt((n-2)/(1-r^2))}
p <- t %>% ggplot(aes(x=r)) + geom_density(size=2) +
  ggtitle("10000 correlations of 30 random pairs") +
  ylim(0,2.5) + xlim(-.8,.8)
p + stat_function(fun=dtr,colour="red",args=list(n=30))
```
]
.pull-right[
- distribution of random $r$s is $t$ distribution, with $n-2$ df

$$ t= r\sqrt{\frac{n-2}{1-r^2}} $$

- makes it "easy" to calculate probability of getting $\ge{}r$ for sample size $n$ by chance
]

---
# Pirates and Global Warming

.center[
![:scale 55%](lecture_5_files/img/fsm.png)
]

- clear _negative_ correlation between number of pirates and mean global temperature
- we need pirates to combat global warming

---
# Simpson's Paradox

.center[
```{r}
#| label: simpson
#| echo: !expr F
#| fig.asp: 0.6
#| fig.width: 6.0
set.seed(13)
dat <- correlation::simulate_simpson(n=50,r=-.5,groups=3) %>% mutate(prob=(V1-min(V1))/(max(V1)-min(V1)),hours=(V2-min(V2))/(max(V2)-min(V2))*10) %>% mutate(group=factor(Group,labels=c('younger','middle','older')))
p <- dat %>% ggplot(aes(x=hours,y=prob)) +
  geom_point(size=2) +
  geom_smooth(method="lm") +
  xlab("hours of exercise") +
  ylab("prob of disease") + theme_presentation(14)
p
```
]

- the more hours of exercise, the greater the risk of disease
---
# Simpson's Paradox

.center[
```{r}
#| label: simpson2
#| fig.asp: 0.6
#| echo: !expr F
#| fig.width: 6.0
p + geom_point(aes(colour=group)) +
  geom_smooth(aes(colour=group),method="lm")
```
]

- age groups mixed together
- an example of a _mediating variable_

---
# Interpreting Correlation

- correlation does not imply causation

- correlation simply suggests that two variables are related

  + there may be mediating variables

- interpretation of that relationship is key

- never rely on statistics such as $r$ without

  + looking at your data

  + thinking about the real world

---
class: inverse, center, middle, animated, bounceInUp

# End of Part 1

---
class: inverse, center, middle

# Part 2

---
# Has Statistics Got You Frazzled?

.pull-left[
![:scale 70%](lecture_5_files/img/playmo_frazzled.jpg)
]

.pull-right[
- we've bandied a lot of terms around in quite a short time

- we've tended to introduce them by example

- time to step back...
]
---
class: inverse, center, middle
# Part Z:  The Zen of Stats

.center[
![:scale 70%](lecture_5_files/img/zen.png)
]

---
# What is NHST all about?

## **N**ull **H**ypothesis **S**tatistical **T**esting

- two premises

  1. much of the variation in the universe is due to _chance_

  1. we can't _prove_ a hypothesis that something else is the cause

---
# Chance

.pull-left[
- when we say _chance_, what we really mean is "stuff we didn't measure"

- we believe that "pure" chance conforms approximately to predictable patterns (like the normal and $t$ distributions)

  - if our data isn't in a predicted pattern, perhaps we haven't captured all of the non-chance elements
]

.pull-right[
### pattens attributable to

```{r}
#| label: chance
#| echo: !expr F
#| fig.asp: 0.6
x <- tibble(x=c(-4,4))
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,size=1) +
  xlab("SDs") + ylab("density") +
  ggtitle("Chance")

db <- function(x) {dnorm(x-2)+dnorm(x+1)}
p2 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=db,size=1) +
  xlab("SDs") + ylab("") +
  ggtitle("Something Else")

p1+p2
```
]
???
- we'll come back to looking at the patterns later on; essentially there is always going to be some part of any variation we can't explain
---
# Proof

.pull-left[
![](lecture_5_files/img/playmo_sun.jpg)

]
.pull-right[
- can't prove a hypothesis to be true

- "the sun will rise tomorrow"
]


---
count: false
# Proof

.pull-left[
![](lecture_5_files/img/playmo_nosun.jpg)

]
.pull-right[
- can't prove a hypothesis to be true

- "the sun will rise tomorrow"

- _just takes one counterexample_
]

---
# Chance and Proof

.br3.pa2.bg-gray.white[if the likelihood that the pattern of data we've observed would be found _by chance_ is low enough, propose an alternative explanation
]

- work from summaries of the data (e.g., $\bar{x}$, $\sigma$)

- use these to approximate chance (e.g., $t$ distribution)

--

  + catch: we can't estimate the probability of an exact value (this is an example of the measurement problem)

  + estimate the probability of finding the measured difference _or more_

---
# Alpha and Beta

- we need an agreed "standard" for proposing an alternative explanation

  + typically in psychology, we set $\alpha$ to 0.05

  + "if the probability of finding this difference or more under chance is $\alpha$ (e.g., 5%) or less, propose an alternative"

- we also need to understand the quality of evidence we're providing

  + can be measured using $\beta$ (psychologists typically aim for 0.80)

  + "given that an effect truly exists in a population, what is the probability of finding $p<\alpha$ in a sample (of size $n$ etc.)?"

---
class: middle
background-image: url(lecture_5_files/img/nuts-and-bolts.jpg)

.br3.pa2.bg-white-80[
# The Rest is Just Nuts and Bolts

- type of measurement

- relevant laws of chance

- suitable estimated distribution (normal, $t$, $\chi^2$, etc.)

- suitable summary statistic ( $z$, $t$, $\chi^2$, $r$, etc.)

- use statistic and distribution to calculate $p$ and compare to $\alpha$

- rinse, repeat

]
---
# The Most Useful Tool

.pull-left[
![:scale 70%](lecture_5_files/img/playmo_spanner.jpg)
]
.pull-right[
- $t$ (or $z$) statistics are really ubiquitous

- formally, mean difference divided by standard error

- conceptually:

  + what is the difference?
  + what is the range of differences I would get by chance?
  + how extreme is this difference compared to the range?

- expressed in terms of _numbers of standard errors_ from the most likely difference (usually, by hypothesis, zero)
]
---
class: inverse, center, middle
# Part N: Nirvana

.center[
![:scale 45%](lecture_5_files/img/nirvana.png)
]

---
# .red[Announcement]

.flex.items-center[
.w-50.pa2[
- no lecture or lab released in week 6 (25-29 October)

- quiz 2 due on Friday 29th

- after the break: linear models

- in the meantime, have a good break and a Happy Halloween
]
.w-50.pa2[
![:scale 70%](lecture_5_files/img/playmo_hallow.jpg)
]]
---
class: inverse, center, middle, animated, bounceInUp

# End

---
# Acknowledgements

- the [papaja package](https://github.com/crsh/papaja) helps with the preparation of APA-ready manuscripts

- icons by Diego Lavecchia from the [Noun Project](https://thenounproject.com/)
