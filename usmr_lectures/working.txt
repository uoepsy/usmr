# Assumptions of Linear Models

.pull-left[
.br3.bg-gray.white.pa2[
### required
- **linearity** of relationship(!)

- for the _residuals_:
  + **normality**
  + **homogeneity of variance**
  + **independence**
]
]
.pull-right[
.br3.bg-gray.white.pa2[
### desirable
<!-- - uncorrelated predictors (no collinearity) -->
- no 'bad' (overly influential) observations
]]

---
# Residuals

.pull-left[
$$y_i=b_0+b_1\cdot{}x_i+\epsilon_i$$
$$\color{red}{\epsilon\sim{}N(0,\sigma)~\text{independently}}$$
- normally distributed (mean should be $\simeq{}$ zero)

- homogeneous (differences from $\hat{y}$ shouldn't be systematically smaller or larger for different $x$)

- independent (residuals shouldn't influence other residuals)
]
.pull-right[
```{r}
#| label: resid
#| fig.asp: 0.6
#| echo: !expr F
d20 %>% ggplot(aes(x=BloodAlc,y=RT,yend=pred)) +
  geom_segment(aes(xend=BloodAlc),linetype="dotted",colour="red",size=1) +
  geom_smooth(method="lm",se=FALSE) +
  geom_point(size=3)
```
]
---
# At A Glance
```{r}
#| label: modresid
#| highlight.output: 7.0
summary(mod)
```

---
# In More Detail

```{r}
#| label: margins
#| include: false
par(mar=c(0,0,0,0))
```

.pull-left[
### linearity

```{r}
#| label: resid1
#| fig.asp: 0.8
#| fig.show: hide
plot(mod,which=1)
```
- plots residuals $\epsilon_i$ against fitted values $\hat{y}_i$

- the 'average residual' is roughly zero across $\hat{y}$, so relationship is likely to be linear
]
.pull-right[
![](lecture_7_files/figure-html/resid1-1.svg)
]
---
# In More Detail

.pull-left[
### normality

```{r}
#| label: resid2
#| fig.asp: 0.8
#| fig.show: hide
hist(resid(mod))
```
]
.pull-right[
![](lecture_7_files/figure-html/resid2-1.svg)
]
---
count: false
# In More Detail

.pull-left[
### normality

```{r}
#| label: resid3
#| fig.asp: 0.8
#| fig.show: hide
plot(density(resid(mod)))
```

- check that residuals $\epsilon$ are approximately normally distributed

- in fact there's a better way of doing this...
]
.pull-right[
![](lecture_7_files/figure-html/resid3-1.svg)
]

---
name: QQ
# In More Detail

.pull-left[
### normality

```{r}
#| label: resid4
#| fig.asp: 0.8
#| fig.show: hide
plot(mod,which=2)
```

- **Q-Q plot** compares the residuals $\epsilon$ against a known distribution (here, normal)

- observations close to the straight line mean residuals are approximately normal

- numbered observations refer to _row numbers_ in the original data, for checking
]
.pull-right[
![](lecture_7_files/figure-html/resid4-1.svg)
]
---
# Q-Q Plots

.pull-left[
#### y axis

- for a normal distribution, what values _should_ (say) 2%, or 4% of the observations lie below?

- expressed in "standard deviations from the mean"

```{r}
#| label: qn1
qnorm(c(.02,.04))
```
]

--

.pull-right[
#### x axis

- from our residuals, what values _are_ (say) 2%, or 4%, of observations found to be less than?

- convert to "standard deviations from the mean"

```{r}
#| label: qn2
quantile(scale(resid(mod)),c(.02,.04))
```
]

--

- Q-Q Plot shows these values plotted against each other

---
template: QQ
---
# In More Detail

.pull-left[
### homogeneity of variance

```{r}
#| label: resid5
#| fig.asp: 0.8
#| fig.show: hide
plot(mod,which=3)
```

- the _size_ of the residuals is approximately the same across values of $\hat{y}$
]
.pull-right[
![](lecture_7_files/figure-html/resid5-1.svg)
]

---
# Visual vs Other Methods

- statistical ways of checking assumptions will be introduced in labs

- they tend to have limitations (for example, they're susceptible to sample size)

- nothing beats looking at plots like these (and `plot(<model>)` makes it easy)

- however, two things:

--

.pt2[
1. there are no criteria for deciding exactly when assumptions are sufficiently met

  + it's a matter of experience and judgement

1. we need to talk about **independence** of residuals
]

---
class: inverse, center, middle, animated, flip
# End of Part 1
---
class: inverse, center, middle
# Part 2
## Independence, Influence
---
# Independence

- no easy way to check independence of residuals

- in part, because it depends on the _source_ of the observations

.pt2[
- one determinant might be a single person being observed multiple times

- e.g., my reaction times might tend to be slower than yours<br/>
  $\rightarrow$ multivariate statistics
]
---
# Independence

- another determinant might be _time_

- observations in a sequence might be autocorrelated

- can be checked using the Durbin-Watson Test from the `car` package

```{r}
#| label: dwt
#| highlight.output: 2.0
library(car)
dwt(mod)
```

- shows no autocorrelation at lag 1
---
# Influence
.center[
```{r}
#| label: baddata1
#| echo: !expr F
#| fig.asp: 0.6
#| fig.width: 6.0
ba <- mean(d20$BloodAlc)
b20 <- d20 %>% select(RT,BloodAlc) %>% add_row(BloodAlc=ba,RT=1000)
p <- d20 %>% ggplot(aes(x=BloodAlc,y=RT)) +
  geom_point(size=2) +
  geom_smooth(method="lm",se=FALSE)
m <- lm(RT~BloodAlc,data=b20)
tt <- tibble(x=c(min(b20$BloodAlc),max(b20$BloodAlc)),
             y=c(min(predict(m)),max(predict(m))))
p + geom_line(data=tt,aes(x=x,y=y),size=.8,colour="red") +
  geom_point(aes(x=ba,y=1000),size=3,colour="red")
```
]
- even substantial **outliers** may only have small effects on the model

- here, only the intercept is affected
---
# Influence
.center[
```{r}
#| label: baddata2
#| echo: !expr F
#| fig.asp: 0.6
#| fig.width: 6.0
m1 <- lm(RT~BloodAlc,data=d20)
rt=coef(m1)[1]+.14*coef(m1)[2]+rnorm(1,sd=50)
b20 <- d20 %>% select(RT,BloodAlc) %>% add_row(BloodAlc=.14,RT=rt)
m <- lm(RT~BloodAlc,data=b20)
tt <- tibble(x=c(min(b20$BloodAlc),max(b20$BloodAlc)),
             y=c(min(predict(m)),max(predict(m))))
p + geom_line(data=tt,aes(x=x,y=y),size=.8,colour="red") +
  geom_point(aes(x=.14,y=rt),size=3,colour="red")
```
]
- observations with high **leverage** are inconsistent with other data, but may not be distorting the model

---
# Influence
.center[
```{r}
#| label: baddata3
#| echo: !expr F
#| fig.asp: 0.6
#| fig.width: 6.0
m1 <- lm(RT~BloodAlc,data=d20)
rt=coef(m1)[1]+.13*coef(m1)[2]+rnorm(1,sd=50)-250
b20 <- d20 %>% select(RT,BloodAlc) %>% add_row(BloodAlc=.13,RT=rt)
m <- lm(RT~BloodAlc,data=b20)
tt <- tibble(x=c(min(b20$BloodAlc),max(b20$BloodAlc)),
             y=c(min(predict(m)),max(predict(m))))
p + geom_line(data=tt,aes(x=x,y=y),size=.8,colour="red") +
  geom_point(aes(x=.13,y=rt),size=3,colour="red")
```
]
- we care about observations with high **influence** (outliers with high leverage)
---
# Cook's Distance

.center[
```{r}
#| label: cook
#| fig.asp: 0.6
#| fig.width: 6.0
#| echo: !expr F
tt <- mutate(tt,y1=c(min(predict(m1)),coef(m1)[1]+max(x)*coef(m1)[2]))
tb <- d20 %>% mutate(y1=predict(m1),y2=predict(m)[-21])
d20 %>% ggplot(aes(x=BloodAlc,y=RT)) +
  geom_point(size=2) + geom_point(aes(x=.13,y=rt),size=3,colour="red") +
  geom_line(data=tt,aes(x=x,y=y),col="red",size=.8) +
  geom_line(data=tt,aes(x=x,y=y1),col="blue",size=.8) +
  geom_segment(data=tb,aes(x=BloodAlc,xend=BloodAlc,y=y1,yend=y2),linetype="dashed")
```
]

- a standardised measure of "how much the model differs without observation $i$"

---
.flex.items-center[.w-10.pa1[![:scale 70%](lecture_7_files/img/danger.svg)] .f1.w-80.pa1[Cook's Distance]]

.pt3[
$$D_i=\frac{\sum_{j=1}^{n}{(\hat{y}_j-\hat{y}_{j(i)})^2}}{(p+1)\hat{\sigma}^2}$$
- $\hat{y}+j$ is the $j$th fitted value
- $\hat{y}_{j(i)}$ is the $j$th value from a fit which doesn't include observation $i$
- $p$ is the number of regression coefficients
- $\hat{\sigma}^2$ is the estimated variance from the fit, i.e., mean squared error
]
---
# Cook's Distance

```{r}
#| label: scramble
#| include: !expr F
b20 <- b20 %>% slice_sample(n=21)
mod <- lm(RT~BloodAlc,data=b20)
```

.pull-left[
```{r}
#| label: cook2
#| fig.asp: 0.8
#| fig.show: hide
plot(mod,which=4)
```

- observations labelled by row

- various rules of thumb, but start looking when Cook's Distance > 0.5
]
.pull-right[
![](lecture_7_files/figure-html/cook2-1.svg)
]

---
class: animated, rollIn
# Learning to Read

```{r}
#| label: makedat
#| include: false
# library(broom)
# doit <- TRUE
#
# while (doit) {
#   reading <- tibble(
#     age=runif(50,5,11),
#     hrs_wk=rnorm(50,5,1),
#     method=gl(2,25,labels=c("phonics","word"))
#   )
#
#   reading <- reading %>% mutate(
#     R_AGE=age + hrs_wk/2 - ifelse(method=="word",2,0)
#   )
#
#   reading <- reading %>% mutate(
#     R_AGE=R_AGE+hrs_wk*ifelse(method=="word",0,.6)
#   )
#
#   reading <- reading %>% mutate(
#     R_AGE=R_AGE-.03*age*hrs_wk
#   )
#
#   reading <- reading %>% mutate(
#     R_AGE=R_AGE+rnorm(50,sd=.5)
#   )
#
#   m <- lm(R_AGE ~ method,data=reading) %>% tidy() %>% mutate(sig=(p.value<.05))
#   if (!m$sig[2]) {
#     next
#   }
#
#   m <- lm(R_AGE~age+hrs_wk,data=reading) %>% tidy() %>% mutate(sig=(p.value<.05))
#
#   if (sum(m$sig[2:3])!=2) {
#     next
#   }
#
#   m <- lm(R_AGE ~ hrs_wk*method,data=reading) %>% tidy() %>% mutate(sig=(p.value<.05))
#   if (!m$sig[4]) {
#     next
#   }
#
#   m <- lm(R_AGE ~ age*hrs_wk,data=reading) %>% tidy() %>% mutate(sig=(p.value<.05))
#   if (!m$sig[4]) {
#     next
#   }
#
#   doit <- FALSE
#
# }
#
# save(reading,file = "R/reading.Rdata")

load("R/reading.Rdata")
```
.pull-left[
![](lecture_7_files/img/playmo_teach.jpg)
]
.pull-right[
- the Playmo School has been evaluating its reading programmes, using 50 students

- ages of students

- hours per week students spend reading of their own volition

- whether they are taught using phonics or whole-word methods

- **outcome: "reading age"**
]

---
count: false
# Learning to Read

.pull-left[
![](lecture_7_files/img/playmo_teach.jpg)
]
.pull-right[
.center[
```{r}
#| label: showdat
#| echo: false
library(gt)
reading %>% slice(c(1:5,46:50)) %>% gt()
```
]]
???
- this is just some of the data in the dataframe

- on the right you can see the outcome variable, `R_AGE` for reading age.

- note that I tend to put my dependent variables in all caps, just so that I can recognise at a glance that they're what we measured.
---
count: false
# Learning to Read

.pull-left[
![](lecture_7_files/img/playmo_teach.jpg)
]
.pull-right[
.center[
```{r}
#| label: showdat2
#| echo: false
library(gt)
reading %>% slice(c(1:5,46:50)) %>% gt() %>%
  data_color(columns=c("hrs_wk","R_AGE"),colors="#d0d9ff",alpha=.8)
```
]]

---
# Does Practice Affect Reading Age?

.pull-left[

```{r}
#| label: plot1
#| fig.asp: 0.6
#| fig.show: hide
p <- reading %>%
  ggplot(aes(x=hrs_wk,y=R_AGE)) +
  geom_point(size=3) +
  ylab("reading age") +
  xlab("hours reading/week")
p
```

]
.pull-right[
![](lecture_7_files/figure-html/plot1-1.svg)
]

--

- hours per week is correlated with reading age: $r=`r cor(reading$hrs_wk,reading$R_AGE)`$,
$p=`r cor.test(reading$hrs_wk,reading$R_AGE)$p.value`$

- we can use a linear model to say something about the effect size

---
# Does Practice Affect Reading Age?

.pull-left[
```{r}
#| label: plot2
#| fig.asp: 0.6
#| fig.show: hide
p + geom_smooth(method="lm")
```
]
.pull-right[
![](lecture_7_files/figure-html/plot2-1.svg)
]

```{r}
#| include: false
mod <- lm(R_AGE~hrs_wk,data=reading)
```

--

- each extra hour spent reading a week adds `r .rround(coef(mod)[2])` years to reading age


---
# A Linear Model

```{r}
#| label: lm1
#| highlight.output: !expr c(7, 11, 12, 17, 18)
mod <- lm(R_AGE ~ hrs_wk, data=reading)
summary(mod)
```

---
class: center, middle, inverse
.f1[but...]
---
.center[
```{r}
#| label: diagp2
#| echo: false
#| fig.asp: 0.7
#| fig.width: 9.0
par(mfrow=c(2,2))
plot(mod,which=1:4)
```
]

---
# Assumptions Not Met!
.pull-left[
- it seems that the assumptions aren't met for this model

- (another demonstration on the right)

- one reason for this can be because there's still systematic structure in the residuals

- i.e., _more than one thing_ which can explain the variance
]
.pull-right[
```{r}
#| label: re
#| echo: !expr F
#| fig.asp: 0.7
plot(density(resid(mod)))
```

]
---
class: inverse, center, middle, animated, flip

# End of Part 2
---
class: inverse, center, middle

# Part 3
## Multiple Regression

---
# Adding Age into the Equation

.pull-left[
- so far, have focused on effects of practice

- but presumably older children read better?

```{r}
#| label: tab3
#| echo: !expr F
reading %>% slice(c(1:3,48:50)) %>% gt() %>%
  data_color(columns=c("age","R_AGE"),colors="#d0d9ff",alpha=.8)
```
]
.pull-right[
```{r}
#| label: try3d
#| echo: !expr F
#| fig.width: 7.0
library(rgl)
par(mar=c(0,0,0,0))
plot3d(x=reading$hrs_wk,y=reading$age,z=reading$R_AGE,
       type='s',
       radius=.1,
       xlab="practise",ylab="age",zlab="READING AGE")
rglwidget()
```
]

???
- **rotate cube leftward to show R_AGE~age**
---
# Another Model

.center[
```{r}
#| label: anmod
#| fig.asp: 0.6
#| echo: !expr F
reading %>% ggplot(aes(x=age,y=R_AGE)) +
  xlab("age") + ylab("reading age") +
  geom_point(size=3) +
  geom_smooth(method="lm")
```

]

---
# Another Model

```{r}
#| label: mod2
#| highlight.output: !expr c(7, 11, 12, 17, 18)
mod2 <- lm(R_AGE ~ age, data=reading)
summary(mod2)
```

???
- point out that intercept is quite meaningless (reading age for age zero)

---
.center[
```{r}
#| label: diag2
#| echo: !expr F
#| fig.asp: 0.7
#| fig.width: 9.0
par(mfrow=c(2,2))
plot(mod2,which=1:4)

```
]

---
# Two Models, No Answers

.pull-left[
- we now have two models that don't map well to assumptions

- each suggests an effect

  + one of `age`

  + one of `hrs_wk`
]
.pull-right[
- if we run them independently, the chances of a type 1 error are

  + $\frac{1}{20}$ (`mod`, including `hrs_wk`)

  + $\frac{1}{20}$ (`mod2`, including `age`)

- or ** $\frac{1}{10}$ ** overall
]

--
.pt1[
&nbsp;
]

.br3.pa2.bg-gray.white.tc[
we need to test multiple predictors in _one_ linear model
]
---
# Model Equations Again

$$\color{red}{\textrm{outcome}_i}=\color{blue}{(\textrm{model})_i}+\textrm{error}_i$$

$$\color{red}{y_i}=\color{blue}{b_0\cdot{}1+b_1\cdot{}x_i}+\epsilon_i$$
--

### linear model with two predictors

$$\color{red}{y_i}=\color{blue}{b_0\cdot{}1+b_1\cdot{}x_{1i}+b_2\cdot{}x_{2i}}+\epsilon_i$$
$$\color{red}{\hat{y}_i}=\color{blue}{b_0\cdot{}1+b_1\cdot{}x_{1i}+b_2\cdot{}x_{2i}}$$

--
.center[
`y ~ 1 + x1 + x2`

`R_AGE ~ 1 + hrs_wk + age` &nbsp;&nbsp; or &nbsp;&nbsp; `R_AGE ~ hrs_wk + age`<sup>1</sup>
]


.footnote[
<sup>1</sup> we'll come back to why order can matter in a bit
]
---
# Running A Multiple Regression

```{r}
#| label: multi
#| highlight.output: !expr '11:13'
mod.m <- lm(R_AGE ~ age + hrs_wk, data=reading)
summary(mod.m)
```

---
# Running a Multiple Regression

```{r}
#| label: multi2
#| echo: !expr F
ca <- coef(mod.m)[2]
ch <- coef(mod.m)[3]
.pp(summary(mod.m),l = list(0,11:13,0))
```

- there are _independent_ effects of age and practice

  + reading age improves by `r ca` for each year of age

  + reading age improves by `r ch` for each weekly hour of practice

- note that the _intercept_ (0 years old, 0 hours/week) is meaningless here

--

- important question:  is this model _better_ than a model based just on age?

---
# Model Fit

```{r}
#| label: multi3
#| echo: !expr F
.pp(summary(mod.m),l=list(0,17:19))

```

- in multiple regression, $R^2$ measures the fit of the entire model
  + sum of individual $R^2$s _if predictors not correlated_

- $R^2=`r summary(mod.m)$r.squared`$ looks better than the $R^2$ for `mod2` (`age` as a predictor) of $`r summary(mod2)$r.squared`$

- but _any_ predictor will improve $R^2$ (chance associations guarantee this)

```{r}
#| label: multi4
#| eval: !expr F
mod.r <- update(mod2, ~ . + runif(50)) #<<
summary(mod.r)
```
```{r}
#| label: multi4b
#| echo: !expr F
mod.r <- update(mod2, ~ . + runif(50))
.pp(summary(mod.r),l=list(0,18,0))
```
???

- Adjusted $R^2$ accounts for multiple predictors

- for `mod2`, it's `r summary(mod2)$adj.r.squared`

- for `mod.r`, it's `r summary(mod.r)$adj.r.squared`

---
# Comparing Models

```{r}
#| label: multic
#| echo: !expr F
#| highlight.output: 3.0
.pp(summary(mod.r),l=list(0,18:19))
```

- $F$ ratio compares model to the null model, which just has an intercept

- but we can also use $F$ ratios to compare models in succession

```{r}
#| label: multic2
anova(mod.m)
```

---
# Order Matters!

- `age` then `hrs_wk`
```{r}
#| label: multic3
#| echo: !expr F
<<multic2>>
```
- `hrs_wk` then `age`
```{r}
#| label: multic4o
#| echo: !expr F
anova(lm(R_AGE~hrs_wk+age,data=reading))
```

---
# Type 1 vs. Type 3 SS

- order matters because R, by default, uses **Type 1** sums of squares
  + calculate each predictor's improvement to the model _in turn_
- compare to **Type 3** sums of squares
  + calculate each predictor's improvement to the model _taking all other predictors into account_

- huge debate about which is "better" (nobody likes Type 2)

- if using Type 1, _predictors should be entered into the model in a theoretically-motivated order_
---
# Type 1 vs. Type 3 SS
.pull-left[
### type 1
.center[
![:scale 70%](lecture_7_files/t13/type0-0.svg)
]]
.pull-right[
### type 3
.center[
![:scale 70%](lecture_7_files/t13/type0-0.svg)
]]
---

```{r}
#| label: t1
#| echo: !expr F
#| results: asis
img <- dir("lecture_7_files/t13/",pattern="type1.+svg$",full.names=TRUE)
slides_txt <- glue::glue("
count: false
# Type 1 vs. Type 3 SS
.pull-left[
### type 1
.center[
![:scale 70%]({img})
]]
.pull-right[
### type 3
.center[
![:scale 70%](lecture_7_files/t13/type0-0.svg)
]]

---

")
cat(slides_txt, sep="")
```

```{r}
#| label: t3
#| echo: !expr F
#| results: asis
img <- dir("lecture_7_files/t13/",pattern="type3.+svg$",full.names=TRUE)
slides_txt <- glue::glue("
count: false
# Type 1 vs. Type 3 SS
.pull-left[
### type 1
.center[
![:scale 70%](lecture_7_files/t13/type1-3.svg)
]]
.pull-right[
### type 3
.center[
![:scale 70%]({img})
]]

---

")
cat(slides_txt, sep="")
```

# The Two-Predictor Model

```{r}
#| label: multiti
#| highlight.output: !expr '11:13'
<<multi>>
```
---

# The Two-Predictor Model

.flex.items-center[
.w-25[
&nbsp;
]
.w-50[
```{r}
#| label: 3d
#| echo: !expr F
plot3d(x=reading$hrs_wk,y=reading$age,z=reading$R_AGE,
       type='s',
       radius=.1,
       xlab="practise",ylab="age",zlab="READING AGE")
coefs <- coef(mod.m)
a <- coefs["hrs_wk"]
b <- coefs["age"]
c <- -1
d <- coefs["(Intercept)"]
planes3d(a,b,c,d, alpha=0.5)
rglwidget()
```
]
.w-25[
&nbsp;
]]
