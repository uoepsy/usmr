
---
class: inverse, middle, center

# Part 3

### The $t$-test

---



---
# The $t$ Distribution

.pull-left[
.center[
![:scale 40%](lecture_3_files/img/gossett.jpg)

"A. Student", or William Sealy Gossett
]]

.pull-right[
```{r}
#| label: normvst
#| echo: !expr F
#| fig.asp: 0.6
df <- tibble(x=c(-3.5,3.5))
df %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,n=151,colour="grey") +
  stat_function(fun=dt,n=151,args=list(df=11),colour="red") +
  xlab("standard deviations") + ylab("density") +
  annotate(geom="text",x=1.7,y=.3,label="t(11)",colour="red",size=8)
```

]

.pt2[
- note that the shape changes according to _degrees of freedom_, hence $t(11)$
]
???
The official name for the $t$-distribution is "Student's t-distribution", after William Gossett's pen-name

Gossett specialised in statistics for relatively small numbers of observations, working with Pearson, Fisher, and others

---

]]

???
here, I'm only showing the right-hand side of each distribution, so that you can see the differences between different degrees of freedom

the distributions are, of course, symmetrical
---

---
# So What is the Probability of a £`r pr` Profit?
.pull-left[
- for 12 people who made a mean profit of £`r pr` with an se of `r se`

- $t(11) = `r pr`/`r se` = `r pr/se`$

- instead of `pnorm()` we use `pt()` for the $t$ distribution

- `pt()` requires the degrees of freedom

```{r}
#| label: pt1
pt(mean(profit)/se,df=11,lower.tail=FALSE)
```
]


.pull-right[
```{r}
#| label: tgraph
#| echo: false
#| fig.asp: 0.6
df <- tibble(x=c(-3.5,3.5))
g <- df %>% ggplot(aes(x=x)) +
  stat_function(fun=dt,n=151,args=list(df=11),size=2) +
  xlab("standard deviations") + ylab("density")

ld <- layer_data(g) %>% filter(x>= mean(profit)/se)
g + geom_area(data=ld,aes(x=x,y=y),fill="red") +
  stat_function(fun=dt,n=151,args=list(df=11),size=2) +
  annotate(geom="text",x=2,y=.3,label="t(11)",size=8)
```

]
---
---
# Types of Hypothesis

```{r}
#| label: ttt
#| eval: false
t.test(profit, mu=0, alternative = "greater")
```

- note the use of `alternative="greater"`

- we've talked about the _null hypothesis_ (also **H<sub>0</sub>**)

  + there is no profit (mean profit = 0)

- the **alternative hypothesis** (**H<sub>1</sub>**, **experimental hypothesis**) is the hypothesis we're interested in

  + here, that the profit is reliably £`r pr` _or more_ (_one-tailed_ hypothesis)

  + could also use `alternative = "less"` or `alternative = "two.sided"`

---
# Putting it Together

- for $t(11)=`r mean(profit)/se`$, $p=`r pt(mean(profit/se),11,lower.tail=FALSE)`$

.br3.pa2.pt2.bg-gray.white.f3[
If you picked 12 people at random from a population of investors who were making no profit, there would be a `r (pc=round(100*pt(mean(profit)/se,11,lower.tail=F),0))`% chance that their average profit would be £`r pr` or more.
]

.pt2[
- is `r pc`% low enough for you to believe that the mean profit probably wasn't due to chance?

- perhaps we'd better face up to this question!

]

---
# Setting the Alpha Level

.pull-left[
- the $\alpha$ level is a criterion for $p$

- if $p$ is lower than the $\alpha$ level

  + we can (decide to) _reject H<sub>0</sub>_

  + we can (implicitly) _accept H<sub>1</sub>_

- what we set $\alpha$ to is a _matter of convention_

- typically, in Psychology, $\color{red}{\alpha}$ .red[is set to .05]

- important to set _before_ any statistical analysis
]
.pull-right[
```{r}
#| label: twonorms
#| echo: !expr F
#| fig.asp: 0.8
library(patchwork)
p <- df %>% ggplot(aes(x=x)) +
  xlab("") + ylab("density") +
  stat_function(fun=dnorm,n=151)
ld <- layer_data(p) %>% filter(x>=qnorm(.95))
p1 <- p + geom_area(data=ld,aes(x=x,y=y),fill="red") +
  ggtitle("one-tailed") +
  stat_function(fun=dnorm,n=151,size=1)
ld1 <- layer_data(p) %>% filter(x>=qnorm(.975))
ld2 <- layer_data(p) %>% filter(x<=qnorm(.025))
p2 <- p + geom_area(data=ld1,aes(x=x,y=y),fill="red") +
  geom_area(data=ld2,aes(x=x,y=y),fill="red") +
  xlab("std devs") + ggtitle("two-tailed") +
  stat_function(fun=dnorm,n=151,size=1)
p1/p2
```
]
???
I've shown you here on the normal curve because it's a more general distribution than the $t$ distribution, which requires degrees of freedom, but the graphs would look quite similar, as you know

- if we're testing a medicine, with possible side-effects, we want alpha to be low

- for general psychology, meh...

- no leading zeroes because $p$ is always less than 1

- no cheating now!

---

???
---
# Most Experiments Involve Differences

.pull-left[
- but of course "profit" _is_ a difference between **paired samples**

```{r}
#| label: table
#| echo: !expr F
df <- tibble(before=runif(12,100,1000),after=before+profit,profit=profit)
before=df$before
after=df$after
library(gt)
df %>% head() %>% gt() %>% fmt_currency(columns=everything(),currency="GBP")
```
]

.pull-right[
- doesn't matter whether _values_ are approx. normal as long as _differences_ are

```{r}
#| label: hist
#| fig.asp: 0.6
hist(before)
```
]
---
# Equivalent $t$-tests

.pull-left[
```{r}
#| label: ttest1
t.test(profit, mu=0,
       alternative="greater")
```

]
.pull-right[
```{r}
#| label: ttests
t.test(after, before, paired=TRUE,
       alternative="greater")
```
]
.flex.items-center[
.w-5.pa1[
![:scale 70%](lecture_1_files/img/danger.svg)
]
.w-95.pa1[
- note that a paired-samples $t$-test is actually (kind of) _multivariate_ (>1 measure per person)
]]

???
- note that the $t$-values have different signs

- this is essentially irrelevant

  + it just reflects which column was subtracted from which


---
class: inverse, center, middle, animated, rubberBand

# End

---
# Acknowledgements

- icons by Diego Lavecchia from the [Noun Project](https://thenounproject.com/)
