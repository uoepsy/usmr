---
title: "[Some Kind of End to the Course]{.r-fit-text}"
---


```{r}
#| label: setup
#| include: false

library(tidyverse)
source('_theme/theme_quarto.R')
```


# Zen and the Art of USMR {background-color="#FCBB06" style="text-align: center;"}
![](img/zen.png){fig-align="center"}

## Null-Hypothesis Statistical Testing

founded on **two principles**

1. you can't prove a hypothesis to be true

:::: {.columns}

::: {.column width="50%"}
![](img/playmo_sun.jpg){width="65%"}

_the sun will rise every day_
:::

::: {.column width="50%" .fragment}
![](img/playmo_nosun.jpg){width="65%"} 

just _one_ counterexample...
:::

::::

## Null-Hypothesis Statistical Testing

:::: {.columns}

::: {.column width="70%"}
founded on **two principles**

1. you can't prove a hypothesis to be true

2. the universe is governed by chance
:::

::: {.column width="30%"}
<!-- NEEDS FIXED -->
{{< video "img/video/galton.mp4" >}}
:::

::::





## [A Theory about the Structure of Chance]{.r-fit-text}

- if we deviate _enough_ from that structure, we assume that something else is at play

```{r}
#| echo: false
#| layout-ncol: 2
#| fig-asp: .8
x <- tibble(x=c(-4,4))
x %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,linewidth=1.5) +
  xlab("variable") + ylab("density") +
  ggtitle("Chance")+
  scale_y_continuous(labels=NULL)

db <- function(x) {dnorm(x-2)+dnorm(x+1)}
x %>% ggplot(aes(x=x)) +
  stat_function(fun=db,linewidth=1.5) +
  xlab("variable") + ylab("density") +
  ggtitle("Something Else?") +
  scale_y_continuous(labels=NULL)

```

## What is Enough?

:::: {.columns}

::: {.column width="40%"}
![](img/playmo_frazzled.jpg){width="80%" fig-align="center"}
:::

::: {.column width="60%"}

- _enough_ is operationalised as a probability

  - based on summary statistics (e.g., $\bar{x}, \sigma, n, \textrm{se}$)
  
- summaries _assume that chance is ruling things_

- summary statistics are often

  - $\frac{x-\bar{x}}{\sigma}$ (single observations)
  - $\frac{\bar{x}}{\textrm{se}}$ (group summaries)
  
- (these are basically the same thing, compared to $z$ or $t$ as appropriate)
:::
::::

---

::: myblock
if the probability of obtaining the data we have observed (or more extreme data) under the null hypothesis is low enough...
:::

[if]{.red}

- we have made the right assumptions about chance

- we have calculated the right summary

- we have looked up the appropriate probability (of obtaining summary statistic or more)

- that probability is below the $\alpha$ level we have previously set

[then]{.red}

- we can, tentatively, suggest that maybe the pattern we have observed is not due to chance

## But Linear Models...?

::: myyellowblock
$$\color{red}{\textrm{outcome}_i} = \color{blue}{(\textrm{model})_i} + \textrm{error}_i$$
:::

- essentially, these are based on _covariance_

- the [_model_]{.blue} expresses the amount that variables covary

- the _error_ is the unexplained variance, and we have a theory about its distribution

- we want the _model_ (SS) to be big compared to the _error_ (SS)

  - the $F$ ratio measures this (so does $R^2$, less directly)
  
- again, we're calculating summary statistics and looking up the probability of obtaining them (or more) in a chance universe

## Coefficients

- coefficients tell you about (hypothetical) lines through data

- the most important (and often least exciting) coefficient is the _intercept_

  - other properties of the line are always relative to this

  - we can change the interpretation of the intercept by _scaling_ predictors

::: fragment

### when predictors are _categories_

- we assign numbers to the category values (one pair of values for each of $n-1$ comparisons)

  - the interpretation of the intercept depends on these values
  
:::

## Non-Linear Predictors

## Goodbye {background-image="img/playmo_goodbye.jpg" background-size="contain"}

<!--![](img/playmo_goodbye.jpg){fig-align="center"}-->
