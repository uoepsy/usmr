{"title":"2A: Measurement & Distributions","markdown":{"yaml":{"title":"2A: Measurement & Distributions","link-citations":true,"params":{"SHOW_SOLS":true,"TOGGLE":true}},"headingText":"Types of Data","containsRefs":false,"markdown":"\n\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n```\n\n<!-- __Reading time: 23 minutes__   -->\n\n:::lo\nThis reading:  \n\n- What different types of data can we collect?\n- How can we summarise and visualise distributions of different types of data?  \n\n__Also:__  \n\n- The \"tidyverse\": a different style of coding in R\n\n\n:::\n\n\n\nIn the example of rolling a single die (like our simulations last week), each roll of the die could take one of a discrete set of responses (1, 2, 3, 4, 5 or 6). A die cannot land on 5.3, or 2.6.  \n\nThere are many different things we can measure / record on observational units, and the data we collect can have different characteristics. Some data will be similar to rolling a die in that values take on **categories**, and others could take any value on a **continuous** scale.  \n\nFor example, think about the short survey we sent out at the start of this course. Amongst other things, the survey captures data on heights (it asks for answers in cm, and respondents can be precise as they like) and eye-colours (chosen from a set of options). We distinguish between these different types of data by talking about variables that are __categorical__ (responses take one of a set of defined categories: \"blue\", \"green\", and so on..) and those that are __numeric__ (responses are in the form of a number). Within each of these, there also are a few important sub-classes.  \n\nWhen we collect data, we typically get a __sample__ of observational units (e.g., the set of people we collect data from) and each variable that we measure gives us a set of values - a __\"distribution\".__ This reading walks through the various different types of data we might encounter, and some of the metrics we use to summarise distributions of different types. Typically, summaries of distributions focus on two features:  \n\n1. **central tendency**: where _most_ of the data falls\n2. **spread**: how widely dispersed the data are  \n\nTo look at a variety of different types of data, we will use a dataset on some of the most popular internet passwords, their strength, and how long it took for an algorithm to crack it. The data are available online at [https://uoepsy.github.io/data/passworddata.csv](https://uoepsy.github.io/data/passworddata.csv).  \n  \n:::frame\n__Data: Passwords__\n\nYou can read in the dataset by using code such as the below: \n```{r}\npwords <- read.csv(\"https://uoepsy.github.io/data/passworddata.csv\")\n```\n  \n| Variable Name | Description            |\n|---------------|--------------------|\n| rank   | Popularity in the database of released passwords |\n| password     | Password |\n| type     | Category of password  |\n| cracked     | Time to crack by online guessing |\n| strength     | Strength = quality of password where 10 is highest, 1 is lowest |\n| strength_cat     | Strength category (weak, medium, strong) |\n\n\n:::\n\n\n## Categorical  \n\n:::statbox\n__Categorical__ variables tell us what group or category each individual belongs to. Each distinct group or category is called a __level__ of the variable.\n\n\n|  __Type__ | __Description__ |  __Example__|\n|:--|:--|:--|\n|  __Nominal (Unordered categorical)__ | A categorical variable with _no_ intrinsic ordering among the levels. | Species: _Dog_, _Cat_, _Parrot_, _Horse_, ... |\n|  __Ordinal (Ordered categorical)__ | A categorical variable which levels possess some kind of order |  Level: _Low_, _Medium_, _High_ |\n|  __Binary categorical__ | A special case of categorical variable with only 2 possible levels |  isDog: _Yes_ or _No_. |\n\n:::\n\nIf we want to summarise a categorical variable into a single number, then the simplest approach is to use the mode:  \n\n+ __Mode:__ The most frequent value (the value that occurs the greatest number of times).  \n\nWhen we have ordinal variables, there is another option, and that is to use the median: \n\n+ __Median:__ This is the value for which 50% of observations are lower and 50% are higher. It is the mid-point of the values when they are rank-ordered. _(Note that \"lower\" and \"higher\" requires our values to have an order to them)_\n\nWhen we use the __median__ as our measure of central tendency (i.e. the middle of the distribution) and we want to discuss how spread out the spread are around it, then we will want to use _quartiles._ The __Inter-Quartile Range (IQR)__ is obtained by rank-ordering all the data, and finding the points at which 25% (one quarter) and 75% (three quarters) of the data falls below (this makes the median the \"2nd quartile\").  \n\n\nIn our dataset on passwords, we have various categorical variables, such as the type of password (categories like \"animal\", \"fluffy\" etc).  \n\nThere are various ways we might want to summarise categorical variables like this. We have already seen the code to do this in our example of the dice simulation - we can simply counting the frequencies in each level:  \n\n```{r}\ntable(pwords$type)\n```\nThis shows us that the __mode__ (most common) is \"name\" related passwords.  \n\nWe could also convert these to proportions, by dividing each of these by the total number of observations. \nFor instance, here are the percentages of passwords of each type^[think about what `sum(table(pwords$type))` is doing. it's counting all the values in the table. so it's going to give us the total]:   \n```{r}\ntable(pwords$type) / sum(table(pwords$type)) * 100\n```\n\n:::rtip\n\nOften, if the entries in a variable are characters (letters), then many functions in R (like `table()`) will treat it the same as if it is a categorical variable.  \nHowever, this is not always the case, so it is good to tell R specifically that each variable __is__ a categorical variable.\n\nThere is a special way that we tell R that a variable is categorical - we set it to be a \"factor\". Note what happens when we make the \"type\" and \"strength_cat\"  variables to be a factor:  \n\n```{r}\npwords$type <- factor(pwords$type)\npwords$strength_cat <- factor(pwords$strength_cat)\nsummary(pwords)\n```\n\nR now recognises that there a set number of possible response options, or \"levels\", for these variables. We can see what they are using:\n```{r}\nlevels(pwords$strength_cat)\n```\nThe \"strength_cat\" variable specifically has an ordering to the levels, so we might be better off also telling R about this ordering. We do this like so:  \n```{r}\npwords$strength_cat <- factor(pwords$strength_cat, ordered = TRUE, levels = c(\"weak\",\"medium\",\"strong\"))\n```\n\n:::\n\nSometimes, we might have a variable that we know is categorical, but we might want to treat it as a set of numbers instead. A very common example in psychological research is Likert data (questions measured on scales such as \"Strongly Disagree\">>\"Disagree\">>...>>\"Strongly Agree\").  \n\nIt is often useful to have these responses as numbers (e.g. 1 = \"Strongly Disagree\" to 5 = \"Strongly Agree\"), as this allows us to use certain functions and analyses more easily. \nFor instance, the `median()` and `IQR()` functions require the data to be numbers.  \n\nThis will not work:\n```{r}\n#| error: true\nmedian(pwords$strength_cat)\n```\nWhen we ask R to convert a factor to a numeric variable, it will give turn the first category into 1, the second category to 2, and so on. As R knows that our `strength_cat` variable is the ordered categories \"weak\">>\"medium\">>\"strong\", then `as.numeric(pwords$strength_cat)` will turn these to 1s, 2s, and 3s. \n```{r}\nmedian(as.numeric(pwords$strength_cat))\n```\n\n:::rtip\n__Converting between types of data:__  \n\nIn R, we can use various functions to convert between different types of data, such as:  \n\n- `factor()` / `as.factor()` - to turn a variable into a factor\n- `as.numeric()` - to turn a variable into numbers\n- `as.character()` - to turn a variable into letters\n\nand we can _check_ what type of data something is coded as, by using `is.factor()`, `is.numeric()`, `is.character()`. \n\n::: {.callout-tip collapse=\"true\"}\n#### be careful with conversions\n\nStudy the code below and the output.  \nThink carefully about why this happens:  \n```{r}\nvec <- c(1,2,4,7)\nas.numeric(as.factor(vec))\n```\n\nWhy is the output different here?  \n```{r}\nas.numeric(as.character(as.factor(vec)))\n```\n\n:::\n\n:::\n\n## Numeric\n\n:::statbox\n__Numeric__ (or quantitative) variables consist of numbers, and represent a _measurable quantity_. Operations like adding and averaging make sense only for numeric variables.\n\n|  __Type__ | __Description__ |  __Example__|\n|:--|:--|:--|\n|  __Continuous__ | Variables which can take any real number within the specified range of measurement |  Height: _172_, _165.2_, _183_, ... |\n| __Discrete__ |  Variables which can only take integer number values. For instance, a _counts_ can only take positive integer values (0, 1, 2, 3, etc.) | Number_of_siblings: _0_, _1_, _2_, _3_, _4_, ... |\n\n:::\n\nOne of the most frequently used measures of central tendency for __numeric__ data is the __mean__.  The mean is calculated by summing all of the observations together and then dividing by the total number of obervations ($n$). \n\n:::sticky\n__Mean:__ $\\bar{x}$  \n\nWhen we have sampled some data, we denote the mean of our sample with the symbol $\\bar{x}$ (sometimes referred to as \"x bar\"). The equation for the mean is:\n\n$$\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}$$\n\n::: {.callout-note collapse=\"true\"}\n#### Help reading mathematical formulae\nThis might be the first mathematical formula you have seen in a while, so let's unpack it.  \n\nThe $\\sum$ symbol is used to denote a _series of additions_ - a __\"summation\".__  \n  \nWhen we include the bits around it: $\\sum\\limits_{i = 1}^{n}x_i$ we are indicating that we add together all the terms $x_i$ for values of $i$ between $1$ and $n$: \n$$\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n$$ \n\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the $1^{st}$ to the $n^{th}$ (where $n$ is the total number of values), and we divide that by $n$. \n:::\n\n:::\n\n\nIf we are using the __mean__ as our as our measure of central tendency, we can think of the spread of the data in terms of the __deviations__ (distances from each value to the mean).\n\nRecall that the mean is denoted by $\\bar{x}$. If we use $x_i$ to denote the $i^{th}$ value of $x$, then we can denote deviation for $x_i$ as $x_i - \\bar{x}$.  \nThe deviations can be visualised by the red lines in @fig-deviations.  \n\n```{r}\n#| label: fig-deviations\n#| echo: false\n#| fig-cap: \"Deviations from the mean\"\nknitr::include_graphics(\"images/numeric/deviations.png\")\n```\n\n:::sticky\n__The sum of the deviations from the mean, $x_i - \\bar x$, is always zero__\n\n$$\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n$$\n\nThe mean is like a center of gravity - the sum of the positive deviations (where $x_i > \\bar{x}$) is equal to the sum of the negative deviations (where $x_i < \\bar{x}$).\n:::\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider __squared deviations__.  \n\nSquaring the deviations makes them all positive. Observations far away from the mean _in either direction_ will have large, positive squared deviations. The average squared deviation is known as the __variance,__ and denoted by $s^2$\n\n:::sticky\n__Variance:__ $s^2$\n\nThe variance is calculated as the average of the squared deviations from the mean.  \n\nWhen we have sampled some data, we denote the mean of our sample with the symbol $\\bar{x}$ (sometimes referred to as \"x bar\"). The equation for the variance is:\n\n$$s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}$$\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: why n minus 1?\n\nThe top part of the equation $\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2$ can be expressed in $n-1$ terms, so we divide by $n-1$ to get the average.  \n<br>\n__Example:__ If we only have two observations $x_1$ and $x_2$, then we can write out the formula for variance in full quite easily. The top part of the equation would be:\n$$\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n$$\n\nThe mean for only two observations can be expressed as $\\bar{x} = \\frac{x_1 + x_2}{2}$, so we can substitute this in to the formula above. \n$$\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n$$\nbecomes:\n$$\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2 \n$$\nWhich simplifies down to one value:\n$$\n\\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n$$\n<br>\nSo although we have $n=2$ datapoints, $x_1$ and $x_2$, the top part of the equation for the variance has 1 fewer units of information. In order to take the average of these bits of information, we divide by $n-1$. \n:::\n\n:::\n\nOne difficulty in interpreting __variance__ as a measure of spread is that it is in units of __squared deviations.__  It reflects the typical _squared_ distance from a value to the mean.  \n\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the __standard deviation.__  \n\n:::sticky\n__Standard Deviation:__ $s$\n\nThe standard deviation, denoted by $s$, is a rough estimate of the typical distance from a value to the mean.  \nIt is the square root of the variance (the typical _squared_ distance from a value to the mean). \n\n$$\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n$$\n\n:::  \n\nIn the passwords dataset, we only have one continuous variable, and that is the \"cracked\" variable, which if we recall is the \"Time to crack by online guessing\".  You might be questioning whether the \"strength\" variable, which ranges from 1 to 10 is numeric? This depends on whether we think that statements like \"a password of strength 10 is twice as strong as a password of strength 5\".  \nFor now, we'll just look at the \"cracked\" variable.  \n\nTo calculate things like means and standard deviations in R is really easy, because there are functions that do them all for us.  \nFor instance, we can do the calculation by summing the _cracked_ variable, and dividing by the number of observations (in our case we have 500 passwords):\n```{r}\n# get the values in the \"cracked\" variable from the \"pwords\" dataframe, and\n# sum them all together. Then divide this by 500\nsum(pwords$cracked)/500\n```\nOr, more easily, we can use the `mean()` function:\n```{r}\nmean(pwords$cracked)\n```\n\nWe can get R to calculate the variance and standard deviation with the `var()` and `sd()` functions:  \n```{r}\nvar(pwords$cracked)\nsd(pwords$cracked)\n```\nand just to prove to ourselves:  \n```{r}\nsd(pwords$cracked)^2 == var(pwords$cracked)\n```\n\n\n:::rtip\nIf a column of our dataset contains only numbers, R will typically just interpret it as a numeric variable. However, we should still be careful; remember what happens if we have just one erroneous entry in there - they can all change to be characters (surrounded by quotation marks):  \n```{r}\nc(1,3,6,\"peppapig\",3)\n```\nWe can force a variable to be numeric by using `as.numeric()`, which will also coerce any non-numbers to be NA (not applicable):\n```{r}\nas.numeric(c(1,3,6,\"peppapig\",3))\n```\n\nIf there is an `NA` in the variable, many functions like `mean()`, `var()` and `sd()` will not compute: \n```{r}\nx <- c(1, 3, 6, NA, 3)\nmean(x)\n```\nHowever, we can ask these functions to remove the NAs prior to the computation:\n```{r}\nmean(x, na.rm = TRUE)\n```\n\n:::\n\n```{r}\n#| label: fig-typesdata\n#| echo: false\n#| fig-show: 'hold'\n#| fig-align: 'center'\n#| fig-cap: \"Artwork by \\\\@allison_horst\"\nknitr::include_graphics(\"images/ahorst/cdnob.png\")\n```\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Advances in R  \n\nBefore we get started on some visualisations and summaries of different types of data, we're going to briefly introduce some crucial bits of R code.   \n\n## This is a pipe!   \n\nWe have seen already seen a few examples of code such as: \n```{r}\n#| eval: false\ndim(somedata)\ntable(somedata$somevariable)\n```\n<!-- 1. show the dimensions of the data -->\n<!-- 2. show the frequency table of values in a variable -->\n\nAnd we have seen how we might wrap functions inside functions: \n```{r}\n#| eval: false\nbarplot(table(somedata$somevariable))\n```\n\nThis sort of writing (functions inside functions inside functions) involves R evaluating code from the inside out. But a lot of us don't intuitively think that way, and actually find it easier to think in terms of a sequence. The code `barplot(table(somedata$somevariable))` could be read as \"take this variable, _then_ make a table of it, _then_ make a barplot of that table\". \n\nWe can actually write code that better maps to this way of reading, using a nice little symbol called a \"pipe\":  \n\n:::statbox\n__Piping__\n\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write __sequentially__:  \n\n![](images/numeric/pipes.png)\n\nNotice that what we are doing is using a new symbol: `|>`  \n\nThis symbol takes the output of whatever is on it's left-hand side, and uses it as an _input_ for whatever is on the right-hand side.   \n\nThe `|>` symbol gets called a \"pipe\".  \n:::\n\nLet's see it in action with the passwords dataset we've been using. \n\n::::panelset\n\n:::panel\n#### inside-out\n\nThe typical way of writing code is requires reading from the inside-out:\n```{r}\n#| out-width: \"40%\"\nbarplot(table(pwords$type))\n```\n\n:::\n\n:::panel\n#### piped\n\nWhen we pipe code, we can read it from left to right:  \n\n```{r}\n#| out-width: \"40%\"\npwords$type |>\n    table() |>\n    barplot()\n```\n:::\n::::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Other pipes: |> and %>%\n\nThe `|>` pipe is a relatively recent addition to R, but will likely be replacing the older `%>%` pipe that was in a specific set of packages, and has been around since about 2014.  \n\n__These two pipes do basically the same thing__  \n\nThere are some subtle differences between the two that only become apparent in very specific situations, none of which are likely to arise on this course.  \n\nHowever, it's important to be aware of them both, because you will like see them both in resources/online forums etc. You can usually just use them interchangeably. \n<div style=\"display:inline-block;width:55%;vertical-align:top\">\n```{r}\n#| eval: false\n# for %>% we need the tidyverse\nlibrary(tidyverse)\n1:10 %>% mean()\n```\n</div>\n<div style=\"display:inline-block;width:40%;vertical-align:top;\">\n```{r}\n#| eval: false\n# the new base R pipe\n1:10 |> mean()\n```\n</div>\n  \n:::\n\n## The Tidyverse\n\nWe're going to use pipes a lot throughout this course, and it pairs really well with a group of functions in the __tidyverse__ packages, which were designed to be used in conjunction with a pipe:\n\n* `select()` extracts columns  \n* `filter()` subsets data based on conditions  \n* `mutate()` adds new variables    \n* `group_by()` group related rows together  \n* `summarise()`/`summarize()` reduces values down to a single summary  \n\nTypically, the __tidyverse__ means that we no longer have to keep telling R in which dataframe to look for the variable. The tidyverse functions are designed to make things is a bit easier. The examples below show how.  \n\nYou'll notice that the code has lots of indentations to make it more readable, which RStudio does for you when you press enter!  \n\nBefore anything else, however, we need to load the tidyverse package:  \n```{r}\nlibrary(tidyverse)\n```\n\n\n:::statbox\n__select()__  \n\nWe know about using `$` to extract a column from a dataframe. The `select()` function is a little bit like that - it allows us to choose certain columns in a dataframe. It will return all rows.  \nBecause we can select multiple columns this way, it doesn't return us a vector (in the way dataframe$variable does), but returns a dataframe:  \n\n\n```{r}\n#| eval: false\n# take the data\n# and select the \"variable1\" and \"variable2\" columns\ndata |>\n  select(variable1, variable2)\n```\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |>\n  select(type, strength)\n```\n\n:::\n::: {.panel}\n#### Base R\n```{r}\n#| eval: false\npwords[, c(\"type\",\"strength\")]\n```\n:::\n:::\n\n:::\n\n:::statbox\n__filter()__ \n\nThe `filter()` function is a bit like the `[]` to choose rows that meet certain conditios - it allows us to _filter_ a dataframe down to those rows which meet a given condition. It will return all columns.  \n\n```{r}\n#| eval: false\n# take the data\n# and filter it to only the rows where the \"variable1\" column is \n# equal to \"value1\". \ndata |> \n  filter(variable1 == value1)\n```\n\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |>\n    filter(strength_cat == \"strong\")\n```\n:::\n::: {.panel}\n#### Base R\n```{r}\n#| eval: false\npwords[pwords$strength_cat == \"strong\", ]\n```\n:::\n:::\n\n:::  \n\n:::statbox\n__mutate()__  \n\nThe `mutate()` function is used to add or modify variables to data.  \n```{r}\n#| eval: false\n# take the data\n# |>\n# mutate it, such that there is a variable called \"newvariable\", which\n# has the values of a variable called \"oldvariable\" multiplied by two.\ndata |>\n  mutate(\n    newvariable = oldvariable * 2\n  )\n```\n\nTo ensure that our additions/modifications of variables are stored in R's environment (rather than simply printed out), we need to *reassign* the name of our dataframe:\n```{r}\n#| eval: false\ndata <- \n  data |>\n  mutate(\n    ...\n  )\n```\n__Note:__ Inside functions like `mutate()`, we don't have to keep using the dollar sign `$`, as we have already told it what data to look for variables in.\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords <- pwords |> \n    mutate(\n        cracked_min = cracked / 60\n    )\n```\n:::\n::: {.panel}\n#### Base R\n```{r}\n#| eval: false\npwords$cracked_min <- pwords$cracked / 60\n```\n:::\n:::\n\n:::\n\n\n:::statbox\n__summarise()__   \n  \nThe `summarise()` function is used to reduce variables down to a single summary value.\n```{r}\n#| eval: false\n# take the data |>\n# summarise() it, such that there is a value called \"summary_value\", which\n# is the sum() of \"variable1\" column, and a value called \n# \"summary_value2\" which is the mean() of the \"variable2\" column.\ndata |>\n  summarise(\n    summary_value = sum(variable1),\n    summary_value2 = mean(variable2)\n  )\n```\n  \n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |> \n    summarise(\n        mean_cracked = mean(cracked),\n        sd_cracked = sd(cracked),\n        nr_strong = sum(strength_cat == \"strong\")\n    )\n```\n:::\n::: {.panel}\n#### Base R\nTo store these all in the same object (like the tidyverse way) we would have to create a `data.frame()` and add these as variables.  \n```{r}\n#| eval: false\nmean(pwords$cracked)\nsd(pwords$cracked)\nsum(pwords$strength_cat == \"strong\")\n```\n:::\n:::\n\n:::\n\n:::statbox\n__group_by()__\n\nThe __group_by()__ function is often used as an intermediate step in order to do something. For instance, if we want to summarise a variable by calculating its mean, but we want to do that for several groups, then we first __group_by()__ and _then_ __summarise()__:  \n\n```{r}\n#| eval: false\n# take the data |> \n# and, grouped by the levels of the \"mygroups\" variable,\n# summarise() it so that there is a column called \"summary_col\", which\n# is the mean of the \"variable1\" column for each group. \ndata |>\n    group_by(mygroups) |>\n    summarise(\n        summary_col = mean(variable1)\n    )\n```\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |> \n    group_by(strength_cat) |>\n    summarise(\n        mean_cracked = mean(cracked)\n    )\n```\n:::\n::: {.panel}\n#### Base R\nThis is less easy. There are functions in Base R that can do similar things, but we're not going to teach those here. You could envisage getting all the same values by doing:  \n```{r}\n#| eval: false\nmean(pwords$cracked[pwords$strength_cat == \"weak\"])\nmean(pwords$cracked[pwords$strength_cat == \"medium\"])\nmean(pwords$cracked[pwords$strength_cat == \"strong\"])\n```\n:::\n:::\n\n\n:::\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# ggplot\n\nWe're going to now make our first steps into the world of data visualisation, and start learning some methods for presenting plots of distributions of various types of data. R is an incredibly capable language for creating visualisations of almost any kind. It is used by many media companies (e.g., the BBC), and has the capability of producing 3d visualisations, animations, interactive graphs, and more.  \n\n:::frame\n_\"By visualizing information, we turn it into a landscape that you can explore with your eyes. A sort of information map. And when you’re lost in information, an information map is kind of useful.\"_ – [David McCandless](https://informationisbeautiful.net/)\n:::\n\nWe are going to use the most popular R package for visualisation: __ggplot2__. This is actually part of the __tidyverse__, so if we have an R script, and we have loaded the __tidyverse__ packages at the start (by using `library(tidyverse)`), then __ggplot2__ will be loaded too).  \n\nRecall our way of plotting frequencies that we have seen so far (we saw this in the dice simulations):  \n```{r}\nbarplot(table(pwords$type))\n```\n\nWe can also use `ggplot()` to visualise this. The benefit is that we can easily then edit *all* aspects of the visualisation.  \n```{r}\n# create the plot, and give the \"mappings\"\nggplot(data = pwords, aes(x = type)) + \n    # add some shapes\n    geom_bar() +\n    # add some titles, change axes labels etc\n    labs(title = \"Password type frequencies\", x = \"password type\") +\n    # edit the scales of the x axis\n    scale_x_discrete(labels = abbreviate)\n```\n\n:::rtip\n__Basic ggplot components__  \n\nNote the key components of the ggplot code. \n\n*  `data = ` where we provide the name of the dataframe. \n*  `aes = ` where we provide the _aesthetics_. These are things which we map from the data to the graph. For instance, the x-axis, or if we wanted to colour the columns/bars according to some aspect of the data. \n\nThen we add (using `+`) some _geometry_. These are the shapes (in our case, the columns/bars), which will be put in the correct place according to what we specified in `aes()`.  \n\n* `+ geom_....` Adds different shapes (e.g., bars) to the plot.  \n\n<br>\nYou can find great documentation on ggplot2 at https://www.statsandr.com/blog/graphics-in-r-with-ggplot2/.  \n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: looking ahead\n\nUse these as reference for when you want to make changes to the plots you create.  \n\nAdditionally, remember that google is your friend - there are endless forums with people asking how to do something in ggplot, and you can just copy and paste bits of code to add to your plots!  \n  \n1. Filling/colouring geoms: \n```{r}\n#| code-fold: true\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") + \n  scale_x_discrete(labels = abbreviate)\n```\n2. Change the limits of the axes:\n```{r}\n#| code-fold: true\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") +\n  scale_x_discrete(labels = abbreviate) +\n  ylim(0,250)\n```\n3. Remove (or reposition) the legend:\n```{r}\n#| code-fold: true\n# setting theme(legend.position = \"bottom\") would put it at the bottom!\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") + \n  scale_x_discrete(labels = abbreviate) +\n  guides(fill=\"none\")\n```\n4. Changing the theme. Theme-type stuff can also be used to do things such as making axes labels rotate (I always have to google how to do this stuff!)  \n```{r}\n#| code-fold: true\n# there are many predefined themes, including: \n# theme_bw(), theme_classic(), theme_light()\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") + \n  guides(fill=\"none\") + \n  scale_x_discrete(labels = abbreviate) +\n  theme_dark() +\n  theme(axis.text.x = element_text(angle = 90))\n```\n5. Other shapes, x and y:\n```{r}\n#| code-fold: true\nggplot(data = pwords, aes(x = type, y = strength)) +\n  geom_boxplot()\n\nggplot(data = pwords, aes(x = type, y = strength)) +\n  geom_violin()\n\nggplot(data = pwords, aes(x = strength, y = cracked)) + \n  geom_point()\n```\n```{r}\n#| label: fig-ahorstggplt\n#| echo: false\n#| fig-cap: \"Artwork by \\\\@allison_horst\"\nknitr::include_graphics(\"images/ahorst/ggplot2_masterpiece.png\")\n```\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Visualising Distributions\n\n## Boxplots {-}  \n\nBoxplots provide a useful way of visualising the __interquartile range (IQR).__ You can see what each part of the boxplot represents in Figure @fig-boxplotdesc.  \n\n```{r}\n#| label: fig-boxplotdesc\n#| echo: false\n#| fig-cap: \"Anatomy of a boxplot\"\n#| out-width: '100%'\n\nset.seed(34875)\npx = rnorm(100,10,10)\nmn=min(px[px>quantile(px, .25)-(1.5*IQR(px))])\nmx=max(px[px<quantile(px, .75)+(1.5*IQR(px))])\nouts_up = px[px>quantile(px, .75)+(1.5*IQR(px))]\nouts_lw = px[px<quantile(px, .25)-(1.5*IQR(px))]\nggplot(data = NULL, aes(x = px)) +\n  geom_boxplot(col=\"red\")+\n  ylim(-1,1)+xlim(-30,45)+\n  theme_classic()+\n  theme(axis.title = element_blank(), axis.text = element_blank(),\n      axis.ticks = element_blank(), axis.line = element_blank())+\n  \n  # Quartiles\n  annotate(\"text\",x=median(px), y=0.7, label=\"Q2\\n(Median)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  annotate(\"text\",x=quantile(px, .25), y=0.6, label=\"Q1\", vjust=1,hjust=1, col=\"grey70\")+\n  annotate(\"text\",x=quantile(px, .75), y=0.6, label=\"Q3\", vjust=1,hjust=0, col=\"grey70\")+\n  geom_segment(aes(x=median(px), xend=median(px), y=0.48, yend=0.38), col=\"grey70\",lty=\"dashed\")+\n  geom_segment(aes(x=quantile(px, .25), xend=quantile(px, .25), y=0.48, yend=0.38), col=\"grey70\",lty=\"dashed\")+\n  geom_segment(aes(x=quantile(px, .75), xend=quantile(px, .75), y=0.48, yend=0.38), col=\"grey70\",lty=\"dashed\")+\n  \n  # Whiskers\n  annotate(\"text\",x=mn, y=-0.2, label=\"Minimum value in data\\nwhich is > Q1 - (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  annotate(\"text\",x=mx, y=-0.2, label=\"Maximum value in data\\nwhich is < Q3 + (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  geom_segment(aes(x=mn, xend=mn, y=0, yend=-0.2),col=\"grey70\",lty=\"dashed\")+\n  geom_segment(aes(x=mx, xend=mx, y=0, yend=-0.2),col=\"grey70\",lty=\"dashed\")+\n  \n  #IQR\n  geom_segment(aes(x=quantile(px, .25), xend=quantile(px, .25), y=0.78, yend=0.85), col=\"grey70\")+\n  geom_segment(aes(x=quantile(px, .75), xend=quantile(px, .75), y=0.78, yend=0.85), col=\"grey70\")+\n  geom_segment(aes(x=quantile(px, .75), xend=quantile(px, .25), y=0.85, yend=0.85), col=\"grey70\")+\n  annotate(\"label\",x=median(px), y=0.9, label=\"IQR\", vjust=1,hjust=0.5, col=\"grey70\")+\n  \n  # Outliers +\n  geom_segment(aes(x=outs_up, xend=mean(outs_up), y=0, yend=0.27), col=\"grey70\", lty=\"dashed\")+\n  annotate(\"text\",x=mean(outs_up), y=0.5, label=\"Outliers\\nDatapoints > Q3 + (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  # Outliers -\n  geom_segment(aes(x=outs_lw, xend=mean(outs_lw), y=0, yend=0.27), col=\"grey70\", lty=\"dashed\")+\n  annotate(\"text\",x=mean(outs_lw), y=0.5, label=\"Outliers\\nDatapoints < Q1 - (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  \n  NULL\n  \nrm(mn,mx,outs_lw,outs_up,px)\n```\n\nWe can create a boxplot of our age variable using the following code:\n```{r}\n#| fig-height: 3\n# Notice, we put strength on the x axis, making the box plot vertical. \n# If we had set aes(y = strength) instead, then it would simply be rotated 90 degrees \nggplot(data = pwords, aes(x = strength)) +\n  geom_boxplot()\n```\n\n\n## Histograms {-}\n\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these map to how visualisations of numeric variables look.  \n\nWe can visualise numeric data using a __histogram__, which shows the frequency of values which fall within _bins_ of an equal width. \n\nTo do this, we're going to use some new data, on 120 participants' IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests. The data are available at [https://uoepsy.github.io/data/wechsler.csv](https://uoepsy.github.io/data/wechsler.csv)  \n```{r}\nwechsler <- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n```\n\n```{r}\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n```\nWe can specifiy the width of the bins:\n```{r}\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n```\n\nLet's take a look at the means and standard deviations of participants' scores on the other tests (the _test1_ and _test2_ variables).  \nNote how nicely we can do this with our newfound tidyverse skills!  \n```{r}\nwechsler |> \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n```\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1.\n```{r}\n#| echo: false\nggplot(data = wechsler, aes(x = test1)) + \n  geom_histogram()+\n  xlim(0,100) +\nggplot(data = wechsler, aes(x = test2)) + \n  geom_histogram()+\n  xlim(0,100)\n```\n\n\n:::statbox\n__Defining moments__  \n\nThe \"moments\" of a distribution are the metrics that relate to the shape of that distribution. \nWe've already seen the primary two moments that define the shapes of these distributions: the mean and the variance. The mean moves the distribution right or left, and the variance makes the distribution wider or narrower.  \n  \nThere are two more, \"skewness\" and \"kurtosis\" which tend to be of less focus of investigation (the questions we ask tend to be mainly concerned with means and variances). Skewness is a measure of _asymmetry_ in a distribution. Distributions can be _positively skewed_ or _negatively skewed_, and this influences our measures of central tendency and of spread to different degrees. The kurtosis is a measure of how \"pointy\" vs \"rounded\" the shape of a distribution is.  \n\n\n```{r}\n#| echo: false\ntibble(\n  x = seq(0, 200, 1),\n  y = dnorm(x, 100, 15),\n  y2 = dnorm(x,150, 15),\n  y3 = dnorm(x,50,15)\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"Mean = 100\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"Mean = 150\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"Mean = 50\"), alpha=0.2)+\n  theme_classic()+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  guides(fill=FALSE)+\n  labs(title=\"The mean defines the location of a distribution\") -> plt_loc\n\ntibble(\n  x = seq(0, 200, 1),\n  y = dnorm(x, 100,10),\n  y2 = dnorm(x,100, 15),\n  y3 = dnorm(x,100,5)\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"Mean = 100\"), alpha=0.3)+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"Mean = 150\"), alpha=0.3)+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"Mean = 50\"), alpha=0.3)+\n  theme_classic()+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  guides(fill=FALSE)+\n  xlim(50,150)+\n  labs(title=\"The variance defines the scale of a distribution\") -> plt_scale\n\nlibrary(sn)\ntibble(\n  x = seq(0, 200, 1),\n  y = dnorm(x, 100, 15),\n  y2 = dsn(x, xi = 180, omega=27.5, alpha = -5),\n  y3 = dsn(x, xi = 20, omega=27.5,alpha = 5),\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"Symmetric\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"Negative Skew\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"Positive Skew\"), alpha=0.2)+\n  theme_classic()+\n  scale_fill_manual(\"\",breaks=c(\"Positive Skew\",\"Symmetric\",\"Negative Skew\"), values=c(\"chartreuse3\",\"blue\",\"red\"))+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  \n  geom_vline(aes(xintercept=100), lty=\"dashed\", col=\"blue\")+\n  annotate(\"label\",x=100, y=0.022, label=\"Mode\", vjust=1,hjust=0.5, col=\"blue\")+\n  annotate(\"label\",x=100, y=0.0275, label=\"Median\", vjust=1,hjust=0.5, col=\"blue\")+\n  annotate(\"label\",x=100, y=0.033, label=\"Mean\", vjust=1,hjust=0.5, col=\"blue\")+\n  \n  geom_vline(xintercept=c(155,165,170), lty=\"dashed\", col=\"red\")+\n  annotate(\"label\",x=175, y=0.022, label=\"Mode\", vjust=1,hjust=0.5, col=\"red\")+\n  annotate(\"label\",x=165, y=0.0275, label=\"Median\", vjust=1,hjust=0.5, col=\"red\")+\n  annotate(\"label\",x=155, y=0.033, label=\"Mean\", vjust=1,hjust=0.5, col=\"red\")+\n  \n  geom_vline(xintercept=c(45,35,30), lty=\"dashed\", col=\"chartreuse3\")+\n  annotate(\"label\",x=25, y=0.022, label=\"Mode\", vjust=1,hjust=0.5, col=\"chartreuse3\")+\n  annotate(\"label\",x=35, y=0.0275, label=\"Median\", vjust=1,hjust=0.5, col=\"chartreuse3\")+\n  annotate(\"label\",x=45, y=0.033, label=\"Mean\", vjust=1,hjust=0.5, col=\"chartreuse3\")+\n  labs(title=\"Skewness defines the asymmetry of a distribution\") + \n  NULL -> plt_skew\n\nlibrary(PearsonDS)\ntibble(\n  x = seq(0, 200, .1),\n  y = dpearson(x, moments = c(mean=100,variance=15,skewness=0,kurtosis=3)),\n  y2 = dpearson(x, moments = c(mean=100,variance=15,skewness=0,kurtosis=7)),\n  y3 = dpearson(x,moments = c(mean=100,variance=15,skewness=0,kurtosis=2.3))\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"100\"), alpha=0.2)+\n  geom_line(aes(y=y,col=\"100\"))+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"150\"), alpha=0.2)+\n  geom_line(aes(y=y2,col=\"150\"))+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"50\"), alpha=0.2)+\n  geom_line(aes(y=y3,col=\"50\"))+\n  theme_classic()+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  guides(fill='none',col='none')+\n  xlim(80,120)+\n  labs(title=\"The kurtosis defines the 'pointy-ness' of a distribution\") -> plt_kurt\n\nplt_loc / plt_scale\nplt_skew / plt_kurt\n```\n\n:::\n\n\n## Density\n\nIn addition to grouping numeric data into _bins_ in order to produce a histogram, we can also visualise a __density curve.__  \n\nBecause there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, ...), we could group the data into infinitely many bins. This is essentially what we are doing with a density curve.  \nYou can think of \"density\" as a bit similar to the notion of \"relative frequency\" (or \"proportion\"), in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.  \n\n```{r}\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()\n```\n\n\n\n:::statbox\n__Area under the curve__  \n\nThink about the barplots we have been looking at in the exercises where we simulate dice rolling :  \n```{r}\n# our function to simulate the roll of a die/some dice\ndice <- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n# simulate 1000 rolls of a single die\nroll1000 <- replicate(1000, dice(1))\n# tabulate and plot:\ntable(roll1000) |>\n  barplot(ylab=\"count\")\n```\nTo think about questions like \"what proportion of 1000 rolls does the die land on 6?\", we are simply interested in the count of 6s divided by the count of all rolls:  \n```{r}\ntab1000 <- table(roll1000)\ntab1000\ntab1000[6] / sum(tab1000)\n```\nSo Another way of thinking of this is that we are just dividing the count in each category by the total number. Or, \nPut another way, imagine we divide the area of each bar by the total area. The area now sums to 1, and our question is asking about the ratio of the red area to the total area (grey + red): \n```{r}\n#| echo: false\ntable(roll1000) |> \n  # convert to proportions\n  prop.table() |>\n  barplot(col=c(rep(\"#bbbbbb\",5),\"#ff0000\"),ylab=\"count/total\")\n```\n\nNothing really changes with a density curve! If we want to ask what proportion of our distribution of IQ scores is >120, then we are asking about the area under the curve that is to the right of 120:  \n\n```{r}\n#| echo: false\np <- ggplot(wechsler,aes(x=iq))+\n    geom_density(fill=\"grey\")\nd <- ggplot_build(p)$data[[1]]\np + geom_area(data = subset(d, x > 110), aes(x=x, y=y), fill=\"red\")\n```\n\nIt looks like about a third, maybe a little less. \nLet's calculate this proportion directly:  \n```{r}\nsum(wechsler$iq>110) / length(wechsler$iq)\n```\n\nIt might seem a little odd to think about area under the curve when we are asking about \"what _proportion_ of the data is ...?\". If we have the data, then we can just calculate the answer (like we did above). \nHowever, a lot of statistics is really concerned with the _probability_ of events. When we discuss probability, we move from talking about a specific set of observed data to thinking about a theoretical/mathematical model that defines the way in which data is generated. This where it becomes more useful to think about distributions in a more abstract sense.  \n\nFor instance, with a fair six-sided die, we have a probability distribution  (@fig-diceprob) in which each side is given the probability $\\frac{1}{6}$:\n$$\n\\begin{gather*}\nP(x) = \\begin{cases}\n  \\frac{1}{6} & \\text{if $x \\in \\{1,2,3,4,5,6\\}$}\\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n\\end{gather*}\n$$\nInstead of rolling a die, suppose that we are picking a person off the street and measuring their IQ. Given that IQ scales are designed to have a mean of 100 and standard deviation of 15, what is the _probability_ that we pick a person with an IQ of greater than 110?  \n```{r}\n#| label: fig-diceprob\n#| fig-cap: \"Left: Discrete probability distribution of a fair six-sided die. Right: Continuous probability distribution of IQ scores\"\n#| echo: false\n#| fig-height: 3.5\ntibble(response = 1:6, prob = rep(1/6,6)) %>%\nggplot(., aes(x=response, y=prob))+\n  geom_point(size=3)+\n  geom_segment(aes(x=response, xend=response, y=0,yend=prob),lty=\"dotted\")+\n  scale_x_continuous(\"possible faces of a die\", breaks=1:6)+\n  scale_y_continuous(\"probability\", limits = c(0,1), breaks=map_dbl(1:6,~./6), labels=c(paste0(1:5,\"/6\"),\"1\"))+\n  theme_classic()+\n  theme(text = element_text(size = 20)) -> dp\n\n\ndf <- tibble(x=c(50,150))\ng <- df %>% ggplot(aes(x=x)) +\n  stat_function(fun=dnorm,args=list(mean=100,sd=15),size=1) +\n  xlab(\"possible IQ scores\") + ylab(\"density\")\nld <- layer_data(g) %>% filter(x>= 110)\ncp <- \n  g + geom_area(data=ld,aes(x=x,y=y),fill=\"red\")+theme_classic()+\n  theme(text = element_text(size = 20))\n\nlibrary(patchwork)\ndp + cp\n\n```\n\n:::\n\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n```\n\n<!-- __Reading time: 23 minutes__   -->\n\n:::lo\nThis reading:  \n\n- What different types of data can we collect?\n- How can we summarise and visualise distributions of different types of data?  \n\n__Also:__  \n\n- The \"tidyverse\": a different style of coding in R\n\n\n:::\n\n\n# Types of Data\n\nIn the example of rolling a single die (like our simulations last week), each roll of the die could take one of a discrete set of responses (1, 2, 3, 4, 5 or 6). A die cannot land on 5.3, or 2.6.  \n\nThere are many different things we can measure / record on observational units, and the data we collect can have different characteristics. Some data will be similar to rolling a die in that values take on **categories**, and others could take any value on a **continuous** scale.  \n\nFor example, think about the short survey we sent out at the start of this course. Amongst other things, the survey captures data on heights (it asks for answers in cm, and respondents can be precise as they like) and eye-colours (chosen from a set of options). We distinguish between these different types of data by talking about variables that are __categorical__ (responses take one of a set of defined categories: \"blue\", \"green\", and so on..) and those that are __numeric__ (responses are in the form of a number). Within each of these, there also are a few important sub-classes.  \n\nWhen we collect data, we typically get a __sample__ of observational units (e.g., the set of people we collect data from) and each variable that we measure gives us a set of values - a __\"distribution\".__ This reading walks through the various different types of data we might encounter, and some of the metrics we use to summarise distributions of different types. Typically, summaries of distributions focus on two features:  \n\n1. **central tendency**: where _most_ of the data falls\n2. **spread**: how widely dispersed the data are  \n\nTo look at a variety of different types of data, we will use a dataset on some of the most popular internet passwords, their strength, and how long it took for an algorithm to crack it. The data are available online at [https://uoepsy.github.io/data/passworddata.csv](https://uoepsy.github.io/data/passworddata.csv).  \n  \n:::frame\n__Data: Passwords__\n\nYou can read in the dataset by using code such as the below: \n```{r}\npwords <- read.csv(\"https://uoepsy.github.io/data/passworddata.csv\")\n```\n  \n| Variable Name | Description            |\n|---------------|--------------------|\n| rank   | Popularity in the database of released passwords |\n| password     | Password |\n| type     | Category of password  |\n| cracked     | Time to crack by online guessing |\n| strength     | Strength = quality of password where 10 is highest, 1 is lowest |\n| strength_cat     | Strength category (weak, medium, strong) |\n\n\n:::\n\n\n## Categorical  \n\n:::statbox\n__Categorical__ variables tell us what group or category each individual belongs to. Each distinct group or category is called a __level__ of the variable.\n\n\n|  __Type__ | __Description__ |  __Example__|\n|:--|:--|:--|\n|  __Nominal (Unordered categorical)__ | A categorical variable with _no_ intrinsic ordering among the levels. | Species: _Dog_, _Cat_, _Parrot_, _Horse_, ... |\n|  __Ordinal (Ordered categorical)__ | A categorical variable which levels possess some kind of order |  Level: _Low_, _Medium_, _High_ |\n|  __Binary categorical__ | A special case of categorical variable with only 2 possible levels |  isDog: _Yes_ or _No_. |\n\n:::\n\nIf we want to summarise a categorical variable into a single number, then the simplest approach is to use the mode:  \n\n+ __Mode:__ The most frequent value (the value that occurs the greatest number of times).  \n\nWhen we have ordinal variables, there is another option, and that is to use the median: \n\n+ __Median:__ This is the value for which 50% of observations are lower and 50% are higher. It is the mid-point of the values when they are rank-ordered. _(Note that \"lower\" and \"higher\" requires our values to have an order to them)_\n\nWhen we use the __median__ as our measure of central tendency (i.e. the middle of the distribution) and we want to discuss how spread out the spread are around it, then we will want to use _quartiles._ The __Inter-Quartile Range (IQR)__ is obtained by rank-ordering all the data, and finding the points at which 25% (one quarter) and 75% (three quarters) of the data falls below (this makes the median the \"2nd quartile\").  \n\n\nIn our dataset on passwords, we have various categorical variables, such as the type of password (categories like \"animal\", \"fluffy\" etc).  \n\nThere are various ways we might want to summarise categorical variables like this. We have already seen the code to do this in our example of the dice simulation - we can simply counting the frequencies in each level:  \n\n```{r}\ntable(pwords$type)\n```\nThis shows us that the __mode__ (most common) is \"name\" related passwords.  \n\nWe could also convert these to proportions, by dividing each of these by the total number of observations. \nFor instance, here are the percentages of passwords of each type^[think about what `sum(table(pwords$type))` is doing. it's counting all the values in the table. so it's going to give us the total]:   \n```{r}\ntable(pwords$type) / sum(table(pwords$type)) * 100\n```\n\n:::rtip\n\nOften, if the entries in a variable are characters (letters), then many functions in R (like `table()`) will treat it the same as if it is a categorical variable.  \nHowever, this is not always the case, so it is good to tell R specifically that each variable __is__ a categorical variable.\n\nThere is a special way that we tell R that a variable is categorical - we set it to be a \"factor\". Note what happens when we make the \"type\" and \"strength_cat\"  variables to be a factor:  \n\n```{r}\npwords$type <- factor(pwords$type)\npwords$strength_cat <- factor(pwords$strength_cat)\nsummary(pwords)\n```\n\nR now recognises that there a set number of possible response options, or \"levels\", for these variables. We can see what they are using:\n```{r}\nlevels(pwords$strength_cat)\n```\nThe \"strength_cat\" variable specifically has an ordering to the levels, so we might be better off also telling R about this ordering. We do this like so:  \n```{r}\npwords$strength_cat <- factor(pwords$strength_cat, ordered = TRUE, levels = c(\"weak\",\"medium\",\"strong\"))\n```\n\n:::\n\nSometimes, we might have a variable that we know is categorical, but we might want to treat it as a set of numbers instead. A very common example in psychological research is Likert data (questions measured on scales such as \"Strongly Disagree\">>\"Disagree\">>...>>\"Strongly Agree\").  \n\nIt is often useful to have these responses as numbers (e.g. 1 = \"Strongly Disagree\" to 5 = \"Strongly Agree\"), as this allows us to use certain functions and analyses more easily. \nFor instance, the `median()` and `IQR()` functions require the data to be numbers.  \n\nThis will not work:\n```{r}\n#| error: true\nmedian(pwords$strength_cat)\n```\nWhen we ask R to convert a factor to a numeric variable, it will give turn the first category into 1, the second category to 2, and so on. As R knows that our `strength_cat` variable is the ordered categories \"weak\">>\"medium\">>\"strong\", then `as.numeric(pwords$strength_cat)` will turn these to 1s, 2s, and 3s. \n```{r}\nmedian(as.numeric(pwords$strength_cat))\n```\n\n:::rtip\n__Converting between types of data:__  \n\nIn R, we can use various functions to convert between different types of data, such as:  \n\n- `factor()` / `as.factor()` - to turn a variable into a factor\n- `as.numeric()` - to turn a variable into numbers\n- `as.character()` - to turn a variable into letters\n\nand we can _check_ what type of data something is coded as, by using `is.factor()`, `is.numeric()`, `is.character()`. \n\n::: {.callout-tip collapse=\"true\"}\n#### be careful with conversions\n\nStudy the code below and the output.  \nThink carefully about why this happens:  \n```{r}\nvec <- c(1,2,4,7)\nas.numeric(as.factor(vec))\n```\n\nWhy is the output different here?  \n```{r}\nas.numeric(as.character(as.factor(vec)))\n```\n\n:::\n\n:::\n\n## Numeric\n\n:::statbox\n__Numeric__ (or quantitative) variables consist of numbers, and represent a _measurable quantity_. Operations like adding and averaging make sense only for numeric variables.\n\n|  __Type__ | __Description__ |  __Example__|\n|:--|:--|:--|\n|  __Continuous__ | Variables which can take any real number within the specified range of measurement |  Height: _172_, _165.2_, _183_, ... |\n| __Discrete__ |  Variables which can only take integer number values. For instance, a _counts_ can only take positive integer values (0, 1, 2, 3, etc.) | Number_of_siblings: _0_, _1_, _2_, _3_, _4_, ... |\n\n:::\n\nOne of the most frequently used measures of central tendency for __numeric__ data is the __mean__.  The mean is calculated by summing all of the observations together and then dividing by the total number of obervations ($n$). \n\n:::sticky\n__Mean:__ $\\bar{x}$  \n\nWhen we have sampled some data, we denote the mean of our sample with the symbol $\\bar{x}$ (sometimes referred to as \"x bar\"). The equation for the mean is:\n\n$$\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}$$\n\n::: {.callout-note collapse=\"true\"}\n#### Help reading mathematical formulae\nThis might be the first mathematical formula you have seen in a while, so let's unpack it.  \n\nThe $\\sum$ symbol is used to denote a _series of additions_ - a __\"summation\".__  \n  \nWhen we include the bits around it: $\\sum\\limits_{i = 1}^{n}x_i$ we are indicating that we add together all the terms $x_i$ for values of $i$ between $1$ and $n$: \n$$\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n$$ \n\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the $1^{st}$ to the $n^{th}$ (where $n$ is the total number of values), and we divide that by $n$. \n:::\n\n:::\n\n\nIf we are using the __mean__ as our as our measure of central tendency, we can think of the spread of the data in terms of the __deviations__ (distances from each value to the mean).\n\nRecall that the mean is denoted by $\\bar{x}$. If we use $x_i$ to denote the $i^{th}$ value of $x$, then we can denote deviation for $x_i$ as $x_i - \\bar{x}$.  \nThe deviations can be visualised by the red lines in @fig-deviations.  \n\n```{r}\n#| label: fig-deviations\n#| echo: false\n#| fig-cap: \"Deviations from the mean\"\nknitr::include_graphics(\"images/numeric/deviations.png\")\n```\n\n:::sticky\n__The sum of the deviations from the mean, $x_i - \\bar x$, is always zero__\n\n$$\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n$$\n\nThe mean is like a center of gravity - the sum of the positive deviations (where $x_i > \\bar{x}$) is equal to the sum of the negative deviations (where $x_i < \\bar{x}$).\n:::\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider __squared deviations__.  \n\nSquaring the deviations makes them all positive. Observations far away from the mean _in either direction_ will have large, positive squared deviations. The average squared deviation is known as the __variance,__ and denoted by $s^2$\n\n:::sticky\n__Variance:__ $s^2$\n\nThe variance is calculated as the average of the squared deviations from the mean.  \n\nWhen we have sampled some data, we denote the mean of our sample with the symbol $\\bar{x}$ (sometimes referred to as \"x bar\"). The equation for the variance is:\n\n$$s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}$$\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: why n minus 1?\n\nThe top part of the equation $\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2$ can be expressed in $n-1$ terms, so we divide by $n-1$ to get the average.  \n<br>\n__Example:__ If we only have two observations $x_1$ and $x_2$, then we can write out the formula for variance in full quite easily. The top part of the equation would be:\n$$\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n$$\n\nThe mean for only two observations can be expressed as $\\bar{x} = \\frac{x_1 + x_2}{2}$, so we can substitute this in to the formula above. \n$$\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n$$\nbecomes:\n$$\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2 \n$$\nWhich simplifies down to one value:\n$$\n\\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n$$\n<br>\nSo although we have $n=2$ datapoints, $x_1$ and $x_2$, the top part of the equation for the variance has 1 fewer units of information. In order to take the average of these bits of information, we divide by $n-1$. \n:::\n\n:::\n\nOne difficulty in interpreting __variance__ as a measure of spread is that it is in units of __squared deviations.__  It reflects the typical _squared_ distance from a value to the mean.  \n\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the __standard deviation.__  \n\n:::sticky\n__Standard Deviation:__ $s$\n\nThe standard deviation, denoted by $s$, is a rough estimate of the typical distance from a value to the mean.  \nIt is the square root of the variance (the typical _squared_ distance from a value to the mean). \n\n$$\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n$$\n\n:::  \n\nIn the passwords dataset, we only have one continuous variable, and that is the \"cracked\" variable, which if we recall is the \"Time to crack by online guessing\".  You might be questioning whether the \"strength\" variable, which ranges from 1 to 10 is numeric? This depends on whether we think that statements like \"a password of strength 10 is twice as strong as a password of strength 5\".  \nFor now, we'll just look at the \"cracked\" variable.  \n\nTo calculate things like means and standard deviations in R is really easy, because there are functions that do them all for us.  \nFor instance, we can do the calculation by summing the _cracked_ variable, and dividing by the number of observations (in our case we have 500 passwords):\n```{r}\n# get the values in the \"cracked\" variable from the \"pwords\" dataframe, and\n# sum them all together. Then divide this by 500\nsum(pwords$cracked)/500\n```\nOr, more easily, we can use the `mean()` function:\n```{r}\nmean(pwords$cracked)\n```\n\nWe can get R to calculate the variance and standard deviation with the `var()` and `sd()` functions:  \n```{r}\nvar(pwords$cracked)\nsd(pwords$cracked)\n```\nand just to prove to ourselves:  \n```{r}\nsd(pwords$cracked)^2 == var(pwords$cracked)\n```\n\n\n:::rtip\nIf a column of our dataset contains only numbers, R will typically just interpret it as a numeric variable. However, we should still be careful; remember what happens if we have just one erroneous entry in there - they can all change to be characters (surrounded by quotation marks):  \n```{r}\nc(1,3,6,\"peppapig\",3)\n```\nWe can force a variable to be numeric by using `as.numeric()`, which will also coerce any non-numbers to be NA (not applicable):\n```{r}\nas.numeric(c(1,3,6,\"peppapig\",3))\n```\n\nIf there is an `NA` in the variable, many functions like `mean()`, `var()` and `sd()` will not compute: \n```{r}\nx <- c(1, 3, 6, NA, 3)\nmean(x)\n```\nHowever, we can ask these functions to remove the NAs prior to the computation:\n```{r}\nmean(x, na.rm = TRUE)\n```\n\n:::\n\n```{r}\n#| label: fig-typesdata\n#| echo: false\n#| fig-show: 'hold'\n#| fig-align: 'center'\n#| fig-cap: \"Artwork by \\\\@allison_horst\"\nknitr::include_graphics(\"images/ahorst/cdnob.png\")\n```\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Advances in R  \n\nBefore we get started on some visualisations and summaries of different types of data, we're going to briefly introduce some crucial bits of R code.   \n\n## This is a pipe!   \n\nWe have seen already seen a few examples of code such as: \n```{r}\n#| eval: false\ndim(somedata)\ntable(somedata$somevariable)\n```\n<!-- 1. show the dimensions of the data -->\n<!-- 2. show the frequency table of values in a variable -->\n\nAnd we have seen how we might wrap functions inside functions: \n```{r}\n#| eval: false\nbarplot(table(somedata$somevariable))\n```\n\nThis sort of writing (functions inside functions inside functions) involves R evaluating code from the inside out. But a lot of us don't intuitively think that way, and actually find it easier to think in terms of a sequence. The code `barplot(table(somedata$somevariable))` could be read as \"take this variable, _then_ make a table of it, _then_ make a barplot of that table\". \n\nWe can actually write code that better maps to this way of reading, using a nice little symbol called a \"pipe\":  \n\n:::statbox\n__Piping__\n\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write __sequentially__:  \n\n![](images/numeric/pipes.png)\n\nNotice that what we are doing is using a new symbol: `|>`  \n\nThis symbol takes the output of whatever is on it's left-hand side, and uses it as an _input_ for whatever is on the right-hand side.   \n\nThe `|>` symbol gets called a \"pipe\".  \n:::\n\nLet's see it in action with the passwords dataset we've been using. \n\n::::panelset\n\n:::panel\n#### inside-out\n\nThe typical way of writing code is requires reading from the inside-out:\n```{r}\n#| out-width: \"40%\"\nbarplot(table(pwords$type))\n```\n\n:::\n\n:::panel\n#### piped\n\nWhen we pipe code, we can read it from left to right:  \n\n```{r}\n#| out-width: \"40%\"\npwords$type |>\n    table() |>\n    barplot()\n```\n:::\n::::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Other pipes: |> and %>%\n\nThe `|>` pipe is a relatively recent addition to R, but will likely be replacing the older `%>%` pipe that was in a specific set of packages, and has been around since about 2014.  \n\n__These two pipes do basically the same thing__  \n\nThere are some subtle differences between the two that only become apparent in very specific situations, none of which are likely to arise on this course.  \n\nHowever, it's important to be aware of them both, because you will like see them both in resources/online forums etc. You can usually just use them interchangeably. \n<div style=\"display:inline-block;width:55%;vertical-align:top\">\n```{r}\n#| eval: false\n# for %>% we need the tidyverse\nlibrary(tidyverse)\n1:10 %>% mean()\n```\n</div>\n<div style=\"display:inline-block;width:40%;vertical-align:top;\">\n```{r}\n#| eval: false\n# the new base R pipe\n1:10 |> mean()\n```\n</div>\n  \n:::\n\n## The Tidyverse\n\nWe're going to use pipes a lot throughout this course, and it pairs really well with a group of functions in the __tidyverse__ packages, which were designed to be used in conjunction with a pipe:\n\n* `select()` extracts columns  \n* `filter()` subsets data based on conditions  \n* `mutate()` adds new variables    \n* `group_by()` group related rows together  \n* `summarise()`/`summarize()` reduces values down to a single summary  \n\nTypically, the __tidyverse__ means that we no longer have to keep telling R in which dataframe to look for the variable. The tidyverse functions are designed to make things is a bit easier. The examples below show how.  \n\nYou'll notice that the code has lots of indentations to make it more readable, which RStudio does for you when you press enter!  \n\nBefore anything else, however, we need to load the tidyverse package:  \n```{r}\nlibrary(tidyverse)\n```\n\n\n:::statbox\n__select()__  \n\nWe know about using `$` to extract a column from a dataframe. The `select()` function is a little bit like that - it allows us to choose certain columns in a dataframe. It will return all rows.  \nBecause we can select multiple columns this way, it doesn't return us a vector (in the way dataframe$variable does), but returns a dataframe:  \n\n\n```{r}\n#| eval: false\n# take the data\n# and select the \"variable1\" and \"variable2\" columns\ndata |>\n  select(variable1, variable2)\n```\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |>\n  select(type, strength)\n```\n\n:::\n::: {.panel}\n#### Base R\n```{r}\n#| eval: false\npwords[, c(\"type\",\"strength\")]\n```\n:::\n:::\n\n:::\n\n:::statbox\n__filter()__ \n\nThe `filter()` function is a bit like the `[]` to choose rows that meet certain conditios - it allows us to _filter_ a dataframe down to those rows which meet a given condition. It will return all columns.  \n\n```{r}\n#| eval: false\n# take the data\n# and filter it to only the rows where the \"variable1\" column is \n# equal to \"value1\". \ndata |> \n  filter(variable1 == value1)\n```\n\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |>\n    filter(strength_cat == \"strong\")\n```\n:::\n::: {.panel}\n#### Base R\n```{r}\n#| eval: false\npwords[pwords$strength_cat == \"strong\", ]\n```\n:::\n:::\n\n:::  \n\n:::statbox\n__mutate()__  \n\nThe `mutate()` function is used to add or modify variables to data.  \n```{r}\n#| eval: false\n# take the data\n# |>\n# mutate it, such that there is a variable called \"newvariable\", which\n# has the values of a variable called \"oldvariable\" multiplied by two.\ndata |>\n  mutate(\n    newvariable = oldvariable * 2\n  )\n```\n\nTo ensure that our additions/modifications of variables are stored in R's environment (rather than simply printed out), we need to *reassign* the name of our dataframe:\n```{r}\n#| eval: false\ndata <- \n  data |>\n  mutate(\n    ...\n  )\n```\n__Note:__ Inside functions like `mutate()`, we don't have to keep using the dollar sign `$`, as we have already told it what data to look for variables in.\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords <- pwords |> \n    mutate(\n        cracked_min = cracked / 60\n    )\n```\n:::\n::: {.panel}\n#### Base R\n```{r}\n#| eval: false\npwords$cracked_min <- pwords$cracked / 60\n```\n:::\n:::\n\n:::\n\n\n:::statbox\n__summarise()__   \n  \nThe `summarise()` function is used to reduce variables down to a single summary value.\n```{r}\n#| eval: false\n# take the data |>\n# summarise() it, such that there is a value called \"summary_value\", which\n# is the sum() of \"variable1\" column, and a value called \n# \"summary_value2\" which is the mean() of the \"variable2\" column.\ndata |>\n  summarise(\n    summary_value = sum(variable1),\n    summary_value2 = mean(variable2)\n  )\n```\n  \n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |> \n    summarise(\n        mean_cracked = mean(cracked),\n        sd_cracked = sd(cracked),\n        nr_strong = sum(strength_cat == \"strong\")\n    )\n```\n:::\n::: {.panel}\n#### Base R\nTo store these all in the same object (like the tidyverse way) we would have to create a `data.frame()` and add these as variables.  \n```{r}\n#| eval: false\nmean(pwords$cracked)\nsd(pwords$cracked)\nsum(pwords$strength_cat == \"strong\")\n```\n:::\n:::\n\n:::\n\n:::statbox\n__group_by()__\n\nThe __group_by()__ function is often used as an intermediate step in order to do something. For instance, if we want to summarise a variable by calculating its mean, but we want to do that for several groups, then we first __group_by()__ and _then_ __summarise()__:  \n\n```{r}\n#| eval: false\n# take the data |> \n# and, grouped by the levels of the \"mygroups\" variable,\n# summarise() it so that there is a column called \"summary_col\", which\n# is the mean of the \"variable1\" column for each group. \ndata |>\n    group_by(mygroups) |>\n    summarise(\n        summary_col = mean(variable1)\n    )\n```\n\n::: {.panelset}\n::: {.panel}\n#### Tidyverse\n```{r}\n#| eval: false\npwords |> \n    group_by(strength_cat) |>\n    summarise(\n        mean_cracked = mean(cracked)\n    )\n```\n:::\n::: {.panel}\n#### Base R\nThis is less easy. There are functions in Base R that can do similar things, but we're not going to teach those here. You could envisage getting all the same values by doing:  \n```{r}\n#| eval: false\nmean(pwords$cracked[pwords$strength_cat == \"weak\"])\nmean(pwords$cracked[pwords$strength_cat == \"medium\"])\nmean(pwords$cracked[pwords$strength_cat == \"strong\"])\n```\n:::\n:::\n\n\n:::\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# ggplot\n\nWe're going to now make our first steps into the world of data visualisation, and start learning some methods for presenting plots of distributions of various types of data. R is an incredibly capable language for creating visualisations of almost any kind. It is used by many media companies (e.g., the BBC), and has the capability of producing 3d visualisations, animations, interactive graphs, and more.  \n\n:::frame\n_\"By visualizing information, we turn it into a landscape that you can explore with your eyes. A sort of information map. And when you’re lost in information, an information map is kind of useful.\"_ – [David McCandless](https://informationisbeautiful.net/)\n:::\n\nWe are going to use the most popular R package for visualisation: __ggplot2__. This is actually part of the __tidyverse__, so if we have an R script, and we have loaded the __tidyverse__ packages at the start (by using `library(tidyverse)`), then __ggplot2__ will be loaded too).  \n\nRecall our way of plotting frequencies that we have seen so far (we saw this in the dice simulations):  \n```{r}\nbarplot(table(pwords$type))\n```\n\nWe can also use `ggplot()` to visualise this. The benefit is that we can easily then edit *all* aspects of the visualisation.  \n```{r}\n# create the plot, and give the \"mappings\"\nggplot(data = pwords, aes(x = type)) + \n    # add some shapes\n    geom_bar() +\n    # add some titles, change axes labels etc\n    labs(title = \"Password type frequencies\", x = \"password type\") +\n    # edit the scales of the x axis\n    scale_x_discrete(labels = abbreviate)\n```\n\n:::rtip\n__Basic ggplot components__  \n\nNote the key components of the ggplot code. \n\n*  `data = ` where we provide the name of the dataframe. \n*  `aes = ` where we provide the _aesthetics_. These are things which we map from the data to the graph. For instance, the x-axis, or if we wanted to colour the columns/bars according to some aspect of the data. \n\nThen we add (using `+`) some _geometry_. These are the shapes (in our case, the columns/bars), which will be put in the correct place according to what we specified in `aes()`.  \n\n* `+ geom_....` Adds different shapes (e.g., bars) to the plot.  \n\n<br>\nYou can find great documentation on ggplot2 at https://www.statsandr.com/blog/graphics-in-r-with-ggplot2/.  \n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: looking ahead\n\nUse these as reference for when you want to make changes to the plots you create.  \n\nAdditionally, remember that google is your friend - there are endless forums with people asking how to do something in ggplot, and you can just copy and paste bits of code to add to your plots!  \n  \n1. Filling/colouring geoms: \n```{r}\n#| code-fold: true\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") + \n  scale_x_discrete(labels = abbreviate)\n```\n2. Change the limits of the axes:\n```{r}\n#| code-fold: true\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") +\n  scale_x_discrete(labels = abbreviate) +\n  ylim(0,250)\n```\n3. Remove (or reposition) the legend:\n```{r}\n#| code-fold: true\n# setting theme(legend.position = \"bottom\") would put it at the bottom!\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") + \n  scale_x_discrete(labels = abbreviate) +\n  guides(fill=\"none\")\n```\n4. Changing the theme. Theme-type stuff can also be used to do things such as making axes labels rotate (I always have to google how to do this stuff!)  \n```{r}\n#| code-fold: true\n# there are many predefined themes, including: \n# theme_bw(), theme_classic(), theme_light()\nggplot(data = pwords, aes(x = type, fill = type)) +\n  geom_bar()+\n  labs(title = \"Password type frequencies\", x = \"password type\") + \n  guides(fill=\"none\") + \n  scale_x_discrete(labels = abbreviate) +\n  theme_dark() +\n  theme(axis.text.x = element_text(angle = 90))\n```\n5. Other shapes, x and y:\n```{r}\n#| code-fold: true\nggplot(data = pwords, aes(x = type, y = strength)) +\n  geom_boxplot()\n\nggplot(data = pwords, aes(x = type, y = strength)) +\n  geom_violin()\n\nggplot(data = pwords, aes(x = strength, y = cracked)) + \n  geom_point()\n```\n```{r}\n#| label: fig-ahorstggplt\n#| echo: false\n#| fig-cap: \"Artwork by \\\\@allison_horst\"\nknitr::include_graphics(\"images/ahorst/ggplot2_masterpiece.png\")\n```\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Visualising Distributions\n\n## Boxplots {-}  \n\nBoxplots provide a useful way of visualising the __interquartile range (IQR).__ You can see what each part of the boxplot represents in Figure @fig-boxplotdesc.  \n\n```{r}\n#| label: fig-boxplotdesc\n#| echo: false\n#| fig-cap: \"Anatomy of a boxplot\"\n#| out-width: '100%'\n\nset.seed(34875)\npx = rnorm(100,10,10)\nmn=min(px[px>quantile(px, .25)-(1.5*IQR(px))])\nmx=max(px[px<quantile(px, .75)+(1.5*IQR(px))])\nouts_up = px[px>quantile(px, .75)+(1.5*IQR(px))]\nouts_lw = px[px<quantile(px, .25)-(1.5*IQR(px))]\nggplot(data = NULL, aes(x = px)) +\n  geom_boxplot(col=\"red\")+\n  ylim(-1,1)+xlim(-30,45)+\n  theme_classic()+\n  theme(axis.title = element_blank(), axis.text = element_blank(),\n      axis.ticks = element_blank(), axis.line = element_blank())+\n  \n  # Quartiles\n  annotate(\"text\",x=median(px), y=0.7, label=\"Q2\\n(Median)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  annotate(\"text\",x=quantile(px, .25), y=0.6, label=\"Q1\", vjust=1,hjust=1, col=\"grey70\")+\n  annotate(\"text\",x=quantile(px, .75), y=0.6, label=\"Q3\", vjust=1,hjust=0, col=\"grey70\")+\n  geom_segment(aes(x=median(px), xend=median(px), y=0.48, yend=0.38), col=\"grey70\",lty=\"dashed\")+\n  geom_segment(aes(x=quantile(px, .25), xend=quantile(px, .25), y=0.48, yend=0.38), col=\"grey70\",lty=\"dashed\")+\n  geom_segment(aes(x=quantile(px, .75), xend=quantile(px, .75), y=0.48, yend=0.38), col=\"grey70\",lty=\"dashed\")+\n  \n  # Whiskers\n  annotate(\"text\",x=mn, y=-0.2, label=\"Minimum value in data\\nwhich is > Q1 - (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  annotate(\"text\",x=mx, y=-0.2, label=\"Maximum value in data\\nwhich is < Q3 + (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  geom_segment(aes(x=mn, xend=mn, y=0, yend=-0.2),col=\"grey70\",lty=\"dashed\")+\n  geom_segment(aes(x=mx, xend=mx, y=0, yend=-0.2),col=\"grey70\",lty=\"dashed\")+\n  \n  #IQR\n  geom_segment(aes(x=quantile(px, .25), xend=quantile(px, .25), y=0.78, yend=0.85), col=\"grey70\")+\n  geom_segment(aes(x=quantile(px, .75), xend=quantile(px, .75), y=0.78, yend=0.85), col=\"grey70\")+\n  geom_segment(aes(x=quantile(px, .75), xend=quantile(px, .25), y=0.85, yend=0.85), col=\"grey70\")+\n  annotate(\"label\",x=median(px), y=0.9, label=\"IQR\", vjust=1,hjust=0.5, col=\"grey70\")+\n  \n  # Outliers +\n  geom_segment(aes(x=outs_up, xend=mean(outs_up), y=0, yend=0.27), col=\"grey70\", lty=\"dashed\")+\n  annotate(\"text\",x=mean(outs_up), y=0.5, label=\"Outliers\\nDatapoints > Q3 + (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  # Outliers -\n  geom_segment(aes(x=outs_lw, xend=mean(outs_lw), y=0, yend=0.27), col=\"grey70\", lty=\"dashed\")+\n  annotate(\"text\",x=mean(outs_lw), y=0.5, label=\"Outliers\\nDatapoints < Q1 - (1.5 * IQR)\", vjust=1,hjust=0.5, col=\"grey70\")+\n  \n  NULL\n  \nrm(mn,mx,outs_lw,outs_up,px)\n```\n\nWe can create a boxplot of our age variable using the following code:\n```{r}\n#| fig-height: 3\n# Notice, we put strength on the x axis, making the box plot vertical. \n# If we had set aes(y = strength) instead, then it would simply be rotated 90 degrees \nggplot(data = pwords, aes(x = strength)) +\n  geom_boxplot()\n```\n\n\n## Histograms {-}\n\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these map to how visualisations of numeric variables look.  \n\nWe can visualise numeric data using a __histogram__, which shows the frequency of values which fall within _bins_ of an equal width. \n\nTo do this, we're going to use some new data, on 120 participants' IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests. The data are available at [https://uoepsy.github.io/data/wechsler.csv](https://uoepsy.github.io/data/wechsler.csv)  \n```{r}\nwechsler <- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n```\n\n```{r}\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n```\nWe can specifiy the width of the bins:\n```{r}\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n```\n\nLet's take a look at the means and standard deviations of participants' scores on the other tests (the _test1_ and _test2_ variables).  \nNote how nicely we can do this with our newfound tidyverse skills!  \n```{r}\nwechsler |> \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n```\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1.\n```{r}\n#| echo: false\nggplot(data = wechsler, aes(x = test1)) + \n  geom_histogram()+\n  xlim(0,100) +\nggplot(data = wechsler, aes(x = test2)) + \n  geom_histogram()+\n  xlim(0,100)\n```\n\n\n:::statbox\n__Defining moments__  \n\nThe \"moments\" of a distribution are the metrics that relate to the shape of that distribution. \nWe've already seen the primary two moments that define the shapes of these distributions: the mean and the variance. The mean moves the distribution right or left, and the variance makes the distribution wider or narrower.  \n  \nThere are two more, \"skewness\" and \"kurtosis\" which tend to be of less focus of investigation (the questions we ask tend to be mainly concerned with means and variances). Skewness is a measure of _asymmetry_ in a distribution. Distributions can be _positively skewed_ or _negatively skewed_, and this influences our measures of central tendency and of spread to different degrees. The kurtosis is a measure of how \"pointy\" vs \"rounded\" the shape of a distribution is.  \n\n\n```{r}\n#| echo: false\ntibble(\n  x = seq(0, 200, 1),\n  y = dnorm(x, 100, 15),\n  y2 = dnorm(x,150, 15),\n  y3 = dnorm(x,50,15)\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"Mean = 100\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"Mean = 150\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"Mean = 50\"), alpha=0.2)+\n  theme_classic()+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  guides(fill=FALSE)+\n  labs(title=\"The mean defines the location of a distribution\") -> plt_loc\n\ntibble(\n  x = seq(0, 200, 1),\n  y = dnorm(x, 100,10),\n  y2 = dnorm(x,100, 15),\n  y3 = dnorm(x,100,5)\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"Mean = 100\"), alpha=0.3)+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"Mean = 150\"), alpha=0.3)+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"Mean = 50\"), alpha=0.3)+\n  theme_classic()+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  guides(fill=FALSE)+\n  xlim(50,150)+\n  labs(title=\"The variance defines the scale of a distribution\") -> plt_scale\n\nlibrary(sn)\ntibble(\n  x = seq(0, 200, 1),\n  y = dnorm(x, 100, 15),\n  y2 = dsn(x, xi = 180, omega=27.5, alpha = -5),\n  y3 = dsn(x, xi = 20, omega=27.5,alpha = 5),\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"Symmetric\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"Negative Skew\"), alpha=0.2)+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"Positive Skew\"), alpha=0.2)+\n  theme_classic()+\n  scale_fill_manual(\"\",breaks=c(\"Positive Skew\",\"Symmetric\",\"Negative Skew\"), values=c(\"chartreuse3\",\"blue\",\"red\"))+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  \n  geom_vline(aes(xintercept=100), lty=\"dashed\", col=\"blue\")+\n  annotate(\"label\",x=100, y=0.022, label=\"Mode\", vjust=1,hjust=0.5, col=\"blue\")+\n  annotate(\"label\",x=100, y=0.0275, label=\"Median\", vjust=1,hjust=0.5, col=\"blue\")+\n  annotate(\"label\",x=100, y=0.033, label=\"Mean\", vjust=1,hjust=0.5, col=\"blue\")+\n  \n  geom_vline(xintercept=c(155,165,170), lty=\"dashed\", col=\"red\")+\n  annotate(\"label\",x=175, y=0.022, label=\"Mode\", vjust=1,hjust=0.5, col=\"red\")+\n  annotate(\"label\",x=165, y=0.0275, label=\"Median\", vjust=1,hjust=0.5, col=\"red\")+\n  annotate(\"label\",x=155, y=0.033, label=\"Mean\", vjust=1,hjust=0.5, col=\"red\")+\n  \n  geom_vline(xintercept=c(45,35,30), lty=\"dashed\", col=\"chartreuse3\")+\n  annotate(\"label\",x=25, y=0.022, label=\"Mode\", vjust=1,hjust=0.5, col=\"chartreuse3\")+\n  annotate(\"label\",x=35, y=0.0275, label=\"Median\", vjust=1,hjust=0.5, col=\"chartreuse3\")+\n  annotate(\"label\",x=45, y=0.033, label=\"Mean\", vjust=1,hjust=0.5, col=\"chartreuse3\")+\n  labs(title=\"Skewness defines the asymmetry of a distribution\") + \n  NULL -> plt_skew\n\nlibrary(PearsonDS)\ntibble(\n  x = seq(0, 200, .1),\n  y = dpearson(x, moments = c(mean=100,variance=15,skewness=0,kurtosis=3)),\n  y2 = dpearson(x, moments = c(mean=100,variance=15,skewness=0,kurtosis=7)),\n  y3 = dpearson(x,moments = c(mean=100,variance=15,skewness=0,kurtosis=2.3))\n) %>%\n  ggplot(.,aes(x=x))+\n  geom_ribbon(aes(ymin=0, ymax=y, fill=\"100\"), alpha=0.2)+\n  geom_line(aes(y=y,col=\"100\"))+\n  geom_ribbon(aes(ymin=0, ymax=y2, fill=\"150\"), alpha=0.2)+\n  geom_line(aes(y=y2,col=\"150\"))+\n  geom_ribbon(aes(ymin=0, ymax=y3, fill=\"50\"), alpha=0.2)+\n  geom_line(aes(y=y3,col=\"50\"))+\n  theme_classic()+\n  scale_y_continuous(NULL, breaks=NULL,)+\n  theme(axis.line.y = element_blank(), axis.text.x = element_blank(), \n        axis.title.x = element_blank(), legend.position=\"bottom\")+\n  guides(fill='none',col='none')+\n  xlim(80,120)+\n  labs(title=\"The kurtosis defines the 'pointy-ness' of a distribution\") -> plt_kurt\n\nplt_loc / plt_scale\nplt_skew / plt_kurt\n```\n\n:::\n\n\n## Density\n\nIn addition to grouping numeric data into _bins_ in order to produce a histogram, we can also visualise a __density curve.__  \n\nBecause there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, ...), we could group the data into infinitely many bins. This is essentially what we are doing with a density curve.  \nYou can think of \"density\" as a bit similar to the notion of \"relative frequency\" (or \"proportion\"), in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.  \n\n```{r}\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()\n```\n\n\n\n:::statbox\n__Area under the curve__  \n\nThink about the barplots we have been looking at in the exercises where we simulate dice rolling :  \n```{r}\n# our function to simulate the roll of a die/some dice\ndice <- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n# simulate 1000 rolls of a single die\nroll1000 <- replicate(1000, dice(1))\n# tabulate and plot:\ntable(roll1000) |>\n  barplot(ylab=\"count\")\n```\nTo think about questions like \"what proportion of 1000 rolls does the die land on 6?\", we are simply interested in the count of 6s divided by the count of all rolls:  \n```{r}\ntab1000 <- table(roll1000)\ntab1000\ntab1000[6] / sum(tab1000)\n```\nSo Another way of thinking of this is that we are just dividing the count in each category by the total number. Or, \nPut another way, imagine we divide the area of each bar by the total area. The area now sums to 1, and our question is asking about the ratio of the red area to the total area (grey + red): \n```{r}\n#| echo: false\ntable(roll1000) |> \n  # convert to proportions\n  prop.table() |>\n  barplot(col=c(rep(\"#bbbbbb\",5),\"#ff0000\"),ylab=\"count/total\")\n```\n\nNothing really changes with a density curve! If we want to ask what proportion of our distribution of IQ scores is >120, then we are asking about the area under the curve that is to the right of 120:  \n\n```{r}\n#| echo: false\np <- ggplot(wechsler,aes(x=iq))+\n    geom_density(fill=\"grey\")\nd <- ggplot_build(p)$data[[1]]\np + geom_area(data = subset(d, x > 110), aes(x=x, y=y), fill=\"red\")\n```\n\nIt looks like about a third, maybe a little less. \nLet's calculate this proportion directly:  \n```{r}\nsum(wechsler$iq>110) / length(wechsler$iq)\n```\n\nIt might seem a little odd to think about area under the curve when we are asking about \"what _proportion_ of the data is ...?\". If we have the data, then we can just calculate the answer (like we did above). \nHowever, a lot of statistics is really concerned with the _probability_ of events. When we discuss probability, we move from talking about a specific set of observed data to thinking about a theoretical/mathematical model that defines the way in which data is generated. This where it becomes more useful to think about distributions in a more abstract sense.  \n\nFor instance, with a fair six-sided die, we have a probability distribution  (@fig-diceprob) in which each side is given the probability $\\frac{1}{6}$:\n$$\n\\begin{gather*}\nP(x) = \\begin{cases}\n  \\frac{1}{6} & \\text{if $x \\in \\{1,2,3,4,5,6\\}$}\\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n\\end{gather*}\n$$\nInstead of rolling a die, suppose that we are picking a person off the street and measuring their IQ. Given that IQ scales are designed to have a mean of 100 and standard deviation of 15, what is the _probability_ that we pick a person with an IQ of greater than 110?  \n```{r}\n#| label: fig-diceprob\n#| fig-cap: \"Left: Discrete probability distribution of a fair six-sided die. Right: Continuous probability distribution of IQ scores\"\n#| echo: false\n#| fig-height: 3.5\ntibble(response = 1:6, prob = rep(1/6,6)) %>%\nggplot(., aes(x=response, y=prob))+\n  geom_point(size=3)+\n  geom_segment(aes(x=response, xend=response, y=0,yend=prob),lty=\"dotted\")+\n  scale_x_continuous(\"possible faces of a die\", breaks=1:6)+\n  scale_y_continuous(\"probability\", limits = c(0,1), breaks=map_dbl(1:6,~./6), labels=c(paste0(1:5,\"/6\"),\"1\"))+\n  theme_classic()+\n  theme(text = element_text(size = 20)) -> dp\n\n\ndf <- tibble(x=c(50,150))\ng <- df %>% ggplot(aes(x=x)) +\n  stat_function(fun=dnorm,args=list(mean=100,sd=15),size=1) +\n  xlab(\"possible IQ scores\") + ylab(\"density\")\nld <- layer_data(g) %>% filter(x>= 110)\ncp <- \n  g + geom_area(data=ld,aes(x=x,y=y),fill=\"red\")+theme_classic()+\n  theme(text = element_text(size = 20))\n\nlibrary(patchwork)\ndp + cp\n\n```\n\n:::\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"02a_measurement.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"code-annotations":"hover","link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"2A: Measurement & Distributions","params":{"SHOW_SOLS":true,"TOGGLE":true}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}