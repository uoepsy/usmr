{"title":"8A: Centering and Scaling","markdown":{"yaml":{"title":"8A: Centering and Scaling","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Centering/Scaling/Standardising","containsRefs":false,"markdown":"\n\n\n```{r}\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\n```\n\n\n```{r}\n#| include: false\nset.seed(12345)\ndf = tibble(\n  name = randomNames::randomNames(70,which.names=\"first\"),\n  age = round(runif(70,18,80)),\n  height = round(rnorm(70,168,12),1),\n  shoe_size = round(rnorm(70,41,3)),\n  hrs_sleep = round(age*-.1 + rnorm(70,14,2),1),\n  ampm = sample(c(\"am\",\"pm\"),70,replace=TRUE,prob=c(.3,.7)),\n  smoke = sample(c(\"y\",\"n\",\"v\"),70,replace=TRUE, prob=c(.1,.7,.2)),\n  HR = round(87 + age*.13 + hrs_sleep*-3 + (smoke==\"n\")*-4 + rnorm(70,0,6))\n) |> as.data.frame()\ndf[3,c(1,2,3,4,7)] <- c(\"Martin\",NA,182,43,\"n\")\ndf[,c(2:5,8)] <- apply(df[,c(2:5,8)], 2, as.numeric)\n\nsummary(df)\nhrdat <- as_tibble(df)\n#write_csv(df, file=\"../../data/usmr_hrsleep.csv\")\n```\n\nFor this section we're going to play with some random data looking at whether peoples' resting heart rates depend on how much sleep they get.   \n\nOur data contains `r nrow(df)` people, for which we have a variety of measures. The only ones we are going to concern ourselves with are heart rate (`HR`) and hours slept (`hrs_sleep`), but there are plenty of other ones for you to play around with if you like  \n\n```{r}\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\nhead(hrdat)\n```\n\n\n\nThere are many transformations we can do to a continuous variable, but far and away the most common ones are _centering_ and _scaling._ \n\n:::sticky\n__Centering__  \n\nCentering simply means moving the entire distribution to be centered on some new value. We achieve this by subtracting our desired center from each value of a variable. \n\n__Mean-centering__  \n\nA common option is to _mean center_ (i.e. to subtract the mean from each value). This makes our new values all relative to the mean.  \n\n:::\n\n\nIn our heart-rates data, the average hours of sleep is `r round(mean(hrdat$hrs_sleep),1)`. If we subtract that from each person's hours of sleep, we get the mean-centered hours slept (the `hrs_sleepC` column below). You can see that the first person in our data (Biancha) sleeps 14 hours a night, which is (according to the `hrs_sleepC` variable) 5.2 hours more than average.  \n\n```{r}\nhrdat |>\n  mutate(\n    hrs_sleepC = hrs_sleep - mean(hrs_sleep), \n      .after = hrs_sleep # put new column after the hrs_sleep variable\n  ) |> head(5L) # just show the first 5\n```\n\nWe can center a variable on other things, such as the minimum or maximum value of the scale we are using, or some judiciously chosen value of interest.  \n\n\n:::sticky\n__Scaling__\n\nScaling changes the _units_ of the variable, and we do this by dividing the observations by some value. E.g., moving from \"36 months\" to \"3 years\" involves multiplying (scaling) the value by 1/12.  \n\n__Standardising__  \n\nFar and away the most common transformation that involves scaling is called 'standardisation'. This involves subtracting the mean and then dividing by the standard deviation. So standardisation centers on the sample mean **and** scales by the sample standard deviation.   \n\n$$\n\\text{standardised }x_i = \\frac{x_i - \\bar x}{s_x}\n$$\n\n:::\n\nThe process of standardisation (subtracting the mean and dividing by the standard deviation) will make it so that all our values are expressed in terms of \"how many standard deviations above/below the mean\". This can be useful because it puts variables on the same conceptual scale (units of standard deviation).   \n\n\n::: {.callout-note collapse=\"true\"}\n#### Martin's height\n\n:::: {.columns}\n::: {.column width=\"70%\"}\nConsider Martin. He goes on a lot about people's heights. He is 182cm tall, and he has size 43 feet (in EU sizes). Is Martin's height more unusual than the size of his feet? If we standardise both variables, we can see that he is 1.2 standard deviations above average height, but only .56 standard deviations above average in shoe size.  \n\n```{r}\n#| eval: false\nhrdat |> \n  mutate(\n    Zheight = (height-mean(height))/sd(height), \n    Zshoe = (shoe_size-mean(shoe_size))/sd(shoe_size), \n      .after = shoe_size\n  ) |> head(5L) # just show the first 5\n```\n:::\n::: {.column width=\"30%\"}\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/playmo_mc.jpg\")\n```\n:::\n::::\n```{r}\n#| echo: false\nhrdat |> \n  mutate(\n    Zheight = (height-mean(height))/sd(height), \n    Zshoe = (shoe_size-mean(shoe_size))/sd(shoe_size), \n      .after = shoe_size\n  ) |> head(5L) # just show the first 5\n```\n\n\n\n\n\n:::\n\n\n\n\n:::rtip\n__handy functions__  \n\nWe can easily center and scale in R by just performing those calculations using something like `(var - mean(var)) / sd(var)`, but there is a handy function that can do it quickly for us:  \n\n```{r}\n#| eval: false\nhrdat |> mutate(\n  hrs_sleepC = scale(hrs_sleep, scale = FALSE), # mean centered\n  hrs_sleepZ = scale(hrs_sleep), # standardised\n)\n```\n\nWe can actually use these _inside_ the call to the `lm()` function, e.g.  \n```{r}\n#| eval: false\nlm(HR ~ scale(hrs_sleep), data = hrdat)\n```\n\n\n:::\n\n\n# Centering and Scaling Predictors\n\nWe know that we can transform our variables in lots of ways, but how does this choice affect the models we are fitting? In short, it doesn't affect our model, but it _does_ change what we get out of it.  \n\nIf we re-center a predictor on some new value (such as the mean), then all this does is change what \"zero\" means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is \"when all predictors are zero\". And we are changing what \"zero\" represents!  \n\nWhen we scale a predictor, this will change the slope. Why? Because it changes what \"moving 1\" represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains _exactly the same_, we are only changing the *units* that we are using to expressing that slope.\n\nFor the remainder of this reading, we're going to start with the model `lm(HR ~ hrs_sleep)`, and then explore how different transformations to our variables change what we get out of the model. Because we are applying transformations to individual variables, all of the logic we're about to see holds in multiple regression models too (i.e. it doesn't matter how many independent predictors we have, all of the below stays the same).  \n\nIn @fig-scalexlm you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don't affect the model itself - the large blue point shows how we are changing where our intercept is estimated, but the slope of the line (and our uncertainty in slope) is the same in all four plots. These models are also shown below, along with a comparison to show that they are all identical in terms of model fit.   \n\n```{r}\n#| label: fig-scalexlm\n#| fig-cap: \"Centering and scaling predictors in linear regression models\"\n#| out-width: \"100%\"\n#| echo: false\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(title=\"Original\", x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~scale(x,scale=F),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=map_dbl(seq(7,-7), ~mean(df$x)-.),\n                     labels=seq(-7,7))+\n  labs(title=\"Mean Centered\", x=\"Hours Slept\\n(mean centered)\",y=\"HR\")\n  \n  \nmod = lm(y~scale(x),df)\np3 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),\n                     breaks=c(mean(df$x)-(2*sd(df$x)), mean(df$x)-sd(df$x), \n                              mean(df$x), \n                              mean(df$x)+sd(df$x), mean(df$x)+(2*sd(df$x))),\n                     labels=c(-2,-1,0,1,2))+\n  labs(title=\"Standardised X\", x=\"Hours Slept\\n(standardised)\",y=\"HR\")\n\nmod = lm(y~x,df %>% mutate(x=x-8))\np4 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=8,xend=8,y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=8,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14, labels=c(0:14)-8)+\n  labs(title=\"Centered on 8 hours\", x=\"Hours Slept\\n(relative to 8 hours)\",y=\"HR\")\np1 + p2 + p3 + p4 \n```\n\n::::panelset\n:::panel\n#### Original\n\n```{r}\n#| eval: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\nsummary(mod_orig)\n```\n```{r}\n#| echo: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n.pp(summary(mod_orig),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who didn't sleep at all\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Mean centered X\n\n```{r}\n#| eval: false\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\nsummary(mod_mc)\n```\n```{r}\n#| echo: false\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n.pp(summary(mod_mc),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who **slept an average number of hours**\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Standardised X\n\n```{r}\n#| eval: false\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\nsummary(mod_z)\n```\n```{r}\n#| echo: false\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\n.pp(summary(mod_z),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who **slept an average number of hours**\n- `hrs_sleep`: estimated change in HR for every 1 additional **standard deviation of sleep time**\n\n:::\n:::panel\n#### X Centered on 8\n\nThe function `I()` can be used to isolate a computation. The `+` and `-` symbols in linear models _mean_ something (they are how we add predictors), so we can use `I()` here to just tell R to do the computation of `hrs_sleep-8` on its own. \n\n```{r}\n#| eval: false\nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat)\nsummary(mod_8)\n```\n```{r}\n#| echo: false\nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat)\n.pp(summary(mod_8),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who **slept 8 hours**\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Comparison\n\nNormally, if models have different sets of predictors which are not nested (one model containing all of the predictors of the other model), then we can't compare them with an F test.  \nHowever, in this case, the `hrs_sleep` variable is in every model (just transformed in some way), so we can do a comparison.  \n\nNote that the residual sums of squares for these models is identical - no model is explaining more variance than the other, because underlyingly they are all just the same model!  \n\n```{r}\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat)\n\nanova(mod_orig, mod_mc, mod_z, mod_8)\n```\n\n:::\n::::\n\n## Transformations, pre/post  \n\nWhen we apply these transformations we can do so either _during_ the process of fitting the model (e.g. by using `scale()` inside the `lm()` function as we have just seen). We can also do this _prior_ to fitting the model, by creating a new variable using code like `hrs_sleepZ = scale(hrs_sleep)`, and then using that variable in the model. \n\nIt is also possible to work out the slope for transformed variables _after_ we've just fitted our original model. This is because when we scale a predictor, all that happens to our coefficient is that it gets scaled accordingly. Consider the example in @fig-hrsmin, where we change from using _hours_ to _minutes_. To do this we can just multiply our `hrs_sleep` variable by 60 (so that, e.g., 1.5 hours becomes 90 minutes).  \n\nThe coefficient from the model changes from  \n\n\"change in HR for every 1 hour of sleep\"  \nto  \n\"change in HR for every 1 minute of sleep\"  \n\nbut we can actually do that calculation ourselves, because 1 minute is 1/60th of an hour. So we already know that $\\frac{\\text{change in HR for every 1 hour of sleep}}{60}$ is the same thing as \"change in HR for every 1 minute of sleep\".  \n\n```{r}\n#| echo: false\n#| label: fig-hrsmin\n#| fig-cap: \"Changing from hours_slept to minutes_slept will make our coefficient 1/60th of the size\"  \ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(subtitle=paste0(\"hrs slept coefficient: \",round(coef(mod)[2],2)), x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~I(x*60),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14,labels=map_dbl(0:14,~.*60))+\n  labs(subtitle=paste0(\"mins slept coefficient: \",round(coef(mod)[2],2)),x=\"Minutes Slept\",y=\"HR\")+\n  theme(axis.text.x = element_text(angle=60,hjust=1))\n\np1 / p2\n```\n\nThe same applies when we standardise a predictor. Since standardising involves dividing a variable by its standard deviation, the coefficient for the standardised variable will be the original coefficient multiplied by the variable's standard deviation.  \n\n:::: {.columns}\n::: {.column width=\"45%\"}\nOriginal coefficients  \n`lm(HR ~ hrs_sleep, data = hrdat)`\n```{r}\ncoef(mod_orig)\n```\n\n:::\n::: {.column width=\"2.5%\"}\n:::\n::: {.column width=\"45%\"}\nStandardised coefficients  \n`lm(HR ~ scale(hrs_sleep), data = hrdat)`\n```{r}\ncoef(mod_z)\n```\n\n:::\n::::\n\nThe standard deviation of `hrs_sleep` we can calculate:  \n```{r}\nsd(hrdat$hrs_sleep) \n```\n\nAnd we can move from the original coefficient to the standardised one!  \n```{r}\n# original coefficient multiplied by sd of hrs sleep\ncoef(mod_orig)[2] * sd(hrdat$hrs_sleep)\n```\n\n# Scaling the outcome  \n\nWe've seen what happens when we scale our _predictors_ in a linear regression model, but what happens when we scale our _outcome_ variable? If we tie it to our plots, all that happens is that the numbers on the y-axis will change (see @fig-scaley).  \n\nThe model again is unchanged, but our coefficient will no longer be the \"estimated change in beats per minute\" but will be \"the estimated change in standard deviations of heart rates\".   \n\nOur original coefficient was interpreted as heart rate decreasing by `r round(coef(mod_orig)[2],1)` beats per minute for every additional hour slept. By standardising the outcome (heart rates), then we just re-express that `r round(coef(mod_orig)[2],1)` in terms of \"how many standard deviations?\". \nThe standard deviation of heart rates in our data is `r round(sd(hrdat$HR),1)`, so the slope in terms of standard deviations should be $\\frac{-3.5}{11} = -0.32$. The interpretation of this number is simply that heart rates decrease by 0.32 **standard deviations** for every additional hour slept.  \n\n```{r}\n#| echo: false\n#| label: fig-scaley\n#| fig-cap: \"Scaling the outcome variable\"\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\ndf = hrdat |> mutate(x=hrs_sleep,y=scale(HR))\nmod = lm(y~x,df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=-100,yend=100)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)+\n  labs(x=\"Hours Slept\",y=\"standardised HR\")\n\np1 / p2\n```\n\n::::panelset\n:::panel\n#### Original\n\n```{r}\n#| eval: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\nsummary(mod_orig)\n```\n```{r}\n#| echo: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n.pp(summary(mod_orig),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who didn't sleep at all\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Standardised Y\n\n```{r}\n#| eval: false\nmod_yz <- lm(scale(HR) ~ hrs_sleep, data = hrdat)\nsummary(mod_yz)\n```\n```{r}\n#| echo: false\nmod_yz <- lm(scale(HR) ~ hrs_sleep, data = hrdat)\n.pp(summary(mod_yz),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated **number of standard deviations above average heart rate (HR)** for someone who didn't sleep at all\n- `hrs_sleep`: estimated change in **standard deviations of** HR for every 1 additional hour of sleep.\n\n:::\n::::\n\n\n# Standardised Coefficients\n\nWhen we standardise variables in a regression model, it means we can talk about all our coefficients in terms of \"standard deviation units\". To the extent that it is possible to do so, this puts our coefficients on scales of the similar magnitude, making qualititative comparisons between the sizes of effects a *little* more easy.  \n\nWe tend to refer to coefficients using standardised variables as (unsurprisingly), \"standardised coefficients\", and we denote them with a $\\beta$^[some people use $\\beta$ for a normal coefficient, and $\\beta^*$ for a standardised coefficient].  \n\nThere are two main ways that people construct standardised coefficients. One of which standardises just the predictor, and the other of which standardises both predictor and outcome.  \n\n| predictor    | outcome      | in lm                     | coefficient                    | interpretation                               |\n| ------------ | ------------ | ------------------------- | --------------------------------- | -------------------------------------------- |\n| standardised | raw          | `y ~ scale(x)`        | $\\beta = b \\cdot s_x$             | \"change in Y for a 1 SD increase in X\"       |\n| standardised | standardised | `scale(y) ~ scale(x)` | $\\beta = b \\cdot \\frac{s_x}{s_y}$ | \"change in SD of Y for a 1 SD increase in X\" |\n\n\n\n## apples and oranges\n\nVery often, people will consider a standardised coefficient to be a unitless measure of \"size of effect\" that they can go and happily compare with other results (be it from another coefficient from the model, or a coefficient from a different study altogether, perhaps even on a different population!).  \n\nHowever, a **very** important thing to remember is that standardised coefficients are *dependent upon the sample standard deviations*. \n\nThis means that any comarisons between standardised coefficients comparisons could be due to an actual difference in magnitude of the underlying relationship, but it could just as easily be due to differences in the standard deviations of the variables. \n\nAs a toy example, I have two datasets, each of 100 people. The first has people aged 18 to 40. The second has people aged 18 to 80. Both datasets have been taken from a population where the underlying linear relationship between age and vocabulary is $vocab_i = 10 + 1 \\cdot age_i$. So the \"association between age and vocabulary\" should be more or less the same for both datasets (as seen in @fig-2hyp).    \n\n```{r}\n#| echo: false\n#| label: fig-2hyp\n#| fig-cap: \"Two hypothetical studies, with different age ranges of participants\"\n\nset.seed(56)\ndfpop <- tibble(\n  age = round(runif(1e5,18,80)),\n  vocabulary = 10 + age*1 + rnorm(1e5,0,3)\n)\ndf1 <- slice_sample(dfpop[dfpop$age<40,], n=100)\ndf2 <- slice_sample(dfpop, n=100)\n\n\nbind_rows(\n  df1 |> mutate(study=\"study 1\"),\n  df2 |> mutate(study=\"study 2\")\n) |>\nggplot(aes(x=age,y=vocabulary,col=study))+\n  geom_point(alpha=.3,size=3)+\n  geom_smooth(method=lm) +\n  xlim(18,80)+ylim(0,100)\n\n```\n\nBut the standardised coefficients for the second dataset will always be bigger, because the variance in the `age` variable is bigger.  \n\nThis is plainly clear from when we remember that the standardised coefficient is simply the original raw `age` coefficient multiplied by the standard deviation of `age` (either $b \\cdot s_{age}$ or $b \\cdot \\frac{s_{age}}{s_{vocab}}$). In the 2nd study, $s_{age}$ is bigger, but (as clearly evident in the plot above) we don't want to be saying that age has a bigger effect on vocabulary in study 2.. \n\n```{r}\n#| echo: false\nm1_o = lm(vocabulary ~ age, data = df1)\nm1_zx = lm(vocabulary ~ scale(age), data = df1)\nm1_zxy = lm(scale(vocabulary) ~ scale(age), data = df1)\n\nm2_o = lm(vocabulary ~ age, data = df2)\nm2_zx = lm(vocabulary ~ scale(age), data = df2)\nm2_zxy = lm(scale(vocabulary) ~ scale(age), data = df2)\n\ntribble(\n  ~\"estimate\",~\"study 1, ages 18-40\",~\"study 2, ages 18-80\",\n  \"sd(age)\",sd(df1$age), sd(df2$age),\n  \"vocab ~ age\",coef(m1_o)[2],coef(m2_o)[2],\n  \"vocab ~ scale(age)\",coef(m1_zx)[2],coef(m2_zx)[2],\n  \"scale(vocab) ~ scale(age)\",coef(m1_zxy)[2],coef(m2_zxy)[2],\n) |> mutate(across(2:3,~round(.,2))) |> gt::gt()\n\n```\n\nThe take-home of this is that sometimes we just can't compare apples and oranges. And sometimes we can't even compare apples to other apples! Choosing whether or not to standardise coefficients is going to depend on many things, and sometimes the easiest thing is simply to report both raw and standardised coefficients.  \n\n","srcMarkdownNoYaml":"\n\n\n```{r}\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\n```\n\n\n```{r}\n#| include: false\nset.seed(12345)\ndf = tibble(\n  name = randomNames::randomNames(70,which.names=\"first\"),\n  age = round(runif(70,18,80)),\n  height = round(rnorm(70,168,12),1),\n  shoe_size = round(rnorm(70,41,3)),\n  hrs_sleep = round(age*-.1 + rnorm(70,14,2),1),\n  ampm = sample(c(\"am\",\"pm\"),70,replace=TRUE,prob=c(.3,.7)),\n  smoke = sample(c(\"y\",\"n\",\"v\"),70,replace=TRUE, prob=c(.1,.7,.2)),\n  HR = round(87 + age*.13 + hrs_sleep*-3 + (smoke==\"n\")*-4 + rnorm(70,0,6))\n) |> as.data.frame()\ndf[3,c(1,2,3,4,7)] <- c(\"Martin\",NA,182,43,\"n\")\ndf[,c(2:5,8)] <- apply(df[,c(2:5,8)], 2, as.numeric)\n\nsummary(df)\nhrdat <- as_tibble(df)\n#write_csv(df, file=\"../../data/usmr_hrsleep.csv\")\n```\n\nFor this section we're going to play with some random data looking at whether peoples' resting heart rates depend on how much sleep they get.   \n\nOur data contains `r nrow(df)` people, for which we have a variety of measures. The only ones we are going to concern ourselves with are heart rate (`HR`) and hours slept (`hrs_sleep`), but there are plenty of other ones for you to play around with if you like  \n\n```{r}\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\nhead(hrdat)\n```\n\n\n# Centering/Scaling/Standardising\n\nThere are many transformations we can do to a continuous variable, but far and away the most common ones are _centering_ and _scaling._ \n\n:::sticky\n__Centering__  \n\nCentering simply means moving the entire distribution to be centered on some new value. We achieve this by subtracting our desired center from each value of a variable. \n\n__Mean-centering__  \n\nA common option is to _mean center_ (i.e. to subtract the mean from each value). This makes our new values all relative to the mean.  \n\n:::\n\n\nIn our heart-rates data, the average hours of sleep is `r round(mean(hrdat$hrs_sleep),1)`. If we subtract that from each person's hours of sleep, we get the mean-centered hours slept (the `hrs_sleepC` column below). You can see that the first person in our data (Biancha) sleeps 14 hours a night, which is (according to the `hrs_sleepC` variable) 5.2 hours more than average.  \n\n```{r}\nhrdat |>\n  mutate(\n    hrs_sleepC = hrs_sleep - mean(hrs_sleep), \n      .after = hrs_sleep # put new column after the hrs_sleep variable\n  ) |> head(5L) # just show the first 5\n```\n\nWe can center a variable on other things, such as the minimum or maximum value of the scale we are using, or some judiciously chosen value of interest.  \n\n\n:::sticky\n__Scaling__\n\nScaling changes the _units_ of the variable, and we do this by dividing the observations by some value. E.g., moving from \"36 months\" to \"3 years\" involves multiplying (scaling) the value by 1/12.  \n\n__Standardising__  \n\nFar and away the most common transformation that involves scaling is called 'standardisation'. This involves subtracting the mean and then dividing by the standard deviation. So standardisation centers on the sample mean **and** scales by the sample standard deviation.   \n\n$$\n\\text{standardised }x_i = \\frac{x_i - \\bar x}{s_x}\n$$\n\n:::\n\nThe process of standardisation (subtracting the mean and dividing by the standard deviation) will make it so that all our values are expressed in terms of \"how many standard deviations above/below the mean\". This can be useful because it puts variables on the same conceptual scale (units of standard deviation).   \n\n\n::: {.callout-note collapse=\"true\"}\n#### Martin's height\n\n:::: {.columns}\n::: {.column width=\"70%\"}\nConsider Martin. He goes on a lot about people's heights. He is 182cm tall, and he has size 43 feet (in EU sizes). Is Martin's height more unusual than the size of his feet? If we standardise both variables, we can see that he is 1.2 standard deviations above average height, but only .56 standard deviations above average in shoe size.  \n\n```{r}\n#| eval: false\nhrdat |> \n  mutate(\n    Zheight = (height-mean(height))/sd(height), \n    Zshoe = (shoe_size-mean(shoe_size))/sd(shoe_size), \n      .after = shoe_size\n  ) |> head(5L) # just show the first 5\n```\n:::\n::: {.column width=\"30%\"}\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/playmo_mc.jpg\")\n```\n:::\n::::\n```{r}\n#| echo: false\nhrdat |> \n  mutate(\n    Zheight = (height-mean(height))/sd(height), \n    Zshoe = (shoe_size-mean(shoe_size))/sd(shoe_size), \n      .after = shoe_size\n  ) |> head(5L) # just show the first 5\n```\n\n\n\n\n\n:::\n\n\n\n\n:::rtip\n__handy functions__  \n\nWe can easily center and scale in R by just performing those calculations using something like `(var - mean(var)) / sd(var)`, but there is a handy function that can do it quickly for us:  \n\n```{r}\n#| eval: false\nhrdat |> mutate(\n  hrs_sleepC = scale(hrs_sleep, scale = FALSE), # mean centered\n  hrs_sleepZ = scale(hrs_sleep), # standardised\n)\n```\n\nWe can actually use these _inside_ the call to the `lm()` function, e.g.  \n```{r}\n#| eval: false\nlm(HR ~ scale(hrs_sleep), data = hrdat)\n```\n\n\n:::\n\n\n# Centering and Scaling Predictors\n\nWe know that we can transform our variables in lots of ways, but how does this choice affect the models we are fitting? In short, it doesn't affect our model, but it _does_ change what we get out of it.  \n\nIf we re-center a predictor on some new value (such as the mean), then all this does is change what \"zero\" means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is \"when all predictors are zero\". And we are changing what \"zero\" represents!  \n\nWhen we scale a predictor, this will change the slope. Why? Because it changes what \"moving 1\" represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains _exactly the same_, we are only changing the *units* that we are using to expressing that slope.\n\nFor the remainder of this reading, we're going to start with the model `lm(HR ~ hrs_sleep)`, and then explore how different transformations to our variables change what we get out of the model. Because we are applying transformations to individual variables, all of the logic we're about to see holds in multiple regression models too (i.e. it doesn't matter how many independent predictors we have, all of the below stays the same).  \n\nIn @fig-scalexlm you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don't affect the model itself - the large blue point shows how we are changing where our intercept is estimated, but the slope of the line (and our uncertainty in slope) is the same in all four plots. These models are also shown below, along with a comparison to show that they are all identical in terms of model fit.   \n\n```{r}\n#| label: fig-scalexlm\n#| fig-cap: \"Centering and scaling predictors in linear regression models\"\n#| out-width: \"100%\"\n#| echo: false\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(title=\"Original\", x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~scale(x,scale=F),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=map_dbl(seq(7,-7), ~mean(df$x)-.),\n                     labels=seq(-7,7))+\n  labs(title=\"Mean Centered\", x=\"Hours Slept\\n(mean centered)\",y=\"HR\")\n  \n  \nmod = lm(y~scale(x),df)\np3 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),\n                     breaks=c(mean(df$x)-(2*sd(df$x)), mean(df$x)-sd(df$x), \n                              mean(df$x), \n                              mean(df$x)+sd(df$x), mean(df$x)+(2*sd(df$x))),\n                     labels=c(-2,-1,0,1,2))+\n  labs(title=\"Standardised X\", x=\"Hours Slept\\n(standardised)\",y=\"HR\")\n\nmod = lm(y~x,df %>% mutate(x=x-8))\np4 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=8,xend=8,y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=8,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14, labels=c(0:14)-8)+\n  labs(title=\"Centered on 8 hours\", x=\"Hours Slept\\n(relative to 8 hours)\",y=\"HR\")\np1 + p2 + p3 + p4 \n```\n\n::::panelset\n:::panel\n#### Original\n\n```{r}\n#| eval: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\nsummary(mod_orig)\n```\n```{r}\n#| echo: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n.pp(summary(mod_orig),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who didn't sleep at all\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Mean centered X\n\n```{r}\n#| eval: false\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\nsummary(mod_mc)\n```\n```{r}\n#| echo: false\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n.pp(summary(mod_mc),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who **slept an average number of hours**\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Standardised X\n\n```{r}\n#| eval: false\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\nsummary(mod_z)\n```\n```{r}\n#| echo: false\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\n.pp(summary(mod_z),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who **slept an average number of hours**\n- `hrs_sleep`: estimated change in HR for every 1 additional **standard deviation of sleep time**\n\n:::\n:::panel\n#### X Centered on 8\n\nThe function `I()` can be used to isolate a computation. The `+` and `-` symbols in linear models _mean_ something (they are how we add predictors), so we can use `I()` here to just tell R to do the computation of `hrs_sleep-8` on its own. \n\n```{r}\n#| eval: false\nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat)\nsummary(mod_8)\n```\n```{r}\n#| echo: false\nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat)\n.pp(summary(mod_8),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who **slept 8 hours**\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Comparison\n\nNormally, if models have different sets of predictors which are not nested (one model containing all of the predictors of the other model), then we can't compare them with an F test.  \nHowever, in this case, the `hrs_sleep` variable is in every model (just transformed in some way), so we can do a comparison.  \n\nNote that the residual sums of squares for these models is identical - no model is explaining more variance than the other, because underlyingly they are all just the same model!  \n\n```{r}\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat)\n\nanova(mod_orig, mod_mc, mod_z, mod_8)\n```\n\n:::\n::::\n\n## Transformations, pre/post  \n\nWhen we apply these transformations we can do so either _during_ the process of fitting the model (e.g. by using `scale()` inside the `lm()` function as we have just seen). We can also do this _prior_ to fitting the model, by creating a new variable using code like `hrs_sleepZ = scale(hrs_sleep)`, and then using that variable in the model. \n\nIt is also possible to work out the slope for transformed variables _after_ we've just fitted our original model. This is because when we scale a predictor, all that happens to our coefficient is that it gets scaled accordingly. Consider the example in @fig-hrsmin, where we change from using _hours_ to _minutes_. To do this we can just multiply our `hrs_sleep` variable by 60 (so that, e.g., 1.5 hours becomes 90 minutes).  \n\nThe coefficient from the model changes from  \n\n\"change in HR for every 1 hour of sleep\"  \nto  \n\"change in HR for every 1 minute of sleep\"  \n\nbut we can actually do that calculation ourselves, because 1 minute is 1/60th of an hour. So we already know that $\\frac{\\text{change in HR for every 1 hour of sleep}}{60}$ is the same thing as \"change in HR for every 1 minute of sleep\".  \n\n```{r}\n#| echo: false\n#| label: fig-hrsmin\n#| fig-cap: \"Changing from hours_slept to minutes_slept will make our coefficient 1/60th of the size\"  \ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(subtitle=paste0(\"hrs slept coefficient: \",round(coef(mod)[2],2)), x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~I(x*60),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14,labels=map_dbl(0:14,~.*60))+\n  labs(subtitle=paste0(\"mins slept coefficient: \",round(coef(mod)[2],2)),x=\"Minutes Slept\",y=\"HR\")+\n  theme(axis.text.x = element_text(angle=60,hjust=1))\n\np1 / p2\n```\n\nThe same applies when we standardise a predictor. Since standardising involves dividing a variable by its standard deviation, the coefficient for the standardised variable will be the original coefficient multiplied by the variable's standard deviation.  \n\n:::: {.columns}\n::: {.column width=\"45%\"}\nOriginal coefficients  \n`lm(HR ~ hrs_sleep, data = hrdat)`\n```{r}\ncoef(mod_orig)\n```\n\n:::\n::: {.column width=\"2.5%\"}\n:::\n::: {.column width=\"45%\"}\nStandardised coefficients  \n`lm(HR ~ scale(hrs_sleep), data = hrdat)`\n```{r}\ncoef(mod_z)\n```\n\n:::\n::::\n\nThe standard deviation of `hrs_sleep` we can calculate:  \n```{r}\nsd(hrdat$hrs_sleep) \n```\n\nAnd we can move from the original coefficient to the standardised one!  \n```{r}\n# original coefficient multiplied by sd of hrs sleep\ncoef(mod_orig)[2] * sd(hrdat$hrs_sleep)\n```\n\n# Scaling the outcome  \n\nWe've seen what happens when we scale our _predictors_ in a linear regression model, but what happens when we scale our _outcome_ variable? If we tie it to our plots, all that happens is that the numbers on the y-axis will change (see @fig-scaley).  \n\nThe model again is unchanged, but our coefficient will no longer be the \"estimated change in beats per minute\" but will be \"the estimated change in standard deviations of heart rates\".   \n\nOur original coefficient was interpreted as heart rate decreasing by `r round(coef(mod_orig)[2],1)` beats per minute for every additional hour slept. By standardising the outcome (heart rates), then we just re-express that `r round(coef(mod_orig)[2],1)` in terms of \"how many standard deviations?\". \nThe standard deviation of heart rates in our data is `r round(sd(hrdat$HR),1)`, so the slope in terms of standard deviations should be $\\frac{-3.5}{11} = -0.32$. The interpretation of this number is simply that heart rates decrease by 0.32 **standard deviations** for every additional hour slept.  \n\n```{r}\n#| echo: false\n#| label: fig-scaley\n#| fig-cap: \"Scaling the outcome variable\"\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\ndf = hrdat |> mutate(x=hrs_sleep,y=scale(HR))\nmod = lm(y~x,df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=-100,yend=100)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)+\n  labs(x=\"Hours Slept\",y=\"standardised HR\")\n\np1 / p2\n```\n\n::::panelset\n:::panel\n#### Original\n\n```{r}\n#| eval: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\nsummary(mod_orig)\n```\n```{r}\n#| echo: false\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n.pp(summary(mod_orig),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated heart rate (HR) for someone who didn't sleep at all\n- `hrs_sleep`: estimated change in HR for every 1 additional hour of sleep.\n\n:::\n:::panel\n#### Standardised Y\n\n```{r}\n#| eval: false\nmod_yz <- lm(scale(HR) ~ hrs_sleep, data = hrdat)\nsummary(mod_yz)\n```\n```{r}\n#| echo: false\nmod_yz <- lm(scale(HR) ~ hrs_sleep, data = hrdat)\n.pp(summary(mod_yz),l=list(3,9:12))\n```\n\n- `(Intercept)`: estimated **number of standard deviations above average heart rate (HR)** for someone who didn't sleep at all\n- `hrs_sleep`: estimated change in **standard deviations of** HR for every 1 additional hour of sleep.\n\n:::\n::::\n\n\n# Standardised Coefficients\n\nWhen we standardise variables in a regression model, it means we can talk about all our coefficients in terms of \"standard deviation units\". To the extent that it is possible to do so, this puts our coefficients on scales of the similar magnitude, making qualititative comparisons between the sizes of effects a *little* more easy.  \n\nWe tend to refer to coefficients using standardised variables as (unsurprisingly), \"standardised coefficients\", and we denote them with a $\\beta$^[some people use $\\beta$ for a normal coefficient, and $\\beta^*$ for a standardised coefficient].  \n\nThere are two main ways that people construct standardised coefficients. One of which standardises just the predictor, and the other of which standardises both predictor and outcome.  \n\n| predictor    | outcome      | in lm                     | coefficient                    | interpretation                               |\n| ------------ | ------------ | ------------------------- | --------------------------------- | -------------------------------------------- |\n| standardised | raw          | `y ~ scale(x)`        | $\\beta = b \\cdot s_x$             | \"change in Y for a 1 SD increase in X\"       |\n| standardised | standardised | `scale(y) ~ scale(x)` | $\\beta = b \\cdot \\frac{s_x}{s_y}$ | \"change in SD of Y for a 1 SD increase in X\" |\n\n\n\n## apples and oranges\n\nVery often, people will consider a standardised coefficient to be a unitless measure of \"size of effect\" that they can go and happily compare with other results (be it from another coefficient from the model, or a coefficient from a different study altogether, perhaps even on a different population!).  \n\nHowever, a **very** important thing to remember is that standardised coefficients are *dependent upon the sample standard deviations*. \n\nThis means that any comarisons between standardised coefficients comparisons could be due to an actual difference in magnitude of the underlying relationship, but it could just as easily be due to differences in the standard deviations of the variables. \n\nAs a toy example, I have two datasets, each of 100 people. The first has people aged 18 to 40. The second has people aged 18 to 80. Both datasets have been taken from a population where the underlying linear relationship between age and vocabulary is $vocab_i = 10 + 1 \\cdot age_i$. So the \"association between age and vocabulary\" should be more or less the same for both datasets (as seen in @fig-2hyp).    \n\n```{r}\n#| echo: false\n#| label: fig-2hyp\n#| fig-cap: \"Two hypothetical studies, with different age ranges of participants\"\n\nset.seed(56)\ndfpop <- tibble(\n  age = round(runif(1e5,18,80)),\n  vocabulary = 10 + age*1 + rnorm(1e5,0,3)\n)\ndf1 <- slice_sample(dfpop[dfpop$age<40,], n=100)\ndf2 <- slice_sample(dfpop, n=100)\n\n\nbind_rows(\n  df1 |> mutate(study=\"study 1\"),\n  df2 |> mutate(study=\"study 2\")\n) |>\nggplot(aes(x=age,y=vocabulary,col=study))+\n  geom_point(alpha=.3,size=3)+\n  geom_smooth(method=lm) +\n  xlim(18,80)+ylim(0,100)\n\n```\n\nBut the standardised coefficients for the second dataset will always be bigger, because the variance in the `age` variable is bigger.  \n\nThis is plainly clear from when we remember that the standardised coefficient is simply the original raw `age` coefficient multiplied by the standard deviation of `age` (either $b \\cdot s_{age}$ or $b \\cdot \\frac{s_{age}}{s_{vocab}}$). In the 2nd study, $s_{age}$ is bigger, but (as clearly evident in the plot above) we don't want to be saying that age has a bigger effect on vocabulary in study 2.. \n\n```{r}\n#| echo: false\nm1_o = lm(vocabulary ~ age, data = df1)\nm1_zx = lm(vocabulary ~ scale(age), data = df1)\nm1_zxy = lm(scale(vocabulary) ~ scale(age), data = df1)\n\nm2_o = lm(vocabulary ~ age, data = df2)\nm2_zx = lm(vocabulary ~ scale(age), data = df2)\nm2_zxy = lm(scale(vocabulary) ~ scale(age), data = df2)\n\ntribble(\n  ~\"estimate\",~\"study 1, ages 18-40\",~\"study 2, ages 18-80\",\n  \"sd(age)\",sd(df1$age), sd(df2$age),\n  \"vocab ~ age\",coef(m1_o)[2],coef(m2_o)[2],\n  \"vocab ~ scale(age)\",coef(m1_zx)[2],coef(m2_zx)[2],\n  \"scale(vocab) ~ scale(age)\",coef(m1_zxy)[2],coef(m2_zxy)[2],\n) |> mutate(across(2:3,~round(.,2))) |> gt::gt()\n\n```\n\nThe take-home of this is that sometimes we just can't compare apples and oranges. And sometimes we can't even compare apples to other apples! Choosing whether or not to standardise coefficients is going to depend on many things, and sometimes the easiest thing is simply to report both raw and standardised coefficients.  \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"08a_scaling.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"code-annotations":"hover","link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"8A: Centering and Scaling","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}