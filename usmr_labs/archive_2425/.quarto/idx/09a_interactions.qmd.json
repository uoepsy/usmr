{"title":"9A: Interactions","markdown":{"yaml":{"title":"9A: Interactions","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"write_csv(ptdat, \"../../data/usmr_partnertime.csv\")","containsRefs":false,"markdown":"\n\n```{r}\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nset.seed(993)\ncontcont <- tibble(\n  x1 = rnorm(60),\n  x2 = rnorm(60,2,1),\n  y = 1*x1 + 2.2*x2 + 1*x1*x2 + rnorm(60),\n  partner_time = round(7+(scale(x1)[,1]*3),2),\n  relationship_qual = round((scale(x2)[,1]*10) + 60),\n  wellbeing = 20 + (scale(y)[,1]*5)\n) %>% mutate(x1=partner_time,x2=relationship_qual,y=wellbeing)\n\nptdat <- contcont %>%\n  select(partner_time,relationship_qual,wellbeing)\nconcont <- contcont %>%\n  select(x1,x2,y)\n\n\ncontcat <- tibble(\n  x1 = rnorm(60),\n  x2 = sample(0:1,60,T),\n  y = -.8*x1 - 3*x2 - .6*x1*x2 + rnorm(60),\n  apoe4 = x2,\n  aqpi = round((scale(x1)[,1]*100) + 180),\n  mmse = round((scale(y)[,1]*4) + 22),\n) %>% mutate(x1=aqpi, x2=apoe4, y=mmse)\n\nairpol <- contcat %>%\n  select(aqpi, apoe4, mmse) %>% mutate(apoe4=ifelse(apoe4==0,\"neg\",\"pos\"))\ncontcat <- contcat %>%\n  select(x1,x2,y)\nset.seed(913)\ncatcat <- tibble(\n  x1 = sample(0:1,60, T),\n  x2 = sample(0:1,60, T),\n  y = round(8 + 2*x1 - .5*x2 + 4*x1*x2 + rnorm(60)),\n  anonymity = ifelse(x1==1,\"anonymous\",\"identifiable\"),\n  asgroup = ifelse(x2==1,\"group\",\"alone\"),\n  candybars = y\n)\ncandy <- catcat %>% select(anonymity, asgroup, candybars)\n# write_csv(candy, \"../../data/usmr_candy.csv\")\n# write_csv(airpol, \"../../data/usmr_airpol.csv\")\n```\n\n\n# Holding Constant...\n\nSo far, when we've been using multiple linear regression models, all our coefficients have been being interpreted as the \"change in $y$ for a 1-unit change in $x$ **while holding constant** the other predictors in the model\".   \n\nConsider a model with the following structure:    \n$$\ny = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\n\nWhen we fit this model structure to some data, we get out our estimated values for the coefficients $b_0$, $b_1$, and $b_2$, e.g.,:   \n\n```{r}\nmydata <- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmymodel <- lm(y ~ x1 + x2, data = mydata)\n```\n```\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.39138    3.67735  -0.650  0.51867   \nx1           0.17570    0.06435   2.730  0.00888 **\nx2          -0.64756    0.19959  -3.244  0.00217 **\n```\n\nThe coefficient we get out for $x_1$ tells us that the model estimates that $y$ will increase by 0.17 for every increase of 1 unit in $x_1$, _provided that we hold $x_2$ constant._  \n\nSo for _any_ value of $x_2$, a one unit increase in $x_1$ is associated with a $b_1$ change in $y$, provided that $x_2$ value stays as it is.  \n\nLet's use our primate brains data for a more realistic example: \n\n```{r}\n#| output: false\nbraindata <- read_csv(\"https://uoepsy.github.io/data/usmr_braindata.csv\")\nbraindata <- braindata %>% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\")\n)\n\nmonkeymod <- lm(mass_brain ~ age + isMonkey, data = braindata)\nsummary(monkeymod)\n```\n```\n...\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.668308   0.050478  13.240 1.57e-14 ***\nage         -0.004081   0.002337  -1.746   0.0904 .  \nisMonkeyYES -0.246933   0.044152  -5.593 3.54e-06 ***\n---\n```\n\nOur coefficient for `isMonkeyYES` coefficient is the estimated difference in brain mass between a human and a monkey **of the same age**.  \nSimilarly, our coefficient for `age` is the estimated change in brain mass for every additional year of age, provided we are not comparing a monkey to a human. \n\nThe idea of this can be seen on the plot in @fig-constplot. Imagine 4 different participants in our data. The coefficient for `isMonkeyYES` is the difference between A and C, or the difference between B and D. It doesn't matter where on the x-axis we go, it is the _vertical_ distance between the two lines.  \nThe `age` coefficient is the comparison between A and B, or between C and D.  \n\n```{r}\n#| label: fig-constplot\n#| fig-cap: \"multiple regression model of brain mass predicted by age and isMonkey. The age coefficient is the slope of the lines, and the isMonkey coefficient is the difference in their heights\"\n#| echo: false\nmm = data.frame(\n  who = c(\"A\\nHuman age 10\",\"C\\nMonkey age 10\",\"B\\nHuman age 11\",\"D\\nMonkey age 11\"),\n  age = c(10,10,11,11),\n  agelab = c(6,6,16,16),\n  isMonkey=rep(c(\"NO\",\"YES\"),2)\n) |> broom::augment(monkeymod, newdata=_)\n\nexpand_grid(\n  age = min(braindata$age):max(braindata$age),\n  isMonkey = c(\"NO\",\"YES\")\n) |>\n  broom::augment(monkeymod, \n                 newdata = _, \n                 interval=\"confidence\") |>\n  ggplot(aes(x=age,y=.fitted))+\n  geom_line(aes(col=isMonkey))+\n  geom_ribbon(aes(col=isMonkey,fill=isMonkey,\n                  ymin=.lower,ymax=.upper), alpha=.2)+\n  geom_point(data=mm)+\n  geom_segment(data=mm,aes(x=agelab,xend=age,y=.fitted-.1,yend=.fitted))+\n  geom_label(data=mm,aes(x=agelab,y=.fitted-.1,label=who), fill=\"white\")\n```\n\n::: {.callout-note collapse=\"true\"}\n#### *another* example (if you want one)\n\nWe're interested in estimating the association between BMI and cognition, after controlling for differences due to age.  \n\nWe want to use this model:  \n$$\n\\text{score on test of cognition} = b_0 + b_1 \\cdot \\text{age} + b_2 \\cdot \\text{BMI} + \\epsilon\n$$\nwhich we can fit using `lm()`:  \n```{r}\n#| eval: false\nlm(score ~ 1 + age + BMI, data = ourdata)\n```\nand we might get some coefficients (estimates for the $b_?$ values) such as those below:  \n_(I've just made up some nice round numbers to make it easier to think about)_  \n\n```\nCoefficients:\n            Estimate    ...   ...\n(Intercept)  160.000    ...   ...\nage         -1.50000    ...   ...\nBMI         -2.50000    ...   ...\n```\n\nThe coefficient of interest, the one for BMI, is telling us that \"a 1 unit increase in BMI is associated with a -2.5 decrease in Scores on the cognitive test, _holding age constant_\".  \n\nConsider 3 people: \n\n```{r}\n#| label: fig-pplconstant\n#| fig-cap: \"Three theoretical people\"  \n#| echo: false\nknitr::include_graphics(\"images/ints/constant.png\")\n```\n\n\nThe coefficient for BMI represents the difference in cognitive scores we would expect between Person A and Person B.  \n\nThink about why this is.  \nFor some person $i$, their model predicted score is:  \n$$\n\\hat{score_i} = b_0 + b_1 \\cdot age_i + b_2 \\cdot BMI_i\n$$\nWhich from our model estimates is:  \n$$\n\\hat{score_i} = 160 - 1.5 \\cdot age_i - 2.5 \\cdot BMI_i\n$$\n\n- Person A's score = $160 - (1.5*50) - (2.5*22) = 30$\n- Person B's score = $160 - (1.5*50) - (2.5*23) = 27.5$\n- Person C's score = $160 - (1.5*60) - (2.5*23) = 12.5$\n\nThe difference in model estimated Score between Person A and Person B is the coefficient of BMI, because those two people _only differ_ on BMI. Person A and Person C _also differ_ on age. This is how the coefficient of BMI is interpreted as \"holding age constant\" - it is a comparison between two hypothetical people who differ on BMI but are identical with respect to the other predictors in the model.  \n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# It Depends...\n\nSometimes, however, we might have reason to think that a certain association should *not* be the same at every value of another variable.  \n\nThere are lots of practical cases where we might think that the relationship between two variables _depends on_ the value of a third. \n\nBelow are some examples of this idea, where the explanatory variables (the predictors) are of different types (e.g. continuous, categorical, etc):  \n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with partner influences wellbeing _depends on_ the quality of the relationship.  \n\nVariables:  \n\n- __Outcome:__ Wellbeing (questionnaire score ranging from 0 to 32)\n- __Continuous Predictor:__ Time spent with partner (hours per day)  \n- __Continuous Predictor:__ Relationship Quality (rating from 0 to 100)  \n\n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ genetic status.  \n\nVariables:  \n\n- __Outcome:__ Cognitive Functioning measured via the MMSE. Possible scores range from 0 to 30. Lower scores indicate poorer cognitive functioning\n- __Continuous Predictor:__ Air-Quality Pollution Index (AQI). Ranges from 0 to 500. The higher the AQI value, the greater the level of air pollution.\n- __Categorical Predictor:__ APOE-4 Genotype status: Negative vs Positive  \n\n\n:::\n:::panel\n#### Example 3\n\n> Trick-or-treating children are more greedy when their costume hides their identity. But this is different _depending on_ whether they are alone or in a group.  \n\n\nVariables:  \n\n- __Outcome__: Number of candy bars taken while trick or treating\n- __Categorical Predictor:__  Whether or not the childs' identity is hidden by their costume (anonymous vs identifiable)\n- __Categorical Predictor:__ Whether the child is trick or treating as a group or alone (alone vs group)  \n\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Interactions!\n\nIn each of the above examples, we can no longer think about \"the relationship between [outcome] and [predictor]\" without discussing the level of the other predictor.  \n\n:::: {.columns}\n::: {.column width=\"55%\"}\nUsing Example 1 above, let's return to thinking about this in terms of two different observations (i.e. two different people in @fig-pplint).  \nHow do we think people's wellbeing will differ between someone who spends 1 hour with their partner and someone who spends 2 hours with their partner?  \n\nIf these are people from group A (these are people who have a really great relationship), then that extra hour will probably make a big impact on wellbeing!  \n\nWill it be the same for people from group B? People in group B are those people who don't have great relationships. For these people, whether you spend 1 hour or 2 hours with your partner doesn't pattern with a difference in wellbeing to the same extent as it does for the people in group A. It might even have a negative impact!  \n\n:::\n::: {.column width=\"45%\"}\n```{r}\n#| label: fig-pplint\n#| fig-cap: \"Comparisons between two theoretical observations who differ on only one predictor now depends on some other predictor\"  \n#| echo: false\n#| out-height: \"400px\"\nknitr::include_graphics(\"images/ints/ints.png\")\n```\n\n:::\n::::\n\n\nTo capture this, what we need is an _extra_ bit of information to tell us \"how _much_ does the association between 'time spent with partner' and wellbeing change as relationship quality changes?\" And this is something we can include in our model, and get an estimate for!   \n\nWe can model the idea of \"the association between $x_1$ and $y$ changes _depending on_ the level of $x_2$\" by including a product (multiplication) term between the two predictors.   \n\nSuch a model would take the form:  \n\n\n$$\ny = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2 + \\epsilon\n$$\n\nWhat this is doing is saying that our outcome $y$ is predicted by: \n\n- some amount of $x_1$\n- some amount of $x_2$\n- and a little addition to each of those amounts depending on the value of of the other variable.   \n\nTo provide a visual intuition and build on how we have been thinking of multiple regression upto this point, when we have two predictors that interact, our regression surface is no longer flat, but _twists_. This is because the slope along values of $x_1$ _changes_ as we move up $x_2$:   \n\n\n::: {.callout-caution collapse=\"true\"}\n#### why is it multiplication?  \n\nIf we think about the simple additive model (additive = **without** the interaction), then what we're saying is that the effect of $x_1$ on $y$ is simply $b_1$, which is a _constant_ (i.e. just a single number).  \n  \n$$\ny = b_0 + \\underbrace{b_1}_{\\text{effect}\\\\ \\text{of } x_1} \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\nBut what we want to model is that \"the effect of $x_1$ _depends_ on $x_2$\", so the effect is not just a single constant number, it's more like \"some number plus an amount of $x_2$\".  \nFor \"an amount of $x_2$\" let's use $b_3 \\cdot x_2$:  \n\n$$\ny = b_0 + \\underbrace{(\\overbrace{b_1}^{\\,\\,\\text{ some}\\\\ \\text{number}} + \\overbrace{b_{3} \\cdot x_2}^{\\text{some amount of }x_2})}_{\\text{effect of } x_1} \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\n\nWhich really expands out to:  \n\n$$\ny = b_0 + b_1 \\cdot x_1 + b_3 \\cdot x_2 \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\n\n:::\n\n\n\n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.^[This example comes from Ian Hajnosz, who previously tutored on USMR]  \n\n```{r}\n#| echo: false\n#| label: fig-i.contcont\n#| fig-cap: \"Interaction between two continuous predictors, viewed from two angles\"\n\nfit<-lm(y~x1*x2, data=contcont)\nsteps=50\nx1 <- with(contcont, seq(min(x1),max(x1),length=steps))\nx2 <- with(contcont, seq(min(x2),max(x2),length=steps))\nnewdat <- expand.grid(x1=x1, x2=x2)\ny <- matrix(predict(fit, newdat), steps, steps)\n\npar(mfrow=c(1,2))\np <- persp(x1,x2,y, theta = 25,phi=20, col = NA,\n           xlab=\"Time spent\\nwith partner\",ylab=\"Relationship Quality\",zlab=\"Wellbeing\")\nobs <- with(contcont, trans3d(x1,x2, y, p))\npred <- with(contcont, trans3d(x1, x2, fitted(fit), p))\npoints(obs, col = \"red\", pch = 16)\n#points(pred, col = \"blue\", pch = 16)\nsegments(obs$x, obs$y, pred$x, pred$y)\n\np <- persp(x1,x2,y, theta = 60,phi=20, col = NA,\n           xlab=\"Time spent\\nwith partner\",ylab=\"Relationship Quality\",zlab=\"Wellbeing\")\nobs <- with(contcont, trans3d(x1,x2, y, p))\npred <- with(contcont, trans3d(x1, x2, fitted(fit), p))\npoints(obs, col = \"red\", pch = 16)\n#points(pred, col = \"blue\", pch = 16)\nsegments(obs$x, obs$y, pred$x, pred$y)\npar(mfrow=c(1,1))\n\n```\n\nAt high values of relationship quality, the amount wellbeing increases with time spent is greater than it is at low values of relationship quality.  \nAnd we can phrase this the other way around: at high amounts of time spent with partner, relationship quality has a bigger effect on wellbeing than it does for low amounts of time spent. \n\nIn the model with this interaction:  \n$$\n\\text{wellbeing} = b_0 + b_1 \\cdot \\text{time} + b_2 \\cdot \\text{quality} + b_3 \\cdot \\text{time} \\cdot \\text{quality}\n$$\nThe interaction coefficient $b_3$ is the adjustment we make to the slope of wellbeing with \"time spent with partner\", as we move 1 up in \"relationship quality\".^[And vice versa! It is also the adjustment we make to the slope of wellbeing with \"relationship quality\", as we move 1 up in \"time spent with partner\"]  \n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.^[This example comes from Otto Jutila, who previously tutored on USMR]  \n\n```{r}\n#| echo: false\n#| label: fig-i.contcat\n#| fig-cap: \"Interaction between a continuous and a binary categorical predictors\"\nplot(contcat$y~contcat$x1,col=ifelse(contcat$x2,\"red\",\"black\"), xlab=\"air quality pollution index (AQPI)\",ylab=\"Cognition (MMSE)\")\nabline(lm(y~x1,contcat[contcat$x2==0,]))\nabline(lm(y~x1,contcat[contcat$x2==1,]),col=\"red\")\nlegend(300, 29, legend=c(\"APOE4 positive\", \"APOE4 negative\"),\n       col=c(\"red\", \"black\"), lty=1)\n```\n\nThis kind of interaction (where one predictor is continuous and the other is categorical), is sometimes the easiest to think about.  \n\nWe can see in @fig-i.contcat that cognition decreases as air pollution increases, __but__ this is different depending on genetic status. In the APOE4-positive group, the association is steeper than in the APOE4-negative group. The interaction is evident in that the two lines are non-parallel.   \n\nIn the model:  \n\n$$\n\\text{Cognition} = b_0 + b_1 \\cdot\\text{pollution} + b_2\\cdot\\text{APOE4} + b_3\\cdot\\text{pollution} \\cdot \\text{APOE4}\n$$\n\nThe interaction term $b_3$ is the estimated adjustment made to the slope of cogition across air pollution when we move from one group to the other. e.g. the slope for the APOE4-positive group is equal to the slope of the APOE4-negative group _plus the interaction term_.\n\n:::\n:::panel\n#### Example 3\n\n> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.^[This example comes from Tia Gong, who previously tutored on USMR]  \n\n```{r}\n#| echo: false\n#| label: fig-i.catcat\n#| fig-cap: \"Interaction between two categorical predictors\"\nggplot(candy, aes(x=anonymity, y=candybars, col=asgroup)) + \n  stat_summary(geom=\"pointrange\", size=1) +\n  stat_summary(geom=\"line\",lwd=1,lty=\"dotted\",aes(group=asgroup))\n```\n\nFor interactions between two categorical variables, we tend to plot the mean of the outcome variable for each combination of levels. We can see the interaction in @fig-i.catcat in the fact that the two dotted lines we have added to join the group-means are not parallel. \n\nChildren who are anonymous tend to take more candybars than those who are identifiable, but this difference is much greater when children are trick of treating in a group than when they are doing so alone!  \n\nIn the model\n$$\n\\text{candy} = b_0 + b_1 \\cdot\\text{isAnon} + b_2\\cdot\\text{inGroup} + b_3\\cdot\\text{isAnon} \\cdot \\text{inGroup}\n$$\n\nThe interaction term $b_3$ is going to be the estimated adjustment to the difference between anonymous vs identifiable for children in a group vs those alone. Put another way, it is how the difference between the two blue dots in alone vs group for is different from the difference between the two red dots.  \ne.g. the difference between anonymous & identifiable children in a group is equal to the difference between anonymous & identifiable children who are alone _plus the interaction term_.\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Fitting interactions in R\n\nIn R, the interaction term gets denoted by a colon `:` between the two predictor variables. We can fit an interaction term in a regression model quite easily in R:  \n\n```{r}\n#| eval: false\n# we don't *need* to put the 1, it's up to you :)\nlm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)\n```\nThis maps to the model equation we saw earlier: \n$$\ny = b_0 + b_1\\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2 + \\epsilon\n$$\n\n:::rtip\n__Shortcut__  \n\nTo express `x1 + x2 + x1:x2`, we can also use just `x1*x2`.  \n\nThese two models are equivalent: \n\n```{r}\n#| eval: false\nlm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)\nlm(y ~ 1 + x1*x2, data = dataset)\n```\n\n:::\n\n:::imp\n\nIf we fit the interaction `x1:x2`, we almost _always_ want to also fit the separate effects `x1` and `x2`.  \n\n_\"Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of $x_1$ depends on the level of $x_2$ but that there is no effect of $x_1$.\"_ [Ramsey & Schafer](https://www.cengage.uk/c/the-statistical-sleuth-3e-ramsey-schafer/9781133490678/){target=\"_blank\"}\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Interpretation\n\nWhen we include an interaction term in our model, we are saying that two effects of the two predictors on the outcome are _dependent upon_ one another. This means that with an interaction in the model `lm(y ~ x1 + x2 + x1:x2)` we can no longer talk about the \"effect of $x_1$ on $y$_'holding $x_2$ constant'_\". Instead we have to talk about **marginal effects** - e.g. the effect of $x_1$ _at a **specific** value_ of $x_2$.   \n\nWhen we fit a model with an interaction in R, we get out coefficients for both predictors, and for the interaction. The coefficients for each individual predictor reflect the effect on the outcome _when the other predictor is zero_. \n\n```\nlm(formula = y ~ x1 * x2, data = df)\n...\n...\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  ...        ....       ...      ...  \nx1           ...        ....       ...      ...  \nx2           ...        ....       ...      ...  \nx1:x2        ...        ....       ...      ...  \n---\n```\n\n\n:::sticky\n**marginal effects when the other predictor is zero**  \n\nThe individual coefficients for each predictor that is involved in an interaction are estimated _when the other predictor in the interaction is zero._  \n\n:::\n\n\nThese coefficients can be interpreted, in turn as:  \n\n| Coefficient      | Interpretation |\n| ----------- | ----------- |\n| `(Intercept)` | the estimated $y$ when all predictors ($x_1$ and $x_2$) are zero is [*estimate*] |\n| `x1`  | **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a [*estimate*] change in $y$ |\n| `x2`  | **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a [*estimate*] change in $y$. |\n| `x1:x2`  | as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by [*estimate*]<br>_**or**_<br>as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by [*estimate*] |\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n#### What if there are other things in the model too?\n\nNote that the interaction `x1:x2` changes how we interpret the individual coefficients for `x1` and `x2`.  \n\nIt does __*not*__ change how we interpret coefficients for other predictors that might be in our model. For variables that **aren't** involved in the interaction term, these are still held constant.  \n\nFor example, suppose we _also_ had another predictor $c_1$ in our model: \n```\nlm(y ~ c1 + x1 + x2 + x1:x2)\n```\n\n| Coefficient      | Interpretation |\n| ----------- | ----------- |\n| `(Intercept)` | the estimated $y$ when all predictors ($c_1$, $x_1$ and $x_2$) are zero is [*estimate*] |\n| `c1` | a 1 unit increase in $c_1$ is associated with a [*estimate*] increase in $y$, holding constant all other variables in the model ($x_1$ and $x_2$) |\n| `x1`  | holding $c_1$ constant, **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a [*estimate*] change in $y$ |\n| `x2`  | holding $c_1$ constant, **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a [*estimate*] change in $y$. |\n| `x1:x2`  | holding $c_1$ constant, as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by [*estimate*]<br>_**or**_<br>holding $c_1$ constant, as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by [*estimate*] |\n\n\n:::\n\n\n\n<br><br>\n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  \n\n```{r}\n# data\nptdat <- read_csv(\"https://uoepsy.github.io/data/usmr_partnertime.csv\")\n# model\neg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = ptdat)\nsummary(eg1mod)\n```\n\n- For someone who spends 0 hours with their partner, and who has a relationship quality score of 0, their estimated wellbeing is `r round(coef(eg1mod)[1],2)`.  \n- For someone who has a relationship quality score of 0, every hour spent with their partner is associated with a `r round(coef(eg1mod)[2],2)` change in wellbeing.  \n- For someone who spends 0 hours with their partner, an increase of 1 in the relationship quality is associated with a `r round(coef(eg1mod)[3],2)` change in wellbeing.  \n- For every 1 increase in relationship quality, an extra hour spent with their partner is associated with an _additional_ `r round(coef(eg1mod)[4],2)` change in wellbeing. \n\n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  \n\n```{r}\n# data\nairpol <- read_csv(\"https://uoepsy.github.io/data/usmr_airpol.csv\")\n# model\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\nsummary(eg2mod)\n```\n\n- For people who are APOE4 negative (the reference level, see [8A#multiple-categories-multiple-regression](08a_mlr.html#multiple-categories-multiple-regression){target=\"_blank\"}), living where the Air Quality Pollution Index is 0 (no pollution), the estimated score on the MMSE (mini mental state exam) is `r round(coef(eg2mod)[1],2)`.  \n- For people who are APOE4 negative, a 1 unit increase in air pollution is associated with a `r round(coef(eg2mod)[2],2)` change in MMSE scores.  \n- For people living with 0 air pollution, being APOE4 positive is associated with `r round(coef(eg2mod)[3],2)` lower MMSE scores compared to APOE4 negative\n- Compared to APOE4 negative, being APOE4 positive is associated with `r round(coef(eg2mod)[4],2)` greater decrease in MMSE scores for every 1 increase in air pollution.\n\n\n:::\n:::panel\n#### Example 3\n\n> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.\n\n```{r}\n# data\ncandy <- read_csv(\"https://uoepsy.github.io/data/usmr_candy.csv\")\n# model\neg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)\nsummary(eg3mod)\n```\n\n- An anonymous child trick or treating alone is estimated to take `r round(coef(eg3mod)[1])` candybars.\n- When trick or treating alone, compared to an anonymous child, an identifiable child is estimated to take `r abs(round(coef(eg3mod)[2]))` fewer candybars.  \n- When anonymous, compared to trick or treating alone, children who are part of a group are estimated to take `r round(coef(eg3mod)[3],1)` more candybars.  \n- The difference between identifiable and anonymous children's candybar taking is `r abs(round(coef(eg3mod)[4]))` fewer when children are trick or treating in a group than when they are alone.  \n\n\n```{r}\n#| eval: false\n#| echo: false\nfit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))\nsjPlot::plot_model(fit, type=\"int\",show.data=T)+geom_line()\n\n# it's just viewing that cube from one side \nlibrary(scatterplot3d)\nplt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,\n                                 x.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 y.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 main=\"y~x1*x2\\n(x1 and x2 are categorical)\"))\nfit = lm(y~x1*x2,catcat)\npp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))\npp$y <- predict(fit, pp)\n\npp1 <- pp[pp$x2==0,]\npp2 <- pp[pp$x2==1,]\npp3 <- pp[pp$x1==0,]\npp4 <- pp[pp$x1==1,]\nplt$points(pp1$x1,pp1$x2,pp1$y,type=\"l\")\nplt$points(pp2$x1,pp2$x2,pp2$y,type=\"l\")\nplt$points(pp3$x1,pp3$x2,pp3$y,type=\"l\")\nplt$points(pp4$x1,pp4$x2,pp4$y,type=\"l\")\n```\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Visualisation\n\nWe have seen lots of 3-dimensional plots above to try and help in building our intuition about how we model an interaction. However, we typically don't really want to visualise models like this (in part because our models often have more than 2 predictors, and so have more than 3 dimensions).  \n\nWhat we therefore need to do is find ways to represent these relationships in 2-dimensional plots. This is slightly different depending on the type of variables we are dealing with. \n\n- For a continuous $\\times$ categorical interaction, we can plot the association of the continuous predictor with the outcome *for each level* of the categorical variable. \n- For a continuous $\\times$ continuous interaction, we can plot the association of one predictor *at some judiciously chosen values* of the other (e.g. at the min, mean and max, or at -1 SD, mean, and +1 SD).  \n- For a categorical $\\times$ categorical interaction, we can plot the various estimated group means, with optional dotted^[dotted is a good way to indicate that there is no data across that line - it is linking two categories. Martin disagrees and thinks you should never have lines] lines to illustrate the non-parallelism   \n\n\n::: {.callout-note collapse=\"true\"}\n#### optional helper packages\n\nWe can make these sort of plots very easily using handy packages that are designed to make our lives easier. The _big_ downsides are that reliance on these packages will a) make it harder to customise plots, b) make it harder to troubleshoot when things go wrong, and c) stop us from having to *think* (and thinking is always good!).  \n\n**Use with caution:**    \n\n- From the **sjPlot** package: `plot_model(model, type = \"int\")` or `plot_model(model, type = \"eff\", terms = c(\"x1\",\"x2\"))`  \n- From the **interactions** package: `interact_plot(model, pred = \"x1\", modx = \"x2\")`, and `cat_plot()` for categorical interactions.     \n\n\n:::\n\n\n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  \n\nPlotting continuous by continuous interactions requires choosing a set of values for one of our predictors, at which we plot the slope of the other.  \n\nHere, we could choose to plot the slope of `wellbeing ~ partner_time` for the mean `relationship_qual`, and for 1 SD above and below that mean.  \n\n```{r}\n#| code-fold: true\n# data\nptdat <- read_csv(\"https://uoepsy.github.io/data/usmr_partnertime.csv\")\n# model\neg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = ptdat)\n\n# plot data\nplotdat <- expand_grid(\n  partner_time = 0:13, #min to max of sample\n  relationship_qual = c(\n    mean(ptdat$relationship_qual)-sd(ptdat$relationship_qual),\n    mean(ptdat$relationship_qual),\n    mean(ptdat$relationship_qual)+sd(ptdat$relationship_qual)\n  )\n)\n# plot\nbroom::augment(eg1mod, newdata = plotdat, interval=\"confidence\") |>\n  ggplot(aes(x=partner_time, y=.fitted, \n             col=relationship_qual,\n             group=relationship_qual))+\n  geom_point(data = ptdat,aes(y=wellbeing))+\n  geom_line()+\n  geom_ribbon(aes(ymin=.lower,ymax=.upper,fill=relationship_qual),\n              alpha=.2)\n\n```\n\n<!-- ::: {.callout-tip collapse=\"true\"} -->\n<!-- #### optional helper packages -->\n\n<!-- The interactions package defaults to showing the y~x1 relationship when x2 is at -1 SD below the mean, at the mean, and +1 SD above the mean.   -->\n<!-- ```{r} -->\n<!-- library(interactions) -->\n<!-- interact_plot(eg1mod, pred = \"partner_time\", modx = \"relationship_qual\", interval = TRUE) -->\n<!-- ``` -->\n\n<!-- The __sjPlot__ package can do this too, but wil default to showing it at the min and max of the other predictor.   -->\n<!-- ```{r} -->\n<!-- library(sjPlot) -->\n<!-- plot_model(eg1mod, type=\"int\") -->\n<!-- ``` -->\n<!-- We can change these manually. For instance, to make `plot_model` show the association when relationship quality is zero, when it is 50, and when it is 100, we could use:  -->\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- plot_model(eg1mod, type = \"eff\", terms=c(\"partner_time\",\"relationship_qual [0, 50, 100]\")) -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  \n\nThis is the most straightforward type of interaction to visualise, as there are only a set number of values that one of the variables can take, meaning only a finite number of lines we need to draw:  \n\n```{r}\n#| code-fold: true\n# data\nairpol <- read_csv(\"https://uoepsy.github.io/data/usmr_airpol.csv\")\n# model\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\n\n# plot data\nplotdat <- expand_grid(\n  aqpi = 0:500,\n  apoe4 = c(\"neg\",\"pos\")\n)\n# plot\nbroom::augment(eg2mod, newdata = plotdat, interval=\"confidence\") |>\n  ggplot(aes(x=aqpi, y = .fitted, \n             col = apoe4, fill = apoe4)) + \n  geom_point(data = airpol, aes(y = mmse), alpha=.3, size=3) +\n  geom_line() +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)\n\n```\n\n\n<!-- ::: {.callout-tip collapse=\"true\"} -->\n<!-- #### optional helper packages -->\n\n<!-- ```{r} -->\n<!-- p1 <- sjPlot::plot_model(eg2mod, type = \"int\") -->\n<!-- p2 <- interactions::interact_plot(eg2mod, pred = \"aqpi\", modx = \"apoe4\", interval = TRUE) -->\n<!-- p1 / p2 -->\n<!-- ``` -->\n<!-- ::: -->\n\n\n:::\n:::panel\n#### Example 3\n\n> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.\n\nFor these types of interactions (between categorical variables), plotting the estimates from our model is best done as our set of estimated group means.   \n```{r}\n#| code-fold: true\n# data\ncandy <- read_csv(\"https://uoepsy.github.io/data/usmr_candy.csv\")\n# model\neg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)\n\n# plot data\nplotdat <- expand_grid(\n  anonymity = c(\"anonymous\",\"identifiable\"),\n  asgroup = c(\"alone\",\"group\")\n)\n\n# plot\nbroom::augment(eg3mod, newdata=plotdat, interval=\"confidence\") |>\n  ggplot(aes(x = anonymity, col = asgroup)) + \n  geom_jitter(data = candy, aes(y = candybars),\n              alpha = .3, size = 3,\n              height = 0, width = .2) +\n  geom_pointrange(aes(y=.fitted, ymin=.lower, ymax=.upper),\n                  position = position_dodge(width=.2))\n```\n\n\n<!-- ::: {.callout-tip collapse=\"true\"} -->\n<!-- #### optional helper packages -->\n\n<!-- ```{r} -->\n<!-- p1 <- sjPlot::plot_model(eg3mod, type = \"int\")  -->\n<!-- p2 <- interactions::cat_plot(eg3mod, pred = \"anonymity\", modx = \"asgroup\", interval = TRUE) -->\n\n<!-- p1 / p2 -->\n<!-- ``` -->\n\n\n<!-- ::: -->\n\n\n\n```{r}\n#| eval: false\n#| echo: false\nfit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))\nsjPlot::plot_model(fit, type=\"int\",show.data=T)+geom_line()\n\n# it's just viewing that cube from one side \nlibrary(scatterplot3d)\nplt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,\n                                 x.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 y.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 main=\"y~x1*x2\\n(x1 and x2 are categorical)\"))\nfit = lm(y~x1*x2,catcat)\npp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))\npp$y <- predict(fit, pp)\n\npp1 <- pp[pp$x2==0,]\npp2 <- pp[pp$x2==1,]\npp3 <- pp[pp$x1==0,]\npp4 <- pp[pp$x1==1,]\nplt$points(pp1$x1,pp1$x2,pp1$y,type=\"l\")\nplt$points(pp2$x1,pp2$x2,pp2$y,type=\"l\")\nplt$points(pp3$x1,pp3$x2,pp3$y,type=\"l\")\nplt$points(pp4$x1,pp4$x2,pp4$y,type=\"l\")\n```\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Getting more from your model\n\nReporting on interactions is a bit like telling a story. Listing the interpretation of the coefficients is fine, but often reads a bit awkwardly. A good presentation of the results would provide the reader with the overall pattern of results, pulling out the key parts that are of interest. There are various things we can do to get out coefficients that might be of more use to us in telling our story.  \n\n## Mean Centering\n\nBy mean centering a continuous predictor, we change what \"0\" means. Normally, when we don't have an interaction, this simply changes the intercept value (see [8A #centering-and-scaling-predictors](08a_scaling.html#centering-and-scaling-predictors){target=\"_blank\"}). If we have the interaction `y ~ x1 + x2 + x1:x2`, then mean centering `x1` will make the coefficient for `x2` now represent \"the association between $x_2$ and $y$ for someone at the _average_ of $x_1$\". \n\nUsing one of our examples from throughout this reading, we might mean-center the air-pollution so that we can consider the difference in MMSE scores between APOE4 positive and negative people _at the average air-pollution level_\n\n```{r}\n# data\nairpol <- read_csv(\"https://uoepsy.github.io/data/usmr_airpol.csv\")\n# model with original variable\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\n# model with mean centered predictor\neg2mod_cent <- lm(mmse ~ scale(aqpi,scale=FALSE) * apoe4, data = airpol)\n```\n\n```{r}\n# coefficients:\ncoef(eg2mod)\ncoef(eg2mod_cent)\n```\n\nThis is because the coefficient for APOE4 compares the heights of the two lines when the other predictor is zero. So if we change what \"zero\" represents, we can change what that estimates. In the model plots below, we can see that _the model doesn't change_, it is just extracting different information (it is the distance to move from the blue dot to the red dot):  \n\n```{r}\n#| out-width: \"100%\"\n#| echo: false\nairpol <- airpol %>% \n  mutate(\n    aqpiC = aqpi - mean(aqpi)\n  )\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\neg2mod_cent <- lm(mmse ~ aqpiC * apoe4, data = airpol)\nsjPlot::plot_model(eg2mod, type=\"int\") +\n  geom_point(x=0,y=coef(eg2mod)[1], size=4, col=\"blue\")+\n  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, col=\"red\")+\n  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Raw AQPI\") +\n  \nsjPlot::plot_model(eg2mod_cent, type=\"int\") +\n  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, col=\"blue\")+\n  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, col=\"red\")+\n  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Mean Centered AQPI\") +\n  plot_layout(guides=\"collect\")\n```\n\n## Relevelling Factors\n\nAnother thing that can be useful (especially when working with categorical variables with lots of levels) is to make sure your variables are `factors` in R, and to set a useful reference level. Typically, the reference level is what we think of as \"normal\", e.g. if we have 3 groups: Placebo, Drug A, and Drug B, then we might compare each drug to the placebo condition, because that's comparable to most people (i.e. who aren't taking the drug).\n\nFor example, when we have two categorical variables:  \n\n- anonymity = \"anonymous\" vs \"identifiable\"\n- asgroup = \"alone\" vs \"group\"\n\nThen the default is to take the alphabetical ordering. We can change the ordering using functions that \"relevel\" a factor. Note, this only works if the variable is _already_ a `factor` in R (see [2A#categorical](02a_measurement.html#categorical){target=\"_blank\"} for a reminder of 'factors').   \n\n```{r}\n# data \ncandy <- read_csv(\"https://uoepsy.github.io/data/usmr_candy.csv\")\n\n# make both predictors 'factors'\ncandy <- \n  candy %>% \n  mutate(\n    anonymity = factor(anonymity),\n    asgroup = factor(asgroup)\n  )\n```\n\nOnce they are factors, we can see the default levels:  \n\n:::: {.columns}\n::: {.column width=\"47.5%\"}\n```{r}\nlevels(candy$anonymity)\n```\n:::\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n::: {.column width=\"47.5%\"}\n```{r}\nlevels(candy$asgroup)\n```\n:::\n\n::::\n\n\nThis is the original model, with the default levels:  \n```{r}\neg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)\ncoef(eg3mod)\n```\n\nLet's relevel anonymity to have \"identifiable\" as the first level, and then refit the model:  \n```{r}\ncandy <- \n  candy %>% \n  mutate(\n    anonymity = fct_relevel(anonymity, \"identifiable\")\n  )\n\neg3mod_rel <- lm(candybars ~ anonymity * asgroup, data = candy)\n\ncoef(eg3mod_rel)\n```\n\nAgain, the model doesn't change, we are simply extracting different bits from it:  \n\n::::panelset\n:::panel\n#### reference level = \"anonymous\"\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\np1 <- sjPlot::plot_model(eg3mod, type = \"int\") +\n  labs(title=\"with 'anonymous' and 'alone' as reference levels\") + \n  \n  \n  geom_segment(x = 1, xend = 1, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,3)]), \n                col=\"green3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 1, xend = 1.95, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,2)]), \n                col=\"orange3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 1.05, xend = 2.05, y = sum(coef(eg3mod)[c(1,3)]), yend=sum(coef(eg3mod)[c(1,2,3)]), \n                col=\"black\", lty=\"dotted\", lwd=.5,alpha=.2) +\n  \n  geom_segment(x = 2.05, xend = 2.05, y = sum(coef(eg3mod)[c(1:3)]), yend=sum(coef(eg3mod)[c(1:4)]), \n                col=\"purple\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  \n  annotate(geom=\"label\", x=1.1,y=coef(eg3mod)[1],label=\"Intercept\")+\n  annotate(geom=\"label\", x=1.01,col=\"green3\", hjust=0,y=coef(eg3mod)[1]+2,label=\"'asgroup' coefficient\") +\n  annotate(geom=\"label\",col=\"orange3\", x=1.5,y=coef(eg3mod)[1]-1,label=\"'anonymity' coefficient\") +\n  annotate(geom=\"label\",col=\"purple\", x=2.04,hjust=1,y=coef(eg3mod)[1],label=\"interaction coefficient\") \n\np1 + theme_bw(base_size = 12)\n\n```\n\n:::\n:::panel\n#### reference level = \"identifiable\"\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\np2 <- sjPlot::plot_model(eg3mod, type = \"int\") +\n  labs(title=\"with 'identifiable' and 'alone' as reference levels\") + \n  \n  geom_segment(x = 1.95, xend = 1.95, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,3)]), \n                col=\"green3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 1.95, xend = 1, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,2)]), \n                col=\"orange3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 2, xend = 1, y = sum(coef(eg3mod_rel)[c(1,3)]), yend=sum(coef(eg3mod_rel)[c(1,2,3)]), \n                col=\"black\", lty=\"dotted\", lwd=.5, alpha=.2) +\n  \n  geom_segment(x = 1, xend = 1, y = sum(coef(eg3mod_rel)[c(1:3)]), yend=sum(coef(eg3mod_rel)[c(1:4)]), \n                col=\"purple\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  \n  annotate(geom=\"label\", x=2,hjust=0,y=coef(eg3mod_rel)[1],label=\"Intercept\")+\n  annotate(geom=\"label\", x=1.93,col=\"green3\", hjust=1,y=coef(eg3mod_rel)[1]-.25,label=\"'asgroup' coefficient\") +\n  annotate(geom=\"label\",col=\"orange3\", x=1.5,y=coef(eg3mod_rel)[1]+1,label=\"'anonymity' coefficient\") +\n  annotate(geom=\"label\",col=\"purple\", x=1.01,hjust=0,y=coef(eg3mod_rel)[1]+4,label=\"interaction coefficient\")\n\n\np2 + theme_bw(base_size = 12)\n```\n:::\n::::\n\n<!-- :::sticky -->\n<!-- __Mapping model estimates to model plots__   -->\n\n<!-- To help with interpretation of the estimated coefficients from a model, a very useful exercise is to try and map them to what we see in our model plots (as seen above).   -->\n\n<!-- As we've talked about already - as models get more complex, there is a very important distinction to make between plotting the data and plotting the model estimates.   -->\n<!-- Whereas previously we could plot a simple regression `lm(y~x)` by plotting the _data_ and adding `geom_smooth(method=lm)`, now that we are working with multiple regression in order to visualise our results we should think of it as plotting the _model_ (and we can add the data if desired).   -->\n\n<!-- ::: -->\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: Interactions and Sum Contrasts  \n\n```{r}\n#| include: false\n# DATA ----\nset.seed(8653)\ndf <- expand_grid(\n  grouping = c(\"1\",\"2\"),\n  condition = c(\"A\",\"B\"),\n  n = 1:30\n) %>%\n  mutate(\n    lp = 2 + -2*(grouping==\"2\") + 1*(condition==\"B\") +\n      -2*(grouping==\"2\" & condition==\"B\"),\n    y = lp + rnorm(n())\n  ) %>% mutate_if(is.character,factor)\n\n# group means and marginals ----\n# tab <- xtabs( ~ grouping + condition, data = df)\n# N <- addmargins(tab)\n# addmargins(xtabs(y ~ grouping + condition, data = df)) / N\n\nmodel <- lm(y ~ condition * grouping, df)\n\n# change contrasts (sum contrasts drops the first level, so we'll relevel it to drop group2 and condB)\ndf$grouping = fct_relevel(df$grouping, \"2\")\ncontrasts(df$grouping) <- \"contr.sum\"\ndf$condition = fct_relevel(df$condition, \"B\")\ncontrasts(df$condition) <- \"contr.sum\"\n\nmodel1 <- lm(y ~ condition * grouping, df)\n\n\n\n\nmodplot <- as.data.frame(effects::effect(\"condition * grouping\",model)) %>%\n  ggplot(.,aes(x=condition, y=fit, ymin=lower,ymax=upper))+\n  geom_pointrange(aes(col=grouping,shape=condition), size=1.5,alpha=.8) +\n  geom_line(aes(col=grouping,group=grouping), lty=\"dashed\",linewidth=1)+\n  # labs(title=\"The model\", subtitle=\"lm()\") + \n  theme_classic()\n```\n\n\nWhen we have categorical predictors, our choice of contrasts coding changes the bits that we're getting our of our model.  \n\nSuppose we have a 2x2 design (condition A and B, in groups 1 and 2):  \n\n```{r}\n#| label: fig-sumplot\n#| fig-cap: \"Categorical x Categorical Interaction plot\"\n#| echo: false\nmodplot\n```\n\nWhen we are using the default contrasts coding (treatment - [see 8B #contrasts](08b_catpred.html#contrasts){target=\"_blank\"}) in R, then our coefficients for the individual predictors represent moving between the dots in @fig-sumplot.  \n\n```\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            1.9098     0.1759  10.855  < 2e-16 ***\nconditionB             1.1841     0.2488   4.759 5.65e-06 ***\ngrouping2             -1.6508     0.2488  -6.635 1.09e-09 ***\nconditionB:grouping2  -2.1627     0.3519  -6.146 1.15e-08 ***\n---\n```\n\n- The intercept is the red circle in @fig-sumplot.  \n- The coefficient for condition is the difference between the red circle and the red triangle in @fig-sumplot.  \n- The coefficient for grouping is the difference between the red circle and the blue circle in @fig-sumplot.  \n- The interaction coefficient is the difference from the slope of the red line to the slope of the blue line.  \n\n\nHowever, when we change to using sum contrasts, we're switching where zero is in our model. So if we change to sum contrasts (here we've changed __both__ predictors to using sum contrasts), then we end up estimating the effect of each predictor averaged across the other.  \n\n```\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           1.13577    0.08796  12.912  < 2e-16 ***\nconditionB            0.05141    0.08796   0.584     0.56    \ngrouping2            -1.36607    0.08796 -15.530  < 2e-16 ***\nconditionB:grouping2 -0.54066    0.08796  -6.146 1.15e-08 ***\n---\n```\n\n- The intercept is the grey X in @fig-sumplot2.  \n- The coefficient for condition is the difference between the grey X and the grey triangle in @fig-sumplot2.  \n- The coefficient for grouping is the difference between the grey X and the blue line in @fig-sumplot2.  \n- The interaction coefficient is the difference from the slope of the grey line to slope of the blue line.  \n\n```{r}\n#| echo: false\n#| label: fig-sumplot2\n#| fig-cap: \"Visualisation of sum-contrasts for categorical x categorical interaction plot\"\nmodplot + \n  geom_segment(x = 1, xend = 2, \n               y = (coef(model1)[1]-coef(model1)[2]), \n               yend = sum(coef(model1)[c(1,2)]),\n               lty = \"dashed\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = sum(coef(model1)[c(1,3)]),\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = coef(model1)[1]-coef(model1)[3],\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_point(x=1,\n             y=(coef(model1)[1]-coef(model1)[2]),\n             size = 7, alpha = .05,\n             aes(shape=\"A\")) +\n  geom_point(x=2,\n             y=sum(coef(model1)[1:2]),\n             size = 7, alpha = .05,\n             aes(shape=\"B\")) +\n  geom_point(x=1.5,\n             y=coef(model1)[1],\n             size = 7, alpha = .2,\n             shape=4) \n\n```\n\n\nIt can get quite confusing when we start switching up the contrasts, but it's all just because we're changing what \"zero\" means, and what \"moving 1\" means:  \n\n::::panelset\n:::panel\n#### Treatment contrasts\n\n```{r}\n#| echo: false\ndd = tibble(\n  #lab=letters[1:4],\n  clab=c(1,1,0,0),\n  glab=c(1,0,1,0),\n  lab=paste0(\"condition: \",clab,\"\\ngroup: \",glab),\n  condition=c(2,2,1,1),\n  ff=c(\n    #c(1,0,0,0) %*% coef(model1),\n    #c(1,1,0,0) %*% coef(model1),\n    #c(1,0,1,0) %*% coef(model1),\n    c(1,1,1,1) %*% coef(model1),\n    #c(1,-1,0,0) %*% coef(model1),\n    #c(1,0,-1,0) %*% coef(model1),\n    c(1,1,-1,-1) %*% coef(model1),\n    c(1,-1,1,-1) %*% coef(model1),\n    c(1,-1,-1,1) %*% coef(model1)\n  )\n)\nmodplot+\n  geom_label(inherit.aes=F,data=dd,aes(x=condition,\n                                       y=ff,label=lab),alpha=.8)+\n  labs(subtitle=\"y~condition*group with treatment contrasts\")\n```\n\n\n\n:::\n:::panel\n#### Sum contrasts\n\n```{r}\n#| echo: false\ndd = tibble(\n  #lab=letters[1:9],\n  clab=c(0,1,0,1,-1,0,1,-1,-1),\n  glab=c(0,0,1,1,0,-1,-1,1,-1),\n  lab=paste0(\"condition: \",clab,\"\\ngroup: \",glab),\n  condition=c(1.5,2,1.5,2,1,1.5,2,1,1),\n  ff=c(\n    c(1,0,0,0) %*% coef(model1),\n    c(1,1,0,0) %*% coef(model1),\n    c(1,0,1,0) %*% coef(model1),\n    c(1,1,1,1) %*% coef(model1),\n    c(1,-1,0,0) %*% coef(model1),\n    c(1,0,-1,0) %*% coef(model1),\n    c(1,1,-1,-1) %*% coef(model1),\n    c(1,-1,1,-1) %*% coef(model1),\n    c(1,-1,-1,1) %*% coef(model1)\n  )\n)\nmodplot+\n  geom_segment(x = 1, xend = 2, \n               y = (coef(model1)[1]-coef(model1)[2]), \n               yend = sum(coef(model1)[c(1,2)]),\n               lty = \"dashed\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = sum(coef(model1)[c(1,3)]),\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = coef(model1)[1]-coef(model1)[3],\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_point(x=1,\n             y=(coef(model1)[1]-coef(model1)[2]),\n             size = 7, alpha = .05,\n             aes(shape=\"A\")) +\n  geom_point(x=2,\n             y=sum(coef(model1)[1:2]),\n             size = 7, alpha = .05,\n             aes(shape=\"B\")) +\n  geom_point(x=1.5,\n             y=coef(model1)[1],\n             size = 7, alpha = .2,\n             shape=4) + \n  geom_label(inherit.aes=F,data=dd,aes(x=condition,\n                                       y=ff,label=lab),alpha=.8)+\n  labs(subtitle=\"y~condition*group with sum contrasts\")\n```\n\n:::\n::::\n\n\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional Extra: Getting non-linear  \n\nYou might be noticing that when we start talking about our regression surface \"twisting\" (e.g. in @fig-i.contcont), we're starting to see curves in our model.  \n\nWe can model _curves_ in a \"linear\" model!?  \n\nAn interaction does in a sense introduce non-linearity to our thinking, because there we no longer think of a linear effect of $x_1$ (it \"depends on\" $x_2$). This a little bit of a trick, because ultimately our model is still linear - we are estimating our outcome $y$ as the _linear combination_ of a set of predictors. In the model $y = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2$, the adjustment that we make $b_3$ to each of the coefficients $b_1$ and $b_2$ is a constant.  \n\nWe can even exploit this to model more clearly \"non-linear\" associations, such as the age and height example below.  \n\n\n\n```{r}\n#| echo: false\n#| label: fig-nonlin\n#| fig-cap: \"Two linear models, one with a quadratic term (right)\"  \n#| out-width: \"100%\"\nset.seed(13)\ndf <- tibble(\n  age = sample(0:16,100,replace=T),\n  height = 70 + (7*age) - (17*scale(age)^2) + rnorm(100,0,10)\n)\nmod1 <- lm(height ~ age, data = df)\nmod2 <- lm(height ~ age + I(age^2), data = df)\ndf <- df %>% mutate(f1 = fitted(mod1),f2=fitted(mod2))\n\nggplot(df,aes(x=age,y=height))+\n  geom_point() + \n  geom_line(aes(y=f1),lwd=1, col=\"blue\")  +\n  labs(title=\"lm(height ~ age)\") + \n  \n  ggplot(df,aes(x=age,y=height))+\n  geom_point() + \n  geom_line(aes(y=f2),lwd=1,col=\"blue\") +\n  labs(title=\"lm(height ~ age + I(age^2))\") &     \n  theme_bw(base_size = 12)\n```\n\nWe will cover this a lot more in the multivariate stats course, so don't worry too much about it right now.  \nHowever, it's useful as a means of seeing how we can extend linear models to fit these sort of relationships. In the model on the right of @fig-nonlin, the model returns three coefficients:\n```{r}\n#| echo: false\n.pp(summary(mod2),l=list(10:13))\n```\n\nIt is estimating a persons' height as:\n\n- an intercept of `r coef(mod2)[1] %>% round(2)` (the estimated height of a newborn baby of age 0)\n- plus `r coef(mod2)[2] %>% round(2)` cm for every year of age they have\n- plus `r coef(mod2)[3] %>% round(2)` cm for every year of age they have, squared.  \n\nSo for a 4 year old, the estimated height is: \n\n$$\nheight = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[2] %>% round(2)` \\times 4) + (`r coef(mod2)[3] %>% round(2)` \\times 4^2) = `r round(coef(mod2),1) %*% c(1,4,16)`\n$$\nand for a 10 year old, it is:  \n\n$$\nheight = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[2] %>% round(2)` \\times 10) + (`r coef(mod2)[3] %>% round(2)` \\times 10^2) = `r round(coef(mod2),1) %*% c(1,10,100)`\n$$\nWe can see how the quadratic \"$\\text{age}^2$ term has a larger (negative) effect as age increases (for the 4 year old it makes the estimated height $`r coef(mod2)[3] %>% round(2)` \\times 4^2 = `r round(coef(mod2)[3]*4^2,1)`$ lower, and for the 10 year old it makes the estimated height $`r coef(mod2)[3] %>% round(2)` \\times 10^2 = `r round(coef(mod2)[3]*10^2,1)`$ lower). This captures the plateauing of heights as children get older.  \n\n:::\n\n<!-- # Optional: further exploration -->\n\n<!-- ## Simple Slopes -->\n\n<!-- we've kind of already done it visually.   -->\n<!-- plot the slope of y~x1 for certain values of x2.   -->\n\n<!-- we've done this visually, but it would be nice to get out a \"is y~x1 significant when x2 = ?\"   -->\n\n\n\n\n<!-- ## Johnson Neyman -->\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nset.seed(993)\ncontcont <- tibble(\n  x1 = rnorm(60),\n  x2 = rnorm(60,2,1),\n  y = 1*x1 + 2.2*x2 + 1*x1*x2 + rnorm(60),\n  partner_time = round(7+(scale(x1)[,1]*3),2),\n  relationship_qual = round((scale(x2)[,1]*10) + 60),\n  wellbeing = 20 + (scale(y)[,1]*5)\n) %>% mutate(x1=partner_time,x2=relationship_qual,y=wellbeing)\n\nptdat <- contcont %>%\n  select(partner_time,relationship_qual,wellbeing)\nconcont <- contcont %>%\n  select(x1,x2,y)\n\n\ncontcat <- tibble(\n  x1 = rnorm(60),\n  x2 = sample(0:1,60,T),\n  y = -.8*x1 - 3*x2 - .6*x1*x2 + rnorm(60),\n  apoe4 = x2,\n  aqpi = round((scale(x1)[,1]*100) + 180),\n  mmse = round((scale(y)[,1]*4) + 22),\n) %>% mutate(x1=aqpi, x2=apoe4, y=mmse)\n\nairpol <- contcat %>%\n  select(aqpi, apoe4, mmse) %>% mutate(apoe4=ifelse(apoe4==0,\"neg\",\"pos\"))\ncontcat <- contcat %>%\n  select(x1,x2,y)\nset.seed(913)\ncatcat <- tibble(\n  x1 = sample(0:1,60, T),\n  x2 = sample(0:1,60, T),\n  y = round(8 + 2*x1 - .5*x2 + 4*x1*x2 + rnorm(60)),\n  anonymity = ifelse(x1==1,\"anonymous\",\"identifiable\"),\n  asgroup = ifelse(x2==1,\"group\",\"alone\"),\n  candybars = y\n)\ncandy <- catcat %>% select(anonymity, asgroup, candybars)\n# write_csv(ptdat, \"../../data/usmr_partnertime.csv\")\n# write_csv(candy, \"../../data/usmr_candy.csv\")\n# write_csv(airpol, \"../../data/usmr_airpol.csv\")\n```\n\n\n# Holding Constant...\n\nSo far, when we've been using multiple linear regression models, all our coefficients have been being interpreted as the \"change in $y$ for a 1-unit change in $x$ **while holding constant** the other predictors in the model\".   \n\nConsider a model with the following structure:    \n$$\ny = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\n\nWhen we fit this model structure to some data, we get out our estimated values for the coefficients $b_0$, $b_1$, and $b_2$, e.g.,:   \n\n```{r}\nmydata <- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmymodel <- lm(y ~ x1 + x2, data = mydata)\n```\n```\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.39138    3.67735  -0.650  0.51867   \nx1           0.17570    0.06435   2.730  0.00888 **\nx2          -0.64756    0.19959  -3.244  0.00217 **\n```\n\nThe coefficient we get out for $x_1$ tells us that the model estimates that $y$ will increase by 0.17 for every increase of 1 unit in $x_1$, _provided that we hold $x_2$ constant._  \n\nSo for _any_ value of $x_2$, a one unit increase in $x_1$ is associated with a $b_1$ change in $y$, provided that $x_2$ value stays as it is.  \n\nLet's use our primate brains data for a more realistic example: \n\n```{r}\n#| output: false\nbraindata <- read_csv(\"https://uoepsy.github.io/data/usmr_braindata.csv\")\nbraindata <- braindata %>% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\")\n)\n\nmonkeymod <- lm(mass_brain ~ age + isMonkey, data = braindata)\nsummary(monkeymod)\n```\n```\n...\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.668308   0.050478  13.240 1.57e-14 ***\nage         -0.004081   0.002337  -1.746   0.0904 .  \nisMonkeyYES -0.246933   0.044152  -5.593 3.54e-06 ***\n---\n```\n\nOur coefficient for `isMonkeyYES` coefficient is the estimated difference in brain mass between a human and a monkey **of the same age**.  \nSimilarly, our coefficient for `age` is the estimated change in brain mass for every additional year of age, provided we are not comparing a monkey to a human. \n\nThe idea of this can be seen on the plot in @fig-constplot. Imagine 4 different participants in our data. The coefficient for `isMonkeyYES` is the difference between A and C, or the difference between B and D. It doesn't matter where on the x-axis we go, it is the _vertical_ distance between the two lines.  \nThe `age` coefficient is the comparison between A and B, or between C and D.  \n\n```{r}\n#| label: fig-constplot\n#| fig-cap: \"multiple regression model of brain mass predicted by age and isMonkey. The age coefficient is the slope of the lines, and the isMonkey coefficient is the difference in their heights\"\n#| echo: false\nmm = data.frame(\n  who = c(\"A\\nHuman age 10\",\"C\\nMonkey age 10\",\"B\\nHuman age 11\",\"D\\nMonkey age 11\"),\n  age = c(10,10,11,11),\n  agelab = c(6,6,16,16),\n  isMonkey=rep(c(\"NO\",\"YES\"),2)\n) |> broom::augment(monkeymod, newdata=_)\n\nexpand_grid(\n  age = min(braindata$age):max(braindata$age),\n  isMonkey = c(\"NO\",\"YES\")\n) |>\n  broom::augment(monkeymod, \n                 newdata = _, \n                 interval=\"confidence\") |>\n  ggplot(aes(x=age,y=.fitted))+\n  geom_line(aes(col=isMonkey))+\n  geom_ribbon(aes(col=isMonkey,fill=isMonkey,\n                  ymin=.lower,ymax=.upper), alpha=.2)+\n  geom_point(data=mm)+\n  geom_segment(data=mm,aes(x=agelab,xend=age,y=.fitted-.1,yend=.fitted))+\n  geom_label(data=mm,aes(x=agelab,y=.fitted-.1,label=who), fill=\"white\")\n```\n\n::: {.callout-note collapse=\"true\"}\n#### *another* example (if you want one)\n\nWe're interested in estimating the association between BMI and cognition, after controlling for differences due to age.  \n\nWe want to use this model:  \n$$\n\\text{score on test of cognition} = b_0 + b_1 \\cdot \\text{age} + b_2 \\cdot \\text{BMI} + \\epsilon\n$$\nwhich we can fit using `lm()`:  \n```{r}\n#| eval: false\nlm(score ~ 1 + age + BMI, data = ourdata)\n```\nand we might get some coefficients (estimates for the $b_?$ values) such as those below:  \n_(I've just made up some nice round numbers to make it easier to think about)_  \n\n```\nCoefficients:\n            Estimate    ...   ...\n(Intercept)  160.000    ...   ...\nage         -1.50000    ...   ...\nBMI         -2.50000    ...   ...\n```\n\nThe coefficient of interest, the one for BMI, is telling us that \"a 1 unit increase in BMI is associated with a -2.5 decrease in Scores on the cognitive test, _holding age constant_\".  \n\nConsider 3 people: \n\n```{r}\n#| label: fig-pplconstant\n#| fig-cap: \"Three theoretical people\"  \n#| echo: false\nknitr::include_graphics(\"images/ints/constant.png\")\n```\n\n\nThe coefficient for BMI represents the difference in cognitive scores we would expect between Person A and Person B.  \n\nThink about why this is.  \nFor some person $i$, their model predicted score is:  \n$$\n\\hat{score_i} = b_0 + b_1 \\cdot age_i + b_2 \\cdot BMI_i\n$$\nWhich from our model estimates is:  \n$$\n\\hat{score_i} = 160 - 1.5 \\cdot age_i - 2.5 \\cdot BMI_i\n$$\n\n- Person A's score = $160 - (1.5*50) - (2.5*22) = 30$\n- Person B's score = $160 - (1.5*50) - (2.5*23) = 27.5$\n- Person C's score = $160 - (1.5*60) - (2.5*23) = 12.5$\n\nThe difference in model estimated Score between Person A and Person B is the coefficient of BMI, because those two people _only differ_ on BMI. Person A and Person C _also differ_ on age. This is how the coefficient of BMI is interpreted as \"holding age constant\" - it is a comparison between two hypothetical people who differ on BMI but are identical with respect to the other predictors in the model.  \n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# It Depends...\n\nSometimes, however, we might have reason to think that a certain association should *not* be the same at every value of another variable.  \n\nThere are lots of practical cases where we might think that the relationship between two variables _depends on_ the value of a third. \n\nBelow are some examples of this idea, where the explanatory variables (the predictors) are of different types (e.g. continuous, categorical, etc):  \n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with partner influences wellbeing _depends on_ the quality of the relationship.  \n\nVariables:  \n\n- __Outcome:__ Wellbeing (questionnaire score ranging from 0 to 32)\n- __Continuous Predictor:__ Time spent with partner (hours per day)  \n- __Continuous Predictor:__ Relationship Quality (rating from 0 to 100)  \n\n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ genetic status.  \n\nVariables:  \n\n- __Outcome:__ Cognitive Functioning measured via the MMSE. Possible scores range from 0 to 30. Lower scores indicate poorer cognitive functioning\n- __Continuous Predictor:__ Air-Quality Pollution Index (AQI). Ranges from 0 to 500. The higher the AQI value, the greater the level of air pollution.\n- __Categorical Predictor:__ APOE-4 Genotype status: Negative vs Positive  \n\n\n:::\n:::panel\n#### Example 3\n\n> Trick-or-treating children are more greedy when their costume hides their identity. But this is different _depending on_ whether they are alone or in a group.  \n\n\nVariables:  \n\n- __Outcome__: Number of candy bars taken while trick or treating\n- __Categorical Predictor:__  Whether or not the childs' identity is hidden by their costume (anonymous vs identifiable)\n- __Categorical Predictor:__ Whether the child is trick or treating as a group or alone (alone vs group)  \n\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Interactions!\n\nIn each of the above examples, we can no longer think about \"the relationship between [outcome] and [predictor]\" without discussing the level of the other predictor.  \n\n:::: {.columns}\n::: {.column width=\"55%\"}\nUsing Example 1 above, let's return to thinking about this in terms of two different observations (i.e. two different people in @fig-pplint).  \nHow do we think people's wellbeing will differ between someone who spends 1 hour with their partner and someone who spends 2 hours with their partner?  \n\nIf these are people from group A (these are people who have a really great relationship), then that extra hour will probably make a big impact on wellbeing!  \n\nWill it be the same for people from group B? People in group B are those people who don't have great relationships. For these people, whether you spend 1 hour or 2 hours with your partner doesn't pattern with a difference in wellbeing to the same extent as it does for the people in group A. It might even have a negative impact!  \n\n:::\n::: {.column width=\"45%\"}\n```{r}\n#| label: fig-pplint\n#| fig-cap: \"Comparisons between two theoretical observations who differ on only one predictor now depends on some other predictor\"  \n#| echo: false\n#| out-height: \"400px\"\nknitr::include_graphics(\"images/ints/ints.png\")\n```\n\n:::\n::::\n\n\nTo capture this, what we need is an _extra_ bit of information to tell us \"how _much_ does the association between 'time spent with partner' and wellbeing change as relationship quality changes?\" And this is something we can include in our model, and get an estimate for!   \n\nWe can model the idea of \"the association between $x_1$ and $y$ changes _depending on_ the level of $x_2$\" by including a product (multiplication) term between the two predictors.   \n\nSuch a model would take the form:  \n\n\n$$\ny = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2 + \\epsilon\n$$\n\nWhat this is doing is saying that our outcome $y$ is predicted by: \n\n- some amount of $x_1$\n- some amount of $x_2$\n- and a little addition to each of those amounts depending on the value of of the other variable.   \n\nTo provide a visual intuition and build on how we have been thinking of multiple regression upto this point, when we have two predictors that interact, our regression surface is no longer flat, but _twists_. This is because the slope along values of $x_1$ _changes_ as we move up $x_2$:   \n\n\n::: {.callout-caution collapse=\"true\"}\n#### why is it multiplication?  \n\nIf we think about the simple additive model (additive = **without** the interaction), then what we're saying is that the effect of $x_1$ on $y$ is simply $b_1$, which is a _constant_ (i.e. just a single number).  \n  \n$$\ny = b_0 + \\underbrace{b_1}_{\\text{effect}\\\\ \\text{of } x_1} \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\nBut what we want to model is that \"the effect of $x_1$ _depends_ on $x_2$\", so the effect is not just a single constant number, it's more like \"some number plus an amount of $x_2$\".  \nFor \"an amount of $x_2$\" let's use $b_3 \\cdot x_2$:  \n\n$$\ny = b_0 + \\underbrace{(\\overbrace{b_1}^{\\,\\,\\text{ some}\\\\ \\text{number}} + \\overbrace{b_{3} \\cdot x_2}^{\\text{some amount of }x_2})}_{\\text{effect of } x_1} \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\n\nWhich really expands out to:  \n\n$$\ny = b_0 + b_1 \\cdot x_1 + b_3 \\cdot x_2 \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\n\n:::\n\n\n\n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.^[This example comes from Ian Hajnosz, who previously tutored on USMR]  \n\n```{r}\n#| echo: false\n#| label: fig-i.contcont\n#| fig-cap: \"Interaction between two continuous predictors, viewed from two angles\"\n\nfit<-lm(y~x1*x2, data=contcont)\nsteps=50\nx1 <- with(contcont, seq(min(x1),max(x1),length=steps))\nx2 <- with(contcont, seq(min(x2),max(x2),length=steps))\nnewdat <- expand.grid(x1=x1, x2=x2)\ny <- matrix(predict(fit, newdat), steps, steps)\n\npar(mfrow=c(1,2))\np <- persp(x1,x2,y, theta = 25,phi=20, col = NA,\n           xlab=\"Time spent\\nwith partner\",ylab=\"Relationship Quality\",zlab=\"Wellbeing\")\nobs <- with(contcont, trans3d(x1,x2, y, p))\npred <- with(contcont, trans3d(x1, x2, fitted(fit), p))\npoints(obs, col = \"red\", pch = 16)\n#points(pred, col = \"blue\", pch = 16)\nsegments(obs$x, obs$y, pred$x, pred$y)\n\np <- persp(x1,x2,y, theta = 60,phi=20, col = NA,\n           xlab=\"Time spent\\nwith partner\",ylab=\"Relationship Quality\",zlab=\"Wellbeing\")\nobs <- with(contcont, trans3d(x1,x2, y, p))\npred <- with(contcont, trans3d(x1, x2, fitted(fit), p))\npoints(obs, col = \"red\", pch = 16)\n#points(pred, col = \"blue\", pch = 16)\nsegments(obs$x, obs$y, pred$x, pred$y)\npar(mfrow=c(1,1))\n\n```\n\nAt high values of relationship quality, the amount wellbeing increases with time spent is greater than it is at low values of relationship quality.  \nAnd we can phrase this the other way around: at high amounts of time spent with partner, relationship quality has a bigger effect on wellbeing than it does for low amounts of time spent. \n\nIn the model with this interaction:  \n$$\n\\text{wellbeing} = b_0 + b_1 \\cdot \\text{time} + b_2 \\cdot \\text{quality} + b_3 \\cdot \\text{time} \\cdot \\text{quality}\n$$\nThe interaction coefficient $b_3$ is the adjustment we make to the slope of wellbeing with \"time spent with partner\", as we move 1 up in \"relationship quality\".^[And vice versa! It is also the adjustment we make to the slope of wellbeing with \"relationship quality\", as we move 1 up in \"time spent with partner\"]  \n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.^[This example comes from Otto Jutila, who previously tutored on USMR]  \n\n```{r}\n#| echo: false\n#| label: fig-i.contcat\n#| fig-cap: \"Interaction between a continuous and a binary categorical predictors\"\nplot(contcat$y~contcat$x1,col=ifelse(contcat$x2,\"red\",\"black\"), xlab=\"air quality pollution index (AQPI)\",ylab=\"Cognition (MMSE)\")\nabline(lm(y~x1,contcat[contcat$x2==0,]))\nabline(lm(y~x1,contcat[contcat$x2==1,]),col=\"red\")\nlegend(300, 29, legend=c(\"APOE4 positive\", \"APOE4 negative\"),\n       col=c(\"red\", \"black\"), lty=1)\n```\n\nThis kind of interaction (where one predictor is continuous and the other is categorical), is sometimes the easiest to think about.  \n\nWe can see in @fig-i.contcat that cognition decreases as air pollution increases, __but__ this is different depending on genetic status. In the APOE4-positive group, the association is steeper than in the APOE4-negative group. The interaction is evident in that the two lines are non-parallel.   \n\nIn the model:  \n\n$$\n\\text{Cognition} = b_0 + b_1 \\cdot\\text{pollution} + b_2\\cdot\\text{APOE4} + b_3\\cdot\\text{pollution} \\cdot \\text{APOE4}\n$$\n\nThe interaction term $b_3$ is the estimated adjustment made to the slope of cogition across air pollution when we move from one group to the other. e.g. the slope for the APOE4-positive group is equal to the slope of the APOE4-negative group _plus the interaction term_.\n\n:::\n:::panel\n#### Example 3\n\n> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.^[This example comes from Tia Gong, who previously tutored on USMR]  \n\n```{r}\n#| echo: false\n#| label: fig-i.catcat\n#| fig-cap: \"Interaction between two categorical predictors\"\nggplot(candy, aes(x=anonymity, y=candybars, col=asgroup)) + \n  stat_summary(geom=\"pointrange\", size=1) +\n  stat_summary(geom=\"line\",lwd=1,lty=\"dotted\",aes(group=asgroup))\n```\n\nFor interactions between two categorical variables, we tend to plot the mean of the outcome variable for each combination of levels. We can see the interaction in @fig-i.catcat in the fact that the two dotted lines we have added to join the group-means are not parallel. \n\nChildren who are anonymous tend to take more candybars than those who are identifiable, but this difference is much greater when children are trick of treating in a group than when they are doing so alone!  \n\nIn the model\n$$\n\\text{candy} = b_0 + b_1 \\cdot\\text{isAnon} + b_2\\cdot\\text{inGroup} + b_3\\cdot\\text{isAnon} \\cdot \\text{inGroup}\n$$\n\nThe interaction term $b_3$ is going to be the estimated adjustment to the difference between anonymous vs identifiable for children in a group vs those alone. Put another way, it is how the difference between the two blue dots in alone vs group for is different from the difference between the two red dots.  \ne.g. the difference between anonymous & identifiable children in a group is equal to the difference between anonymous & identifiable children who are alone _plus the interaction term_.\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Fitting interactions in R\n\nIn R, the interaction term gets denoted by a colon `:` between the two predictor variables. We can fit an interaction term in a regression model quite easily in R:  \n\n```{r}\n#| eval: false\n# we don't *need* to put the 1, it's up to you :)\nlm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)\n```\nThis maps to the model equation we saw earlier: \n$$\ny = b_0 + b_1\\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2 + \\epsilon\n$$\n\n:::rtip\n__Shortcut__  \n\nTo express `x1 + x2 + x1:x2`, we can also use just `x1*x2`.  \n\nThese two models are equivalent: \n\n```{r}\n#| eval: false\nlm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)\nlm(y ~ 1 + x1*x2, data = dataset)\n```\n\n:::\n\n:::imp\n\nIf we fit the interaction `x1:x2`, we almost _always_ want to also fit the separate effects `x1` and `x2`.  \n\n_\"Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of $x_1$ depends on the level of $x_2$ but that there is no effect of $x_1$.\"_ [Ramsey & Schafer](https://www.cengage.uk/c/the-statistical-sleuth-3e-ramsey-schafer/9781133490678/){target=\"_blank\"}\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Interpretation\n\nWhen we include an interaction term in our model, we are saying that two effects of the two predictors on the outcome are _dependent upon_ one another. This means that with an interaction in the model `lm(y ~ x1 + x2 + x1:x2)` we can no longer talk about the \"effect of $x_1$ on $y$_'holding $x_2$ constant'_\". Instead we have to talk about **marginal effects** - e.g. the effect of $x_1$ _at a **specific** value_ of $x_2$.   \n\nWhen we fit a model with an interaction in R, we get out coefficients for both predictors, and for the interaction. The coefficients for each individual predictor reflect the effect on the outcome _when the other predictor is zero_. \n\n```\nlm(formula = y ~ x1 * x2, data = df)\n...\n...\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  ...        ....       ...      ...  \nx1           ...        ....       ...      ...  \nx2           ...        ....       ...      ...  \nx1:x2        ...        ....       ...      ...  \n---\n```\n\n\n:::sticky\n**marginal effects when the other predictor is zero**  \n\nThe individual coefficients for each predictor that is involved in an interaction are estimated _when the other predictor in the interaction is zero._  \n\n:::\n\n\nThese coefficients can be interpreted, in turn as:  \n\n| Coefficient      | Interpretation |\n| ----------- | ----------- |\n| `(Intercept)` | the estimated $y$ when all predictors ($x_1$ and $x_2$) are zero is [*estimate*] |\n| `x1`  | **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a [*estimate*] change in $y$ |\n| `x2`  | **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a [*estimate*] change in $y$. |\n| `x1:x2`  | as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by [*estimate*]<br>_**or**_<br>as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by [*estimate*] |\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n#### What if there are other things in the model too?\n\nNote that the interaction `x1:x2` changes how we interpret the individual coefficients for `x1` and `x2`.  \n\nIt does __*not*__ change how we interpret coefficients for other predictors that might be in our model. For variables that **aren't** involved in the interaction term, these are still held constant.  \n\nFor example, suppose we _also_ had another predictor $c_1$ in our model: \n```\nlm(y ~ c1 + x1 + x2 + x1:x2)\n```\n\n| Coefficient      | Interpretation |\n| ----------- | ----------- |\n| `(Intercept)` | the estimated $y$ when all predictors ($c_1$, $x_1$ and $x_2$) are zero is [*estimate*] |\n| `c1` | a 1 unit increase in $c_1$ is associated with a [*estimate*] increase in $y$, holding constant all other variables in the model ($x_1$ and $x_2$) |\n| `x1`  | holding $c_1$ constant, **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a [*estimate*] change in $y$ |\n| `x2`  | holding $c_1$ constant, **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a [*estimate*] change in $y$. |\n| `x1:x2`  | holding $c_1$ constant, as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by [*estimate*]<br>_**or**_<br>holding $c_1$ constant, as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by [*estimate*] |\n\n\n:::\n\n\n\n<br><br>\n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  \n\n```{r}\n# data\nptdat <- read_csv(\"https://uoepsy.github.io/data/usmr_partnertime.csv\")\n# model\neg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = ptdat)\nsummary(eg1mod)\n```\n\n- For someone who spends 0 hours with their partner, and who has a relationship quality score of 0, their estimated wellbeing is `r round(coef(eg1mod)[1],2)`.  \n- For someone who has a relationship quality score of 0, every hour spent with their partner is associated with a `r round(coef(eg1mod)[2],2)` change in wellbeing.  \n- For someone who spends 0 hours with their partner, an increase of 1 in the relationship quality is associated with a `r round(coef(eg1mod)[3],2)` change in wellbeing.  \n- For every 1 increase in relationship quality, an extra hour spent with their partner is associated with an _additional_ `r round(coef(eg1mod)[4],2)` change in wellbeing. \n\n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  \n\n```{r}\n# data\nairpol <- read_csv(\"https://uoepsy.github.io/data/usmr_airpol.csv\")\n# model\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\nsummary(eg2mod)\n```\n\n- For people who are APOE4 negative (the reference level, see [8A#multiple-categories-multiple-regression](08a_mlr.html#multiple-categories-multiple-regression){target=\"_blank\"}), living where the Air Quality Pollution Index is 0 (no pollution), the estimated score on the MMSE (mini mental state exam) is `r round(coef(eg2mod)[1],2)`.  \n- For people who are APOE4 negative, a 1 unit increase in air pollution is associated with a `r round(coef(eg2mod)[2],2)` change in MMSE scores.  \n- For people living with 0 air pollution, being APOE4 positive is associated with `r round(coef(eg2mod)[3],2)` lower MMSE scores compared to APOE4 negative\n- Compared to APOE4 negative, being APOE4 positive is associated with `r round(coef(eg2mod)[4],2)` greater decrease in MMSE scores for every 1 increase in air pollution.\n\n\n:::\n:::panel\n#### Example 3\n\n> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.\n\n```{r}\n# data\ncandy <- read_csv(\"https://uoepsy.github.io/data/usmr_candy.csv\")\n# model\neg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)\nsummary(eg3mod)\n```\n\n- An anonymous child trick or treating alone is estimated to take `r round(coef(eg3mod)[1])` candybars.\n- When trick or treating alone, compared to an anonymous child, an identifiable child is estimated to take `r abs(round(coef(eg3mod)[2]))` fewer candybars.  \n- When anonymous, compared to trick or treating alone, children who are part of a group are estimated to take `r round(coef(eg3mod)[3],1)` more candybars.  \n- The difference between identifiable and anonymous children's candybar taking is `r abs(round(coef(eg3mod)[4]))` fewer when children are trick or treating in a group than when they are alone.  \n\n\n```{r}\n#| eval: false\n#| echo: false\nfit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))\nsjPlot::plot_model(fit, type=\"int\",show.data=T)+geom_line()\n\n# it's just viewing that cube from one side \nlibrary(scatterplot3d)\nplt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,\n                                 x.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 y.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 main=\"y~x1*x2\\n(x1 and x2 are categorical)\"))\nfit = lm(y~x1*x2,catcat)\npp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))\npp$y <- predict(fit, pp)\n\npp1 <- pp[pp$x2==0,]\npp2 <- pp[pp$x2==1,]\npp3 <- pp[pp$x1==0,]\npp4 <- pp[pp$x1==1,]\nplt$points(pp1$x1,pp1$x2,pp1$y,type=\"l\")\nplt$points(pp2$x1,pp2$x2,pp2$y,type=\"l\")\nplt$points(pp3$x1,pp3$x2,pp3$y,type=\"l\")\nplt$points(pp4$x1,pp4$x2,pp4$y,type=\"l\")\n```\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Visualisation\n\nWe have seen lots of 3-dimensional plots above to try and help in building our intuition about how we model an interaction. However, we typically don't really want to visualise models like this (in part because our models often have more than 2 predictors, and so have more than 3 dimensions).  \n\nWhat we therefore need to do is find ways to represent these relationships in 2-dimensional plots. This is slightly different depending on the type of variables we are dealing with. \n\n- For a continuous $\\times$ categorical interaction, we can plot the association of the continuous predictor with the outcome *for each level* of the categorical variable. \n- For a continuous $\\times$ continuous interaction, we can plot the association of one predictor *at some judiciously chosen values* of the other (e.g. at the min, mean and max, or at -1 SD, mean, and +1 SD).  \n- For a categorical $\\times$ categorical interaction, we can plot the various estimated group means, with optional dotted^[dotted is a good way to indicate that there is no data across that line - it is linking two categories. Martin disagrees and thinks you should never have lines] lines to illustrate the non-parallelism   \n\n\n::: {.callout-note collapse=\"true\"}\n#### optional helper packages\n\nWe can make these sort of plots very easily using handy packages that are designed to make our lives easier. The _big_ downsides are that reliance on these packages will a) make it harder to customise plots, b) make it harder to troubleshoot when things go wrong, and c) stop us from having to *think* (and thinking is always good!).  \n\n**Use with caution:**    \n\n- From the **sjPlot** package: `plot_model(model, type = \"int\")` or `plot_model(model, type = \"eff\", terms = c(\"x1\",\"x2\"))`  \n- From the **interactions** package: `interact_plot(model, pred = \"x1\", modx = \"x2\")`, and `cat_plot()` for categorical interactions.     \n\n\n:::\n\n\n\n:::panelset\n\n:::panel\n#### Example 1\n\n> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  \n\nPlotting continuous by continuous interactions requires choosing a set of values for one of our predictors, at which we plot the slope of the other.  \n\nHere, we could choose to plot the slope of `wellbeing ~ partner_time` for the mean `relationship_qual`, and for 1 SD above and below that mean.  \n\n```{r}\n#| code-fold: true\n# data\nptdat <- read_csv(\"https://uoepsy.github.io/data/usmr_partnertime.csv\")\n# model\neg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = ptdat)\n\n# plot data\nplotdat <- expand_grid(\n  partner_time = 0:13, #min to max of sample\n  relationship_qual = c(\n    mean(ptdat$relationship_qual)-sd(ptdat$relationship_qual),\n    mean(ptdat$relationship_qual),\n    mean(ptdat$relationship_qual)+sd(ptdat$relationship_qual)\n  )\n)\n# plot\nbroom::augment(eg1mod, newdata = plotdat, interval=\"confidence\") |>\n  ggplot(aes(x=partner_time, y=.fitted, \n             col=relationship_qual,\n             group=relationship_qual))+\n  geom_point(data = ptdat,aes(y=wellbeing))+\n  geom_line()+\n  geom_ribbon(aes(ymin=.lower,ymax=.upper,fill=relationship_qual),\n              alpha=.2)\n\n```\n\n<!-- ::: {.callout-tip collapse=\"true\"} -->\n<!-- #### optional helper packages -->\n\n<!-- The interactions package defaults to showing the y~x1 relationship when x2 is at -1 SD below the mean, at the mean, and +1 SD above the mean.   -->\n<!-- ```{r} -->\n<!-- library(interactions) -->\n<!-- interact_plot(eg1mod, pred = \"partner_time\", modx = \"relationship_qual\", interval = TRUE) -->\n<!-- ``` -->\n\n<!-- The __sjPlot__ package can do this too, but wil default to showing it at the min and max of the other predictor.   -->\n<!-- ```{r} -->\n<!-- library(sjPlot) -->\n<!-- plot_model(eg1mod, type=\"int\") -->\n<!-- ``` -->\n<!-- We can change these manually. For instance, to make `plot_model` show the association when relationship quality is zero, when it is 50, and when it is 100, we could use:  -->\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- plot_model(eg1mod, type = \"eff\", terms=c(\"partner_time\",\"relationship_qual [0, 50, 100]\")) -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n\n:::\n:::panel\n#### Example 2\n\n> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  \n\nThis is the most straightforward type of interaction to visualise, as there are only a set number of values that one of the variables can take, meaning only a finite number of lines we need to draw:  \n\n```{r}\n#| code-fold: true\n# data\nairpol <- read_csv(\"https://uoepsy.github.io/data/usmr_airpol.csv\")\n# model\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\n\n# plot data\nplotdat <- expand_grid(\n  aqpi = 0:500,\n  apoe4 = c(\"neg\",\"pos\")\n)\n# plot\nbroom::augment(eg2mod, newdata = plotdat, interval=\"confidence\") |>\n  ggplot(aes(x=aqpi, y = .fitted, \n             col = apoe4, fill = apoe4)) + \n  geom_point(data = airpol, aes(y = mmse), alpha=.3, size=3) +\n  geom_line() +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)\n\n```\n\n\n<!-- ::: {.callout-tip collapse=\"true\"} -->\n<!-- #### optional helper packages -->\n\n<!-- ```{r} -->\n<!-- p1 <- sjPlot::plot_model(eg2mod, type = \"int\") -->\n<!-- p2 <- interactions::interact_plot(eg2mod, pred = \"aqpi\", modx = \"apoe4\", interval = TRUE) -->\n<!-- p1 / p2 -->\n<!-- ``` -->\n<!-- ::: -->\n\n\n:::\n:::panel\n#### Example 3\n\n> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.\n\nFor these types of interactions (between categorical variables), plotting the estimates from our model is best done as our set of estimated group means.   \n```{r}\n#| code-fold: true\n# data\ncandy <- read_csv(\"https://uoepsy.github.io/data/usmr_candy.csv\")\n# model\neg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)\n\n# plot data\nplotdat <- expand_grid(\n  anonymity = c(\"anonymous\",\"identifiable\"),\n  asgroup = c(\"alone\",\"group\")\n)\n\n# plot\nbroom::augment(eg3mod, newdata=plotdat, interval=\"confidence\") |>\n  ggplot(aes(x = anonymity, col = asgroup)) + \n  geom_jitter(data = candy, aes(y = candybars),\n              alpha = .3, size = 3,\n              height = 0, width = .2) +\n  geom_pointrange(aes(y=.fitted, ymin=.lower, ymax=.upper),\n                  position = position_dodge(width=.2))\n```\n\n\n<!-- ::: {.callout-tip collapse=\"true\"} -->\n<!-- #### optional helper packages -->\n\n<!-- ```{r} -->\n<!-- p1 <- sjPlot::plot_model(eg3mod, type = \"int\")  -->\n<!-- p2 <- interactions::cat_plot(eg3mod, pred = \"anonymity\", modx = \"asgroup\", interval = TRUE) -->\n\n<!-- p1 / p2 -->\n<!-- ``` -->\n\n\n<!-- ::: -->\n\n\n\n```{r}\n#| eval: false\n#| echo: false\nfit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))\nsjPlot::plot_model(fit, type=\"int\",show.data=T)+geom_line()\n\n# it's just viewing that cube from one side \nlibrary(scatterplot3d)\nplt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,\n                                 x.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 y.ticklabs = c(0,NA,NA,NA,NA,1), \n                                 main=\"y~x1*x2\\n(x1 and x2 are categorical)\"))\nfit = lm(y~x1*x2,catcat)\npp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))\npp$y <- predict(fit, pp)\n\npp1 <- pp[pp$x2==0,]\npp2 <- pp[pp$x2==1,]\npp3 <- pp[pp$x1==0,]\npp4 <- pp[pp$x1==1,]\nplt$points(pp1$x1,pp1$x2,pp1$y,type=\"l\")\nplt$points(pp2$x1,pp2$x2,pp2$y,type=\"l\")\nplt$points(pp3$x1,pp3$x2,pp3$y,type=\"l\")\nplt$points(pp4$x1,pp4$x2,pp4$y,type=\"l\")\n```\n\n:::\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Getting more from your model\n\nReporting on interactions is a bit like telling a story. Listing the interpretation of the coefficients is fine, but often reads a bit awkwardly. A good presentation of the results would provide the reader with the overall pattern of results, pulling out the key parts that are of interest. There are various things we can do to get out coefficients that might be of more use to us in telling our story.  \n\n## Mean Centering\n\nBy mean centering a continuous predictor, we change what \"0\" means. Normally, when we don't have an interaction, this simply changes the intercept value (see [8A #centering-and-scaling-predictors](08a_scaling.html#centering-and-scaling-predictors){target=\"_blank\"}). If we have the interaction `y ~ x1 + x2 + x1:x2`, then mean centering `x1` will make the coefficient for `x2` now represent \"the association between $x_2$ and $y$ for someone at the _average_ of $x_1$\". \n\nUsing one of our examples from throughout this reading, we might mean-center the air-pollution so that we can consider the difference in MMSE scores between APOE4 positive and negative people _at the average air-pollution level_\n\n```{r}\n# data\nairpol <- read_csv(\"https://uoepsy.github.io/data/usmr_airpol.csv\")\n# model with original variable\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\n# model with mean centered predictor\neg2mod_cent <- lm(mmse ~ scale(aqpi,scale=FALSE) * apoe4, data = airpol)\n```\n\n```{r}\n# coefficients:\ncoef(eg2mod)\ncoef(eg2mod_cent)\n```\n\nThis is because the coefficient for APOE4 compares the heights of the two lines when the other predictor is zero. So if we change what \"zero\" represents, we can change what that estimates. In the model plots below, we can see that _the model doesn't change_, it is just extracting different information (it is the distance to move from the blue dot to the red dot):  \n\n```{r}\n#| out-width: \"100%\"\n#| echo: false\nairpol <- airpol %>% \n  mutate(\n    aqpiC = aqpi - mean(aqpi)\n  )\neg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)\neg2mod_cent <- lm(mmse ~ aqpiC * apoe4, data = airpol)\nsjPlot::plot_model(eg2mod, type=\"int\") +\n  geom_point(x=0,y=coef(eg2mod)[1], size=4, col=\"blue\")+\n  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, col=\"red\")+\n  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Raw AQPI\") +\n  \nsjPlot::plot_model(eg2mod_cent, type=\"int\") +\n  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, col=\"blue\")+\n  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, col=\"red\")+\n  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Mean Centered AQPI\") +\n  plot_layout(guides=\"collect\")\n```\n\n## Relevelling Factors\n\nAnother thing that can be useful (especially when working with categorical variables with lots of levels) is to make sure your variables are `factors` in R, and to set a useful reference level. Typically, the reference level is what we think of as \"normal\", e.g. if we have 3 groups: Placebo, Drug A, and Drug B, then we might compare each drug to the placebo condition, because that's comparable to most people (i.e. who aren't taking the drug).\n\nFor example, when we have two categorical variables:  \n\n- anonymity = \"anonymous\" vs \"identifiable\"\n- asgroup = \"alone\" vs \"group\"\n\nThen the default is to take the alphabetical ordering. We can change the ordering using functions that \"relevel\" a factor. Note, this only works if the variable is _already_ a `factor` in R (see [2A#categorical](02a_measurement.html#categorical){target=\"_blank\"} for a reminder of 'factors').   \n\n```{r}\n# data \ncandy <- read_csv(\"https://uoepsy.github.io/data/usmr_candy.csv\")\n\n# make both predictors 'factors'\ncandy <- \n  candy %>% \n  mutate(\n    anonymity = factor(anonymity),\n    asgroup = factor(asgroup)\n  )\n```\n\nOnce they are factors, we can see the default levels:  \n\n:::: {.columns}\n::: {.column width=\"47.5%\"}\n```{r}\nlevels(candy$anonymity)\n```\n:::\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n::: {.column width=\"47.5%\"}\n```{r}\nlevels(candy$asgroup)\n```\n:::\n\n::::\n\n\nThis is the original model, with the default levels:  \n```{r}\neg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)\ncoef(eg3mod)\n```\n\nLet's relevel anonymity to have \"identifiable\" as the first level, and then refit the model:  \n```{r}\ncandy <- \n  candy %>% \n  mutate(\n    anonymity = fct_relevel(anonymity, \"identifiable\")\n  )\n\neg3mod_rel <- lm(candybars ~ anonymity * asgroup, data = candy)\n\ncoef(eg3mod_rel)\n```\n\nAgain, the model doesn't change, we are simply extracting different bits from it:  \n\n::::panelset\n:::panel\n#### reference level = \"anonymous\"\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\np1 <- sjPlot::plot_model(eg3mod, type = \"int\") +\n  labs(title=\"with 'anonymous' and 'alone' as reference levels\") + \n  \n  \n  geom_segment(x = 1, xend = 1, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,3)]), \n                col=\"green3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 1, xend = 1.95, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,2)]), \n                col=\"orange3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 1.05, xend = 2.05, y = sum(coef(eg3mod)[c(1,3)]), yend=sum(coef(eg3mod)[c(1,2,3)]), \n                col=\"black\", lty=\"dotted\", lwd=.5,alpha=.2) +\n  \n  geom_segment(x = 2.05, xend = 2.05, y = sum(coef(eg3mod)[c(1:3)]), yend=sum(coef(eg3mod)[c(1:4)]), \n                col=\"purple\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  \n  annotate(geom=\"label\", x=1.1,y=coef(eg3mod)[1],label=\"Intercept\")+\n  annotate(geom=\"label\", x=1.01,col=\"green3\", hjust=0,y=coef(eg3mod)[1]+2,label=\"'asgroup' coefficient\") +\n  annotate(geom=\"label\",col=\"orange3\", x=1.5,y=coef(eg3mod)[1]-1,label=\"'anonymity' coefficient\") +\n  annotate(geom=\"label\",col=\"purple\", x=2.04,hjust=1,y=coef(eg3mod)[1],label=\"interaction coefficient\") \n\np1 + theme_bw(base_size = 12)\n\n```\n\n:::\n:::panel\n#### reference level = \"identifiable\"\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\np2 <- sjPlot::plot_model(eg3mod, type = \"int\") +\n  labs(title=\"with 'identifiable' and 'alone' as reference levels\") + \n  \n  geom_segment(x = 1.95, xend = 1.95, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,3)]), \n                col=\"green3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 1.95, xend = 1, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,2)]), \n                col=\"orange3\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_segment(x = 2, xend = 1, y = sum(coef(eg3mod_rel)[c(1,3)]), yend=sum(coef(eg3mod_rel)[c(1,2,3)]), \n                col=\"black\", lty=\"dotted\", lwd=.5, alpha=.2) +\n  \n  geom_segment(x = 1, xend = 1, y = sum(coef(eg3mod_rel)[c(1:3)]), yend=sum(coef(eg3mod_rel)[c(1:4)]), \n                col=\"purple\", lwd=1,\n                arrow = arrow(length = unit(0.5, \"cm\"))) +\n  \n  annotate(geom=\"label\", x=2,hjust=0,y=coef(eg3mod_rel)[1],label=\"Intercept\")+\n  annotate(geom=\"label\", x=1.93,col=\"green3\", hjust=1,y=coef(eg3mod_rel)[1]-.25,label=\"'asgroup' coefficient\") +\n  annotate(geom=\"label\",col=\"orange3\", x=1.5,y=coef(eg3mod_rel)[1]+1,label=\"'anonymity' coefficient\") +\n  annotate(geom=\"label\",col=\"purple\", x=1.01,hjust=0,y=coef(eg3mod_rel)[1]+4,label=\"interaction coefficient\")\n\n\np2 + theme_bw(base_size = 12)\n```\n:::\n::::\n\n<!-- :::sticky -->\n<!-- __Mapping model estimates to model plots__   -->\n\n<!-- To help with interpretation of the estimated coefficients from a model, a very useful exercise is to try and map them to what we see in our model plots (as seen above).   -->\n\n<!-- As we've talked about already - as models get more complex, there is a very important distinction to make between plotting the data and plotting the model estimates.   -->\n<!-- Whereas previously we could plot a simple regression `lm(y~x)` by plotting the _data_ and adding `geom_smooth(method=lm)`, now that we are working with multiple regression in order to visualise our results we should think of it as plotting the _model_ (and we can add the data if desired).   -->\n\n<!-- ::: -->\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: Interactions and Sum Contrasts  \n\n```{r}\n#| include: false\n# DATA ----\nset.seed(8653)\ndf <- expand_grid(\n  grouping = c(\"1\",\"2\"),\n  condition = c(\"A\",\"B\"),\n  n = 1:30\n) %>%\n  mutate(\n    lp = 2 + -2*(grouping==\"2\") + 1*(condition==\"B\") +\n      -2*(grouping==\"2\" & condition==\"B\"),\n    y = lp + rnorm(n())\n  ) %>% mutate_if(is.character,factor)\n\n# group means and marginals ----\n# tab <- xtabs( ~ grouping + condition, data = df)\n# N <- addmargins(tab)\n# addmargins(xtabs(y ~ grouping + condition, data = df)) / N\n\nmodel <- lm(y ~ condition * grouping, df)\n\n# change contrasts (sum contrasts drops the first level, so we'll relevel it to drop group2 and condB)\ndf$grouping = fct_relevel(df$grouping, \"2\")\ncontrasts(df$grouping) <- \"contr.sum\"\ndf$condition = fct_relevel(df$condition, \"B\")\ncontrasts(df$condition) <- \"contr.sum\"\n\nmodel1 <- lm(y ~ condition * grouping, df)\n\n\n\n\nmodplot <- as.data.frame(effects::effect(\"condition * grouping\",model)) %>%\n  ggplot(.,aes(x=condition, y=fit, ymin=lower,ymax=upper))+\n  geom_pointrange(aes(col=grouping,shape=condition), size=1.5,alpha=.8) +\n  geom_line(aes(col=grouping,group=grouping), lty=\"dashed\",linewidth=1)+\n  # labs(title=\"The model\", subtitle=\"lm()\") + \n  theme_classic()\n```\n\n\nWhen we have categorical predictors, our choice of contrasts coding changes the bits that we're getting our of our model.  \n\nSuppose we have a 2x2 design (condition A and B, in groups 1 and 2):  \n\n```{r}\n#| label: fig-sumplot\n#| fig-cap: \"Categorical x Categorical Interaction plot\"\n#| echo: false\nmodplot\n```\n\nWhen we are using the default contrasts coding (treatment - [see 8B #contrasts](08b_catpred.html#contrasts){target=\"_blank\"}) in R, then our coefficients for the individual predictors represent moving between the dots in @fig-sumplot.  \n\n```\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            1.9098     0.1759  10.855  < 2e-16 ***\nconditionB             1.1841     0.2488   4.759 5.65e-06 ***\ngrouping2             -1.6508     0.2488  -6.635 1.09e-09 ***\nconditionB:grouping2  -2.1627     0.3519  -6.146 1.15e-08 ***\n---\n```\n\n- The intercept is the red circle in @fig-sumplot.  \n- The coefficient for condition is the difference between the red circle and the red triangle in @fig-sumplot.  \n- The coefficient for grouping is the difference between the red circle and the blue circle in @fig-sumplot.  \n- The interaction coefficient is the difference from the slope of the red line to the slope of the blue line.  \n\n\nHowever, when we change to using sum contrasts, we're switching where zero is in our model. So if we change to sum contrasts (here we've changed __both__ predictors to using sum contrasts), then we end up estimating the effect of each predictor averaged across the other.  \n\n```\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           1.13577    0.08796  12.912  < 2e-16 ***\nconditionB            0.05141    0.08796   0.584     0.56    \ngrouping2            -1.36607    0.08796 -15.530  < 2e-16 ***\nconditionB:grouping2 -0.54066    0.08796  -6.146 1.15e-08 ***\n---\n```\n\n- The intercept is the grey X in @fig-sumplot2.  \n- The coefficient for condition is the difference between the grey X and the grey triangle in @fig-sumplot2.  \n- The coefficient for grouping is the difference between the grey X and the blue line in @fig-sumplot2.  \n- The interaction coefficient is the difference from the slope of the grey line to slope of the blue line.  \n\n```{r}\n#| echo: false\n#| label: fig-sumplot2\n#| fig-cap: \"Visualisation of sum-contrasts for categorical x categorical interaction plot\"\nmodplot + \n  geom_segment(x = 1, xend = 2, \n               y = (coef(model1)[1]-coef(model1)[2]), \n               yend = sum(coef(model1)[c(1,2)]),\n               lty = \"dashed\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = sum(coef(model1)[c(1,3)]),\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = coef(model1)[1]-coef(model1)[3],\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_point(x=1,\n             y=(coef(model1)[1]-coef(model1)[2]),\n             size = 7, alpha = .05,\n             aes(shape=\"A\")) +\n  geom_point(x=2,\n             y=sum(coef(model1)[1:2]),\n             size = 7, alpha = .05,\n             aes(shape=\"B\")) +\n  geom_point(x=1.5,\n             y=coef(model1)[1],\n             size = 7, alpha = .2,\n             shape=4) \n\n```\n\n\nIt can get quite confusing when we start switching up the contrasts, but it's all just because we're changing what \"zero\" means, and what \"moving 1\" means:  \n\n::::panelset\n:::panel\n#### Treatment contrasts\n\n```{r}\n#| echo: false\ndd = tibble(\n  #lab=letters[1:4],\n  clab=c(1,1,0,0),\n  glab=c(1,0,1,0),\n  lab=paste0(\"condition: \",clab,\"\\ngroup: \",glab),\n  condition=c(2,2,1,1),\n  ff=c(\n    #c(1,0,0,0) %*% coef(model1),\n    #c(1,1,0,0) %*% coef(model1),\n    #c(1,0,1,0) %*% coef(model1),\n    c(1,1,1,1) %*% coef(model1),\n    #c(1,-1,0,0) %*% coef(model1),\n    #c(1,0,-1,0) %*% coef(model1),\n    c(1,1,-1,-1) %*% coef(model1),\n    c(1,-1,1,-1) %*% coef(model1),\n    c(1,-1,-1,1) %*% coef(model1)\n  )\n)\nmodplot+\n  geom_label(inherit.aes=F,data=dd,aes(x=condition,\n                                       y=ff,label=lab),alpha=.8)+\n  labs(subtitle=\"y~condition*group with treatment contrasts\")\n```\n\n\n\n:::\n:::panel\n#### Sum contrasts\n\n```{r}\n#| echo: false\ndd = tibble(\n  #lab=letters[1:9],\n  clab=c(0,1,0,1,-1,0,1,-1,-1),\n  glab=c(0,0,1,1,0,-1,-1,1,-1),\n  lab=paste0(\"condition: \",clab,\"\\ngroup: \",glab),\n  condition=c(1.5,2,1.5,2,1,1.5,2,1,1),\n  ff=c(\n    c(1,0,0,0) %*% coef(model1),\n    c(1,1,0,0) %*% coef(model1),\n    c(1,0,1,0) %*% coef(model1),\n    c(1,1,1,1) %*% coef(model1),\n    c(1,-1,0,0) %*% coef(model1),\n    c(1,0,-1,0) %*% coef(model1),\n    c(1,1,-1,-1) %*% coef(model1),\n    c(1,-1,1,-1) %*% coef(model1),\n    c(1,-1,-1,1) %*% coef(model1)\n  )\n)\nmodplot+\n  geom_segment(x = 1, xend = 2, \n               y = (coef(model1)[1]-coef(model1)[2]), \n               yend = sum(coef(model1)[c(1,2)]),\n               lty = \"dashed\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = sum(coef(model1)[c(1,3)]),\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_segment(x = 1.5, xend = 1.5, \n               y = coef(model1)[1], yend = coef(model1)[1]-coef(model1)[3],\n               lty = \"dotted\", linewidth = 1, alpha = .05, \n               col=\"black\") +\n  geom_point(x=1,\n             y=(coef(model1)[1]-coef(model1)[2]),\n             size = 7, alpha = .05,\n             aes(shape=\"A\")) +\n  geom_point(x=2,\n             y=sum(coef(model1)[1:2]),\n             size = 7, alpha = .05,\n             aes(shape=\"B\")) +\n  geom_point(x=1.5,\n             y=coef(model1)[1],\n             size = 7, alpha = .2,\n             shape=4) + \n  geom_label(inherit.aes=F,data=dd,aes(x=condition,\n                                       y=ff,label=lab),alpha=.8)+\n  labs(subtitle=\"y~condition*group with sum contrasts\")\n```\n\n:::\n::::\n\n\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional Extra: Getting non-linear  \n\nYou might be noticing that when we start talking about our regression surface \"twisting\" (e.g. in @fig-i.contcont), we're starting to see curves in our model.  \n\nWe can model _curves_ in a \"linear\" model!?  \n\nAn interaction does in a sense introduce non-linearity to our thinking, because there we no longer think of a linear effect of $x_1$ (it \"depends on\" $x_2$). This a little bit of a trick, because ultimately our model is still linear - we are estimating our outcome $y$ as the _linear combination_ of a set of predictors. In the model $y = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2$, the adjustment that we make $b_3$ to each of the coefficients $b_1$ and $b_2$ is a constant.  \n\nWe can even exploit this to model more clearly \"non-linear\" associations, such as the age and height example below.  \n\n\n\n```{r}\n#| echo: false\n#| label: fig-nonlin\n#| fig-cap: \"Two linear models, one with a quadratic term (right)\"  \n#| out-width: \"100%\"\nset.seed(13)\ndf <- tibble(\n  age = sample(0:16,100,replace=T),\n  height = 70 + (7*age) - (17*scale(age)^2) + rnorm(100,0,10)\n)\nmod1 <- lm(height ~ age, data = df)\nmod2 <- lm(height ~ age + I(age^2), data = df)\ndf <- df %>% mutate(f1 = fitted(mod1),f2=fitted(mod2))\n\nggplot(df,aes(x=age,y=height))+\n  geom_point() + \n  geom_line(aes(y=f1),lwd=1, col=\"blue\")  +\n  labs(title=\"lm(height ~ age)\") + \n  \n  ggplot(df,aes(x=age,y=height))+\n  geom_point() + \n  geom_line(aes(y=f2),lwd=1,col=\"blue\") +\n  labs(title=\"lm(height ~ age + I(age^2))\") &     \n  theme_bw(base_size = 12)\n```\n\nWe will cover this a lot more in the multivariate stats course, so don't worry too much about it right now.  \nHowever, it's useful as a means of seeing how we can extend linear models to fit these sort of relationships. In the model on the right of @fig-nonlin, the model returns three coefficients:\n```{r}\n#| echo: false\n.pp(summary(mod2),l=list(10:13))\n```\n\nIt is estimating a persons' height as:\n\n- an intercept of `r coef(mod2)[1] %>% round(2)` (the estimated height of a newborn baby of age 0)\n- plus `r coef(mod2)[2] %>% round(2)` cm for every year of age they have\n- plus `r coef(mod2)[3] %>% round(2)` cm for every year of age they have, squared.  \n\nSo for a 4 year old, the estimated height is: \n\n$$\nheight = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[2] %>% round(2)` \\times 4) + (`r coef(mod2)[3] %>% round(2)` \\times 4^2) = `r round(coef(mod2),1) %*% c(1,4,16)`\n$$\nand for a 10 year old, it is:  \n\n$$\nheight = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[2] %>% round(2)` \\times 10) + (`r coef(mod2)[3] %>% round(2)` \\times 10^2) = `r round(coef(mod2),1) %*% c(1,10,100)`\n$$\nWe can see how the quadratic \"$\\text{age}^2$ term has a larger (negative) effect as age increases (for the 4 year old it makes the estimated height $`r coef(mod2)[3] %>% round(2)` \\times 4^2 = `r round(coef(mod2)[3]*4^2,1)`$ lower, and for the 10 year old it makes the estimated height $`r coef(mod2)[3] %>% round(2)` \\times 10^2 = `r round(coef(mod2)[3]*10^2,1)`$ lower). This captures the plateauing of heights as children get older.  \n\n:::\n\n<!-- # Optional: further exploration -->\n\n<!-- ## Simple Slopes -->\n\n<!-- we've kind of already done it visually.   -->\n<!-- plot the slope of y~x1 for certain values of x2.   -->\n\n<!-- we've done this visually, but it would be nice to get out a \"is y~x1 significant when x2 = ?\"   -->\n\n\n\n\n<!-- ## Johnson Neyman -->\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"09a_interactions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"code-annotations":"hover","link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"9A: Interactions","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}