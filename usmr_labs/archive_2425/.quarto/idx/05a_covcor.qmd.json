{"title":"5A: Covariance and Correlation","markdown":{"yaml":{"title":"5A: Covariance and Correlation","params":{"SHOW_SOLS":true,"TOGGLE":true}},"headingText":"Covariance","containsRefs":false,"markdown":"\n\n:::lo\nThis reading:  \n\n- How can we describe the relationship between two continuous variables?  \n- How can we test the relationship between two continuous variables?  \n\n:::\n\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nset.seed(017)\n```\n\n\n\nIn the last couple of weeks we have covered a range of the basic statistical tests that can be conducted when we have a single outcome variable, and sometimes also a single explanatory variable. Our outcome variables have been both continuous ($t$-tests) and categorical ($\\chi^2$-tests). We're going to look at one more relationship now, which is that between two continuous variables. This will also provide us with our starting point for the second block of the course.  \n\n```{r}\n#| echo: false\ntribble(\n  ~outcome, ~explanatory, ~test, ~examines,\n  \"continuous\",\"\",\"t.test(y, mu = ?)\",\"is the mean of y different from [specified value]?\",\n  \"continuous\",\"binary\",\"t.test(y ~ x)\",\"is the mean of y different between the two groups?\",\n  \"categorical\",\"\",\"chisq.test(table(y), prob = c(?,...,?))\",\"is the distribution of categories of y different from [specified proportions]?\",\n  \"categorical\",\"categorical\",\"chisq.test(table(y,x))\",\"is the distribution of categories of y dependent on the category of x?\",\n  \"continuous\",\"continuous\",\"???\",\"???\"\n) %>% knitr::kable() %>%\n  kableExtra::row_spec(5,background=\"#F3E3E5\")\n```\n\n\n\n\n\n:::frame\n> **Research Question:** Is there a correlation between accuracy and self-perceived confidence of memory recall?  \n\nOur data for this walkthrough is from a (hypothetical) study on memory. Twenty participants studied passages of text (c500 words long), and were tested a week later. The testing phase presented participants with 100 statements about the text. They had to answer whether each statement was true or false, as well as rate their confidence in each answer (on a sliding scale from 0 to 100). The dataset contains, for each participant, the percentage of items correctly answered, and the average confidence rating. Participants' ages were also recorded.   \n\nThe data are available at [\"https://uoepsy.github.io/data/recalldata.csv](https://uoepsy.github.io/data/recalldata.csv).  \n\n:::\n\nLet's take a look visually at the relationships between the percentage of items answered correctly (`recall_accuracy`) and participants' average self-rating of confidence in their answers (`recall_confidence`). Let's also look at the relationship between accuracy and age.  \n\n```{r}\n#| message: false\n#| warning: false\nlibrary(tidyverse)\nlibrary(patchwork)\n\nrecalldata <- read_csv('https://uoepsy.github.io/data/recalldata.csv')\n\nggplot(recalldata, aes(x=recall_confidence, recall_accuracy))+\n  geom_point() + \nggplot(recalldata, aes(x=age, recall_accuracy))+\n  geom_point()\n```\n\nThese two relationships look quite different.  \n\n+ For participants who tended to be more confident in their answers, the percentage of items they correctly answered tends to be higher.  \n+ The older participants were, the lower the percentage of items they correctly answered tended to be.  \n\nWhich relationship should we be more confident in and why? \n\nIdeally, we would have some means of quantifying the strength and direction of these sorts of relationship. This is where we come to the two summary statistics which we can use to talk about the association between two numeric variables: __Covariance__ and __Correlation__.  \n\n\nCovariance is the measure of how two variables vary together. \nIt is the change in one variable associated with the change in another variable.   \n\nCovariance is calculated using the following formula:\n\n$$\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}$$\nwhere:\n\n- $x$ and $y$ are two variables; e.g., `age` and `recall_accuracy`;\n- $i$ denotes the observational unit, such that $x_i$ is value that the $x$ variable takes on the $i$th observational unit, and similarly for $y_i$;\n- $n$ is the sample size.\n\n\nThis can initially look like quite a big equation, so let's break it down.  \n\n:::frame\n__Covariance explained visually__  \n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\nset.seed(7135)\ntibble(x=runif(10,20,80),y=rnorm(10,160,10)) %>%\n  mutate(y=y+x/2,\n         coldir = ifelse( (x>mean(x) & y>mean(y)) | (x<mean(x) & y<mean(y)), \"pos\",\"neg\")\n  ) -> df\n\np1<-ggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()\n  \np2<-ggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)+1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))\n  \np3<-ggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))+\n  annotate(\"text\",x=mean(df$x)+7, y=202,label=expression(x[i]-bar(\"x\")))+\n  annotate(\"text\",x=71.5, y=mean(df$y)+7,label=expression(y[i]-bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color=\"tomato1\",data = df)\n\n\np4<-ggplot(df,aes(x=x,y=y))+\n  geom_point(aes(col=coldir))+\n  scale_color_manual(\"\",values=c(\"black\",\"red\"))+\n  theme_classic()+\n  theme(legend.position = \"none\")+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color=\"tomato1\",data = df)\n\np5<-ggplot(df,aes(x=x,y=y))+\n  geom_point(aes(col=coldir))+\n  scale_color_manual(\"\",values=c(\"blue\",\"red\"))+\n  theme_classic()+\n  theme(legend.position = \"none\")+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 194.4188, xend = 45.32536, yend = 194.4188), color=\"skyblue3\", data = df)+\n  geom_segment(aes(x = 45.32536, y = mean(y), xend = 45.32536, yend = 194.4188), color=\"skyblue3\",data = df)+\n  geom_segment(aes(x = mean(x), y = 182.6440, xend = 66.73541, yend = 182.6440), color=\"skyblue3\", data = df)+\n  geom_segment(aes(x = 66.73541, y = mean(y), xend = 66.73541, yend = 182.6440), color=\"skyblue3\",data = df)\n```\n\nConsider the following scatterplot:  \n```{r}\n#| echo: false\n#| message: false\np1\n```\n<br>\nNow let's superimpose a vertical dashed line at the mean of $x$ ($\\bar{x}$) and a horizontal dashed line at the mean of $y$ ($\\bar{y}$):\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np2\n```\n<br>\nNow let's pick one of the points, call it $x_i$, and show $(x_{i}-\\bar{x})$ and $(y_{i}-\\bar{y})$.  \nNotice that this makes a rectangle.  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np3\n```\nAs $(x_{i}-\\bar{x})$ and $(y_{i}-\\bar{y})$ are both positive values, their product --- $(x_{i}-\\bar{x})(y_{i}-\\bar{y})$ --- is positive. \n\nIn fact, for any pair of values $x_i$ and $y_i$ where both values are above their respective means, or both are below, then product $(x_{i}-\\bar{x})(y_{i}-\\bar{y})$ will be positive (remember that a negative multiplied by a negative gives a positive):  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np4\n```\n\nAnd for all those other points (in blue) the product $(x_{i}-\\bar{x})(y_{i}-\\bar{y})$ is negative:  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np5\n```\n<br>\nNow let's take another look at the formula for covariance:  \n\n$$\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}$$\n  \nIt is the sum of all these products divided by $n-1$. It is the average of the products! You can almost think of this as the average area of all the rectangles!  \nSo for plots like this where we have been colouring them in blue or red, the measure of covariance moves up and down depending upon how much blue vs how much red there is! It will be zero when there is the same amount of blue as there is red. \n\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/cov.gif\")\n```\n\n```{r}\n#| eval: false\n#| include: false\nset.seed(899)\nmkd <- function(cv){\n  MASS::mvrnorm(n=40,mu=c(0,0),Sigma=Matrix::nearPD(matrix(c(40,cv,cv,70),nrow=2))$mat) |>\n    as.data.frame() |> \n    mutate(V1=V1+15,V2=V2+25)\n}\n\ninit = tibble(\n  frm = 0,\n  data = mkd(-50)\n) |> unnest()\nss = init |> transmute(\n  V1,\n  V2,\n  V2e = V2 - 2*coef(lm(V2~V1,init))[2]*V1,\n  frm = list(seq(1,100)),\n  V2l = map2(V2,V2e,~seq(..1,..2,length.out=100)),\n) |> unnest() |>\n  group_by(frm) |> \n  mutate(V2l = V2l - (mean(V2l)-25),\n         v1m=mean(V1),\n         v2m=mean(V2l),\n         cov = round(cov(V1,V2l),2),\n         cor = round(cor(V1,V2l),2),\n         col=case_when(\n           V1>v1m & V2l>v2m ~ \"p\",\n           V1<v1m & V2l<v2m ~ \"p\",\n           TRUE ~ \"n\"\n         )) |> ungroup()\n\n\nlibrary(gganimate)\nanimp <- ggplot(ss, aes(x=V1,y=V2l))+\n  geom_point(size=3)+\n  geom_hline(aes(yintercept=v2m),lty=\"dotted\")+\n  geom_vline(aes(xintercept=v1m),lty=\"dotted\")+\n  geom_segment(aes(x=V1,xend=V1,y=V2l,yend=v2m,col=col),alpha=.4)+\n  geom_segment(aes(x=V1,xend=v1m,y=V2l,yend=V2l,col=col),alpha=.4)+\n  geom_rect(aes(xmin=V1,xmax=v1m,ymin=V2l,ymax=v2m,fill=col),alpha=.2)+\n  scale_color_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  scale_fill_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  guides(col=\"none\",fill=\"none\")+\n  labs(x=\"X\",y=\"Y\", title=paste0(\"cov = \",\"{closest_state}\"))+\n  #facet_wrap(~frm)\n  transition_states(cov)+ease_aes('sine-in-out')\n\n\nss = init |> transmute(\n  V1,\n  V2,\n  V2e = V2 - 2*coef(lm(V2~V1,init))[2]*V1,\n  frm = list(seq(1,100)),\n  V2l = map2(V2,V2e,~seq(..1,..2,length.out=100)),\n) |> unnest() |>\n  group_by(frm) |> \n  mutate(\n    V1 = scale(V1)[,1],\n    V2l = scale(V2l)[,1],\n    v1m=mean(V1),\n         v2m=mean(V2l),\n         cov = round(cov(V1,V2l),2),\n         cor = round(cor(V1,V2l),2),\n         col=case_when(\n           V1>v1m & V2l>v2m ~ \"p\",\n           V1<v1m & V2l<v2m ~ \"p\",\n           TRUE ~ \"n\"\n         )) |> ungroup()\n\nanimp2 <- ggplot(ss, aes(x=V1,y=V2l))+\n  geom_point(size=3)+\n  geom_hline(aes(yintercept=v2m),lty=\"dotted\")+\n  geom_vline(aes(xintercept=v1m),lty=\"dotted\")+\n  geom_segment(aes(x=V1,xend=V1,y=V2l,yend=v2m,col=col),alpha=.4)+\n  geom_segment(aes(x=V1,xend=v1m,y=V2l,yend=V2l,col=col),alpha=.4)+\n  geom_rect(aes(xmin=V1,xmax=v1m,ymin=V2l,ymax=v2m,fill=col),alpha=.2)+\n  scale_color_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  scale_fill_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  guides(col=\"none\",fill=\"none\")+\n  labs(x=\"X\",y=\"Y\", title=paste0(\"cor = \",\"{closest_state}\"))+\n  #facet_wrap(~frm)\n  transition_states(cor)+ease_aes('sine-in-out')\n\nanim_save(\"images/cov.gif\",animp)\nanim_save(\"images/cor.gif\",animp2)\n```\n\n:::\n\n:::rtip\n**Covariance in R**  \n\nWe can calculate covariance in R using the `cov()` function.  \n`cov()` can take two variables `cov(x = , y = )`.  \n\n```{r}\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n  \nIf necessary, we can choose use only the complete observations (i.e. ignoring all rows where either value is an `NA`) by specifying:  \n```{r}\n#| eval: false\ncov(x = ..., y = ..., use = \"complete.obs\")\n```\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n### Step-by-step calculations of covariance\n\n1. Create 2 new columns in the memory recall data, one of which is the mean recall accuracy, and one which is the mean recall confidence. \n```{r}\nrecalldata <-\n  recalldata |> mutate(\n    maccuracy = mean(recall_accuracy),\n    mconfidence = mean(recall_confidence)\n  )\n```\n\n2. Now create three new columns which are:\n\n    i. recall accuracy minus the mean recall accuracy - this is the $(x_i - \\bar{x})$ part.   \n    ii. confidence minus the mean confidence - and this is the $(y_i - \\bar{y})$ part.   \n    iii. the product of i. and ii. - this is calculating $(x_i - \\bar{x})$$(y_i - \\bar{y})$.    \n\n```{r}\nrecalldata <- \n  recalldata |> \n    mutate(\n      acc_minus_mean_acc = recall_accuracy - maccuracy,\n      conf_minus_mean_conf = recall_confidence - mconfidence,\n      prod_acc_conf = acc_minus_mean_acc * conf_minus_mean_conf\n    )\n\nrecalldata\n```\n\n3. Finally, sum the products, and divide by $n-1$\n\n```{r}\nrecalldata |>\n  summarise(\n    prod_sum = sum(prod_acc_conf),\n    n = n()\n  )\n\n2243.46 / (20-1)\n```\n\nWhich is the same result as using `cov()`:\n```{r}\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n\n:::\n\n# Correlation\n\nOne thing to note with covariance is that it is sensitive to the units of measurement. The covariance between height-in-centimeters and weight-in-grams will be a much bigger number than the covariance between height-in-meters and weight-in-kilograms.  \n\nCorrelation solves this issue, and you can think of correlation as a standardised covariance. It has a scale from negative one to one, on which the distance from zero indicates the strength of the relationship.  \nJust like covariance, positive/negative values reflect the nature of the relationship.  \n\nThe __correlation coefficient__ is a standardised number which quantifies the strength and direction of the **linear** relationship between two variables. In a population it is denoted by $\\rho$, and in a sample it is denoted by $r$.  \n  \nWe can calculate $r$ using the following formula:  \n$$\nr_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\n$$ \n\nWe can actually rearrange this formula to show that the correlation is simply the covariance, but with the values $(x_i - \\bar{x})$ divided by the standard deviation ($s_x$), and the values $(y_i - \\bar{y})$ divided by $s_y$: \n$$\nr_{(x,y)}=\\frac{1}{n-1} \\sum_{i=1}^n \\left( \\frac{x_{i}-\\bar{x}}{s_x} \\right) \\left( \\frac{y_{i}-\\bar{y}}{s_y} \\right)\n$$\n<br>\nThe correlation is the simply the covariance of **standardised** variables (variables expressed as the distance _in standard deviations_ from the mean).    \n\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/cor.gif\")\n```\n\n\n:::sticky\n**Properties of correlation coefficients**\n\n+ $-1 \\leq r \\leq 1$\n+ The sign indicates the direction of association\n  + _positive association_ ($r > 0$) means that values of one variable tend to be higher when values of the other variable are higher\n  + _negative association_ ($r < 0$) means that values of one variable tend to be lower when values of the other variable are higher\n  + _no linear association_ ($r \\approx 0$) means that higher/lower values of one variable do not tend to occur with higher/lower values of the other variable \n+ The closer $r$ is to $\\pm 1$, the stronger the linear association\n+ $r$ has no units and does not depend on the units of measurement\n+ The correlation between $x$ and $y$ is the same as the correlation between $y$ and $x$\n\n:::\n\n:::rtip\n**Correlation in R**  \nJust like R has a `cov()` function for calculating covariance, there is a `cor()` function for calculating correlation: \n```{r}\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n\nIf necessary, we can choose use only the complete observations (i.e. ignoring all rows where either value is an `NA`) by specifying:  \n```{r}\n#| eval: false\ncor(x = ..., y = ..., use = \"complete.obs\")\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nWe calculated above that $\\text{cov}(\\text{recall-accuracy}, \\text{recall-confidence})$ = `r cov(recalldata$recall_accuracy, recalldata$recall_confidence) |> round(3)`.  \n  \nTo calculate the _correlation_, we can simply divide this by the standard deviations of the two variables $s_{\\text{recall-accuracy}} \\times s_{\\text{recall-confidence}}$\n\n```{r}\nrecalldata |> summarise(\n  s_ra = sd(recall_accuracy),\n  s_rc = sd(recall_confidence)\n)\n\n118.08 / (14.527 * 11.622)\n```\n\nWhich is the same result as using `cor()`:\n```{r}\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Correlation Tests  \n\nNow that we've seen the formulae for _covariance_ and _correlation_, as well as how to quickly calculate them in R using `cov()` and `cor()`, we can use a statistical test to establish the probability of finding an association this strong by chance alone.  \n\n:::statbox\n__Hypotheses:__  \n\nThe hypotheses of the correlation test are, as always, statements about the _population_ parameter (in this case the correlation between the two variables in the population - i.e., $\\rho$).  \n\n\nIf we are conducting a two tailed test, then \n\n- $H_0: \\rho = 0$. There is _not_ a linear relationship between $x$ and $y$ in the population.  \n- $H_1: \\rho \\neq 0$ There is a linear relationship between $x$ and $y$.  \n  \nIf we instead conduct a one-tailed test, then we are testing either\n\n- $H_0: \\rho \\leq 0$ There is a negative or no linear relationship between $x$ and $y$   \nvs  \n$H_1: \\rho > 0$ There is a positive linear relationship between $x$ and $y$.\n- $H_0: \\rho \\geq 0$ There is a positive or no linear relationship between $x$ and $y$   \nvs  \n$H_1: \\rho < 0$ There is a negative linear relationship between $x$ and $y$.  \n\n__Test Statistic__  \n\nThe __test statistic__ for this test here is _another_ $t$ statistic, the formula for which depends on both the observed correlation ($r$) and the sample size ($n$):\n\n$$t = r \\sqrt{\\frac{n-2}{1-r^2}}$$\n\n\n__p-value__  \n\nWe calculate the p-value for our $t$-statistic as the long-run probability of a $t$-statistic with $n-2$ degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed $t$-statistic.  \n\n__Assumptions__  \n\nFor a test of Pearson's correlation coefficient $r$, we need to make sure a few conditions are met:  \n\n+ Both variables are quantitative\n+ Both variables should be drawn from normally distributed populations.\n+ The relationship between the two variables should be linear.  \n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Quick and easy `cor.test()`  \n\nWe can test the significance of the correlation coefficient really easily with the function `cor.test()`:  \n\n```{r}\ncor.test(recalldata$recall_accuracy, recalldata$recall_confidence)\n```\n\nby default, `cor.test()` will include only observations that have no missing data on either variable.  \ne.g., running `cor.test()` on `x` and `y` in the dataframe below will include only the yellow rows:  \n```{r}\n#| echo: false\ndata.frame(\n  x=c(1,2,NA,4,5),\n  y=c(NA,6,8,7,9)\n) |> knitr::kable() |>\n  kableExtra::row_spec(c(2,4,5),bold=TRUE,background=\"#FCBB06\")\n```\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nOr, if we want to calculate our test statistic manually: \n```{r}\n#calculate r\nr = cor(recalldata$recall_accuracy, recalldata$recall_confidence)\n\n#get n\nn = nrow(recalldata)\n\n#calculate t    \ntstat = r * sqrt((n - 2) / (1 - r^2))\n\n#calculate p-value for t, with df = n-2 \n2*(1-pt(tstat, df=n-2))\n```\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: (completely optional!) why t?\n\nWhy exactly do we have a $t$ statistic? We're calculating $r$, not $t$??  \n\nRemember that in hypothesis testing, we need a distribution against which to compare a statistic. But $r$ is bounded (it can only be between -1 and 1). This means the distributions of \"$r$'s that we would expect if under repeated sampling\" is not easily defined in a standard way. Consider how the shape changes when our sample size changes:  \n```{r}\n#| echo: false\nsim<-function(s){\n  x<-runif(s,0,100)\n  y<-runif(s,0,100)\n  cor(x,y)\n}\ntibble(n=c(5,10,20,50,100)) |>\n  mutate(\n    r = map(n, ~tibble(r=replicate(1e4,sim(.))))\n  ) |> unnest(r) |>\n  mutate(n = factor(n)) |>\n  ggplot(aes(x=r,col=n))+\n  geom_density()\n```\n\nSo what we do is convert the $r$ statistic to a $t$ statistic, and then we can compare _that_ to a $t$ distribution!  \n\n$t$ statistics are generally calculated by using $\\frac{estimate - 0}{standard\\, error}$.  \nThe standard error for a correlation $r$ is quantifiable as $\\sqrt{\\frac{(1-r^2)}{(n-2)}}$.  \nWe can think of this as what variance gets left-over ($1-r^2$) in relation to how much data is free to vary ($n-2$ because we have calculated 2 means in the process of getting $r$). This logic maps to how our standard error of the mean was calculated $\\frac{\\sigma}{\\sqrt{n}}$, in that it is looking at $\\frac{\\text{leftover variation}}{\\text{free datapoints}}$.  \n\nWhat this means is we can convert $r$ into a $t$ that we can then test!  \n\n$$\nt = \\, \\, \\frac{r}{SE_r} \\,\\,=\\,\\, \\frac{r}{\\sqrt{\\frac{(1-r^2)}{(n-2)}}} \\,\\,=\\,\\, r \\sqrt{\\frac{n-2}{1-r^2}}\n$$\n\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Cautions!\n\nCorrelation is an invaluable tool for quantifying relationships between variables, but __must be used with care__.  \n\nBelow are a few things to be aware of when we talk about correlation. \n\n:::frame\n__Correlation can be heavily affected by outliers. Always plot your data!__  \n\nThe two plots below only differ with respect to the inclusion of _one_ observation. However, the correlation coefficient for the two sets of observations is markedly different.  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\ndf2<-df\ndf2[2,1]<-180\n\npp1 <- ggplot(df2,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df2$x)+1.5, y=max(df2$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df2$x)+5, y=mean(df2$y)+1,label=expr(bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = df2[[2,2]], xend = df2[[2,1]], yend = df2[[2,2]]), color=\"skyblue3\", data = df2)+\n  geom_segment(aes(x = df2[[2,1]], y = mean(y), xend = df2[[2,1]], yend = df2[[2,2]]), color=\"skyblue3\",data = df2)+\n  labs(title=paste0(\"Cov = \",cov(df2[,1],df2[,2]) %>% round(2),\"   r = \",cor(df2[,1],df2[,2]) %>% round(2)))+\n  xlim(35,185)\n\ndf3<-df2[-2,]\npp2 <- ggplot(df3,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  #geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  #annotate(\"text\",x=mean(df3$x)+1.5, y=max(df3$y)-5,label=expr(bar(\"x\")))+\n  #geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  #annotate(\"text\",x=min(df3$x)+5, y=mean(df3$y)+1,label=expr(bar(\"y\")))+\n  #geom_segment(aes(x = mean(x), y = df3[[2,2]], xend = df3[[2,1]], yend = df3[[2,2]]), color=\"skyblue3\", data = df3)+\n  #geom_segment(aes(x = df3[[2,1]], y = mean(y), xend = df3[[2,1]], yend = df3[[2,2]]), color=\"skyblue3\",data = df3)+\n  labs(title=paste0(\"Cov = \",cov(df3[,1],df3[,2]) %>% round(2),\"   r = \",cor(df3[,1],df3[,2]) %>% round(2)))+\n  xlim(35,185)\n\npp2 / pp1\n```\n\n:::\n\n:::frame\n__r = 0 means no linear association. The variables could still be otherwise associated. Always plot your data!__  \n\nThe correlation coefficient in @fig-joface below is negligible, suggesting no _linear_ association. The word \"linear\" here is crucial - the data are very clearly related. \n\n```{r}\n#| label: fig-joface\n#| fig.cap: \"Unrelated data?\"\n#| fig-height: 3\n#| fig-width: 3\n#| echo: false\n#| message: false\n#| warning: false\nfaced<-read_csv(\"data/face.csv\")\n\nggplot(faced,aes(x=lm_x,y=lm_y))+\n  geom_point()+\n  theme_classic()+\n  xlim(-3,3)+\n  labs(title=paste0(\"Cov = \",cov(faced$lm_x,faced$lm_y) %>% round(2),\"   r = \",cor(faced$lm_x,faced$lm_y) %>% round(2)))\n```\n\nSimilarly, take look at all the sets of data in @fig-datasaurus below. The summary statistics (means and standard deviations of each variable, and the correlation) are almost identical, but the visualisations suggest that the data are very different from one another.\n```{r}\n#| label: fig-datasaurus\n#| fig.cap: \"Datasaurus! From Matejka, J., & Fitzmaurice, G. (2017, May): Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing.\"\n#| echo: false\nknitr::include_graphics(\"https://media1.giphy.com/media/UN2kVJQeMFUje/source.gif\")\n```\n:::\n\n:::frame\n__Correlation does not imply causation!__  \n\n```{r}\n#| label: fig-xkcdcor\n#| fig.cap: \"https://twitter.com/quantitudepod/status/1309135514839248896\"\n#| echo: false\nknitr::include_graphics(\"images/covcor/corcaus.jpeg\")\n```\n\nYou will have likely heard the phrase \"correlation does not imply causation\". There is even a whole [wikipedia entry](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation) devoted to the topic.  \n\n__Just because you observe an association between x and y, we should not deduce that x causes y__\n\nAn often cited [paper](http://www.nejm.org/doi/full/10.1056/NEJMon1211064){target=\"_blank\"} which appears to fall foul of this error took a correlation between a country's chocolate consumption and its number of nobel prize winners (see @fig-choco) to suggest a _causal relationship_ between the two (\"chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates\"):  \n\n```{r}\n#| label: fig-choco\n#| fig.cap: \"Chocolate consumption causes more Nobel Laureates?\"\n#| echo: false\nknitr::include_graphics(\"images/covcor/choco.jpeg\")\n```\n\n:::\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>\n\n","srcMarkdownNoYaml":"\n\n:::lo\nThis reading:  \n\n- How can we describe the relationship between two continuous variables?  \n- How can we test the relationship between two continuous variables?  \n\n:::\n\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nset.seed(017)\n```\n\n\n\nIn the last couple of weeks we have covered a range of the basic statistical tests that can be conducted when we have a single outcome variable, and sometimes also a single explanatory variable. Our outcome variables have been both continuous ($t$-tests) and categorical ($\\chi^2$-tests). We're going to look at one more relationship now, which is that between two continuous variables. This will also provide us with our starting point for the second block of the course.  \n\n```{r}\n#| echo: false\ntribble(\n  ~outcome, ~explanatory, ~test, ~examines,\n  \"continuous\",\"\",\"t.test(y, mu = ?)\",\"is the mean of y different from [specified value]?\",\n  \"continuous\",\"binary\",\"t.test(y ~ x)\",\"is the mean of y different between the two groups?\",\n  \"categorical\",\"\",\"chisq.test(table(y), prob = c(?,...,?))\",\"is the distribution of categories of y different from [specified proportions]?\",\n  \"categorical\",\"categorical\",\"chisq.test(table(y,x))\",\"is the distribution of categories of y dependent on the category of x?\",\n  \"continuous\",\"continuous\",\"???\",\"???\"\n) %>% knitr::kable() %>%\n  kableExtra::row_spec(5,background=\"#F3E3E5\")\n```\n\n\n\n\n\n:::frame\n> **Research Question:** Is there a correlation between accuracy and self-perceived confidence of memory recall?  \n\nOur data for this walkthrough is from a (hypothetical) study on memory. Twenty participants studied passages of text (c500 words long), and were tested a week later. The testing phase presented participants with 100 statements about the text. They had to answer whether each statement was true or false, as well as rate their confidence in each answer (on a sliding scale from 0 to 100). The dataset contains, for each participant, the percentage of items correctly answered, and the average confidence rating. Participants' ages were also recorded.   \n\nThe data are available at [\"https://uoepsy.github.io/data/recalldata.csv](https://uoepsy.github.io/data/recalldata.csv).  \n\n:::\n\nLet's take a look visually at the relationships between the percentage of items answered correctly (`recall_accuracy`) and participants' average self-rating of confidence in their answers (`recall_confidence`). Let's also look at the relationship between accuracy and age.  \n\n```{r}\n#| message: false\n#| warning: false\nlibrary(tidyverse)\nlibrary(patchwork)\n\nrecalldata <- read_csv('https://uoepsy.github.io/data/recalldata.csv')\n\nggplot(recalldata, aes(x=recall_confidence, recall_accuracy))+\n  geom_point() + \nggplot(recalldata, aes(x=age, recall_accuracy))+\n  geom_point()\n```\n\nThese two relationships look quite different.  \n\n+ For participants who tended to be more confident in their answers, the percentage of items they correctly answered tends to be higher.  \n+ The older participants were, the lower the percentage of items they correctly answered tended to be.  \n\nWhich relationship should we be more confident in and why? \n\nIdeally, we would have some means of quantifying the strength and direction of these sorts of relationship. This is where we come to the two summary statistics which we can use to talk about the association between two numeric variables: __Covariance__ and __Correlation__.  \n\n# Covariance \n\nCovariance is the measure of how two variables vary together. \nIt is the change in one variable associated with the change in another variable.   \n\nCovariance is calculated using the following formula:\n\n$$\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}$$\nwhere:\n\n- $x$ and $y$ are two variables; e.g., `age` and `recall_accuracy`;\n- $i$ denotes the observational unit, such that $x_i$ is value that the $x$ variable takes on the $i$th observational unit, and similarly for $y_i$;\n- $n$ is the sample size.\n\n\nThis can initially look like quite a big equation, so let's break it down.  \n\n:::frame\n__Covariance explained visually__  \n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\nset.seed(7135)\ntibble(x=runif(10,20,80),y=rnorm(10,160,10)) %>%\n  mutate(y=y+x/2,\n         coldir = ifelse( (x>mean(x) & y>mean(y)) | (x<mean(x) & y<mean(y)), \"pos\",\"neg\")\n  ) -> df\n\np1<-ggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()\n  \np2<-ggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)+1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))\n  \np3<-ggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))+\n  annotate(\"text\",x=mean(df$x)+7, y=202,label=expression(x[i]-bar(\"x\")))+\n  annotate(\"text\",x=71.5, y=mean(df$y)+7,label=expression(y[i]-bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color=\"tomato1\",data = df)\n\n\np4<-ggplot(df,aes(x=x,y=y))+\n  geom_point(aes(col=coldir))+\n  scale_color_manual(\"\",values=c(\"black\",\"red\"))+\n  theme_classic()+\n  theme(legend.position = \"none\")+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color=\"tomato1\",data = df)\n\np5<-ggplot(df,aes(x=x,y=y))+\n  geom_point(aes(col=coldir))+\n  scale_color_manual(\"\",values=c(\"blue\",\"red\"))+\n  theme_classic()+\n  theme(legend.position = \"none\")+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color=\"tomato1\", data = df)+\n  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color=\"tomato1\",data = df)+\n  geom_segment(aes(x = mean(x), y = 194.4188, xend = 45.32536, yend = 194.4188), color=\"skyblue3\", data = df)+\n  geom_segment(aes(x = 45.32536, y = mean(y), xend = 45.32536, yend = 194.4188), color=\"skyblue3\",data = df)+\n  geom_segment(aes(x = mean(x), y = 182.6440, xend = 66.73541, yend = 182.6440), color=\"skyblue3\", data = df)+\n  geom_segment(aes(x = 66.73541, y = mean(y), xend = 66.73541, yend = 182.6440), color=\"skyblue3\",data = df)\n```\n\nConsider the following scatterplot:  \n```{r}\n#| echo: false\n#| message: false\np1\n```\n<br>\nNow let's superimpose a vertical dashed line at the mean of $x$ ($\\bar{x}$) and a horizontal dashed line at the mean of $y$ ($\\bar{y}$):\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np2\n```\n<br>\nNow let's pick one of the points, call it $x_i$, and show $(x_{i}-\\bar{x})$ and $(y_{i}-\\bar{y})$.  \nNotice that this makes a rectangle.  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np3\n```\nAs $(x_{i}-\\bar{x})$ and $(y_{i}-\\bar{y})$ are both positive values, their product --- $(x_{i}-\\bar{x})(y_{i}-\\bar{y})$ --- is positive. \n\nIn fact, for any pair of values $x_i$ and $y_i$ where both values are above their respective means, or both are below, then product $(x_{i}-\\bar{x})(y_{i}-\\bar{y})$ will be positive (remember that a negative multiplied by a negative gives a positive):  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np4\n```\n\nAnd for all those other points (in blue) the product $(x_{i}-\\bar{x})(y_{i}-\\bar{y})$ is negative:  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\np5\n```\n<br>\nNow let's take another look at the formula for covariance:  \n\n$$\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}$$\n  \nIt is the sum of all these products divided by $n-1$. It is the average of the products! You can almost think of this as the average area of all the rectangles!  \nSo for plots like this where we have been colouring them in blue or red, the measure of covariance moves up and down depending upon how much blue vs how much red there is! It will be zero when there is the same amount of blue as there is red. \n\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/cov.gif\")\n```\n\n```{r}\n#| eval: false\n#| include: false\nset.seed(899)\nmkd <- function(cv){\n  MASS::mvrnorm(n=40,mu=c(0,0),Sigma=Matrix::nearPD(matrix(c(40,cv,cv,70),nrow=2))$mat) |>\n    as.data.frame() |> \n    mutate(V1=V1+15,V2=V2+25)\n}\n\ninit = tibble(\n  frm = 0,\n  data = mkd(-50)\n) |> unnest()\nss = init |> transmute(\n  V1,\n  V2,\n  V2e = V2 - 2*coef(lm(V2~V1,init))[2]*V1,\n  frm = list(seq(1,100)),\n  V2l = map2(V2,V2e,~seq(..1,..2,length.out=100)),\n) |> unnest() |>\n  group_by(frm) |> \n  mutate(V2l = V2l - (mean(V2l)-25),\n         v1m=mean(V1),\n         v2m=mean(V2l),\n         cov = round(cov(V1,V2l),2),\n         cor = round(cor(V1,V2l),2),\n         col=case_when(\n           V1>v1m & V2l>v2m ~ \"p\",\n           V1<v1m & V2l<v2m ~ \"p\",\n           TRUE ~ \"n\"\n         )) |> ungroup()\n\n\nlibrary(gganimate)\nanimp <- ggplot(ss, aes(x=V1,y=V2l))+\n  geom_point(size=3)+\n  geom_hline(aes(yintercept=v2m),lty=\"dotted\")+\n  geom_vline(aes(xintercept=v1m),lty=\"dotted\")+\n  geom_segment(aes(x=V1,xend=V1,y=V2l,yend=v2m,col=col),alpha=.4)+\n  geom_segment(aes(x=V1,xend=v1m,y=V2l,yend=V2l,col=col),alpha=.4)+\n  geom_rect(aes(xmin=V1,xmax=v1m,ymin=V2l,ymax=v2m,fill=col),alpha=.2)+\n  scale_color_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  scale_fill_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  guides(col=\"none\",fill=\"none\")+\n  labs(x=\"X\",y=\"Y\", title=paste0(\"cov = \",\"{closest_state}\"))+\n  #facet_wrap(~frm)\n  transition_states(cov)+ease_aes('sine-in-out')\n\n\nss = init |> transmute(\n  V1,\n  V2,\n  V2e = V2 - 2*coef(lm(V2~V1,init))[2]*V1,\n  frm = list(seq(1,100)),\n  V2l = map2(V2,V2e,~seq(..1,..2,length.out=100)),\n) |> unnest() |>\n  group_by(frm) |> \n  mutate(\n    V1 = scale(V1)[,1],\n    V2l = scale(V2l)[,1],\n    v1m=mean(V1),\n         v2m=mean(V2l),\n         cov = round(cov(V1,V2l),2),\n         cor = round(cor(V1,V2l),2),\n         col=case_when(\n           V1>v1m & V2l>v2m ~ \"p\",\n           V1<v1m & V2l<v2m ~ \"p\",\n           TRUE ~ \"n\"\n         )) |> ungroup()\n\nanimp2 <- ggplot(ss, aes(x=V1,y=V2l))+\n  geom_point(size=3)+\n  geom_hline(aes(yintercept=v2m),lty=\"dotted\")+\n  geom_vline(aes(xintercept=v1m),lty=\"dotted\")+\n  geom_segment(aes(x=V1,xend=V1,y=V2l,yend=v2m,col=col),alpha=.4)+\n  geom_segment(aes(x=V1,xend=v1m,y=V2l,yend=V2l,col=col),alpha=.4)+\n  geom_rect(aes(xmin=V1,xmax=v1m,ymin=V2l,ymax=v2m,fill=col),alpha=.2)+\n  scale_color_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  scale_fill_manual(values=c(\"skyblue3\",\"tomato1\"))+\n  guides(col=\"none\",fill=\"none\")+\n  labs(x=\"X\",y=\"Y\", title=paste0(\"cor = \",\"{closest_state}\"))+\n  #facet_wrap(~frm)\n  transition_states(cor)+ease_aes('sine-in-out')\n\nanim_save(\"images/cov.gif\",animp)\nanim_save(\"images/cor.gif\",animp2)\n```\n\n:::\n\n:::rtip\n**Covariance in R**  \n\nWe can calculate covariance in R using the `cov()` function.  \n`cov()` can take two variables `cov(x = , y = )`.  \n\n```{r}\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n  \nIf necessary, we can choose use only the complete observations (i.e. ignoring all rows where either value is an `NA`) by specifying:  \n```{r}\n#| eval: false\ncov(x = ..., y = ..., use = \"complete.obs\")\n```\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n### Step-by-step calculations of covariance\n\n1. Create 2 new columns in the memory recall data, one of which is the mean recall accuracy, and one which is the mean recall confidence. \n```{r}\nrecalldata <-\n  recalldata |> mutate(\n    maccuracy = mean(recall_accuracy),\n    mconfidence = mean(recall_confidence)\n  )\n```\n\n2. Now create three new columns which are:\n\n    i. recall accuracy minus the mean recall accuracy - this is the $(x_i - \\bar{x})$ part.   \n    ii. confidence minus the mean confidence - and this is the $(y_i - \\bar{y})$ part.   \n    iii. the product of i. and ii. - this is calculating $(x_i - \\bar{x})$$(y_i - \\bar{y})$.    \n\n```{r}\nrecalldata <- \n  recalldata |> \n    mutate(\n      acc_minus_mean_acc = recall_accuracy - maccuracy,\n      conf_minus_mean_conf = recall_confidence - mconfidence,\n      prod_acc_conf = acc_minus_mean_acc * conf_minus_mean_conf\n    )\n\nrecalldata\n```\n\n3. Finally, sum the products, and divide by $n-1$\n\n```{r}\nrecalldata |>\n  summarise(\n    prod_sum = sum(prod_acc_conf),\n    n = n()\n  )\n\n2243.46 / (20-1)\n```\n\nWhich is the same result as using `cov()`:\n```{r}\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n\n:::\n\n# Correlation\n\nOne thing to note with covariance is that it is sensitive to the units of measurement. The covariance between height-in-centimeters and weight-in-grams will be a much bigger number than the covariance between height-in-meters and weight-in-kilograms.  \n\nCorrelation solves this issue, and you can think of correlation as a standardised covariance. It has a scale from negative one to one, on which the distance from zero indicates the strength of the relationship.  \nJust like covariance, positive/negative values reflect the nature of the relationship.  \n\nThe __correlation coefficient__ is a standardised number which quantifies the strength and direction of the **linear** relationship between two variables. In a population it is denoted by $\\rho$, and in a sample it is denoted by $r$.  \n  \nWe can calculate $r$ using the following formula:  \n$$\nr_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\n$$ \n\nWe can actually rearrange this formula to show that the correlation is simply the covariance, but with the values $(x_i - \\bar{x})$ divided by the standard deviation ($s_x$), and the values $(y_i - \\bar{y})$ divided by $s_y$: \n$$\nr_{(x,y)}=\\frac{1}{n-1} \\sum_{i=1}^n \\left( \\frac{x_{i}-\\bar{x}}{s_x} \\right) \\left( \\frac{y_{i}-\\bar{y}}{s_y} \\right)\n$$\n<br>\nThe correlation is the simply the covariance of **standardised** variables (variables expressed as the distance _in standard deviations_ from the mean).    \n\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/cor.gif\")\n```\n\n\n:::sticky\n**Properties of correlation coefficients**\n\n+ $-1 \\leq r \\leq 1$\n+ The sign indicates the direction of association\n  + _positive association_ ($r > 0$) means that values of one variable tend to be higher when values of the other variable are higher\n  + _negative association_ ($r < 0$) means that values of one variable tend to be lower when values of the other variable are higher\n  + _no linear association_ ($r \\approx 0$) means that higher/lower values of one variable do not tend to occur with higher/lower values of the other variable \n+ The closer $r$ is to $\\pm 1$, the stronger the linear association\n+ $r$ has no units and does not depend on the units of measurement\n+ The correlation between $x$ and $y$ is the same as the correlation between $y$ and $x$\n\n:::\n\n:::rtip\n**Correlation in R**  \nJust like R has a `cov()` function for calculating covariance, there is a `cor()` function for calculating correlation: \n```{r}\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n\nIf necessary, we can choose use only the complete observations (i.e. ignoring all rows where either value is an `NA`) by specifying:  \n```{r}\n#| eval: false\ncor(x = ..., y = ..., use = \"complete.obs\")\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nWe calculated above that $\\text{cov}(\\text{recall-accuracy}, \\text{recall-confidence})$ = `r cov(recalldata$recall_accuracy, recalldata$recall_confidence) |> round(3)`.  \n  \nTo calculate the _correlation_, we can simply divide this by the standard deviations of the two variables $s_{\\text{recall-accuracy}} \\times s_{\\text{recall-confidence}}$\n\n```{r}\nrecalldata |> summarise(\n  s_ra = sd(recall_accuracy),\n  s_rc = sd(recall_confidence)\n)\n\n118.08 / (14.527 * 11.622)\n```\n\nWhich is the same result as using `cor()`:\n```{r}\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n```\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Correlation Tests  \n\nNow that we've seen the formulae for _covariance_ and _correlation_, as well as how to quickly calculate them in R using `cov()` and `cor()`, we can use a statistical test to establish the probability of finding an association this strong by chance alone.  \n\n:::statbox\n__Hypotheses:__  \n\nThe hypotheses of the correlation test are, as always, statements about the _population_ parameter (in this case the correlation between the two variables in the population - i.e., $\\rho$).  \n\n\nIf we are conducting a two tailed test, then \n\n- $H_0: \\rho = 0$. There is _not_ a linear relationship between $x$ and $y$ in the population.  \n- $H_1: \\rho \\neq 0$ There is a linear relationship between $x$ and $y$.  \n  \nIf we instead conduct a one-tailed test, then we are testing either\n\n- $H_0: \\rho \\leq 0$ There is a negative or no linear relationship between $x$ and $y$   \nvs  \n$H_1: \\rho > 0$ There is a positive linear relationship between $x$ and $y$.\n- $H_0: \\rho \\geq 0$ There is a positive or no linear relationship between $x$ and $y$   \nvs  \n$H_1: \\rho < 0$ There is a negative linear relationship between $x$ and $y$.  \n\n__Test Statistic__  \n\nThe __test statistic__ for this test here is _another_ $t$ statistic, the formula for which depends on both the observed correlation ($r$) and the sample size ($n$):\n\n$$t = r \\sqrt{\\frac{n-2}{1-r^2}}$$\n\n\n__p-value__  \n\nWe calculate the p-value for our $t$-statistic as the long-run probability of a $t$-statistic with $n-2$ degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed $t$-statistic.  \n\n__Assumptions__  \n\nFor a test of Pearson's correlation coefficient $r$, we need to make sure a few conditions are met:  \n\n+ Both variables are quantitative\n+ Both variables should be drawn from normally distributed populations.\n+ The relationship between the two variables should be linear.  \n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Quick and easy `cor.test()`  \n\nWe can test the significance of the correlation coefficient really easily with the function `cor.test()`:  \n\n```{r}\ncor.test(recalldata$recall_accuracy, recalldata$recall_confidence)\n```\n\nby default, `cor.test()` will include only observations that have no missing data on either variable.  \ne.g., running `cor.test()` on `x` and `y` in the dataframe below will include only the yellow rows:  \n```{r}\n#| echo: false\ndata.frame(\n  x=c(1,2,NA,4,5),\n  y=c(NA,6,8,7,9)\n) |> knitr::kable() |>\n  kableExtra::row_spec(c(2,4,5),bold=TRUE,background=\"#FCBB06\")\n```\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nOr, if we want to calculate our test statistic manually: \n```{r}\n#calculate r\nr = cor(recalldata$recall_accuracy, recalldata$recall_confidence)\n\n#get n\nn = nrow(recalldata)\n\n#calculate t    \ntstat = r * sqrt((n - 2) / (1 - r^2))\n\n#calculate p-value for t, with df = n-2 \n2*(1-pt(tstat, df=n-2))\n```\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: (completely optional!) why t?\n\nWhy exactly do we have a $t$ statistic? We're calculating $r$, not $t$??  \n\nRemember that in hypothesis testing, we need a distribution against which to compare a statistic. But $r$ is bounded (it can only be between -1 and 1). This means the distributions of \"$r$'s that we would expect if under repeated sampling\" is not easily defined in a standard way. Consider how the shape changes when our sample size changes:  \n```{r}\n#| echo: false\nsim<-function(s){\n  x<-runif(s,0,100)\n  y<-runif(s,0,100)\n  cor(x,y)\n}\ntibble(n=c(5,10,20,50,100)) |>\n  mutate(\n    r = map(n, ~tibble(r=replicate(1e4,sim(.))))\n  ) |> unnest(r) |>\n  mutate(n = factor(n)) |>\n  ggplot(aes(x=r,col=n))+\n  geom_density()\n```\n\nSo what we do is convert the $r$ statistic to a $t$ statistic, and then we can compare _that_ to a $t$ distribution!  \n\n$t$ statistics are generally calculated by using $\\frac{estimate - 0}{standard\\, error}$.  \nThe standard error for a correlation $r$ is quantifiable as $\\sqrt{\\frac{(1-r^2)}{(n-2)}}$.  \nWe can think of this as what variance gets left-over ($1-r^2$) in relation to how much data is free to vary ($n-2$ because we have calculated 2 means in the process of getting $r$). This logic maps to how our standard error of the mean was calculated $\\frac{\\sigma}{\\sqrt{n}}$, in that it is looking at $\\frac{\\text{leftover variation}}{\\text{free datapoints}}$.  \n\nWhat this means is we can convert $r$ into a $t$ that we can then test!  \n\n$$\nt = \\, \\, \\frac{r}{SE_r} \\,\\,=\\,\\, \\frac{r}{\\sqrt{\\frac{(1-r^2)}{(n-2)}}} \\,\\,=\\,\\, r \\sqrt{\\frac{n-2}{1-r^2}}\n$$\n\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Cautions!\n\nCorrelation is an invaluable tool for quantifying relationships between variables, but __must be used with care__.  \n\nBelow are a few things to be aware of when we talk about correlation. \n\n:::frame\n__Correlation can be heavily affected by outliers. Always plot your data!__  \n\nThe two plots below only differ with respect to the inclusion of _one_ observation. However, the correlation coefficient for the two sets of observations is markedly different.  \n```{r}\n#| echo: false\n#| message: false\n#| warning: false\ndf2<-df\ndf2[2,1]<-180\n\npp1 <- ggplot(df2,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  annotate(\"text\",x=mean(df2$x)+1.5, y=max(df2$y)-5,label=expr(bar(\"x\")))+\n  geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  annotate(\"text\",x=min(df2$x)+5, y=mean(df2$y)+1,label=expr(bar(\"y\")))+\n  geom_segment(aes(x = mean(x), y = df2[[2,2]], xend = df2[[2,1]], yend = df2[[2,2]]), color=\"skyblue3\", data = df2)+\n  geom_segment(aes(x = df2[[2,1]], y = mean(y), xend = df2[[2,1]], yend = df2[[2,2]]), color=\"skyblue3\",data = df2)+\n  labs(title=paste0(\"Cov = \",cov(df2[,1],df2[,2]) %>% round(2),\"   r = \",cor(df2[,1],df2[,2]) %>% round(2)))+\n  xlim(35,185)\n\ndf3<-df2[-2,]\npp2 <- ggplot(df3,aes(x=x,y=y))+\n  geom_point()+\n  theme_classic()+\n  #geom_vline(aes(xintercept=mean(x)), lty=\"dashed\")+\n  #annotate(\"text\",x=mean(df3$x)+1.5, y=max(df3$y)-5,label=expr(bar(\"x\")))+\n  #geom_hline(aes(yintercept=mean(y)), lty=\"dashed\")+\n  #annotate(\"text\",x=min(df3$x)+5, y=mean(df3$y)+1,label=expr(bar(\"y\")))+\n  #geom_segment(aes(x = mean(x), y = df3[[2,2]], xend = df3[[2,1]], yend = df3[[2,2]]), color=\"skyblue3\", data = df3)+\n  #geom_segment(aes(x = df3[[2,1]], y = mean(y), xend = df3[[2,1]], yend = df3[[2,2]]), color=\"skyblue3\",data = df3)+\n  labs(title=paste0(\"Cov = \",cov(df3[,1],df3[,2]) %>% round(2),\"   r = \",cor(df3[,1],df3[,2]) %>% round(2)))+\n  xlim(35,185)\n\npp2 / pp1\n```\n\n:::\n\n:::frame\n__r = 0 means no linear association. The variables could still be otherwise associated. Always plot your data!__  \n\nThe correlation coefficient in @fig-joface below is negligible, suggesting no _linear_ association. The word \"linear\" here is crucial - the data are very clearly related. \n\n```{r}\n#| label: fig-joface\n#| fig.cap: \"Unrelated data?\"\n#| fig-height: 3\n#| fig-width: 3\n#| echo: false\n#| message: false\n#| warning: false\nfaced<-read_csv(\"data/face.csv\")\n\nggplot(faced,aes(x=lm_x,y=lm_y))+\n  geom_point()+\n  theme_classic()+\n  xlim(-3,3)+\n  labs(title=paste0(\"Cov = \",cov(faced$lm_x,faced$lm_y) %>% round(2),\"   r = \",cor(faced$lm_x,faced$lm_y) %>% round(2)))\n```\n\nSimilarly, take look at all the sets of data in @fig-datasaurus below. The summary statistics (means and standard deviations of each variable, and the correlation) are almost identical, but the visualisations suggest that the data are very different from one another.\n```{r}\n#| label: fig-datasaurus\n#| fig.cap: \"Datasaurus! From Matejka, J., & Fitzmaurice, G. (2017, May): Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing.\"\n#| echo: false\nknitr::include_graphics(\"https://media1.giphy.com/media/UN2kVJQeMFUje/source.gif\")\n```\n:::\n\n:::frame\n__Correlation does not imply causation!__  \n\n```{r}\n#| label: fig-xkcdcor\n#| fig.cap: \"https://twitter.com/quantitudepod/status/1309135514839248896\"\n#| echo: false\nknitr::include_graphics(\"images/covcor/corcaus.jpeg\")\n```\n\nYou will have likely heard the phrase \"correlation does not imply causation\". There is even a whole [wikipedia entry](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation) devoted to the topic.  \n\n__Just because you observe an association between x and y, we should not deduce that x causes y__\n\nAn often cited [paper](http://www.nejm.org/doi/full/10.1056/NEJMon1211064){target=\"_blank\"} which appears to fall foul of this error took a correlation between a country's chocolate consumption and its number of nobel prize winners (see @fig-choco) to suggest a _causal relationship_ between the two (\"chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates\"):  \n\n```{r}\n#| label: fig-choco\n#| fig.cap: \"Chocolate consumption causes more Nobel Laureates?\"\n#| echo: false\nknitr::include_graphics(\"images/covcor/choco.jpeg\")\n```\n\n:::\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"05a_covcor.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"code-annotations":"hover","link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"5A: Covariance and Correlation","params":{"SHOW_SOLS":true,"TOGGLE":true}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}