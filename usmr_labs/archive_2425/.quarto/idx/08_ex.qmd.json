{"title":"Exercises: Scaling | Categorical Predictors","markdown":{"yaml":{"title":"Exercises: Scaling | Categorical Predictors","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Pictures of a Brain","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n\n\n:::frame\n__neuronews.csv__  \n\n```{r}\n#| include: false\nset.seed(9)\ndf = tibble(\n  pid = paste0(\"ppt_\",1:120),\n  name = randomNames::randomNames(120, which.names=\"first\"),\n  condition = rep(c(\"text-only\",\"text+brain\",\"text+brain+graph\"),e=40),\n  credibility = 58.2 + \n    (condition==\"text+brain\")*4.7 +\n    (condition==\"text+brain+graph\")*5.322 + \n    rnorm(120, 0, 10.3)\n)\n#write_csv(df,file=\"../../data/usmr_neuronews.csv\")\nlm(credibility~condition,df) |> summary()\n```\n\nThis dataset is from a study^[not a real one, but inspired very loosely by [this one](https://bpspsychub.onlinelibrary.wiley.com/doi/full/10.1111/bjep.12162){target=\"_blank\"}] looking at the influence of presenting scientific news with different pictures on how believable readers interpret the news.  \n\n`r nrow(df)` participants took part in the study. Participation involved reading a short article about some research in neuroscience, and then rating how credible they found the research. Participants were randomly placed into one of three conditions, in which the article was presented a) in text alone, b) with a picture of a brain, or c) with a picture of a brain and a fancy looking (but unrelated to the research) graph. They rated credibility using a sliding scale from 0 to 100, with higher values indicating more credibility.  \n\nThe data is available at [https://uoepsy.github.io/data/usmr_neuronews.csv](https://uoepsy.github.io/data/usmr_neuronews.csv){target=\"_blank\"}.  \n\n```{r}\n#| echo: false\n#| tbl-cap: \"usmr_neuronews.csv data dictionary\"\ntibble(\n  variable=names(df),\n  description = c(\n    \"Participant ID\",\n    \"Participant Name\",\n    \"Condition (text-only / text+brain / text+brain+graph)\",\n    \"Credibility rating (0-100)\"\n  )\n) |> gt::gt()\n```\n\n\n:::\n\n`r qbegin(qcounter())`\nRead in the data and take a look around (this is almost always the first thing to do!)  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nnndat <- read_csv(\"https://uoepsy.github.io/data/usmr_neuronews.csv\")\n\nhist(nndat$credibility)\n# geom jitter is a way of randomly 'jittering' points so that the don't overlap\n# i want them to be the right height, so no jitter in height, but i'll give them a little width jitter\nggplot(nndat, aes(x=condition,y=credibility)) +\n   geom_jitter(height = 0, width = .2)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nFit a model examining whether credibility of the research article differs between conditions.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nmod1 <- lm(credibility ~ condition, data = nndat)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nDo conditions differ in credibility ratings?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThis is an overall question, not asking about differences between specific levels. You can find a way to test this question either at the bottom of the `summary()` output, or by comparing it with a model without condition differences in it (see [8B #testing-group-differences](08b_catpred.html#testing-group-differences){target=\"_blank\"}).  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe can either do this as a model comparison:\n```{r}\nmod0 <- lm(credibility ~ 1, data = nndat)\nanova(mod0, mod1)\n```\n\nand this info is also at the bottom of the summary, because we just have one predictor in the model\n```{r}\n#| eval: false\nsummary(mod1)\n```\n```\nF-statistic: 3.246 on 2 and 117 DF,  p-value: 0.04245\n```\n\n:::int\nConditions significantly differed in credibility ratings $F(2,117) = 3.25, p = .0425$\n:::\n\n`r solend()`\n\n`r qbegin(qcounter())`\n__How__ do groups differ?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nNote that this is a subtly different question to the previous one. It will require us to look at something that tests between specific groups ([8B #testing-differences-between-specific-groups](08b_catpred.html#testing-differences-between-specific-groups){target=\"_blank\"}).  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| include: false\nres = broom::tidy(mod1)\nres$estimate = round(res$estimate,2)\nres$statistic = round(res$statistic,2)\nres$p.value = format.pval(res$p.value,eps=.001,digits=2)\nres$p.value = ifelse(grepl(\"<\",res$p.value), res$p.value, paste0(\"=\",res$p.value))\n```\n\n\n```{r}\nsummary(mod1)\n```\n\n:::int\nCompared to when presented in text-only, conditions where the article was presented alongside a picture of a brain, or alongside both a brain-picture and a graph, resulted in higher average credibility ratings. Including a picture of a brain was associated with a `r res[2,2]` increase in credibility over the text-only article ($b = `r res[2,2]`$, $t(`r mod1['df.residual']`)=`r res[2,4]`$, $p`r res[2,5]`$), and including both a brain-picture and a graph was associated with a `r res[3,2]` higher average credibility rating ($b = `r res[3,2]`$, $t(`r mod1['df.residual']`)=`r res[3,4]`$, $p`r res[3,5]`$).  \n:::\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nLet's prove something to ourselves.  \nBecause we have no other predictors in the model, it should be possible to see how the coefficients from our model map exactly to the group means.  \n\nCalculate the mean credibility for each condition, and compare with your model coefficients.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nTo calculate group means, we can use `group_by()` and `summarise()`! \n\n:::\n\n\n`r qend()`\n`r solbegin(label=\"Solution Part 1 - calculate group means\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\nnndat |> \n  group_by(condition) |>\n  summarise(\n    meancred = mean(credibility)\n  )\n```\n\n`r solend()`\n`r solbegin(label=\"Solution Part 2 - compare to the coefficients\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\nHere are our model coefficients:\n```{r}\ncoef(mod1)\n```\n\nAnd here are our means:\n```{r}\nnndat |> \n  group_by(condition) |>\n  summarise(\n    meancred = mean(credibility)\n  )\n```\n\nWe can see that the intercept is the mean of the `text-only` group.  \n\nThe next coefficient - named \"conditiontext+brain\" is the difference from the `text-only` group to the `text+brain` group.  \n\nThe final coefficient - named \"conditiontext+brain+graph\" is the difference from the `text-only` group to the `text+brain+graph` group.  \n\nSo this means we can get to our group means by using:\n```{r}\ncoef(mod1)[c(1,2)] |> sum() # the mean of text+brain\ncoef(mod1)[c(1,3)] |> sum() # the mean of text+brain+graph\n```\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Detectorists\n\n`r qbegin(qcounter())`\nWe saw this study briefly at the end of last week and we're going to continue where we left off.  \n\nBelow is the description of the study, and the code that we had given you to clean the data and fit a model that assessed whether different strategies (e.g. breathing, closing eyes) were associated with changes in heart rate during a lie-detection test. The model also accounts for differences in heart rates due to age and pre-test anxiety.  \n\nRun the code given below to ensure that we are all at the same place.  \n\n::: {.callout-note collapse=\"true\"}\n#### Study: Lie detectors\n\n```{r}\n#| include: false\nss=88173#round(runif(1,1e3,1e5))\nset.seed(ss)\nldf = tibble(\n  age = round(runif(142,22,64)),\n  anx = round(rnorm(142,0,1),2),\n  strategy = rbinom(142, 3, plogis(scale(anx)*2)),\n  hr = 45 + .3*age +3*anx+ (strategy==2)*-3.4 + (strategy==3)*-5.6 + rnorm(142,0,3)\n) \n#psych::pairs.panels(ldf)\n#summary(lm(hr~age+anx+factor(strategy),ldf))\n#plot(lm(hr~age+anx+factor(strategy),ldf),which=4)\nldf$hr <- round(ldf$hr,1)\nldf$strategy[60] <- 5\nldf$hr[ldf$hr>75]<- 37.0\n#write_csv(ldf,file=\"../../data/usmr_polygraph.csv\")\n```\n\nLaw enforcement in some countries regularly rely on 'polygraph' tests as a form of 'lie detection'. These tests involve measuring heart rate, blood pressure, respiratory rate and sweat. However, there is very little evidence to suggest that these methods are remotely accurate in being able to determine whether or not someone is lying.  \n\nResearchers are interested in if peoples' heart rates during polygraph tests can be influenced by various pre-test strategies, including deep breathing, or closing their eyes. They recruited `r nrow(ldf)` participants (ages `r min(ldf$age)` to `r max(ldf$age)`). Participants were told they were playing a game in which their task was to deceive the polygraph test, and they would receive financial rewards if they managed successfully. At the outset of the study, they completed a questionnaire which asked about their anxiety in relation to taking part. Participants then chose one of 4 strategies to prepare themselves for the test, each lasting 1 minute. These were \"do nothing\", \"deep breathing\", \"close your eyes\" or \"cough\"^[apparently coughing is a method of immediately lowering heart rate!]. The average heart rate of each participant was recorded during their test. \n\n```{r}\n#| echo: false\n#| tbl-cap: \"usmr_polygraph.csv data dictionary\"\ntibble(\n  variable = names(ldf),\n  description = c(\n    \"Age of participant (years)\",\n    \"Anxiety measure (Z-scored)\",\n    \"Pre-test Strategy (0 = do nothing, 1 = close eyes, 2 = cough, 3 = deep breathing)\",\n    \"Average Heart Rate (bpm) during test\"\n  )\n) |> gt::gt()\n```\n\n__Analysis__  \n\nExpand the code box below to see some analysis!  \n```{r}\n#| eval: false\n#| code-fold: true\n# load libraries\nlibrary(tidyverse)\nlibrary(psych)\n# read in the data\nliedf <- read_csv(\"https://uoepsy.github.io/data/usmr_polygraph.csv\")\n\n# there seems to be a 5 there.. \ntable(liedf$strategy)\n# the other variables look okay though\ndescribe(liedf)\npairs.panels(liedf)\n\nliedf <- liedf |> \n  filter(strategy!=5) |>\n  mutate(\n    # strategy is a factor. but currently numbers\n    # i'm going to give them better labels too.. \n    # to do this is need to tell factor() what \"levels\" to look for\n    # and then give it some \"labels\" to apply to those.\n    strategy = factor(strategy, \n                      levels = c(\"0\",\"1\",\"2\",\"3\"),\n                      labels = c(\"do nothing\", \"close eyes\",\n                                 \"cough\", \"deep breathing\")\n                      )\n  )\n\nliemod <- lm(hr ~ age + anx + strategy, data = liedf)\n\n# Does HR differ between strategies?\nanova(liemod)\n# the above is a shortcut for getting this comparison out:\nanova(\n  lm(hr ~ age + anx, data = liedf),\n  lm(hr ~ age + anx + strategy, data = liedf)\n)\n```\n\n:::\n\n`r qend()`\n```{r}\n#| include: false\nliedf <- read_csv(\"https://uoepsy.github.io/data/usmr_polygraph.csv\")\nliedf <- liedf |> \n  filter(strategy!=5) |>\n  mutate(\n    strategy = factor(strategy, \n                      levels = c(\"0\",\"1\",\"2\",\"3\"),\n                      labels = c(\"do nothing\", \"close eyes\",\n                                 \"cough\", \"deep breathing\")\n                      )\n  )\nliemod <- lm(hr ~ age + anx + strategy, data = liedf)\n```\n\n\n`r qbegin(qcounter())`\nOur model includes a predictor with 4 levels (the 4 different strategies).  \n\nThis means we will have 3 coefficients pertaining to this predictor. Take a look at them. Write a brief sentence explaining what each one represents.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe're still all using R's defaults ([8B #treatment-contrasts-the-default](08b_catpred.html#treatment-contrasts-the-default){target=\"_blank\"}), so these follow the logic we have seen already above. \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\ncoef(liemod)\n```\n\n```{r}\n#| echo: false\nbroom::tidy(liemod) |>\n  transmute(\n    term, estimate = round(estimate,2),\n    interpretation = c(\n      \"estimated HR for someone age 0, anxiety 0 (the mean), who 'does nothing' prior to the test\",\n      \"estimated change in HR for an additional year of age, holding constant anxiety and strategy\",\n      \"estimated change in HR for an additional SD of anxiety, holding constant age and strategy\",\n      \"estimated difference in HR between 'doing nothing' strategy and 'close eyes' strategy, holding constant age and anxiety\",\n      \"estimated difference in HR between 'doing nothing' strategy and 'cough' strategy, holding constant age and anxiety\",\n      \"estimated difference in HR between 'doing nothing' strategy and 'deep breathing' strategy, holding constant age and anxiety\"\n    )\n  ) |> gt::gt()\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nCalculate the mean heart rate for each strategy group.  \nDo they match to our model coefficients?  \n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n```{r}\nliedf |> \n  group_by(strategy) |>\n  summarise(\n    meanHR = mean(hr)\n  )\n```\n\nThey don't match! For instance, in our group means above the 'close eyes' strategy has a _higher_ average heart rate than the 'do nothing' strategy. But that's not what our model coefficients said - they said the opposite! \n\nWhy is this? It's because we have other things in our model, so these model estimated differences are now \"holding age and anxiety constant\". \n\n`r solend()`\n\n`r qbegin(qcounter())`\nAt the end last week's exercises, we wanted you to make a plot of the model estimated differences in heart rates between the strategies.  \n\nChances are that you followed the logic of 1) create a little plotting data frame, 2) use `augment()` from the broom package, and 3) shove it into ggplot.  \n\nYou may have ended up with something like this:  \n```{r}\n#| code-fold: true\n#| out-height: \"300px\"\nplotdat <- data.frame(\n  age = mean(liedf$age),\n  anx = mean(liedf$anx),\n  strategy = unique(liedf$strategy)\n)\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\")\n```\n\nThese plots are great, but they don't really show the underlying spread of the data. They make it seem like _everybody_ in the 'do nothing' strategy will have a heart rate between 56 and 59bpm. But that interval is where we expect the _mean_ to be, not where we expect the individuals scores to be.  \n\nCan you add the original raw datapoints to the plot, to present a better picture?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThis is tricky, and we haven't actually seen it anywhere in the readings or lectures.  \nThe thing that we're giving to ggplot (the output of the `augment` function) doesn't have the data in it.  \nWith ggplot we can actually pull in data from different sources:  \n```{r}\n#| eval: false\nggplot(data1, aes(x=x,y=y)) + \n  geom_point() +\n  geom_point(data = data2, aes(x=x,y=newy))\n```\n:::\n\n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\nSo here was our nice plot:  \n```{r}\nplotdat <- data.frame(\n  age = mean(liedf$age),\n  anx = mean(liedf$anx),\n  strategy = unique(liedf$strategy)\n)\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\")\n```\n\nWe want to add to that the original data.. but all that original data is in a different dataset!  \n\nHowever, that doesn't mean we can't use it.  \n\nLet's try to add points for all our heart rates, and we'll tell `geom_point()` to look in our original dataset.  \n\nThe thing we need to be careful with is that our original dataset doesn't have anything called `.fitted` in it, which is currently on our y-axis. So we need to specify the y we want it to use (in this case, the `hr` variable)\n\n```{r}\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\") +\n  geom_point(data = liedf, aes(y = hr))\n```\n\nOkay, the data is on there.. now we just need to make it look nice. My instinct here is to jitter again.\nI'm also going to make the plots black, and a little transparent, so that we can still see the means.  \n\n```{r}\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\") +\n  geom_jitter(data = liedf, aes(y = hr), width = .2, height = 0,\n              col = \"black\", alpha = .3)\n```\n\nThis is just _one_ way to better visualise our findings.  \n\nIf you're feeling adventurous, try: \n\n- a violin plot to show the distributions, rather than jittered points\n- what happens when you put `position = position_nudge(x=.3)` inside the `geom_pointrange()`?  \n- what happens if you add `coord_flip()` to the plot?  \n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Job Satisfaction, Guaranteed\n\n:::frame\n__jobsatpred.csv__  \n\nA company is worried about employee turnover, and they are trying to figure out what are the main aspects of work life that predict their employees' job satisfaction. 86 of their employees complete a questionnaire that leads to a 'job satisfaction score', that can range between 6 to 30 (higher scores indicate more satisfaction).     \n\nThe company then linked their responses on the questionnaire with various bits of information they could get from their human resources department, as seen in @tbl-jobsatdict.  \n\nThe data are available at [https://uoepsy.github.io/data/usmr_jobsatpred.csv](https://uoepsy.github.io/data/usmr_jobsatpred.csv){target=\"_blank\"}.  \n\n```{r}\n#| echo: false\n#| label: tbl-jobsatdict\n#| tbl-cap: \"usmr_jobsatpred.csv data dictionary\"\n\nN = 93\nset.seed(974255.8)\ndf = tibble(\n  name = randomNames::randomNames(N,which.names=\"last\"),\n  commute = rchisq(N,4),\n  hrs_worked = round(sample(c(16,28,35,40),86,T,prob=c(.1,.05,.6,.25)) +\n                       rnorm(N,0,10)),\n  team_size = rpois(N,3.2),\n  remote = sample(0:1,N,T,c(.8,.2)),\n  salary_band = sample(1:10, N, T, prob = (10:1)/(sum(1:10)))\n) |>\n  mutate(\n    hrs_worked = pmax(0,hrs_worked),\n    jobsat = commute*0 + hrs_worked*-.1 + team_size*1.2 + remote*-4 + salary_band*3 + rnorm(N,0,4),\n    jobsat = round(scale(jobsat)[,1]*4.5 + 19.5),\n    commute = round(commute,1)\n  )\n\ndf$commute[which.max(df$commute)] <- df$commute[which.max(df$commute)]*3\ndf$commute = format(df$commute,digits=1)\ndf$commute = gsub(\"\\\\.\",\",\",df$commute)\ndf$remote[sample(1:N,1)] <- 2\ndf$jobsat[sample(1:N,2)] <- 0\n#write_csv(df,file=\"../../data/usmr_jobsatpred.csv\")\njsdat <- df\n# summary(df)\n# psych::multi.hist(df[,-c(1,2)], global=FALSE)\n# lm(jobsat ~ ., df) |> summary()\ntibble(\n  variable = names(df),\n  description  = c(\n    \"Employee Surname\",\n    \"Distance (km) to employee residence (note, records have used a comma in place of a decimal point)\",\n    \"Hours worked in previous week\",\n    \"Size of the team in which the employee works\",\n    \"Whether the employee works in the office or remotely (0 = office, 1 = remote)\",\n    \"Salary band of employee (range 1 to 10, with those on band 10 earning the most)\",\n    \"Job Satisfaction Score (sum of 6 questions each scored on 1-5 likert scale)\"\n  )\n) |> gt::gt()\n```\n\n:::\n\n\n`r qbegin(qcounter())`\nRead in the data. Look around, do any cleaning that might be needed.\n\n`r qend()`\n`r solbegin(label=\"Solution Part 1 - identifying problems\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\njsdat <- read_csv(\"https://uoepsy.github.io/data/usmr_jobsatpred.csv\")\n```\nJust looking at the top of the data, I already know that `commute` is a problem..  \n```{r}\nhead(jsdat)\n```\n\nThe summary shows us a few other things..\n\n- `remote` has a max of 2, but it should be 0s and 1s, because it's a binary variable according to our data dictionary.  \n- `jobsat` should surely be between 6 and 30 (this is that idea of 6 questions where you score between 1 and 6 means your minimum should be 6, and maximum should be 30). Here we have a min of 0 and a max of 31 so something's amiss there.  \n\n```{r}\nsummary(jsdat)\n```\n\n\n`r solend()`\n`r solbegin(label=\"Solution Part 2 - figuring out fixes\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\nVariables are all appropriate types apart from `commute`.  \nIf we're being good, we should probably also set `remote` to be a factor as it is two distinct categories.  \n\nFor the commute variable, we need to first (prior to making it a numeric variable), replace all the commas with a point - \".\". We can do this using `gsub()`.  \n\nFor the remote variable, we know it only has two possible levels - 0 and 1. However, we saw earlier that there was a 2 in there as well. But there's only one of them, so it's probably a typo or something.  \n```{r}\nsum(df$remote==2)\n```\nWe should probably replace that with an NA as it is an impossible value, and we have no idea whether it is supposed to be a 0 or a 1.  \nAs it happens, by giving the `factor()` function only the levels we want to keep, it will replace any others with `NA`s.  \n```{r}\nfactor(c(\"dog\",\"cat\",\"parrot\"), levels=c(\"dog\",\"cat\"))\n```\n\nWe also noted that the `jobsat` variable had some values that were $<6$ and $>30$. \nWe can replace those with NAs as well, using that handy `ifelse()` function!  \n```{r}\nx <- c(1,16,50,72)\n# if x is less than 10 or x is less than 70, give me an NA, \n# otherwise give me the value of x\nifelse(x<10 | x>70, NA, x)\n```\n\n`r solend()`\n`r solbegin(label=\"Solution Part 3 - implementing\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\nLet's do this bit of cleaning all in one mutate:  \n```{r}\njsdat <- \n  jsdat |> \n    mutate(\n      commute = as.numeric(gsub(\",\",\".\",commute)),\n      remote = factor(remote, levels = c(\"0\",\"1\"),\n                      labels = c(\"office\",\"remote\")),\n      jobsat = ifelse(jobsat < 6 | jobsat > 30, NA, jobsat)\n    )\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nExplore! Make some quick plots and calculate some quick descriptives.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(tableone)\nCreateTableOne(data = jsdat |> select(-name))\n```\n\n\n```{r}\nlibrary(psych)\npairs.panels(jsdat)\n```\n\nIt looks like `commute` has a __big__ outlier!! \n```{r}\nmax(jsdat$commute)\n```\nIt's not impossible though (`r round(max(jsdat$commute))` km commute?), it's just quite different.. chances are this may come out as an influential datapoint in our model, but I have no obvious reason to exclude this observation just because they live far away! \n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nFit a model to examine the independent associations between job satisfaction and all of the aspects of working patterns collected by the company.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe want to look at the independent associations, so we want a multiple regression model.  \nWe don't really have any theoretical order in which to put the predictors in our model. However, this mainly matters when we use `anova(model)` - if we are just going to look at the coefficients (which is what we will be doing), then we can put them in any order for now. \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\njsmod <- lm(jobsat ~ commute + hrs_worked + team_size + remote + salary_band, jsdat)\nsummary(jsmod)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nCheck the assumption plots from the model. Remember we had that employee who was commuting quite far - are they exerting too much influence on our model?  \nIf so, maybe refit the model without them.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe can see below that there is that one very influential observation (see bottom right plot). It's almost 6 times more than the next most influential one.  \n```{r echo=c(2)}\npar(mfrow=c(2,2))\nplot(jsmod, which=c(1:4))\npar(mfrow=c(1,1))\n```\n\nWe can see from the plots that it is row 1 in the data, and below we confirm this:  \n```{r}\nsummary(influence.measures(jsmod))\n```\n\nWe'll refit the model without that observation:  \n```{r}\njsmod1 <- lm(jobsat ~ commute + hrs_worked + \n               team_size + remote + salary_band, \n             data = jsdat[-1,])\n```\nAnd check again. These look much better in terms of influencers. The main assumption plots don't look _terrible_, but they're not perfect (but then nothing ever is!).  \n```{r echo=c(2)}\npar(mfrow=c(2,2))\nplot(jsmod1, which=c(1:4))\npar(mfrow=c(1,1))\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nNow fit a second model, in which the predictors are standardised (those that can be).  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- You can either standardise just the predictors, or standardise both predictors and outcome (see [8A #standardised-coefficients](08a_scaling.html#standardised-coefficients){target=\"_blank\"}).\n- If you've excluded some data in the model from the previous question, you should exclude them here too.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\njsmodZ <- lm(scale(jobsat) ~ scale(commute) + scale(hrs_worked) + \n               scale(team_size) + remote + scale(salary_band), \n             data = jsdat[-1,])\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nLooking at the standardised coefficients, which looks to have the biggest impact on employees' job satisfaction?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nSalary has a big positive impact.\nThen remote working (negative impact)\nThen hours worked (negative impact)\nThen team size (positive)\nThen commuting distance (positive, but non-significant)\n```{r}\nsummary(jsmodZ)\n```\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nThe company can't afford to pay employees anymore, and they are committed to letting staff work remotely. \nWhat would you suggest they do to improve job satisfaction?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nYou might argue here that because `hrs_worked` has the next biggest effect, we should suggest the company reduces the working hours.  \n\nThe standardised coefficients are telling us that:  \n\n- a person who works 1 SD hours more is .19 SD lower on job satisfaction\n- a person in a team 1 SD bigger is .16 SD higher on job satisfaction  \n\nSo what _are_ the standard deviations of `hrs_worked` and of `team_size`? Are they even worth comparing?  \n\n```{r}\nsd(jsdat$hrs_worked)\nsd(jsdat$team_size)\n```\n\nSo what our standardised coefficients are really saying is:\n\n- a person works 13.4 hours more is .19 SD lower on job satisfaction\n- a person in a team 1.67 people bigger is .16 SD higher on job satisfaction  \n\nYou could easily argue from this that a much easier way the company could improve job satisfaction is to make team sizes a bit bigger - rather than letting people work 13 hours less than they currently do! \n\n`r solend()`\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n\n# Pictures of a Brain\n\n:::frame\n__neuronews.csv__  \n\n```{r}\n#| include: false\nset.seed(9)\ndf = tibble(\n  pid = paste0(\"ppt_\",1:120),\n  name = randomNames::randomNames(120, which.names=\"first\"),\n  condition = rep(c(\"text-only\",\"text+brain\",\"text+brain+graph\"),e=40),\n  credibility = 58.2 + \n    (condition==\"text+brain\")*4.7 +\n    (condition==\"text+brain+graph\")*5.322 + \n    rnorm(120, 0, 10.3)\n)\n#write_csv(df,file=\"../../data/usmr_neuronews.csv\")\nlm(credibility~condition,df) |> summary()\n```\n\nThis dataset is from a study^[not a real one, but inspired very loosely by [this one](https://bpspsychub.onlinelibrary.wiley.com/doi/full/10.1111/bjep.12162){target=\"_blank\"}] looking at the influence of presenting scientific news with different pictures on how believable readers interpret the news.  \n\n`r nrow(df)` participants took part in the study. Participation involved reading a short article about some research in neuroscience, and then rating how credible they found the research. Participants were randomly placed into one of three conditions, in which the article was presented a) in text alone, b) with a picture of a brain, or c) with a picture of a brain and a fancy looking (but unrelated to the research) graph. They rated credibility using a sliding scale from 0 to 100, with higher values indicating more credibility.  \n\nThe data is available at [https://uoepsy.github.io/data/usmr_neuronews.csv](https://uoepsy.github.io/data/usmr_neuronews.csv){target=\"_blank\"}.  \n\n```{r}\n#| echo: false\n#| tbl-cap: \"usmr_neuronews.csv data dictionary\"\ntibble(\n  variable=names(df),\n  description = c(\n    \"Participant ID\",\n    \"Participant Name\",\n    \"Condition (text-only / text+brain / text+brain+graph)\",\n    \"Credibility rating (0-100)\"\n  )\n) |> gt::gt()\n```\n\n\n:::\n\n`r qbegin(qcounter())`\nRead in the data and take a look around (this is almost always the first thing to do!)  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nnndat <- read_csv(\"https://uoepsy.github.io/data/usmr_neuronews.csv\")\n\nhist(nndat$credibility)\n# geom jitter is a way of randomly 'jittering' points so that the don't overlap\n# i want them to be the right height, so no jitter in height, but i'll give them a little width jitter\nggplot(nndat, aes(x=condition,y=credibility)) +\n   geom_jitter(height = 0, width = .2)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nFit a model examining whether credibility of the research article differs between conditions.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nmod1 <- lm(credibility ~ condition, data = nndat)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nDo conditions differ in credibility ratings?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThis is an overall question, not asking about differences between specific levels. You can find a way to test this question either at the bottom of the `summary()` output, or by comparing it with a model without condition differences in it (see [8B #testing-group-differences](08b_catpred.html#testing-group-differences){target=\"_blank\"}).  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe can either do this as a model comparison:\n```{r}\nmod0 <- lm(credibility ~ 1, data = nndat)\nanova(mod0, mod1)\n```\n\nand this info is also at the bottom of the summary, because we just have one predictor in the model\n```{r}\n#| eval: false\nsummary(mod1)\n```\n```\nF-statistic: 3.246 on 2 and 117 DF,  p-value: 0.04245\n```\n\n:::int\nConditions significantly differed in credibility ratings $F(2,117) = 3.25, p = .0425$\n:::\n\n`r solend()`\n\n`r qbegin(qcounter())`\n__How__ do groups differ?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nNote that this is a subtly different question to the previous one. It will require us to look at something that tests between specific groups ([8B #testing-differences-between-specific-groups](08b_catpred.html#testing-differences-between-specific-groups){target=\"_blank\"}).  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| include: false\nres = broom::tidy(mod1)\nres$estimate = round(res$estimate,2)\nres$statistic = round(res$statistic,2)\nres$p.value = format.pval(res$p.value,eps=.001,digits=2)\nres$p.value = ifelse(grepl(\"<\",res$p.value), res$p.value, paste0(\"=\",res$p.value))\n```\n\n\n```{r}\nsummary(mod1)\n```\n\n:::int\nCompared to when presented in text-only, conditions where the article was presented alongside a picture of a brain, or alongside both a brain-picture and a graph, resulted in higher average credibility ratings. Including a picture of a brain was associated with a `r res[2,2]` increase in credibility over the text-only article ($b = `r res[2,2]`$, $t(`r mod1['df.residual']`)=`r res[2,4]`$, $p`r res[2,5]`$), and including both a brain-picture and a graph was associated with a `r res[3,2]` higher average credibility rating ($b = `r res[3,2]`$, $t(`r mod1['df.residual']`)=`r res[3,4]`$, $p`r res[3,5]`$).  \n:::\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nLet's prove something to ourselves.  \nBecause we have no other predictors in the model, it should be possible to see how the coefficients from our model map exactly to the group means.  \n\nCalculate the mean credibility for each condition, and compare with your model coefficients.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nTo calculate group means, we can use `group_by()` and `summarise()`! \n\n:::\n\n\n`r qend()`\n`r solbegin(label=\"Solution Part 1 - calculate group means\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\nnndat |> \n  group_by(condition) |>\n  summarise(\n    meancred = mean(credibility)\n  )\n```\n\n`r solend()`\n`r solbegin(label=\"Solution Part 2 - compare to the coefficients\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\nHere are our model coefficients:\n```{r}\ncoef(mod1)\n```\n\nAnd here are our means:\n```{r}\nnndat |> \n  group_by(condition) |>\n  summarise(\n    meancred = mean(credibility)\n  )\n```\n\nWe can see that the intercept is the mean of the `text-only` group.  \n\nThe next coefficient - named \"conditiontext+brain\" is the difference from the `text-only` group to the `text+brain` group.  \n\nThe final coefficient - named \"conditiontext+brain+graph\" is the difference from the `text-only` group to the `text+brain+graph` group.  \n\nSo this means we can get to our group means by using:\n```{r}\ncoef(mod1)[c(1,2)] |> sum() # the mean of text+brain\ncoef(mod1)[c(1,3)] |> sum() # the mean of text+brain+graph\n```\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Detectorists\n\n`r qbegin(qcounter())`\nWe saw this study briefly at the end of last week and we're going to continue where we left off.  \n\nBelow is the description of the study, and the code that we had given you to clean the data and fit a model that assessed whether different strategies (e.g. breathing, closing eyes) were associated with changes in heart rate during a lie-detection test. The model also accounts for differences in heart rates due to age and pre-test anxiety.  \n\nRun the code given below to ensure that we are all at the same place.  \n\n::: {.callout-note collapse=\"true\"}\n#### Study: Lie detectors\n\n```{r}\n#| include: false\nss=88173#round(runif(1,1e3,1e5))\nset.seed(ss)\nldf = tibble(\n  age = round(runif(142,22,64)),\n  anx = round(rnorm(142,0,1),2),\n  strategy = rbinom(142, 3, plogis(scale(anx)*2)),\n  hr = 45 + .3*age +3*anx+ (strategy==2)*-3.4 + (strategy==3)*-5.6 + rnorm(142,0,3)\n) \n#psych::pairs.panels(ldf)\n#summary(lm(hr~age+anx+factor(strategy),ldf))\n#plot(lm(hr~age+anx+factor(strategy),ldf),which=4)\nldf$hr <- round(ldf$hr,1)\nldf$strategy[60] <- 5\nldf$hr[ldf$hr>75]<- 37.0\n#write_csv(ldf,file=\"../../data/usmr_polygraph.csv\")\n```\n\nLaw enforcement in some countries regularly rely on 'polygraph' tests as a form of 'lie detection'. These tests involve measuring heart rate, blood pressure, respiratory rate and sweat. However, there is very little evidence to suggest that these methods are remotely accurate in being able to determine whether or not someone is lying.  \n\nResearchers are interested in if peoples' heart rates during polygraph tests can be influenced by various pre-test strategies, including deep breathing, or closing their eyes. They recruited `r nrow(ldf)` participants (ages `r min(ldf$age)` to `r max(ldf$age)`). Participants were told they were playing a game in which their task was to deceive the polygraph test, and they would receive financial rewards if they managed successfully. At the outset of the study, they completed a questionnaire which asked about their anxiety in relation to taking part. Participants then chose one of 4 strategies to prepare themselves for the test, each lasting 1 minute. These were \"do nothing\", \"deep breathing\", \"close your eyes\" or \"cough\"^[apparently coughing is a method of immediately lowering heart rate!]. The average heart rate of each participant was recorded during their test. \n\n```{r}\n#| echo: false\n#| tbl-cap: \"usmr_polygraph.csv data dictionary\"\ntibble(\n  variable = names(ldf),\n  description = c(\n    \"Age of participant (years)\",\n    \"Anxiety measure (Z-scored)\",\n    \"Pre-test Strategy (0 = do nothing, 1 = close eyes, 2 = cough, 3 = deep breathing)\",\n    \"Average Heart Rate (bpm) during test\"\n  )\n) |> gt::gt()\n```\n\n__Analysis__  \n\nExpand the code box below to see some analysis!  \n```{r}\n#| eval: false\n#| code-fold: true\n# load libraries\nlibrary(tidyverse)\nlibrary(psych)\n# read in the data\nliedf <- read_csv(\"https://uoepsy.github.io/data/usmr_polygraph.csv\")\n\n# there seems to be a 5 there.. \ntable(liedf$strategy)\n# the other variables look okay though\ndescribe(liedf)\npairs.panels(liedf)\n\nliedf <- liedf |> \n  filter(strategy!=5) |>\n  mutate(\n    # strategy is a factor. but currently numbers\n    # i'm going to give them better labels too.. \n    # to do this is need to tell factor() what \"levels\" to look for\n    # and then give it some \"labels\" to apply to those.\n    strategy = factor(strategy, \n                      levels = c(\"0\",\"1\",\"2\",\"3\"),\n                      labels = c(\"do nothing\", \"close eyes\",\n                                 \"cough\", \"deep breathing\")\n                      )\n  )\n\nliemod <- lm(hr ~ age + anx + strategy, data = liedf)\n\n# Does HR differ between strategies?\nanova(liemod)\n# the above is a shortcut for getting this comparison out:\nanova(\n  lm(hr ~ age + anx, data = liedf),\n  lm(hr ~ age + anx + strategy, data = liedf)\n)\n```\n\n:::\n\n`r qend()`\n```{r}\n#| include: false\nliedf <- read_csv(\"https://uoepsy.github.io/data/usmr_polygraph.csv\")\nliedf <- liedf |> \n  filter(strategy!=5) |>\n  mutate(\n    strategy = factor(strategy, \n                      levels = c(\"0\",\"1\",\"2\",\"3\"),\n                      labels = c(\"do nothing\", \"close eyes\",\n                                 \"cough\", \"deep breathing\")\n                      )\n  )\nliemod <- lm(hr ~ age + anx + strategy, data = liedf)\n```\n\n\n`r qbegin(qcounter())`\nOur model includes a predictor with 4 levels (the 4 different strategies).  \n\nThis means we will have 3 coefficients pertaining to this predictor. Take a look at them. Write a brief sentence explaining what each one represents.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe're still all using R's defaults ([8B #treatment-contrasts-the-default](08b_catpred.html#treatment-contrasts-the-default){target=\"_blank\"}), so these follow the logic we have seen already above. \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\ncoef(liemod)\n```\n\n```{r}\n#| echo: false\nbroom::tidy(liemod) |>\n  transmute(\n    term, estimate = round(estimate,2),\n    interpretation = c(\n      \"estimated HR for someone age 0, anxiety 0 (the mean), who 'does nothing' prior to the test\",\n      \"estimated change in HR for an additional year of age, holding constant anxiety and strategy\",\n      \"estimated change in HR for an additional SD of anxiety, holding constant age and strategy\",\n      \"estimated difference in HR between 'doing nothing' strategy and 'close eyes' strategy, holding constant age and anxiety\",\n      \"estimated difference in HR between 'doing nothing' strategy and 'cough' strategy, holding constant age and anxiety\",\n      \"estimated difference in HR between 'doing nothing' strategy and 'deep breathing' strategy, holding constant age and anxiety\"\n    )\n  ) |> gt::gt()\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nCalculate the mean heart rate for each strategy group.  \nDo they match to our model coefficients?  \n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n```{r}\nliedf |> \n  group_by(strategy) |>\n  summarise(\n    meanHR = mean(hr)\n  )\n```\n\nThey don't match! For instance, in our group means above the 'close eyes' strategy has a _higher_ average heart rate than the 'do nothing' strategy. But that's not what our model coefficients said - they said the opposite! \n\nWhy is this? It's because we have other things in our model, so these model estimated differences are now \"holding age and anxiety constant\". \n\n`r solend()`\n\n`r qbegin(qcounter())`\nAt the end last week's exercises, we wanted you to make a plot of the model estimated differences in heart rates between the strategies.  \n\nChances are that you followed the logic of 1) create a little plotting data frame, 2) use `augment()` from the broom package, and 3) shove it into ggplot.  \n\nYou may have ended up with something like this:  \n```{r}\n#| code-fold: true\n#| out-height: \"300px\"\nplotdat <- data.frame(\n  age = mean(liedf$age),\n  anx = mean(liedf$anx),\n  strategy = unique(liedf$strategy)\n)\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\")\n```\n\nThese plots are great, but they don't really show the underlying spread of the data. They make it seem like _everybody_ in the 'do nothing' strategy will have a heart rate between 56 and 59bpm. But that interval is where we expect the _mean_ to be, not where we expect the individuals scores to be.  \n\nCan you add the original raw datapoints to the plot, to present a better picture?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThis is tricky, and we haven't actually seen it anywhere in the readings or lectures.  \nThe thing that we're giving to ggplot (the output of the `augment` function) doesn't have the data in it.  \nWith ggplot we can actually pull in data from different sources:  \n```{r}\n#| eval: false\nggplot(data1, aes(x=x,y=y)) + \n  geom_point() +\n  geom_point(data = data2, aes(x=x,y=newy))\n```\n:::\n\n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\nSo here was our nice plot:  \n```{r}\nplotdat <- data.frame(\n  age = mean(liedf$age),\n  anx = mean(liedf$anx),\n  strategy = unique(liedf$strategy)\n)\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\")\n```\n\nWe want to add to that the original data.. but all that original data is in a different dataset!  \n\nHowever, that doesn't mean we can't use it.  \n\nLet's try to add points for all our heart rates, and we'll tell `geom_point()` to look in our original dataset.  \n\nThe thing we need to be careful with is that our original dataset doesn't have anything called `.fitted` in it, which is currently on our y-axis. So we need to specify the y we want it to use (in this case, the `hr` variable)\n\n```{r}\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\") +\n  geom_point(data = liedf, aes(y = hr))\n```\n\nOkay, the data is on there.. now we just need to make it look nice. My instinct here is to jitter again.\nI'm also going to make the plots black, and a little transparent, so that we can still see the means.  \n\n```{r}\nbroom::augment(liemod, \n               newdata=plotdat, \n               interval=\"confidence\") |>\n  ggplot(aes(x=strategy,y=.fitted, col=strategy))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper)) +\n  guides(col=\"none\") +\n  geom_jitter(data = liedf, aes(y = hr), width = .2, height = 0,\n              col = \"black\", alpha = .3)\n```\n\nThis is just _one_ way to better visualise our findings.  \n\nIf you're feeling adventurous, try: \n\n- a violin plot to show the distributions, rather than jittered points\n- what happens when you put `position = position_nudge(x=.3)` inside the `geom_pointrange()`?  \n- what happens if you add `coord_flip()` to the plot?  \n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Job Satisfaction, Guaranteed\n\n:::frame\n__jobsatpred.csv__  \n\nA company is worried about employee turnover, and they are trying to figure out what are the main aspects of work life that predict their employees' job satisfaction. 86 of their employees complete a questionnaire that leads to a 'job satisfaction score', that can range between 6 to 30 (higher scores indicate more satisfaction).     \n\nThe company then linked their responses on the questionnaire with various bits of information they could get from their human resources department, as seen in @tbl-jobsatdict.  \n\nThe data are available at [https://uoepsy.github.io/data/usmr_jobsatpred.csv](https://uoepsy.github.io/data/usmr_jobsatpred.csv){target=\"_blank\"}.  \n\n```{r}\n#| echo: false\n#| label: tbl-jobsatdict\n#| tbl-cap: \"usmr_jobsatpred.csv data dictionary\"\n\nN = 93\nset.seed(974255.8)\ndf = tibble(\n  name = randomNames::randomNames(N,which.names=\"last\"),\n  commute = rchisq(N,4),\n  hrs_worked = round(sample(c(16,28,35,40),86,T,prob=c(.1,.05,.6,.25)) +\n                       rnorm(N,0,10)),\n  team_size = rpois(N,3.2),\n  remote = sample(0:1,N,T,c(.8,.2)),\n  salary_band = sample(1:10, N, T, prob = (10:1)/(sum(1:10)))\n) |>\n  mutate(\n    hrs_worked = pmax(0,hrs_worked),\n    jobsat = commute*0 + hrs_worked*-.1 + team_size*1.2 + remote*-4 + salary_band*3 + rnorm(N,0,4),\n    jobsat = round(scale(jobsat)[,1]*4.5 + 19.5),\n    commute = round(commute,1)\n  )\n\ndf$commute[which.max(df$commute)] <- df$commute[which.max(df$commute)]*3\ndf$commute = format(df$commute,digits=1)\ndf$commute = gsub(\"\\\\.\",\",\",df$commute)\ndf$remote[sample(1:N,1)] <- 2\ndf$jobsat[sample(1:N,2)] <- 0\n#write_csv(df,file=\"../../data/usmr_jobsatpred.csv\")\njsdat <- df\n# summary(df)\n# psych::multi.hist(df[,-c(1,2)], global=FALSE)\n# lm(jobsat ~ ., df) |> summary()\ntibble(\n  variable = names(df),\n  description  = c(\n    \"Employee Surname\",\n    \"Distance (km) to employee residence (note, records have used a comma in place of a decimal point)\",\n    \"Hours worked in previous week\",\n    \"Size of the team in which the employee works\",\n    \"Whether the employee works in the office or remotely (0 = office, 1 = remote)\",\n    \"Salary band of employee (range 1 to 10, with those on band 10 earning the most)\",\n    \"Job Satisfaction Score (sum of 6 questions each scored on 1-5 likert scale)\"\n  )\n) |> gt::gt()\n```\n\n:::\n\n\n`r qbegin(qcounter())`\nRead in the data. Look around, do any cleaning that might be needed.\n\n`r qend()`\n`r solbegin(label=\"Solution Part 1 - identifying problems\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\njsdat <- read_csv(\"https://uoepsy.github.io/data/usmr_jobsatpred.csv\")\n```\nJust looking at the top of the data, I already know that `commute` is a problem..  \n```{r}\nhead(jsdat)\n```\n\nThe summary shows us a few other things..\n\n- `remote` has a max of 2, but it should be 0s and 1s, because it's a binary variable according to our data dictionary.  \n- `jobsat` should surely be between 6 and 30 (this is that idea of 6 questions where you score between 1 and 6 means your minimum should be 6, and maximum should be 30). Here we have a min of 0 and a max of 31 so something's amiss there.  \n\n```{r}\nsummary(jsdat)\n```\n\n\n`r solend()`\n`r solbegin(label=\"Solution Part 2 - figuring out fixes\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\nVariables are all appropriate types apart from `commute`.  \nIf we're being good, we should probably also set `remote` to be a factor as it is two distinct categories.  \n\nFor the commute variable, we need to first (prior to making it a numeric variable), replace all the commas with a point - \".\". We can do this using `gsub()`.  \n\nFor the remote variable, we know it only has two possible levels - 0 and 1. However, we saw earlier that there was a 2 in there as well. But there's only one of them, so it's probably a typo or something.  \n```{r}\nsum(df$remote==2)\n```\nWe should probably replace that with an NA as it is an impossible value, and we have no idea whether it is supposed to be a 0 or a 1.  \nAs it happens, by giving the `factor()` function only the levels we want to keep, it will replace any others with `NA`s.  \n```{r}\nfactor(c(\"dog\",\"cat\",\"parrot\"), levels=c(\"dog\",\"cat\"))\n```\n\nWe also noted that the `jobsat` variable had some values that were $<6$ and $>30$. \nWe can replace those with NAs as well, using that handy `ifelse()` function!  \n```{r}\nx <- c(1,16,50,72)\n# if x is less than 10 or x is less than 70, give me an NA, \n# otherwise give me the value of x\nifelse(x<10 | x>70, NA, x)\n```\n\n`r solend()`\n`r solbegin(label=\"Solution Part 3 - implementing\", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`\n\nLet's do this bit of cleaning all in one mutate:  \n```{r}\njsdat <- \n  jsdat |> \n    mutate(\n      commute = as.numeric(gsub(\",\",\".\",commute)),\n      remote = factor(remote, levels = c(\"0\",\"1\"),\n                      labels = c(\"office\",\"remote\")),\n      jobsat = ifelse(jobsat < 6 | jobsat > 30, NA, jobsat)\n    )\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nExplore! Make some quick plots and calculate some quick descriptives.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(tableone)\nCreateTableOne(data = jsdat |> select(-name))\n```\n\n\n```{r}\nlibrary(psych)\npairs.panels(jsdat)\n```\n\nIt looks like `commute` has a __big__ outlier!! \n```{r}\nmax(jsdat$commute)\n```\nIt's not impossible though (`r round(max(jsdat$commute))` km commute?), it's just quite different.. chances are this may come out as an influential datapoint in our model, but I have no obvious reason to exclude this observation just because they live far away! \n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nFit a model to examine the independent associations between job satisfaction and all of the aspects of working patterns collected by the company.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe want to look at the independent associations, so we want a multiple regression model.  \nWe don't really have any theoretical order in which to put the predictors in our model. However, this mainly matters when we use `anova(model)` - if we are just going to look at the coefficients (which is what we will be doing), then we can put them in any order for now. \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\njsmod <- lm(jobsat ~ commute + hrs_worked + team_size + remote + salary_band, jsdat)\nsummary(jsmod)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nCheck the assumption plots from the model. Remember we had that employee who was commuting quite far - are they exerting too much influence on our model?  \nIf so, maybe refit the model without them.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe can see below that there is that one very influential observation (see bottom right plot). It's almost 6 times more than the next most influential one.  \n```{r echo=c(2)}\npar(mfrow=c(2,2))\nplot(jsmod, which=c(1:4))\npar(mfrow=c(1,1))\n```\n\nWe can see from the plots that it is row 1 in the data, and below we confirm this:  \n```{r}\nsummary(influence.measures(jsmod))\n```\n\nWe'll refit the model without that observation:  \n```{r}\njsmod1 <- lm(jobsat ~ commute + hrs_worked + \n               team_size + remote + salary_band, \n             data = jsdat[-1,])\n```\nAnd check again. These look much better in terms of influencers. The main assumption plots don't look _terrible_, but they're not perfect (but then nothing ever is!).  \n```{r echo=c(2)}\npar(mfrow=c(2,2))\nplot(jsmod1, which=c(1:4))\npar(mfrow=c(1,1))\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nNow fit a second model, in which the predictors are standardised (those that can be).  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- You can either standardise just the predictors, or standardise both predictors and outcome (see [8A #standardised-coefficients](08a_scaling.html#standardised-coefficients){target=\"_blank\"}).\n- If you've excluded some data in the model from the previous question, you should exclude them here too.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\njsmodZ <- lm(scale(jobsat) ~ scale(commute) + scale(hrs_worked) + \n               scale(team_size) + remote + scale(salary_band), \n             data = jsdat[-1,])\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nLooking at the standardised coefficients, which looks to have the biggest impact on employees' job satisfaction?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nSalary has a big positive impact.\nThen remote working (negative impact)\nThen hours worked (negative impact)\nThen team size (positive)\nThen commuting distance (positive, but non-significant)\n```{r}\nsummary(jsmodZ)\n```\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nThe company can't afford to pay employees anymore, and they are committed to letting staff work remotely. \nWhat would you suggest they do to improve job satisfaction?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nYou might argue here that because `hrs_worked` has the next biggest effect, we should suggest the company reduces the working hours.  \n\nThe standardised coefficients are telling us that:  \n\n- a person who works 1 SD hours more is .19 SD lower on job satisfaction\n- a person in a team 1 SD bigger is .16 SD higher on job satisfaction  \n\nSo what _are_ the standard deviations of `hrs_worked` and of `team_size`? Are they even worth comparing?  \n\n```{r}\nsd(jsdat$hrs_worked)\nsd(jsdat$team_size)\n```\n\nSo what our standardised coefficients are really saying is:\n\n- a person works 13.4 hours more is .19 SD lower on job satisfaction\n- a person in a team 1.67 people bigger is .16 SD higher on job satisfaction  \n\nYou could easily argue from this that a much easier way the company could improve job satisfaction is to make team sizes a bit bigger - rather than letting people work 13 hours less than they currently do! \n\n`r solend()`\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"08_ex.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"code-annotations":"hover","link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"Exercises: Scaling | Categorical Predictors","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}