{"title":"8B: Categorical Predictors","markdown":{"yaml":{"title":"8B: Categorical Predictors","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Binary Predictors","containsRefs":false,"markdown":"\n\n```{r}\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\n```\n\n\n```{r}\n#| include: false\nset.seed(50)\nmm <- MASS::Animals\nmm <- mm[c(\"Rhesus monkey\",\"Potar monkey\",\"Chimpanzee\"), ]\nmm$n <- rdunif(3,8,14)\nmm$sbody <- c(2,7,6)\nmm$sbrain <- rep(100,3)\n\nmm <- \n  mm %>% \n  mutate(\n    species = row.names(mm),\n    mass_body = pmap(list(n,body,sbody),~round(rnorm(..1,..2,..3))),\n    mass_brain = pmap(list(n,brain,sbrain),~round(rnorm(..1,..2,..3))/1000)\n  ) %>%\n  select(species,mass_body,mass_brain) %>%\n  unnest() %>% \n  sample_n(n())\n\nmm$mass_body = mm$mass_body + 10\nmm$mass_brain = mm$mass_brain + .2\nmm$species[mm$species==\"Chimpanzee\"]<-\"Human\"\nmm$mass_brain[mm$species==\"Rhesus monkey\"]<-mm$mass_brain[mm$species==\"Rhesus monkey\"]+.06\nmm$age = round(4 + mm$mass_body * .06 + rnorm(nrow(mm),0,3))\nmm$age = round(runif(nrow(mm),2,34))\nbraindata = mm |> select(-mass_body) |> sample_n(n())\n#write_csv(braindata, \"../../data/usmr_braindata.csv\")\n```\n\n\n\nSince we've started working with linear regression, we've seen a few examples of categorical variables as predictors in linear models, and most (if not all) of these have been binary predictors - i.e. having just *two* categories (yes/no, dog/cat, phonics/word). We talked initially ([5B#binary-predictors](05b_slr.html#binary-predictors){target=\"_blank\"}) about how these get entered in the model as 0s and 1s.  \n\nLet's consider a dataset which includes the brain mass and body mass of different primates. Below, we can see the `species` variable has 3 levels indicating whether the observation is a \"Human\", a \"Rhesus monkey\", or a \"Potar monkey\". We can easily create a binary variable such as the `isMonkey` variable (created below), which will allow us to compare monkeys (of _any_ type) vs not-monkeys (or \"Humans\", as we are better known!).  \n\n```{r}\nlibrary(tidyverse)\nbraindata <- read_csv(\"https://uoepsy.github.io/data/usmr_braindata.csv\")\n\nbraindata <- braindata %>% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\")\n)\nhead(braindata)\n```\n\nTo recap what we've seen already - when we use a binary variable like `isMonkey` in a model as a predictor, it gets inputted into the model as a column of 0s and 1s. Because the coefficients of regression models are always interpreted as \"the change in $y$ associated with a 1 unit change in $x$\", the use of 0s and 1s for different levels of a categorical variable allows us to make \"a 1 unit change in $x$\" represent moving from one level to another. Internally, our model is relying on a variable like the `isMonkeyYES` variable below:   \n\n```{r}\n#| echo: false\nbraindata %>% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\"),\n  isMonkeyYES = (isMonkey==\"YES\")*1\n) |> head()\n\n```\n\n\nThis means that from the model below we can get out an estimate of the difference in brain mass from the group `isMonkey == \"NO\"` to the group `isMonkey == \"YES\"`, because this is moving from 0 to 1. We can see this in the model and visualisation @fig-binpredplot.  \n\n\n::::panelset\n:::panel\n#### Model\n```{r}\n#| eval: false\nmonkmod <- lm(mass_brain~isMonkey, data = braindata)\nsummary(monkmod)\n```\n```{r}\n#| echo: false\nmonkmod <- lm(mass_brain~isMonkey, data = braindata)\n.pp(summary(monkmod),l=list(9:12))\n```\n\n- `(Intercept)`: the estimated brain mass of Humans (when the isMonkeyYES variable is zero)\n- `isMonkeyYES`: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1). \n\n\n:::\n:::panel\n#### Visual\n```{r}\n#| label: fig-binpredplot\n#| fig-cap: \"A binary categorical predictor\"\n#| echo: false\nfit <- lm(mass_brain~isMonkey, braindata)\nggplot(braindata, aes(x=isMonkey,y=mass_brain))+\n  geom_jitter(width=.05, alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=isMonkey)) + \n  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"solid\") +\n  geom_segment(aes(x=2,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  geom_segment(aes(x=2,xend=1,y=sum(coef(fit)[1]),yend=sum(coef(fit)[1])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  guides(col=\"none\") +\n  scale_x_discrete(labels=c(\"0\\n'NO'\",\"1\\n'YES'\")) +\n  labs(title = \"lm(mass_brain ~ isMonkey)\") +\n  annotate(geom=\"text\",x=1,y=coef(fit)[1]+.03,hjust=-0.1, label=\"Intercept\", hjust=1.1)+\n  annotate(geom=\"text\",x=2.1,y=coef(fit)[1]-.2, label=\"isMonkeyYES\\n coefficient\", hjust=0, angle=90)\n```\n\n:::\n::::\n  \n<br>\nWhen used as predictors in **multiple** regression models (i.e. when there are other predictors involved), binary variables behave much the same way. The coefficient will give us the estimated change in $y$ when moving from one level to the other^[and the intercept will be the estimated $y$ when **all** predictors are zero], _while holding other predictors constant_ (see [7A#interpreting-coefficients](07a_mlr.html#interpreting-multiple-regression-coefficients)).  \n\n```{r}\n#| eval: false\nmonkmod2 <- lm(mass_brain~age + isMonkey, data = braindata)\nsummary(monkmod2)\n```\n```{r}\n#| echo: false\nmonkmod2 <- lm(mass_brain~age + isMonkey, data = braindata)\n.pp(summary(monkmod2),l=list(9:13))\n```\n\n- `(Intercept)`: the estimated brain mass of *new-born* Humans (when both age is zero and isMonkeyYES is zero)\n- `age`: the estimated change in brain mass for every 1 year increase in age, holding isMonkey constant.\n- `isMonkeyYES`: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1), holding age constant.  \n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: a visual\n\nIf we want a visual intuition to how a binary predictor works in multiple regression - it's actually just like any other predictor, in that it's just another dimension to the model. The only difference is that it is on a __discrete__ scale - observations fall on _either_ 0 _or_ 1, not on the continuum in between.  \n  \nSo for the model `mass_brain ~ age + isMonkey`, rather than a 3D surface, we might think of our model as two edges of a surface, as shown in the left-hand panel of @fig-binpredmult. \nAnd if 'monkey-ness' was some sort of measurable continuum (\"I'm 70% monkey?\") then this just joins those edges up back to a surface (right panel).  \n\n\n```{r}\n#| echo: false\n#| label: fig-binpredmult\n#| fig.cap: \"a binary predictor is just another dimension, but data can only exist at either 0 or 1. If a variable were continuous, then observations can take any value along a line\"  \npar(mfrow=c(1,2))\ndf <- braindata |> \n  transmute(x1 = age,x2=(isMonkey==\"YES\")*1,y=mass_brain)\n\nfit<-lm(y~x1+x2,df)\nsteps=20\nx1 <- with(df, seq(min(x1),max(x1),length=steps))\nx2 <- with(df, seq(min(x2),max(x2),length=steps))\nnewdat <- expand.grid(x1=x1, x2=x2)\ny <- matrix(predict(fit, newdat), steps, steps)\np <- persp(x1,x2,y, theta = 55,phi=10, col = NA,\n           border=NA,\n           xlab=\"age\",ylab=\"isMonkey\",zlab=\"mass_brain\",\n           main=\"mass_brain~age+isMonkey\",zlim=c(0,1))\nobs <- with(df, trans3d(x1 ,x2, y, p))\ndf1 = data.frame(x1=2:34,x2=0)\ndf1$y = predict(fit,df1)\ndf2 = data.frame(x1=2:34,x2=1)\ndf2$y = predict(fit,df2)\npred1 <- with(df1, trans3d(x1 ,x2, y, p))\npred2 <- with(df2, trans3d(x1 ,x2, y, p))\npoints(pred1, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(pred2, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(obs, col = \"red\", pch = 16)\n\nfit<-lm(y~x1+x2,df)\nsteps=20\nx1 <- with(df, seq(min(x1),max(x1),length=steps))\nx2 <- with(df, seq(min(x2),max(x2),length=steps))\nnewdat <- expand.grid(x1=x1, x2=x2)\ny <- matrix(predict(fit, newdat), steps, steps)\np <- persp(x1,x2,y, theta = 55,phi=10, col = NA,\n           #border=NA,\n           xlab=\"age\",\n           ylab=\"'monkey-ness continuum'\", \n           zlab=\"mass_brain\",zlim=c(0,1))\nobs <- with(df, trans3d(x1 ,x2, y, p))\ndf1 = data.frame(x1=2:34,x2=0)\ndf1$y = predict(fit,df1)\ndf2 = data.frame(x1=2:34,x2=1)\ndf2$y = predict(fit,df2)\npred1 <- with(df1, trans3d(x1 ,x2, y, p))\npred2 <- with(df2, trans3d(x1 ,x2, y, p))\npoints(pred1, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(pred2, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(obs, col = \"red\", pch = 16)\n\npar(mfrow=c(1,1))\n\n```\n\n:::\n\n\n# Multiple Categories  \n\nWhat about when we have a predictor with *more than* two categories? We might have lots of different conditions in our experiment, or we might have observations from lots of different distinct groups of people. \n\nIn the primate brains example, we have various different species of primates - we have 3 (\"Potar monkey\", \"Rhesus Monkey\" and \"Human\").  What happens when we consider a model which has a predictor like `species`?  \n\nFitting the model `lm(mass_brain ~ species)` will by default use one of our categories as a \"reference level\", and then our coefficients will show comparisons between each of the remaining levels to that reference level. So if the reference level is \"Human\", the coefficients we get out include:\n\n- the intercept (which is the estimated brain mass of humans); \n- the estimated difference in brain mass when we move from humans to potar monkeys; \n- and the difference when we move from humans to rhesus monkeys.  \n\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~species, braindata)),l=list(3,9:13))\n```\n\n\n:::rtip\n__How do i know which one is my reference level??__  \n\nNotice that our coefficients are named `speciesPotar monkey` and `speciesRhesus monkey`. When faced with a categorical predictor, the lm function will name coefficients in the format `variableLevel`. We can see that we have coefficients for both \"Potar monkey\" and \"Rhesus monkey\", so we can infer that the reference level is the one we can't see (\"Human\").^[This is all assuming that we have things set to their defaults in R. We'll see below that we can change things up using something called 'contrasts'.]  \n\nR will default to using alphabetical ordering, hence the reference level being set as \"Human\". We could override this by making it a factor with an ordering to it's levels (see the use of `factor()` and `levels()` in [2A#categorical](02a_measurement.html#categorical){target=\"_blank\"}). Functions like `fct_relevel()` might be handy too.  \n\n:::\n\n\nNote that while we have **3** species in our variable, we only have **2** coefficients in our model.  \nThis logic scales up - when we have categorical predictors with multiple levels, we will have $\\text{number-of-levels}-1$ coefficients in our model. This is because to determine which of the $k$ categories an observation is in, we actually only need to have $k-1$ binary variables.  \n\nFor instance, we can re-express all the the information contained in the `species` variable (with 3 levels) with just two binary variables:   \n```{r}\nbraindata <- braindata %>% mutate(\n  speciesPotar = ifelse(species == \"Potar monkey\", 1, 0),\n  speciesRhesus = ifelse(species == \"Rhesus monkey\", 1, 0),\n)\nbraindata %>% \n  select(mass_brain, species, speciesPotar, speciesRhesus) %>%\n  head()\n```\n\n- For a human, both `speciesPotar == 0` and `speciesRhesus == 0`\n- For a Potar monkey, `speciesPotar == 1` and `speciesRhesus == 0`\n- For a Rhesus monkey, `speciesPotar == 0` and `speciesRhesus == 1`\n\nThese two binary variables are actually pretty much what R will put into our model even though we only gave it the one `species` variable!  \n\nRecall that the intercept is the estimated outcome _when all predictors are zero_. In this case, when both variables are zero, we are looking at the Humans group. And when we move 1 on the `speciesPotar` scale, we move from the humans to the Potar monkeys. When we move 1 on the `speciesRhesus` scale, we move from humans to Rhesus monkeys. So each coefficient is comparing a level to the \"reference level\" (as in @fig-kpredplot).  \n\n```{r}\n#| label: fig-kpredplot\n#| fig-cap: \"A categorical predictor with 3 levels\"\n#| echo: false\nfit <- lm(mass_brain~species, braindata)\nggplot(braindata, aes(x=species,y=mass_brain))+\n  geom_jitter(width=.05,alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=species)) + \n  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=1,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1)])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=2,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=3,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1,3)])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=.5) +\n  geom_segment(aes(x=1,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1,3)])),col=\"blue\",lwd=.5) +\n  annotate(geom=\"text\",x=1,y=coef(fit)[1]+.03,hjust=-0.1,label=\"Intercept\") + \n  annotate(geom=\"text\",x=2.1,y=coef(fit)[1]-.3,hjust=0,label=\"speciesPotar\\n coefficient\", angle=90) + \n  annotate(geom=\"text\",x=3.1,y=coef(fit)[1]-.15,hjust=0,label=\"speciesRhesus\\n coefficient\", angle=90) + \n  guides(col=\"none\") +\n  labs(title = \"lm(mass_brain ~ species)\")+\n  scale_x_discrete(labels=c(\n    \"speciesPotar = 0\\nspeciesRhesus = 0\\nHuman\",\n    \"speciesPotar = 1\\nspeciesRhesus = 0\\nPotar monkey\",\n    \"speciesPotar = 0\\nspeciesRhesus = 1\\nRhesus monkey\"\n  ))\n```\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: proving it to ourselves\n\nTry fitting a model that uses the two binary variables we made above instead of using `species`. Take a look at the coefficients, try to compare the models to one another.   \nThey are identical! \n```{r}\n#| eval: false\nspecmod2 <- lm(mass_brain ~ speciesPotar + speciesRhesus, data = braindata)\nsummary(specmod2)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain ~ speciesPotar + speciesRhesus, data = braindata)),l=list(3,9:13))\n```\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: more categories = more dimensions\n\nWe've just seen how a categorical variable with $k$ levels gets inputted into our model as a set of $k-1$ predictors. Each of these $k-1$ predictors is actually just another dimension to the model.  \n\nWe could try (we maybe shouldn't) to re-express @fig-kpredplot as a 3D plot. It would look a little like this:  \n\n\n```{r}\n#| echo: false\n#| out-width: \"70%\"\nfit<-lm(mass_brain ~ species,braindata)\nbraindata2 <- as.data.frame(model.matrix(fit)[,2:3]) \nbraindata2$mass_brain = braindata$mass_brain\n\n\nlibrary(scatterplot3d)\nplt <- with(braindata2,scatterplot3d(`speciesRhesus monkey`,`speciesPotar monkey`,mass_brain, scale.y=1,angle=30,ylab=\"\",\n                                     y.ticklabs = c(0,NA,NA,NA,NA,1),\n                                     x.ticklabs = c(0,NA,NA,NA,NA,1),\n                                     main = \"mass_brain ~ species(human/potar monkey/rhesus monkey)\"))\ntext(x = 6.75, y = 0.45, \"speciesPotar monkey\", srt =15)\n\n\npp1 <- tibble(\n  `speciesRhesus monkey`=0,\n  `speciesPotar monkey`=seq(0,1,.1),\n  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[2],length.out=11)\n)\npp2 <- tibble(\n  `speciesRhesus monkey`=seq(0,1,.1),\n  `speciesPotar monkey`=0,\n  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[3],length.out=11)\n)\n\nplt$points(pp1$`speciesRhesus monkey`,pp1$`speciesPotar monkey`,pp1$y,type=\"l\",col=\"blue\")\nplt$points(pp2$`speciesRhesus monkey`,pp2$`speciesPotar monkey`,pp2$y,type=\"l\",col=\"blue\")\n```\n\n:::\n\n\n:::rtip\n__Good practice__  \n\nIn the primate-brains example, all this is happening as part of the model fitting process. This is because we are using variables with sets of characters (\"YES\" and \"NO\", or \"Human\", \"Potar monkey\" and \"Rhesus monkey\"). When we put these in a model, R doesn't know how else to interpret it other than as a set of categories - i.e. it gets interpreted as a 'factor' (see [2A#categorical](02a_measurement.html#categorical){target=\"_blank\"}) without us actually having to explicitly tell R that it is a factor.    \n\nIf we have a variable where a set of categories is represented by numbers, then the model will interpret them as numerical values (i.e. 2 is twice 1 etc).  \n\nIf a variable is categorical, it is good practice to make it a factor when you read in your data. That way you don't get into errors later on when modelling. \n\n:::\n\n## Tests  \n\nWe have just seen how entering a categorical predictor with $k$ levels in as a predictor to a regression model is essentially equivalent to adding in $k-1$ different binary variables. We get out $k-1$ coefficients relating to each of those variables.  \n\nIn our primate-brains example, our model `lm(mass_brain ~ species)` might look like it only has one predictor (`species`), but because it has 3 levels, we end up with 2 coefficients - we are essentially making a _multiple_ regression model. \nSo what can we *test* when it comes to `species`? The coefficients can let us test comparisons between specific levels, but we can also ask an overall question of \"do species differ?\" \n\n#### Testing differences between specific groups\n\n```{r}\n#| echo: false\nmod = lm(mass_brain~species,braindata)\nres = broom::tidy(mod)\nres[,2:4] = apply(res[,2:4],2,function(x) round(x,3))\nres$p.value = format.pval(res$p.value, digits=2,eps=.001)\nres$p.value = ifelse(!grepl(\"<\",res$p.value), paste0(\"=\",res$p.value),res$p.value)\nres$p.value = gsub(\"0\\\\.\",\"\\\\.\",res$p.value)\n```\n\nThe coefficients from our regression model represent specific comparisons between levels of a categorical predictor. The default behaviour in R is (as we have seen) to compare each level to the reference level.  \n\nThe tests (Std. Error, t value, df, p value) associated with these coefficient estimates therefore provide us with a means of testing whether the difference in $y$ between two levels is significantly different from zero.  \n\nFor example, from our model output below, we can say that..  \n\n:::int\n... both Potar monkeys and Rhesus monkeys had significantly lower brain mass than Humans ($b=`r res[[2,2]]`,t(`r mod[[\"df.residual\"]]`)=`r res[[2,4]]`, p`r res[[2,5]]`$ and $b=`r res[[3,2]]`,t(`r mod[[\"df.residual\"]]`)=`r res[[3,4]]`, p`r res[[3,5]]`$ respectively).  \n:::\n\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~species, braindata)),l=list(3,9:13))\n```\n\n#### Testing 'group differences'\n\nIt will hopefully come as little surprise that the way to test an overall question of \"are there differences between groups\" can be assessed by a model comparison between models with and without the predictor.  \n\nWhy? because this allows us to use the F test as a \"joint test\" (see [7A #model-comparisons](07a_mlr.html#model-comparisons){target=\"_blank\"}) of whether including all the information about groups explains a significant amount of additional variance in our outcome (i.e. if their inclusion provides a significant reduction in residual sums of squares).\n\nSo from our model comparison below, we can say something like...   \n\n:::int\n... species differences (Human/Potar monkey/Rhesus monkey) explained a significant amount of variance in brain mass over the null model ($F(2, 32) = 37.22,\\ p <.001$).\n:::\n\n```{r}\nm0 <- lm(mass_brain ~ 1, data = braindata)\nm1 <- lm(mass_brain ~ species, data = braindata)\nanova(m0, m1)\n```\n\n```{r}\n#| eval: false\n#| include: false\n\n# significant F, non-significant ts\nset.seed(7)\nx = rnorm(100)\ng = factor(rbinom(100,2,prob=plogis(x)))\ny = -.5*x + .5*(g==1)  + rnorm(100)\ndf = data.frame(x=x,y=y,g=g)\nlm(y~x) |> summary()\nlm(y~x+g) |> summary()\nanova(lm(y~x+g))\n```\n\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Contrasts\n\nWe can actually do *lots* of clever things with categorical predictors in regression models, in order to compare different groups to one another.  \n\nThe first thing we need to do is to explicitly tell R that they are categorical variables (i.e. we need to make them 'factors'): \n\n```{r}\nbraindata <- braindata %>% \n  mutate(\n    isMonkey = factor(isMonkey),\n    species = factor(species)\n  )\n```\n\nOnce we have done this we can see (and also manipulate) the way in which it gets treated by our model. This is because factors in R have some special attributes called \"contrasts\". Contrasts are ultimately the thing that the model will use to decide what you want to compare to what.  \n\nThe following code shows us the \"contrast matrix\" for a given variable. The rows of this show each level of our variable, and the columns are the coefficients (the comparisons which are estimated when we put the variable into a model).  \nWe can see that the default contrasts match the two binary variables we had created manually earlier on:  \n```{r}\n#| eval: false\ncontrasts(braindata$species)\n```\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/contrasts.png\")\n```\n\nWe can easily \"relevel\" factor, thereby changing which one is the 'reference level' (the level against which all other levels are compared), by using handy functions like `fct_relevel()`. For instance, if we wanted to see how each species compared to Rhesus monkeys, re-levelling the factor changes the contrasts accordingly:  \n```{r}\nbraindata <- braindata %>% \n  mutate(\n    species2 = fct_relevel(species, \"Rhesus monkey\")\n  )\ncontrasts(braindata$species2)\n```\n\nWe're not going to delve too far into contrasts in this course (it's a bit of a rabbit hole!), but it's worth knowing about a couple of different types, and what we can use them to extract from our model. \n\n\n:::sticky\n__Setting contrasts in R__  \n\nAs we will see in action below, in order to change the contrasts used in a model, we can assign specific types of contrasts to the variable in the data, by using code such as: \n```{r}\n#| eval: false\ncontrasts(data$variable) <- ...\n```\n\nThis **changes an attribute of the data**, which means that any model _subsequently_ fitted to that data will now use the assigned contrasts.  \n\nTo revert to the default, we can either a) read in the data again, or b) tell R that we now want to use the default contrasts, known as 'treatment contrasts', by using:\n\n```{r}\n#| eval: false\n# To reset the contrasts to the default used in R\ncontrasts(data$variable) <- \"contr.treatment\"\n```\n\n:::\n\n## Treatment Contrasts (the default)\n\n\"Treatment contrasts\" are the default that R uses. These are the ones we've already discussed above. It compares each level to a reference level. A common example is to compare people taking different 'treatments' (drug A, drug B and drug C) to a placebo group (the reference level).  \n\nWhen we use this approach:  \n\n- the intercept is the estimated y when _all_ predictors are zero. Because the reference level is kind of like \"0\" in our contrast matrix, this is part of the intercept estimate.  \n- we get out a coefficient for each subsequent level, which are the estimated differences from each level to the reference group.  \n\n\n## Sum Contrasts\n\n\"sum contrasts\" (sometimes called \"deviation contrasts\" and \"effects coding\") are the next most commonly used in psychological research. These are a way of comparing each level to the overall mean.  \n\nThis involves a bit of trickery that uses -1s and 1s rather than 0s and 1s, in order to make \"0\" be mid-way between all the levels - the average of the levels.  \n\nWe can adjust the coding scheme that we use like so:  \n```{r}\ncontrasts(braindata$isMonkey) <- \"contr.sum\"\ncontrasts(braindata$isMonkey)\n```\n\n:::imp\nnote that the column of the contrast matrix no longer has a name! It's just got a `[,1]`. This means that the coefficient we get out is not going to have a name either!!! \n:::\n\n```{r}\n#| eval: false\nmonkmod_sum <- lm(mass_brain~isMonkey, braindata)\nsummary(monkmod_sum)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~isMonkey, braindata)), l=list(3,9:12))\n```\n\nThe intercept from this model is the estimated average brain mass averaged across monkeys and non-monkeys. i.e. the estimated 'grand mean' brain mass.  \nThe coefficient represents moving from the overall mean brain mass to the `isMonkey==\"NO\"` mean brain mass.^[we know it is to this group because a 1 increase in the column of our contrast matrix takes us to this group] This is visualised in @fig-binpredplot2.  \n\n```{r}\n#| label: fig-binpredplot2\n#| fig-cap: \"A binary categorical predictor with sum contrasts\"\n#| echo: false\nfit <- lm(mass_brain~isMonkey, braindata, contrasts=list(isMonkey=\"contr.sum\"))\nbraindata %>% mutate(n = as.numeric(isMonkey==\"NO\")) %>%\nggplot(., aes(x=n,y=mass_brain))+\n  geom_jitter(width=.05, alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=isMonkey)) + \n  geom_segment(aes(x=.5,xend=1,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"solid\") +\n  geom_segment(aes(x=1,xend=1,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  geom_segment(aes(x=1,xend=.5,y=sum(coef(fit)[1]),yend=sum(coef(fit)[1])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  guides(col=\"none\") +\n  scale_x_continuous(\"isMonkey\", breaks=c(0,.5,1),labels=c(\"-1\\n'YES'\",\"0\",\"1\\n'NO'\")) +\n  labs(title = \"lm(mass_brain ~ isMonkey), sum contrasts\") +\n  annotate(geom=\"text\",x=.5,y=coef(fit)[1]+.03,hjust=1.1, label=\"Intercept\")+\n  annotate(geom=\"text\",x=1.05,y=coef(fit)[1]+.01, label=\"isMonkey1\\n coefficient\", hjust=0, angle=90)\n```\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: -1/1 vs -.5/.5\n\nFor sum contrasts, sometimes people prefer to use -.5 and .5 instead of -1 and 1. This is because it keeps the intercept as the \"grand mean\", but makes the coefficient represent the difference between the two groups (which might be more useful as a number to talk about).  \n\nThis works because with just these two groups, the distance from `isMonkey==YES` to `isMonkey==NO` is _twice as far_ as the distance from the grand mean to the `isMonkey==NO` group (because the grand mean is the mid-point).  \nBy halving the contrast, it doubles our coefficient (because 'a change in 1' is now twice as far).  \n\n```{r}\ncontrasts(braindata$isMonkey) <- c(.5, -.5)\ncontrasts(braindata$isMonkey)\n```\n```{r}\n#| eval: false\nmonkmod_sum <- lm(mass_brain~isMonkey, braindata)\nsummary(monkmod_sum)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~isMonkey, braindata)), l=list(3,9:12))\n```\n\n```{r}\n#| label: fig-binpredplot3\n#| fig-cap: \"A binary categorical predictor with sum contrasts using -.5 and .5\"\n#| echo: false\nfit <- lm(mass_brain~isMonkey, braindata, contrasts=list(isMonkey=\"contr.sum\"))\nbraindata %>% mutate(n = as.numeric(isMonkey==\"NO\")) %>%\nggplot(., aes(x=n,y=mass_brain))+\n  geom_jitter(width=.05, alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=isMonkey)) + \n  geom_point(x=.5,y=coef(fit)[1],size=3,col=\"blue\")+\n  geom_segment(aes(x=0,xend=1,y=coef(fit)[1]-coef(fit)[2],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"solid\") +\n  geom_segment(aes(x=1,xend=1,y=coef(fit)[1]-coef(fit)[2],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  geom_segment(aes(x=0,xend=1,y=coef(fit)[1]-coef(fit)[2],yend=coef(fit)[1]-coef(fit)[2]),col=\"blue\",lwd=1, lty=\"dotted\")+\n  guides(col=\"none\") +\n  scale_x_continuous(\"isMonkey\", breaks=c(0,.5,1),labels=c(\"-.5\\n'YES'\",\"0\",\".5\\n'NO'\")) +\n  labs(title = \"lm(mass_brain ~ isMonkey), sum contrasts\") +\n  annotate(geom=\"text\",x=.5,y=coef(fit)[1]+.03,hjust=1.1, label=\"Intercept\")+\n  annotate(geom=\"text\",x=1.05,y=coef(fit)[1], label=\"isMonkey1\\n coefficient\", hjust=1, angle=90)\n```\n\n\n\n:::\n\n\n\n\nWhen we move to using variables with more than 2 levels, sum contrasts can look a lot more confusing, but the logic of how we interpret coefficients stays very much the same.  \nFor instance, using sum contrasts with the `species` variable:  \n```{r}\ncontrasts(braindata$species) <- \"contr.sum\"\ncontrasts(braindata$species)\n```\n```{r}\n#| eval: false\nspecmod_sum <- lm(mass_brain ~ species, braindata)\nsummary(specmod_sum)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~species, braindata)), l=list(3,9:13))\n```\n\n- Our intercept is the 'grand mean' (the mean of each species' estimated mean brain mass). \n- Our first coefficient is the difference from the grand mean to the mean of humans. \n- Our second coefficient is the difference from the grand mean to the mean of Potar monkeys. \n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: where have my rhesus monkeys gone? \n\nIt feels a bit odd, but we no longer have an estimate for our Rhesus monkeys. \n\nThis felt okay when we knew they were just being collapsed into our intercept, but where are they now? Our intercept is the grand mean. Where are my Rhesus Monkeys?? \n\nAs stated above, our intercept is the mean of each species' estimated mean brain mass (the 'grand mean'). \nWe can write this as:  \n$$\n\\begin{align}\n\\text{(Intercept)} &: \\frac{\\bar{R}+\\bar{P}+\\bar{H}}{3} = 0.43\\\\\n\\text{where:}&\\\\\n&\\bar{R}, \\,\\bar{P}, \\, \\bar{H} \\text{ are the mean brain mass for }\\\\\n&\\text{Rhesus monkeys, Potar monkeys, and Humans respectively}\n\\end{align}\n$$\n\nThe second coefficient of our model represents the difference from this value to the mean brain mass of humans, and the third represents the difference from this value to the mean brain mass of Potar monkeys:  \n\n$$\n\\begin{align}\n\\text{species1} &: \\bar{H} - 0.43 = 0.17 \\\\\n\\text{species2} &: \\bar{P} - 0.43 = -0.19 \\\\\n\\end{align}\n$$\n\nWe can rewrite these to find those means for Humans and Potars as:  \n\n$$\n\\begin{align}\n\\bar{H} &= \\underbrace{0.43}_{\\text{intercept}} &+ \\underbrace{0.17}_{\\text{species1 coefficient}}  &= &0.6 \\\\\n\\quad & & \\\\\n\\bar{P} &= \\underbrace{0.43}_{\\text{intercept}} &+ \\underbrace{-0.19}_{\\text{species2 coefficient}}  &= &0.24 \\\\\n\\end{align}\n$$\n\nOur Rhesus monkeys are actually still there in our intercept! They're just only represented as a third of the intercept (the other two thirds being the humans and potar monkeys). \nIf we substitute in our $\\bar H$ and $\\bar P$ values to our intercept:  \n\n$$\n\\begin{align}\n\\text{(Intercept)} :&\\, \\frac{\\bar{R}+\\bar{P}+\\bar{H}}{3} = 0.43\\\\\n\\, \\\\\n& \\frac{\\bar{R}+0.24+0.6}{3} = 0.43\\\\\n\\, \\\\\n& \\bar{R}+0.24+0.6 = 3 \\times 0.43\\\\\n& \\bar{R}+0.24+0.6 = 1.29\\\\\n& \\bar{R} = 1.29 - 0.24 - 0.6\\\\\n& \\bar{R} = 0.45\\\\\n\\end{align}\n$$\nAnd there we have our Rhesus monkeys! Because there are no other predictors in our model, this should match exactly (with rounding error in the above calculations!) with what the mean brain mass of Rhesus monkeys is in our data:   \n```{r}\nmean(braindata$mass_brain[braindata$species==\"Rhesus monkey\"])\n```\n\n:::\n\n\n\n\n## Optional: and many more.. \n\nThere are a whole load of other types of contrasts we can use, and we can even set custom ones of our own. The choices are many, _and confusing_, and it really depends on what exactly we want to get out of our model, which is going to depend on our research.  \n\nSome useful resources for your future research:  \n\n- A page showing many many different contrast coding schemes (with R code and interpretation): [https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/){target=\"_blank\"} \n- The **emmeans** package (\"estimated marginal means\") can come in handy for lots and lots of ways to compare groups. The package 'vignette' is at [https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html](https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html){target=\"_blank\"}  \n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\n```\n\n\n```{r}\n#| include: false\nset.seed(50)\nmm <- MASS::Animals\nmm <- mm[c(\"Rhesus monkey\",\"Potar monkey\",\"Chimpanzee\"), ]\nmm$n <- rdunif(3,8,14)\nmm$sbody <- c(2,7,6)\nmm$sbrain <- rep(100,3)\n\nmm <- \n  mm %>% \n  mutate(\n    species = row.names(mm),\n    mass_body = pmap(list(n,body,sbody),~round(rnorm(..1,..2,..3))),\n    mass_brain = pmap(list(n,brain,sbrain),~round(rnorm(..1,..2,..3))/1000)\n  ) %>%\n  select(species,mass_body,mass_brain) %>%\n  unnest() %>% \n  sample_n(n())\n\nmm$mass_body = mm$mass_body + 10\nmm$mass_brain = mm$mass_brain + .2\nmm$species[mm$species==\"Chimpanzee\"]<-\"Human\"\nmm$mass_brain[mm$species==\"Rhesus monkey\"]<-mm$mass_brain[mm$species==\"Rhesus monkey\"]+.06\nmm$age = round(4 + mm$mass_body * .06 + rnorm(nrow(mm),0,3))\nmm$age = round(runif(nrow(mm),2,34))\nbraindata = mm |> select(-mass_body) |> sample_n(n())\n#write_csv(braindata, \"../../data/usmr_braindata.csv\")\n```\n\n\n# Binary Predictors  \n\nSince we've started working with linear regression, we've seen a few examples of categorical variables as predictors in linear models, and most (if not all) of these have been binary predictors - i.e. having just *two* categories (yes/no, dog/cat, phonics/word). We talked initially ([5B#binary-predictors](05b_slr.html#binary-predictors){target=\"_blank\"}) about how these get entered in the model as 0s and 1s.  \n\nLet's consider a dataset which includes the brain mass and body mass of different primates. Below, we can see the `species` variable has 3 levels indicating whether the observation is a \"Human\", a \"Rhesus monkey\", or a \"Potar monkey\". We can easily create a binary variable such as the `isMonkey` variable (created below), which will allow us to compare monkeys (of _any_ type) vs not-monkeys (or \"Humans\", as we are better known!).  \n\n```{r}\nlibrary(tidyverse)\nbraindata <- read_csv(\"https://uoepsy.github.io/data/usmr_braindata.csv\")\n\nbraindata <- braindata %>% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\")\n)\nhead(braindata)\n```\n\nTo recap what we've seen already - when we use a binary variable like `isMonkey` in a model as a predictor, it gets inputted into the model as a column of 0s and 1s. Because the coefficients of regression models are always interpreted as \"the change in $y$ associated with a 1 unit change in $x$\", the use of 0s and 1s for different levels of a categorical variable allows us to make \"a 1 unit change in $x$\" represent moving from one level to another. Internally, our model is relying on a variable like the `isMonkeyYES` variable below:   \n\n```{r}\n#| echo: false\nbraindata %>% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\"),\n  isMonkeyYES = (isMonkey==\"YES\")*1\n) |> head()\n\n```\n\n\nThis means that from the model below we can get out an estimate of the difference in brain mass from the group `isMonkey == \"NO\"` to the group `isMonkey == \"YES\"`, because this is moving from 0 to 1. We can see this in the model and visualisation @fig-binpredplot.  \n\n\n::::panelset\n:::panel\n#### Model\n```{r}\n#| eval: false\nmonkmod <- lm(mass_brain~isMonkey, data = braindata)\nsummary(monkmod)\n```\n```{r}\n#| echo: false\nmonkmod <- lm(mass_brain~isMonkey, data = braindata)\n.pp(summary(monkmod),l=list(9:12))\n```\n\n- `(Intercept)`: the estimated brain mass of Humans (when the isMonkeyYES variable is zero)\n- `isMonkeyYES`: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1). \n\n\n:::\n:::panel\n#### Visual\n```{r}\n#| label: fig-binpredplot\n#| fig-cap: \"A binary categorical predictor\"\n#| echo: false\nfit <- lm(mass_brain~isMonkey, braindata)\nggplot(braindata, aes(x=isMonkey,y=mass_brain))+\n  geom_jitter(width=.05, alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=isMonkey)) + \n  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"solid\") +\n  geom_segment(aes(x=2,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  geom_segment(aes(x=2,xend=1,y=sum(coef(fit)[1]),yend=sum(coef(fit)[1])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  guides(col=\"none\") +\n  scale_x_discrete(labels=c(\"0\\n'NO'\",\"1\\n'YES'\")) +\n  labs(title = \"lm(mass_brain ~ isMonkey)\") +\n  annotate(geom=\"text\",x=1,y=coef(fit)[1]+.03,hjust=-0.1, label=\"Intercept\", hjust=1.1)+\n  annotate(geom=\"text\",x=2.1,y=coef(fit)[1]-.2, label=\"isMonkeyYES\\n coefficient\", hjust=0, angle=90)\n```\n\n:::\n::::\n  \n<br>\nWhen used as predictors in **multiple** regression models (i.e. when there are other predictors involved), binary variables behave much the same way. The coefficient will give us the estimated change in $y$ when moving from one level to the other^[and the intercept will be the estimated $y$ when **all** predictors are zero], _while holding other predictors constant_ (see [7A#interpreting-coefficients](07a_mlr.html#interpreting-multiple-regression-coefficients)).  \n\n```{r}\n#| eval: false\nmonkmod2 <- lm(mass_brain~age + isMonkey, data = braindata)\nsummary(monkmod2)\n```\n```{r}\n#| echo: false\nmonkmod2 <- lm(mass_brain~age + isMonkey, data = braindata)\n.pp(summary(monkmod2),l=list(9:13))\n```\n\n- `(Intercept)`: the estimated brain mass of *new-born* Humans (when both age is zero and isMonkeyYES is zero)\n- `age`: the estimated change in brain mass for every 1 year increase in age, holding isMonkey constant.\n- `isMonkeyYES`: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1), holding age constant.  \n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: a visual\n\nIf we want a visual intuition to how a binary predictor works in multiple regression - it's actually just like any other predictor, in that it's just another dimension to the model. The only difference is that it is on a __discrete__ scale - observations fall on _either_ 0 _or_ 1, not on the continuum in between.  \n  \nSo for the model `mass_brain ~ age + isMonkey`, rather than a 3D surface, we might think of our model as two edges of a surface, as shown in the left-hand panel of @fig-binpredmult. \nAnd if 'monkey-ness' was some sort of measurable continuum (\"I'm 70% monkey?\") then this just joins those edges up back to a surface (right panel).  \n\n\n```{r}\n#| echo: false\n#| label: fig-binpredmult\n#| fig.cap: \"a binary predictor is just another dimension, but data can only exist at either 0 or 1. If a variable were continuous, then observations can take any value along a line\"  \npar(mfrow=c(1,2))\ndf <- braindata |> \n  transmute(x1 = age,x2=(isMonkey==\"YES\")*1,y=mass_brain)\n\nfit<-lm(y~x1+x2,df)\nsteps=20\nx1 <- with(df, seq(min(x1),max(x1),length=steps))\nx2 <- with(df, seq(min(x2),max(x2),length=steps))\nnewdat <- expand.grid(x1=x1, x2=x2)\ny <- matrix(predict(fit, newdat), steps, steps)\np <- persp(x1,x2,y, theta = 55,phi=10, col = NA,\n           border=NA,\n           xlab=\"age\",ylab=\"isMonkey\",zlab=\"mass_brain\",\n           main=\"mass_brain~age+isMonkey\",zlim=c(0,1))\nobs <- with(df, trans3d(x1 ,x2, y, p))\ndf1 = data.frame(x1=2:34,x2=0)\ndf1$y = predict(fit,df1)\ndf2 = data.frame(x1=2:34,x2=1)\ndf2$y = predict(fit,df2)\npred1 <- with(df1, trans3d(x1 ,x2, y, p))\npred2 <- with(df2, trans3d(x1 ,x2, y, p))\npoints(pred1, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(pred2, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(obs, col = \"red\", pch = 16)\n\nfit<-lm(y~x1+x2,df)\nsteps=20\nx1 <- with(df, seq(min(x1),max(x1),length=steps))\nx2 <- with(df, seq(min(x2),max(x2),length=steps))\nnewdat <- expand.grid(x1=x1, x2=x2)\ny <- matrix(predict(fit, newdat), steps, steps)\np <- persp(x1,x2,y, theta = 55,phi=10, col = NA,\n           #border=NA,\n           xlab=\"age\",\n           ylab=\"'monkey-ness continuum'\", \n           zlab=\"mass_brain\",zlim=c(0,1))\nobs <- with(df, trans3d(x1 ,x2, y, p))\ndf1 = data.frame(x1=2:34,x2=0)\ndf1$y = predict(fit,df1)\ndf2 = data.frame(x1=2:34,x2=1)\ndf2$y = predict(fit,df2)\npred1 <- with(df1, trans3d(x1 ,x2, y, p))\npred2 <- with(df2, trans3d(x1 ,x2, y, p))\npoints(pred1, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(pred2, col = \"red\", pch = 16,type=\"l\",lwd=3)\npoints(obs, col = \"red\", pch = 16)\n\npar(mfrow=c(1,1))\n\n```\n\n:::\n\n\n# Multiple Categories  \n\nWhat about when we have a predictor with *more than* two categories? We might have lots of different conditions in our experiment, or we might have observations from lots of different distinct groups of people. \n\nIn the primate brains example, we have various different species of primates - we have 3 (\"Potar monkey\", \"Rhesus Monkey\" and \"Human\").  What happens when we consider a model which has a predictor like `species`?  \n\nFitting the model `lm(mass_brain ~ species)` will by default use one of our categories as a \"reference level\", and then our coefficients will show comparisons between each of the remaining levels to that reference level. So if the reference level is \"Human\", the coefficients we get out include:\n\n- the intercept (which is the estimated brain mass of humans); \n- the estimated difference in brain mass when we move from humans to potar monkeys; \n- and the difference when we move from humans to rhesus monkeys.  \n\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~species, braindata)),l=list(3,9:13))\n```\n\n\n:::rtip\n__How do i know which one is my reference level??__  \n\nNotice that our coefficients are named `speciesPotar monkey` and `speciesRhesus monkey`. When faced with a categorical predictor, the lm function will name coefficients in the format `variableLevel`. We can see that we have coefficients for both \"Potar monkey\" and \"Rhesus monkey\", so we can infer that the reference level is the one we can't see (\"Human\").^[This is all assuming that we have things set to their defaults in R. We'll see below that we can change things up using something called 'contrasts'.]  \n\nR will default to using alphabetical ordering, hence the reference level being set as \"Human\". We could override this by making it a factor with an ordering to it's levels (see the use of `factor()` and `levels()` in [2A#categorical](02a_measurement.html#categorical){target=\"_blank\"}). Functions like `fct_relevel()` might be handy too.  \n\n:::\n\n\nNote that while we have **3** species in our variable, we only have **2** coefficients in our model.  \nThis logic scales up - when we have categorical predictors with multiple levels, we will have $\\text{number-of-levels}-1$ coefficients in our model. This is because to determine which of the $k$ categories an observation is in, we actually only need to have $k-1$ binary variables.  \n\nFor instance, we can re-express all the the information contained in the `species` variable (with 3 levels) with just two binary variables:   \n```{r}\nbraindata <- braindata %>% mutate(\n  speciesPotar = ifelse(species == \"Potar monkey\", 1, 0),\n  speciesRhesus = ifelse(species == \"Rhesus monkey\", 1, 0),\n)\nbraindata %>% \n  select(mass_brain, species, speciesPotar, speciesRhesus) %>%\n  head()\n```\n\n- For a human, both `speciesPotar == 0` and `speciesRhesus == 0`\n- For a Potar monkey, `speciesPotar == 1` and `speciesRhesus == 0`\n- For a Rhesus monkey, `speciesPotar == 0` and `speciesRhesus == 1`\n\nThese two binary variables are actually pretty much what R will put into our model even though we only gave it the one `species` variable!  \n\nRecall that the intercept is the estimated outcome _when all predictors are zero_. In this case, when both variables are zero, we are looking at the Humans group. And when we move 1 on the `speciesPotar` scale, we move from the humans to the Potar monkeys. When we move 1 on the `speciesRhesus` scale, we move from humans to Rhesus monkeys. So each coefficient is comparing a level to the \"reference level\" (as in @fig-kpredplot).  \n\n```{r}\n#| label: fig-kpredplot\n#| fig-cap: \"A categorical predictor with 3 levels\"\n#| echo: false\nfit <- lm(mass_brain~species, braindata)\nggplot(braindata, aes(x=species,y=mass_brain))+\n  geom_jitter(width=.05,alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=species)) + \n  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=1,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1)])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=2,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=3,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1,3)])),col=\"blue\",lwd=1,lty=\"dotted\") +\n  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=.5) +\n  geom_segment(aes(x=1,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1,3)])),col=\"blue\",lwd=.5) +\n  annotate(geom=\"text\",x=1,y=coef(fit)[1]+.03,hjust=-0.1,label=\"Intercept\") + \n  annotate(geom=\"text\",x=2.1,y=coef(fit)[1]-.3,hjust=0,label=\"speciesPotar\\n coefficient\", angle=90) + \n  annotate(geom=\"text\",x=3.1,y=coef(fit)[1]-.15,hjust=0,label=\"speciesRhesus\\n coefficient\", angle=90) + \n  guides(col=\"none\") +\n  labs(title = \"lm(mass_brain ~ species)\")+\n  scale_x_discrete(labels=c(\n    \"speciesPotar = 0\\nspeciesRhesus = 0\\nHuman\",\n    \"speciesPotar = 1\\nspeciesRhesus = 0\\nPotar monkey\",\n    \"speciesPotar = 0\\nspeciesRhesus = 1\\nRhesus monkey\"\n  ))\n```\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: proving it to ourselves\n\nTry fitting a model that uses the two binary variables we made above instead of using `species`. Take a look at the coefficients, try to compare the models to one another.   \nThey are identical! \n```{r}\n#| eval: false\nspecmod2 <- lm(mass_brain ~ speciesPotar + speciesRhesus, data = braindata)\nsummary(specmod2)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain ~ speciesPotar + speciesRhesus, data = braindata)),l=list(3,9:13))\n```\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: more categories = more dimensions\n\nWe've just seen how a categorical variable with $k$ levels gets inputted into our model as a set of $k-1$ predictors. Each of these $k-1$ predictors is actually just another dimension to the model.  \n\nWe could try (we maybe shouldn't) to re-express @fig-kpredplot as a 3D plot. It would look a little like this:  \n\n\n```{r}\n#| echo: false\n#| out-width: \"70%\"\nfit<-lm(mass_brain ~ species,braindata)\nbraindata2 <- as.data.frame(model.matrix(fit)[,2:3]) \nbraindata2$mass_brain = braindata$mass_brain\n\n\nlibrary(scatterplot3d)\nplt <- with(braindata2,scatterplot3d(`speciesRhesus monkey`,`speciesPotar monkey`,mass_brain, scale.y=1,angle=30,ylab=\"\",\n                                     y.ticklabs = c(0,NA,NA,NA,NA,1),\n                                     x.ticklabs = c(0,NA,NA,NA,NA,1),\n                                     main = \"mass_brain ~ species(human/potar monkey/rhesus monkey)\"))\ntext(x = 6.75, y = 0.45, \"speciesPotar monkey\", srt =15)\n\n\npp1 <- tibble(\n  `speciesRhesus monkey`=0,\n  `speciesPotar monkey`=seq(0,1,.1),\n  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[2],length.out=11)\n)\npp2 <- tibble(\n  `speciesRhesus monkey`=seq(0,1,.1),\n  `speciesPotar monkey`=0,\n  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[3],length.out=11)\n)\n\nplt$points(pp1$`speciesRhesus monkey`,pp1$`speciesPotar monkey`,pp1$y,type=\"l\",col=\"blue\")\nplt$points(pp2$`speciesRhesus monkey`,pp2$`speciesPotar monkey`,pp2$y,type=\"l\",col=\"blue\")\n```\n\n:::\n\n\n:::rtip\n__Good practice__  \n\nIn the primate-brains example, all this is happening as part of the model fitting process. This is because we are using variables with sets of characters (\"YES\" and \"NO\", or \"Human\", \"Potar monkey\" and \"Rhesus monkey\"). When we put these in a model, R doesn't know how else to interpret it other than as a set of categories - i.e. it gets interpreted as a 'factor' (see [2A#categorical](02a_measurement.html#categorical){target=\"_blank\"}) without us actually having to explicitly tell R that it is a factor.    \n\nIf we have a variable where a set of categories is represented by numbers, then the model will interpret them as numerical values (i.e. 2 is twice 1 etc).  \n\nIf a variable is categorical, it is good practice to make it a factor when you read in your data. That way you don't get into errors later on when modelling. \n\n:::\n\n## Tests  \n\nWe have just seen how entering a categorical predictor with $k$ levels in as a predictor to a regression model is essentially equivalent to adding in $k-1$ different binary variables. We get out $k-1$ coefficients relating to each of those variables.  \n\nIn our primate-brains example, our model `lm(mass_brain ~ species)` might look like it only has one predictor (`species`), but because it has 3 levels, we end up with 2 coefficients - we are essentially making a _multiple_ regression model. \nSo what can we *test* when it comes to `species`? The coefficients can let us test comparisons between specific levels, but we can also ask an overall question of \"do species differ?\" \n\n#### Testing differences between specific groups\n\n```{r}\n#| echo: false\nmod = lm(mass_brain~species,braindata)\nres = broom::tidy(mod)\nres[,2:4] = apply(res[,2:4],2,function(x) round(x,3))\nres$p.value = format.pval(res$p.value, digits=2,eps=.001)\nres$p.value = ifelse(!grepl(\"<\",res$p.value), paste0(\"=\",res$p.value),res$p.value)\nres$p.value = gsub(\"0\\\\.\",\"\\\\.\",res$p.value)\n```\n\nThe coefficients from our regression model represent specific comparisons between levels of a categorical predictor. The default behaviour in R is (as we have seen) to compare each level to the reference level.  \n\nThe tests (Std. Error, t value, df, p value) associated with these coefficient estimates therefore provide us with a means of testing whether the difference in $y$ between two levels is significantly different from zero.  \n\nFor example, from our model output below, we can say that..  \n\n:::int\n... both Potar monkeys and Rhesus monkeys had significantly lower brain mass than Humans ($b=`r res[[2,2]]`,t(`r mod[[\"df.residual\"]]`)=`r res[[2,4]]`, p`r res[[2,5]]`$ and $b=`r res[[3,2]]`,t(`r mod[[\"df.residual\"]]`)=`r res[[3,4]]`, p`r res[[3,5]]`$ respectively).  \n:::\n\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~species, braindata)),l=list(3,9:13))\n```\n\n#### Testing 'group differences'\n\nIt will hopefully come as little surprise that the way to test an overall question of \"are there differences between groups\" can be assessed by a model comparison between models with and without the predictor.  \n\nWhy? because this allows us to use the F test as a \"joint test\" (see [7A #model-comparisons](07a_mlr.html#model-comparisons){target=\"_blank\"}) of whether including all the information about groups explains a significant amount of additional variance in our outcome (i.e. if their inclusion provides a significant reduction in residual sums of squares).\n\nSo from our model comparison below, we can say something like...   \n\n:::int\n... species differences (Human/Potar monkey/Rhesus monkey) explained a significant amount of variance in brain mass over the null model ($F(2, 32) = 37.22,\\ p <.001$).\n:::\n\n```{r}\nm0 <- lm(mass_brain ~ 1, data = braindata)\nm1 <- lm(mass_brain ~ species, data = braindata)\nanova(m0, m1)\n```\n\n```{r}\n#| eval: false\n#| include: false\n\n# significant F, non-significant ts\nset.seed(7)\nx = rnorm(100)\ng = factor(rbinom(100,2,prob=plogis(x)))\ny = -.5*x + .5*(g==1)  + rnorm(100)\ndf = data.frame(x=x,y=y,g=g)\nlm(y~x) |> summary()\nlm(y~x+g) |> summary()\nanova(lm(y~x+g))\n```\n\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Contrasts\n\nWe can actually do *lots* of clever things with categorical predictors in regression models, in order to compare different groups to one another.  \n\nThe first thing we need to do is to explicitly tell R that they are categorical variables (i.e. we need to make them 'factors'): \n\n```{r}\nbraindata <- braindata %>% \n  mutate(\n    isMonkey = factor(isMonkey),\n    species = factor(species)\n  )\n```\n\nOnce we have done this we can see (and also manipulate) the way in which it gets treated by our model. This is because factors in R have some special attributes called \"contrasts\". Contrasts are ultimately the thing that the model will use to decide what you want to compare to what.  \n\nThe following code shows us the \"contrast matrix\" for a given variable. The rows of this show each level of our variable, and the columns are the coefficients (the comparisons which are estimated when we put the variable into a model).  \nWe can see that the default contrasts match the two binary variables we had created manually earlier on:  \n```{r}\n#| eval: false\ncontrasts(braindata$species)\n```\n```{r}\n#| echo: false\nknitr::include_graphics(\"images/contrasts.png\")\n```\n\nWe can easily \"relevel\" factor, thereby changing which one is the 'reference level' (the level against which all other levels are compared), by using handy functions like `fct_relevel()`. For instance, if we wanted to see how each species compared to Rhesus monkeys, re-levelling the factor changes the contrasts accordingly:  \n```{r}\nbraindata <- braindata %>% \n  mutate(\n    species2 = fct_relevel(species, \"Rhesus monkey\")\n  )\ncontrasts(braindata$species2)\n```\n\nWe're not going to delve too far into contrasts in this course (it's a bit of a rabbit hole!), but it's worth knowing about a couple of different types, and what we can use them to extract from our model. \n\n\n:::sticky\n__Setting contrasts in R__  \n\nAs we will see in action below, in order to change the contrasts used in a model, we can assign specific types of contrasts to the variable in the data, by using code such as: \n```{r}\n#| eval: false\ncontrasts(data$variable) <- ...\n```\n\nThis **changes an attribute of the data**, which means that any model _subsequently_ fitted to that data will now use the assigned contrasts.  \n\nTo revert to the default, we can either a) read in the data again, or b) tell R that we now want to use the default contrasts, known as 'treatment contrasts', by using:\n\n```{r}\n#| eval: false\n# To reset the contrasts to the default used in R\ncontrasts(data$variable) <- \"contr.treatment\"\n```\n\n:::\n\n## Treatment Contrasts (the default)\n\n\"Treatment contrasts\" are the default that R uses. These are the ones we've already discussed above. It compares each level to a reference level. A common example is to compare people taking different 'treatments' (drug A, drug B and drug C) to a placebo group (the reference level).  \n\nWhen we use this approach:  \n\n- the intercept is the estimated y when _all_ predictors are zero. Because the reference level is kind of like \"0\" in our contrast matrix, this is part of the intercept estimate.  \n- we get out a coefficient for each subsequent level, which are the estimated differences from each level to the reference group.  \n\n\n## Sum Contrasts\n\n\"sum contrasts\" (sometimes called \"deviation contrasts\" and \"effects coding\") are the next most commonly used in psychological research. These are a way of comparing each level to the overall mean.  \n\nThis involves a bit of trickery that uses -1s and 1s rather than 0s and 1s, in order to make \"0\" be mid-way between all the levels - the average of the levels.  \n\nWe can adjust the coding scheme that we use like so:  \n```{r}\ncontrasts(braindata$isMonkey) <- \"contr.sum\"\ncontrasts(braindata$isMonkey)\n```\n\n:::imp\nnote that the column of the contrast matrix no longer has a name! It's just got a `[,1]`. This means that the coefficient we get out is not going to have a name either!!! \n:::\n\n```{r}\n#| eval: false\nmonkmod_sum <- lm(mass_brain~isMonkey, braindata)\nsummary(monkmod_sum)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~isMonkey, braindata)), l=list(3,9:12))\n```\n\nThe intercept from this model is the estimated average brain mass averaged across monkeys and non-monkeys. i.e. the estimated 'grand mean' brain mass.  \nThe coefficient represents moving from the overall mean brain mass to the `isMonkey==\"NO\"` mean brain mass.^[we know it is to this group because a 1 increase in the column of our contrast matrix takes us to this group] This is visualised in @fig-binpredplot2.  \n\n```{r}\n#| label: fig-binpredplot2\n#| fig-cap: \"A binary categorical predictor with sum contrasts\"\n#| echo: false\nfit <- lm(mass_brain~isMonkey, braindata, contrasts=list(isMonkey=\"contr.sum\"))\nbraindata %>% mutate(n = as.numeric(isMonkey==\"NO\")) %>%\nggplot(., aes(x=n,y=mass_brain))+\n  geom_jitter(width=.05, alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=isMonkey)) + \n  geom_segment(aes(x=.5,xend=1,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"solid\") +\n  geom_segment(aes(x=1,xend=1,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  geom_segment(aes(x=1,xend=.5,y=sum(coef(fit)[1]),yend=sum(coef(fit)[1])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  guides(col=\"none\") +\n  scale_x_continuous(\"isMonkey\", breaks=c(0,.5,1),labels=c(\"-1\\n'YES'\",\"0\",\"1\\n'NO'\")) +\n  labs(title = \"lm(mass_brain ~ isMonkey), sum contrasts\") +\n  annotate(geom=\"text\",x=.5,y=coef(fit)[1]+.03,hjust=1.1, label=\"Intercept\")+\n  annotate(geom=\"text\",x=1.05,y=coef(fit)[1]+.01, label=\"isMonkey1\\n coefficient\", hjust=0, angle=90)\n```\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: -1/1 vs -.5/.5\n\nFor sum contrasts, sometimes people prefer to use -.5 and .5 instead of -1 and 1. This is because it keeps the intercept as the \"grand mean\", but makes the coefficient represent the difference between the two groups (which might be more useful as a number to talk about).  \n\nThis works because with just these two groups, the distance from `isMonkey==YES` to `isMonkey==NO` is _twice as far_ as the distance from the grand mean to the `isMonkey==NO` group (because the grand mean is the mid-point).  \nBy halving the contrast, it doubles our coefficient (because 'a change in 1' is now twice as far).  \n\n```{r}\ncontrasts(braindata$isMonkey) <- c(.5, -.5)\ncontrasts(braindata$isMonkey)\n```\n```{r}\n#| eval: false\nmonkmod_sum <- lm(mass_brain~isMonkey, braindata)\nsummary(monkmod_sum)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~isMonkey, braindata)), l=list(3,9:12))\n```\n\n```{r}\n#| label: fig-binpredplot3\n#| fig-cap: \"A binary categorical predictor with sum contrasts using -.5 and .5\"\n#| echo: false\nfit <- lm(mass_brain~isMonkey, braindata, contrasts=list(isMonkey=\"contr.sum\"))\nbraindata %>% mutate(n = as.numeric(isMonkey==\"NO\")) %>%\nggplot(., aes(x=n,y=mass_brain))+\n  geom_jitter(width=.05, alpha=.1,size=3)+\n  stat_summary(geom=\"pointrange\", size = 1, aes(col=isMonkey)) + \n  geom_point(x=.5,y=coef(fit)[1],size=3,col=\"blue\")+\n  geom_segment(aes(x=0,xend=1,y=coef(fit)[1]-coef(fit)[2],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"solid\") +\n  geom_segment(aes(x=1,xend=1,y=coef(fit)[1]-coef(fit)[2],yend=sum(coef(fit)[1:2])),col=\"blue\",lwd=1, lty=\"dotted\") +\n  geom_segment(aes(x=0,xend=1,y=coef(fit)[1]-coef(fit)[2],yend=coef(fit)[1]-coef(fit)[2]),col=\"blue\",lwd=1, lty=\"dotted\")+\n  guides(col=\"none\") +\n  scale_x_continuous(\"isMonkey\", breaks=c(0,.5,1),labels=c(\"-.5\\n'YES'\",\"0\",\".5\\n'NO'\")) +\n  labs(title = \"lm(mass_brain ~ isMonkey), sum contrasts\") +\n  annotate(geom=\"text\",x=.5,y=coef(fit)[1]+.03,hjust=1.1, label=\"Intercept\")+\n  annotate(geom=\"text\",x=1.05,y=coef(fit)[1], label=\"isMonkey1\\n coefficient\", hjust=1, angle=90)\n```\n\n\n\n:::\n\n\n\n\nWhen we move to using variables with more than 2 levels, sum contrasts can look a lot more confusing, but the logic of how we interpret coefficients stays very much the same.  \nFor instance, using sum contrasts with the `species` variable:  \n```{r}\ncontrasts(braindata$species) <- \"contr.sum\"\ncontrasts(braindata$species)\n```\n```{r}\n#| eval: false\nspecmod_sum <- lm(mass_brain ~ species, braindata)\nsummary(specmod_sum)\n```\n```{r}\n#| echo: false\n.pp(summary(lm(mass_brain~species, braindata)), l=list(3,9:13))\n```\n\n- Our intercept is the 'grand mean' (the mean of each species' estimated mean brain mass). \n- Our first coefficient is the difference from the grand mean to the mean of humans. \n- Our second coefficient is the difference from the grand mean to the mean of Potar monkeys. \n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: where have my rhesus monkeys gone? \n\nIt feels a bit odd, but we no longer have an estimate for our Rhesus monkeys. \n\nThis felt okay when we knew they were just being collapsed into our intercept, but where are they now? Our intercept is the grand mean. Where are my Rhesus Monkeys?? \n\nAs stated above, our intercept is the mean of each species' estimated mean brain mass (the 'grand mean'). \nWe can write this as:  \n$$\n\\begin{align}\n\\text{(Intercept)} &: \\frac{\\bar{R}+\\bar{P}+\\bar{H}}{3} = 0.43\\\\\n\\text{where:}&\\\\\n&\\bar{R}, \\,\\bar{P}, \\, \\bar{H} \\text{ are the mean brain mass for }\\\\\n&\\text{Rhesus monkeys, Potar monkeys, and Humans respectively}\n\\end{align}\n$$\n\nThe second coefficient of our model represents the difference from this value to the mean brain mass of humans, and the third represents the difference from this value to the mean brain mass of Potar monkeys:  \n\n$$\n\\begin{align}\n\\text{species1} &: \\bar{H} - 0.43 = 0.17 \\\\\n\\text{species2} &: \\bar{P} - 0.43 = -0.19 \\\\\n\\end{align}\n$$\n\nWe can rewrite these to find those means for Humans and Potars as:  \n\n$$\n\\begin{align}\n\\bar{H} &= \\underbrace{0.43}_{\\text{intercept}} &+ \\underbrace{0.17}_{\\text{species1 coefficient}}  &= &0.6 \\\\\n\\quad & & \\\\\n\\bar{P} &= \\underbrace{0.43}_{\\text{intercept}} &+ \\underbrace{-0.19}_{\\text{species2 coefficient}}  &= &0.24 \\\\\n\\end{align}\n$$\n\nOur Rhesus monkeys are actually still there in our intercept! They're just only represented as a third of the intercept (the other two thirds being the humans and potar monkeys). \nIf we substitute in our $\\bar H$ and $\\bar P$ values to our intercept:  \n\n$$\n\\begin{align}\n\\text{(Intercept)} :&\\, \\frac{\\bar{R}+\\bar{P}+\\bar{H}}{3} = 0.43\\\\\n\\, \\\\\n& \\frac{\\bar{R}+0.24+0.6}{3} = 0.43\\\\\n\\, \\\\\n& \\bar{R}+0.24+0.6 = 3 \\times 0.43\\\\\n& \\bar{R}+0.24+0.6 = 1.29\\\\\n& \\bar{R} = 1.29 - 0.24 - 0.6\\\\\n& \\bar{R} = 0.45\\\\\n\\end{align}\n$$\nAnd there we have our Rhesus monkeys! Because there are no other predictors in our model, this should match exactly (with rounding error in the above calculations!) with what the mean brain mass of Rhesus monkeys is in our data:   \n```{r}\nmean(braindata$mass_brain[braindata$species==\"Rhesus monkey\"])\n```\n\n:::\n\n\n\n\n## Optional: and many more.. \n\nThere are a whole load of other types of contrasts we can use, and we can even set custom ones of our own. The choices are many, _and confusing_, and it really depends on what exactly we want to get out of our model, which is going to depend on our research.  \n\nSome useful resources for your future research:  \n\n- A page showing many many different contrast coding schemes (with R code and interpretation): [https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/){target=\"_blank\"} \n- The **emmeans** package (\"estimated marginal means\") can come in handy for lots and lots of ways to compare groups. The package 'vignette' is at [https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html](https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html){target=\"_blank\"}  \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"08b_catpred.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"code-annotations":"hover","link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"8B: Categorical Predictors","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}