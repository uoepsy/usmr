{
  "hash": "12f5fd70f1fb72357882ef5e1dc5c1cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"7A: Multiple Linear Regression\"\nparams: \n    SHOW_SOLS: FALSE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n:::lo\nThis reading:  \n\n- \"Variance explained\" in a linear model\n- Multiple regression: Building models where more than one thing explains variance\n- Comparing models\n- Associations in multiple regression\n\n:::\n\n\nWhen we introduced the linear regression model (see [5B: simple linear regression](05b_slr.html)), we talked mainly about the intercept and the slope (collectively referred to as \"the coefficients\"). We saw that the model summary output (from `summary(model)`) gave us some tests next to each coefficient: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nslr_data <- read_csv(\"https://uoepsy.github.io/data/usmr_slr.csv\")\nsimplemod <- lm(y ~ x, data = slr_data)\nsummary(simplemod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = slr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4383 -0.6593  0.1075  0.5945  2.1867 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.54277    0.32005   4.820 5.24e-06 ***\nx            0.77952    0.09959   7.827 5.92e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9308 on 98 degrees of freedom\nMultiple R-squared:  0.3847,\tAdjusted R-squared:  0.3784 \nF-statistic: 61.26 on 1 and 98 DF,  p-value: 5.918e-12\n```\n\n\n:::\n:::\n\n\n\n\nHowever, there is another way we can look at our regression model, and that is by considering how the model, *as a whole*, explains variation in our outcome model.  \n\n\n# Variance Explained in Simple Regression\n\nTo quantify and assess a model's utility in explaining variance in an outcome variable, we can split the total variability of that outcome variable into two terms: the variability explained by the model plus the variability left unexplained in the residuals.\n\n$$\n\\begin{align}\n& \\qquad \\qquad \\qquad \\qquad \\text{total variability in outcome } =  \\\\\n& \\text{variability explained by model } + \\text{unexplained variability in residuals}\n\\end{align}\n$$\n\nThe illustration in @fig-sstssrssm gets at the intuition behind this: the top panel shows the total variability in the outcome variable $y$ - for each datapoint we see the distance from the mean of $y$. These distances can be split into the bit from the mean to the model predicted value (seen in the bottom left panel of @fig-sstssrssm), and the bit from that value to the actual value (bottom right panel). \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Total Sums of Squares = Model Sums of Squares + Residual Sums of Squares](07a_mlr_files/figure-html/fig-sstssrssm-1.png){#fig-sstssrssm fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nEach of these terms can be quantified as a 'sum of squares' (i.e. summing the squared differences). For the 'total sums of squares', this is the sum of squared differences from each observation to the mean. For the 'model sums of squares', this is the sum of the squared differences from each model-predicted value to the mean. For the 'residual sums of squares', it is the sum of the squared differences from each observation to the relative model-predicted value.  \n\n$$\n\\begin{aligned}\nSS_{Total} &= SS_{Model} \\quad\\quad + \\quad SS_{Residual} \\\\\n\\sum_{i=1}^n (y_i - \\bar y)^2 &= \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 + \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\\\\n\\quad \\\\\n\\text{Where:} \\\\\n& y_i = \\text{observed value} \\\\\n&\\bar{y} = \\text{mean} \\\\\n& \\hat{y}_i = \\text{model predicted value} \\\\\n\\end{aligned}\n$$\n\n## $R^2$: proportion of variance explained\n\nA useful statistic is the $R^2$, which tell us the proportion of the total variability in the outcome (`y`) that is explained by the linear relationship with the predictor (`x`).\n\n:::sticky\nThe $R^2$ coefficient is defined as the proportion of the total variability in the outcome variable which is explained by our model:  \n$$\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n$$\n:::\n\nWe can find the $R^2$ easily in the `summary()` of the model! \n\nThe output of `summary()` displays the R-squared value in the following line:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(simplemod)\n```\n:::\n\n\n\n\n```\n...\n...\nMultiple R-squared:  0.3847,  Adjusted R-squared:  0.3784\n...\n\n```\n\n:::column-margin\nFor the moment, ignore \"Adjusted R-squared\". We will come back to this in a little bit.  \n:::\n\n:::int\nApproximately 38\\% of the total variability in `y` is explained by the linear association with `x`.\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: manual calculation of R-Squared   \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nslr_data <- read_csv(\"https://uoepsy.github.io/data/usmr_slr.csv\")\nsimplemod <- lm(y ~ x, data = slr_data)\n\nsimplemod_fitted <- slr_data |>\n  mutate(\n    y_hat = predict(simplemod),\n    resid = y - y_hat\n  )\nhead(simplemod_fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n      x     y y_hat  resid\n  <dbl> <dbl> <dbl>  <dbl>\n1  3.19  4.42  4.03  0.388\n2  2.57  4.48  3.54  0.941\n3  3.91  2.72  4.59 -1.87 \n4  4.79  5.39  5.28  0.107\n5  4.00  3.85  4.66 -0.809\n6  4.11  4.42  4.74 -0.327\n```\n\n\n:::\n\n```{.r .cell-code}\nsimplemod_fitted |>\n  summarise(\n    SSModel = sum( (y_hat - mean(y))^2 ),\n    SSTotal = sum( (y - mean(y))^2 )\n  ) |>\n  summarise(\n    RSquared = SSModel / SSTotal\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  RSquared\n     <dbl>\n1    0.385\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n## The $F$ Statistic\n\nWe can also perform a test to investigate if the model is 'useful' --- that is, a test to see if our explanatory variable explains more variance in our outcome than we would expect by just some random chance variable.  \n\nOur test is framed in terms of the following hypotheses:\n\n$$\n\\begin{aligned}\nH_0 &: \\text{the model is ineffective, } b_1 = 0 \\\\\nH_1 &: \\text{the model is effective, } b_1 \\neq 0\n\\end{aligned}\n$$\nThe relevant test-statistic is the F-statistic, which uses \"Mean Squares\" (these are Sums of Squares divided by the relevant degrees of freedom). We then compare that against (you guessed it) an F-distribution! F-distributions vary according to two parameters, which are both degrees of freedom.  \n\nWe won't go into details of why we use $F$, but the logic of the significance test is just the same as it was for $z$, $t$ and $\\chi^2$:  \n\n- we calculate a test statistic\n- we have a distribution of what we would expect test statistics from a sample of this size to be, if the null hypothesis is true (the \"null distribution\"). \n- we can then ask how unlikely it is that we would observe our statistic (or more extreme) if the null hypothesis were true.  \n\n\n:::sticky\n**$F$-statistic for simple linear regression**\n\n$$\n\\begin{split}\nF = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}\n\\end{split}\n$$\n\nThis is a comparison between amount of variation in the outcome explained by the model and the amount of variation 'explained by' (or leftover in) the residuals.\n\nThe sample F-statistic is compared to an F-distribution with $df_{1} = 1$ and $df_{2} = n - 2$ degrees of freedom.^[\n- $SS_{Total}$ has $n - 1$ degrees of freedom as one degree of freedom is used up by estimating the mean $\\bar{y}$.  \n- $SS_{Residual}$ has $n - 2$ degrees of freedom. There are $n$ datapoints in total, and once we define the regression line, $n-2$ of these could be _anywhere at all_ (they are 'free to vary'). The remaining 2 are determined by the other $n-2$ in order to get that line.\n- By difference, $SS_{Model}$ has $(n - 1) - (n - 2) = 1$ degree of freedom. In other words, the model here is really just one thing - a line!  \n]\n\n:::\n\nLike the R-squared, the `summary()` of our model prints out the F-statistic, degrees of freedom, and p-value. These are right at the bottom of the summary output, printed as:  \n\n```\n...\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.54277    0.32005   4.820 5.24e-06 ***\nx            0.77952    0.09959   7.827 5.92e-12 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.9308 on 98 degrees of freedom\nMultiple R-squared:  0.3847,\tAdjusted R-squared:  0.3784 \nF-statistic: 61.26 on 1 and 98 DF,  p-value: 5.918e-12\n```\n\n:::int\nThe F-test of model utility was significant ($R^2=0.38, F(1,98) = 61.26,\\ p <.001$), suggesting that predictor $x$ is effective in explaining variance in the outcome.  \n:::\n\nNote that the p-value here is exactly the same as the one for the coefficient. This is because in testing \"the model is (in)effective\", the \"model\" is really _only_ the relationship between the outcome and our **one** predictor. We're about to start adding more explanatory variables into our model, which means that our hypotheses for the $F$-test will be about a *set* of $b$'s, and so the tests will be of different things.  \n\n::: {.callout-caution collapse=\"true\"}\n#### optional: with only one predictor variable, the F-test is equivalent to the t-test of the slope  \n\n**In simple linear regression only** (where we have just __one__ predictor), the F-statistic for overall model significance is equal to the square of the t-statistic for $H_0: b_1 = 0$.\n\nYou can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nslr_data <- read_csv(\"https://uoepsy.github.io/data/usmr_slr.csv\")\nsimplemod <- lm(y ~ x, data = slr_data)\n\nsummary(simplemod)$fstatistic['value']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   value \n61.26497 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(simplemod)$coefficients['x','t value']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.827194\n```\n\n\n:::\n:::\n\n\n\n$$\n\\begin{align}\n& t^2 = F \\\\\n& 7.827194^2 = 61.26497\n\\end{align}\n$$\n\nWe can also show the equivalence of the F-test for model effectiveness and t-test for the slope through their respecive formulae. \n\nRecall the formula of the sum of squares due to the model. We are going to re-express this in an equivalent form below:\n$$\n\\begin{aligned}\nSS_{Model} &= \\sum_i (\\hat y_i - \\bar y)^2 \\\\\n&= \\sum_i (\\hat b_0 + \\hat b_1 x_i - \\bar y)^2 \\\\\n&= \\sum_i (\\bar y - \\hat b_1 \\bar x + \\hat b_1 x_i - \\bar y)^2 \\\\\n&= \\sum_i (\\hat b_1 (x_i - \\bar x))^2 \\\\\n&= \\hat b_1^2 \\sum_i (x_i - \\bar x)^2\n\\end{aligned}\n$$\n\nThe F-statistic is given by:\n$$\n\\begin{aligned}\nF = \\frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} \n= \\frac{\\hat b_1^2 \\sum_i (x_i - \\bar x)^2}{\\hat \\sigma^2} \n= \\frac{\\hat b_1^2 }{\\hat \\sigma^2 / \\sum_i (x_i - \\bar x)^2}\n\\end{aligned}\n$$\n\nNow recall the formula of the t-statistic,\n$$\nt = \\frac{\\hat b_1}{SE(\\hat b_1)} = \\frac{\\hat b_1}{\\hat \\sigma / \\sqrt{\\sum_i (x_i - \\bar x)^2}}\n$$\n\nIt is evident that the latter is obtained as the square root of the former.\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: expressing $F$ in terms of $R^2$\n\nWith some algebra we can also show that:\n$$\nF = \\frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \\frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }\n$$\n\nProof:\n\n$$\n\\begin{aligned}\nF = \\frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} \n= \\frac{\\frac{SS_{Model}}{SS_{Total}}}{\\frac{SS_{Residual}}{SS_{Total}} \\cdot \\frac{1}{(n - 2)}} \n= \\frac{R^2 / 1}{(1 - R^2) / (n - 2)}\n\\end{aligned}\n$$\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# The Multiple Regression Model\n\nThe real power of regression models comes into effect when we start to concern ourselves with more than just \"one outcome explained by one predictor\".  \n  \nEnter... $x_2$!  \n\nWe're going to start with looking at the case of \"one outcome, two predictors\", but the beauty of this is that the logic scales up to however many predictor variables we want to include in our model.  \n\nWhen we fitted the simple regression model with __one__ predictor: \n$$\ny = b_0 + b_1 \\cdot x + \\epsilon\n$$\nwe were fitting a _line_ to a scatterplot of points that we plotted in _2 dimensions_ (an x-axis and a y-axis). \n\nWhen we fit a multiple regression model with __two__ predictors:  \n$$\ny = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n$$\nwe are fitting a __surface__ (or \"plane\") to a 3-dimensional cloud of datapoints (@fig-regsurf). There are three dimensions: x1, x2, and y.    \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Regression surface for y~x1+x2, from two different angles](07a_mlr_files/figure-html/fig-regsurf-1.png){#fig-regsurf fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nDon't worry about trying to figure out how to visualise it if we had more predictors! We can only conceive of 3 spatial dimensions.^[One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it's not worth trying!] However, the logic stays the same when we increase this to having $k$ predictors, but we have a model that is a $k$-dimensional surface, and each coefficient is the angle of that surface with respect to each predictor.  \n\nWhen we have two predictors, such as in the visualisation of @fig-regsurf, our model is now determined by three numbers:  \n\n- the __intercept__, denoted $b_0$.  \nThis is the point at which the plane hits the y-axis (i.e. where $x_1=0$ __and__ $x_2=0$)\n- the __slope of x1__, in this case denoted $b_1$.  \nThis is the angle of the regression plane with respect to the axis of $x_1$. It is the amount which the plane increases for every 1 increase in $x_1$.  \n- the __slope of x2__, in this case denoted $b_2$.  \nThis is the angle of the regression plane with respect to the axis of $x_2$. It is the amount which the plane increases for every 1 increase in $x_2$.  \n\n\n## Fitting Multiple Regression Models in R  \n\nAs we did for simple linear regression, we can fit our multiple regression model using the `lm()` function. We can add as many explanatory variables as we like, separating them with a `+`.  \n\n```\nmodel_name <- lm(y ~ 1 + x1 + x2 + ... + xk, data = dataframe)\n```\n\nAnd we can use all the same functions that we have already seen such as `summary()`, `predict()`, `fitted()`, `coef()` etc.  \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_data <- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = mlr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.5201  -4.2912  -0.0268   3.3044  16.2154 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.39138    3.67735  -0.650  0.51867   \nx1           0.17570    0.06435   2.730  0.00888 **\nx2          -0.64756    0.19959  -3.244  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.069 on 47 degrees of freedom\nMultiple R-squared:  0.2643,\tAdjusted R-squared:  0.233 \nF-statistic: 8.443 on 2 and 47 DF,  p-value: 0.0007369\n```\n\n\n:::\n:::\n\n\n\n\nJust like we saw for the regression model with one predictor, the `summary()` output of a multiple regression model shows us all the same information: residuals, coefficients, $R^2$ and and $F$-test. We'll get to the coefficients a little later on, but first we're going to take a look at the overall model in terms of how much variance is now explained.  \n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Variance Explained in Multiple Regression  \n\n\n\n## The Adjusted $R^2$\n\nWe know from above that in simple linear regression the R-squared can be obtained as:\n$$\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n$$\n\nIn multiple regression, the \"multiple $R^2$\" uses this exact same formula. However, when we add more and more predictors into a multiple regression model, $SS_{Residual}$ _cannot_ increase. In fact, it will _always_ decrease, regardless of how useful our new predictors are. This means that $R^2$ will _always increase_ (because $SS_{Total}$ is constant, so $1-\\frac{SS_{Residual}}{SS_{Total}}$ will increase as $SS_{Residual}$ decreases).  \n\nIf we added randomly generated 1000 new predictors (completely random, so they have nothing to do with the outcome), then by chance alone they will explain _some_ variance in the outcome $y$, and the multiple $R^2$ will always increase.  \n  \nAn alternative, the \"Adjusted-$R^2$\", does not necessarily increase with the addition of more explanatory variables, by the inclusion of a penalty according to the number of explanatory variables in the model. The number by itself isn't directly meaningful, but can be useful in determining the amount of additional variance explained by adding predictor(s) into a model.  \n\n:::sticky\nThe **Adjusted** $R^2$ is a measure of the proportion of variability in the outcome that is explained by our model, *adjusted for the number of predictors in the model.*  \n\n$$\n\\begin{align}\n& Adjusted{-}R^2=1-\\frac{(1-R^2)(n-1)}{n-k-1} \\\\\n& \\quad \\\\\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n\n:::\n  \n\n**In R,** we can view both the mutiple and adjusted $R^2$ at the bottom of the output of `summary()`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/mlr/output_mlr_rsq.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n## The $F$-statistic: a joint test\n\nWe saw just above that with one predictor, the F-statistic is used to test the null hypothesis that the regression slope for that predictor is zero. In multiple regression, the logic is the same, but we are now testing against the null hypothesis that **all** regression slopes are zero (now that we have multiple predictors, \"all\" is more than 1).  \n\n$$\n\\begin{aligned}\nH_0: & \\text{the model is ineffective, } \\\\\n& b_1, ..., b_k = 0 \\\\\nH_1: &\\text{the model is effective, } \\\\\n& \\text{any of }b_1, ..., b_k \\neq 0\n\\end{aligned}\n$$\n\nThe $F$-statistic is sometimes called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per parameter) versus how much of the variation is unexplained (per remaining degrees of freedom). We can generalise the formula for the $F$-statistic in simple regression that we saw above, to encompass situations where there are more predictors:    \n\n:::sticky\n**$F$-ratio**\n\n$$\n\\begin{align}\n& F_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n& \\quad \\\\\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n:::\n\n\n**In R,** at the bottom of the output of `summary()`, you can view the F statistic, along with the hypothesis test against the null hypothesis that all the coefficients are 0:^[under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1]\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/mlr/output_mlr_f.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n:::int\nthe linear model with $x1$ and $x2$ explained a significant amount of variance in $y$ beyond what we would expect by chance ($R^2=0.26, F(2, 47) = 8.44,\\ p <.001$). \n:::\n  \n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Model Comparisons\n\nNow we have seen that we can add in more predictors to a linear regression model, what are we going to _do_ with our modelling? Well, one useful thing we can do is to compare models with and without some predictor(s).  \n\nAn easy starting point would be to compare how the $R^2$ values change when we add predictors - showing the extent to which different models 'explain variance' in our outcome. The two models below explain 10% and 26%, respectively, of the variance in y.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm(y ~ x1, data = mlr_data)\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09953216\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(model2)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2643057\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nBut this comparison is _descriptive_, in that we are not performing any _test_ of whether the differences between the models' explanatory power is more than we might just expect by chance. To do this, we need to utilise our $F$-tests again!  \n\nAs it happens, the $F$-statistic we see at the bottom of `summary(model)` is actually a comparison between two models: our model (with some explanatory variables in predicting $y$) and __the null model.__ \n\n:::sticky\n**the null model / \"intercept-only model\"**   \n\nThe null model can be thought of as the model in which all explanatory variables have zero regression coefficients. Put another way, when all predictor variable coefficients are zero, then we are only estimating $y$ via an intercept (which will be the mean: $\\bar y$).  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnullmod <- lm(y ~ 1, data = mlrdata)\n```\n:::\n\n\n\n\n:::\n\nWe aren't limited to comparing our model to the null model. We can compare any the intermediate models which vary in the complexity, from the null model to our full model.  \n \nIf (*and only if*) two models are __nested__ (one model contains all the predictors of the other and is fitted to the same data), we can compare them using an _incremental F-test._ This is a formal test of whether the __additional predictors__ provide a better fitting model. \"Better fitting\" here refers to a reduction in the residual sums of squares, hence why this is just another form of the $F$-ratio we've seen above.    \n\n:::sticky\n__Incremental F-test__  \n\n+ $H_0:$ coefficients for the added/ommitted variables are all zero.\n+ $H_1:$ at least one of the added/ommitted variables has a coefficient that is not zero. \n\n::: {.callout-caution collapse=\"true\"}\n#### optional: F-ratio written for model comparison  \n\nThe F-ratio for comparing the residual sums of squares between two models can be written as:\n\n$$\n\\begin{align}\n& F_{(df_R-df_F),df_F} = \\frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F} \\\\\n& \\quad \\\\\n& \\text{Where:} \\\\\n& SSR_R = \\text{residual sums of squares for the restricted model} \\\\\n& SSR_F = \\text{residual sums of squares for the full model} \\\\\n& df_R = \\text{residual degrees of freedom from the restricted model} \\\\\n& df_F = \\text{residual degrees of freedom from the full model} \\\\\n\\end{align}\n$$\n:::\n\n:::\n\n\n**In R,** we can conduct an incremental F-test to compare two models by fitting both models using `lm()`, and passing them to the `anova()` function: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm( ... )\nmodel2 <- lm( ... )\nanova(model1, model2)\n```\n:::\n\n\n\n\n\n<!-- ::: {.callout-caution collapse=\"true\"} -->\n<!-- #### why 'anova' -->\n\n<!-- What we are actually doing when we conduct a model comparison is asking \"is there a reduction in residual sums of squares?\".^[The residuals are, remember, the distances from the actual observed values to the model predicted values. If we square those distances (to make them all positive), and sum them up, we get the \"residual sums of squares\".]   -->\n<!-- Another way of phrasing \"is there are reduction in residual sums of squares?\" is to say \"is more variance explained?\". The `anova()` function stands for \"analysis of variance\".   -->\n\n<!-- ::: -->\n\n\nIf we wanted to, for example, compare a model with just one predictor, $x_1$, to a model with 3 predictors: $x_1,\\ x_2,\\ x_3$, we can assess the extent to which the variables $x_2$ and $x_3$ *jointly* improve model fit by explaining more variance (i.e. by reducing the residual sums of squares):    \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm(y ~ x1, data = mlr_data)\nmodel3 <- lm(y ~ x1 + x2 + x3, data = mlr_data)\nanova(model1, model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ x1\nModel 2: y ~ x1 + x2 + x3\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     48 2874.8                              \n2     45 2294.3  3    580.55 3.7956 0.01648 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n:::int\n$x2$ and $x3$ explained a significant amount of additional variance in $y$ beyond $x1$ ($F(3,45) = 3.8,\\ p =.017$). \n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Associations in multiple regression\n\nOkay, so we can build multiple regression models, adding in as many predictors as we like, and we can compare models with and without certain predictors. But __why__ is this a useful thing that we may want to do?  \n\nOne important reason is that it allows us to exercise _statistical control_. We often conduct research where we are interested mainly in one relationship, but we know that there are other things also at play - there are other variables that will probably strongly influence results if they aren't held constant.  \n\nStatistical control allows us to examine the relationship of interest _after accounting for variance explained by other predictors._ Including additional predictors also often has the benefit of improving the predictive accuracy of our model (although this is not often the primary goal in Psychological studies, which are less concerned with 'the best fitting model' and more geared towards understanding and estimating the relevant relationship while exercising appropriate control).\n\nMultiple regression allows to do this because when we have multiple predictor variables in our model, we can examine the association between the outcome $y$ and **the bit of our focal predictor variable that is unique from the other predictors** (i.e. \"after accounting for\" those other predictors).  \n\nFor example, suppose we are interested in the relationship between 'commuting time' and 'job satisfaction' (e.g. people might be more/less satisfied with jobs depending upon the distance of their commute). In this scenario, we are looking at whether our outcome (job satisfaction) varies according to a focal predictor (the distance of commute). _But,_ it's extremely likely that people with more flexible working patterns will tend to have longer commutes (they will spend more time at home, and so be more prepared to travel further on the days they _do_ go in). And this flexible working has other features that will influence job satisfaction. So we need to make it clear that we are interested in the bit of 'commuting time' that is _separate from_ 'work flexibility' - we want to _control for_ work flexibility.\n\n:::statbox\n__Terminology__ \n\nAs with all areas of statistics, people seem to use lots of different terms here. It can be confusing!    \n\n- **outcome/response/dependent variable**: variable on the left hand side of the model equation \n*(thing that varies and we're trying to explain how/why)*\n- **predictor:** any variable on the right hand side of the model equation \n*(things that we thing explain differences in the outcome)*\n- **focal predictor/independent variable:** the predictor of interest\n- **covariates/confounders/control variables:** other variables that we are less interested in but believe to be relevant to how the data comes about, and that may influence both the outcome and the focal predictor.  \n:::\n\nOne way to build this intuition is to consider a Venn diagram with a circle showing the variance in each variable. @fig-vennslr shows a simple linear regression with one predictor (i.e. `lm(y ~ x1)`). The circle for $y$ shows the total variance in $y$ (the same for the $x_1$ circle). The overlap between circles (labelled \"A\") shows the variance in $y$ that is explained by $x_1$ (i.e. the covariance).  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Venn Diagram for Simple Regression y ~ x1](images/mlr/venn_slr.png){#fig-vennslr fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nWhen we add in a new predictor, $x_2$, where do we add it? If $x_1$ and $x_2$ are _completely_ uncorrelated with one another, then it would look something like @fig-vennmlr1, where there is no overlap between the $x_1$ and $x_2$ circles. The total variance explained in $y$ by both predictors is $A + B$, and in this case, nothing changes in our estimate of the relationship between $y$ and $x_1$. It's just the same as before (the area labelled \"A\" is the same in both @fig-vennslr and @fig-vennmlr1). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Venn Diagram for Multiple Regression y ~ x1 + x2 where x1 and x2 are completely uncorrelated](images/mlr/venn_mlr1.png){#fig-vennmlr1 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nHowever, in practice the predictors in our regression model are likely to overlap a bit (it's hard to make sure our focal predictor is completely unrelated to other predictors). In this case, our Venn diagram is going to look like @fig-vennmlr2. The correlation between $x_1$ and $x_2$ is shown by the overlap of those two circles (the area $C + D$ in the diagram). \nThe total variance explained in $y$ is now separated into the areas $A + B + C$ (and $E$ is the _unexplained_ variance - the residuals).  \n\nAreas $A$ and $B$ are no longer the same as in the previous diagrams - there's a little bit (area $C$) that we don't want to double count in its explanatory power as it can't be attributable to specifically one variable or the other.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Venn Diagram for Multiple Regression y ~ x1 + x2 where x1 and x2 are somewhat correlated](images/mlr/venn_mlr2.png){#fig-vennmlr2 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n- $A$ is the variance in $y$ _uniquely_ explained by $x_1$  \n- $B$ is the variance in $y$ _uniquely_ explained by $x_2$\n- $C$ is the variance in $y$ that is explained by both $x_1$ and $x_2$ but not attributable to either one uniquely.  \n\nWe can map the model comparisons we talked about above to this diagram too. The model comparison below would allow us to ask whether the area $B$ is explaining a significant amount of $y$ (excluding the area $A+C$).  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm(y ~ x1, data = mlr_data)\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nanova(model1, model2)\n```\n:::\n\n\n\n\n::: {.callout-note}\n## Example: Happy Commuters\n\nWe have a sample of 100 people who all completed a questionnaire that has given us measures of their job satisfaction (range 5-35, higher scores indicate more satisfaction), their average commuting time (in minutes), their work flexibility (range 5-35, with higher scores indicating more flexibility in working patterns).  \n\nWe're interested in estimating how length of commute is associated with differences in job satisfaction. However, we also have reason to believe that job satisfaction differs depending upon how flexible the working patterns are _and_ that people on more flexible contracts tend to have longer commutes. So we want to isolate the differences in job satisfaction due to commuting from those due to work flexbility.  \n\n\n\n\n\n\n\n\n\nThe toy dataset for our happy commuters example is at [https://uoepsy.github.io/data/happycommute.csv](https://uoepsy.github.io/data/happycommute.csv). We can see plots of the different relationships in @fig-commuteplot. It looks from these like job satisfaction _increases_ with longer commuting time, and also _increases_ with more work flexibility. Note also that commuting time increases with more work flexibility.  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nhappycommute <- read_csv(\"https://uoepsy.github.io/data/happycommute.csv\")\nlibrary(patchwork)\nggplot(happycommute, aes(x=commute,y=jobsat))+\n  geom_point() +\n  labs(x=\"Commuting Time (minutes)\",y=\"Job Satisfaction\") + \nggplot(happycommute, aes(x=work_flex,y=jobsat))+\n  geom_point() +\n  labs(x=\"Work Flexibility Rating\",y=\"Job Satisfaction\") +\nggplot(happycommute, aes(x=work_flex,y=commute))+\n  geom_point() +\n  labs(x=\"Work Flexibility Rating\",y=\"Commuting Time (minutes)\")\n```\n\n::: {.cell-output-display}\n![bi-variate relationships between each of Job Satisfaction, Commuting Time and Work Flexibility](07a_mlr_files/figure-html/fig-commuteplot-1.png){#fig-commuteplot fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nIf we fit a simple regression $JobSat \\sim b_0 + b_1(CommuteTime)$, we get a nice line, and our model is significant. The coefficient is positive suggesting to us that people who commute for longer are more satisfied with their jobs. Good news for me, it takes me 2 hours to get into work!  \n\n::::panelset\n:::panel\n#### Plot\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(happycommute, aes(x=commute,y=jobsat))+\n  geom_point() +\n  geom_smooth(method=lm)+\n  labs(x=\"Commuting Time (minutes)\",y=\"Job Satisfaction\")\n```\n\n::: {.cell-output-display}\n![](07a_mlr_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n:::\n:::panel\n#### Model summary\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlm(jobsat ~ commute, data = happycommute) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = jobsat ~ commute, data = happycommute)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2759  -2.6475   0.0467   2.9659   9.4927 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.11847    1.29422   7.046 2.59e-10 ***\ncommute      0.24794    0.03425   7.240 1.02e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.247 on 98 degrees of freedom\nMultiple R-squared:  0.3485,\tAdjusted R-squared:  0.3418 \nF-statistic: 52.42 on 1 and 98 DF,  p-value: 1.02e-10\n```\n\n\n:::\n:::\n\n\n\n\n:::\n::::\n\n<br>\nBut... what if the reason that we are seeing \"more commuting >> more job satisfaction\" is _not_ because long commutes are enjoyable, but because the people who have longer commutes are those who tend to be in more flexible working patterns (and so are more satisfied in their work for a myriad of other reasons)?    \n\nWhen we compare two models, one with job satisfaction predicted by work flexibility, and one with job satisfaction predicted by both work flexibility _and_ commuting time, we now conclude that commuting time is **not** a useful predictor.. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njobmodel1 <- lm(jobsat ~ work_flex, data = happycommute)\njobmodel2 <- lm(jobsat ~ work_flex + commute, data = happycommute)\nanova(jobmodel1, jobmodel2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: jobsat ~ work_flex\nModel 2: jobsat ~ work_flex + commute\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     98 336.77                              \n2     97 325.30  1    11.472 3.4209 0.06742 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nWhy? Because after we take into account how flexible peoples' work is, knowing their commuting time doesn't actually provide any new information about their job satisfaction.  \n\nIf it helps, we might think of this model as the diagram in @fig-venncommute. When we don't have `work flexibility` in our model, then the estimated association between `commuting time` and `job satisfaction` is the areas $B + C$. When we _do_ have `work flexibility` in the model, the _additional_ variance in `job satisfaction` explained _uniquely_ by `commuting time` is just the tiny area $B$ (not a useful amount). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![lm(jobsat ~ work_flex + commute)](images/mlr/venn_commute.png){#fig-venncommute fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n:::frame\nThis example is a very extreme one where the relationship completely disappears. in real data associations tend to be more subtle/less clear cut. Including $x_2$ may increase or decrease the association between $y$ and $x_1$, depending on the extent to which $x_1$ and $x_2$ are correlated. \n:::\n\n:::\n\n\n\n\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: control on the front-end\n\nIf we haven't collected the data yet, one good option is to __control by design__. This would involve trying to collect our data such that the predictor of interest is _independent_ from other possibly confounding variables.  \n\nWe could do this by __randomisation__, where we randomly allocate people to different levels of our focal predictor, meaning that other variables will not be related to the focal predictor. This is what a \"randomized control trial\" does, randomly allocating people to take drug X or a placebo means that the two groups should be similar in aspects such as age.  \n\nAlternatively, we could do achieve it by __\"case-matching\"__. This involves finding people at different levels of the focal predictor who match on possible confounders. For example, for every 60 year who takes drug X, we also collect data from a 60 year old who does not.  \n\nThere are lots of different types of study design, and it is not something to be rushed through. Careful design can make for great science, and clever statistics cannot save a poorly designed study!  \n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Shortcuts for model comparisons  \n\n[Above](#model-comparisons) we saw that we can compare two models by testing the reduction in residual sums of squares. To do this, we fitted each model and then gave both those models to the   `anova()` function.  \n\nThere are some shortcuts that will sometimes save us time, as they don't involve fitting more than the one model.  \n\n\n::: {.callout-note collapse=\"true\"}\n#### Comparisons incrementally adding each predictor\n\n*Using the order in which predictors are entered into our model*, we can set up a series of models that incrementally add each next predictor.  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel0 <- lm(y ~ 1, data = mlr_data)\nmodel1 <- lm(y ~ 1 + x1, data = mlr_data)\nmodel2 <- lm(y ~ 1 + x1 + x2, data = mlr_data)\nmodel3 <- lm(y ~ 1 + x1 + x2 + x3, data = mlr_data)\n```\n:::\n\n\n\n\nWe can then easily compare them by giving them _all_ to the `anova()` function (which can take as many models as we want to give it).  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(model0, model1, model2, model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ 1\nModel 2: y ~ 1 + x1\nModel 3: y ~ 1 + x1 + x2\nModel 4: y ~ 1 + x1 + x2 + x3\n  Res.Df    RSS Df Sum of Sq       F   Pr(>F)   \n1     49 3192.6                                 \n2     48 2874.8  1    317.77  6.2327 0.016271 * \n3     47 2348.8  1    526.06 10.3181 0.002435 **\n4     45 2294.3  2     54.50  0.5344 0.589671   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nThe output here compares each model to the model before it. This means each test is of the **incremental** addition of of each predictor after the predictors before it.   \n\nHowever, if we just give the fullest model to `anova()`, then it will do this procedure for us!  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value   Pr(>F)   \nx1         1  317.77  317.77  6.2327 0.016271 * \nx2         1  526.06  526.06 10.3181 0.002435 **\nx3         2   54.50   27.25  0.5344 0.589671   \nResiduals 45 2294.29   50.98                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nThese are testing, in order - \n- the inclusion of x1 over the null model (without any predictors)\n- the inclusion of x1 over the model with just x1\n- the inclusion of x3 over the model with x1 and x2\n\n\n:::imp\n**THE ORDER MATTERS**  \n\nWhen we give one model the `anova()`, the order that we write our predictors in our model makes a difference!  \n\n:::\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n#### Comparisons with and without each predictor\n\nWe might instead want to set up a series of comparisons that individually drop each predictor:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodelall   <- lm(y ~ 1 + x1 + x2 + x3, data = mlr_data)\nmodeldrop1 <- lm(y ~ 1 +      x2 + x3, data = mlr_data)\nmodeldrop2 <- lm(y ~ 1 + x1 +      x3, data = mlr_data)\nmodeldrop3 <- lm(y ~ 1 + x1 + x2,      data = mlr_data)\n\n\nanova(modeldrop1, modelall) # models with and without x1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ 1 + x2 + x3\nModel 2: y ~ 1 + x1 + x2 + x3\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)   \n1     46 2710.5                               \n2     45 2294.3  1     416.2 8.1634 0.00645 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(modeldrop2, modelall) # models with and without x2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ 1 + x1 + x3\nModel 2: y ~ 1 + x1 + x2 + x3\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     46 2739.4                                \n2     45 2294.3  1     445.1 8.7301 0.004965 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(modeldrop3, modelall) # models with and without x3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ 1 + x1 + x2\nModel 2: y ~ 1 + x1 + x2 + x3\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     47 2348.8                           \n2     45 2294.3  2    54.496 0.5344 0.5897\n```\n\n\n:::\n:::\n\n\n\n\nThere is also a shorthand way of doing this! \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndrop1(modelall, test=\"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\ny ~ 1 + x1 + x2 + x3\n       Df Sum of Sq    RSS    AIC F value   Pr(>F)   \n<none>              2294.3 201.31                    \nx1      1     416.2 2710.5 207.64  8.1634 0.006450 **\nx2      1     445.1 2739.4 208.17  8.7301 0.004965 **\nx3      2      54.5 2348.8 198.48  0.5344 0.589671   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n:::imp\nThe order that our predictors are put into the model doesn't matter here because we're just taking each predictor and comparing a model with and without it.  \n\nHowever, for more complicated models containing \"interactions\" (we learn about these in a couple of weeks), then this approach is sensitive to how we put categorical predictors into our model.  \n:::\n\nNote that this is like taking each predictor as if it were the \"last one in\" in the \"incremental\" approach above. For proof, you'll note that the test of x3 is the same as above).  \n\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: types of Sums of Squares\n\nBeing able to test different sorts of associations for the same predictors in multiple regression is referred to as \"Types of Sums of Squares\" because it involves different methods for partitioning and attributing the variability. \n\n- Type 1 Sums of Squares is the incremental addition of predictors, as seen with `anova(model)` above. \n\n- Type 3 Sums of Squares calculates the unique contribution of each predictor to the model, adjusting for all other predictors (as seen in `drop1(model)` above)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Types of Sums of Squares. Type 1 (Left) incrementally adds each predictor in turn. Type 3 (Right) assesses the marginal associations.](images/mlr/venn_ss_types.png){#fig-sstype fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\nThe coefficients from a multiple regression model are more closely tied to the Type 3 approach - they represent the association between outcome and the predictor after accounting for all other predictors.  \n\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Interpreting Multiple Regression Coefficients  \n\nThus far in this chapter we have mainly been looking at our models in terms of their overall fit - i.e. assessing and comparing them in terms of 'variance explained'. But the estimated coefficients also provide us with a wealth of information. They tell us _how_ things are associated with $y$ (i.e. they quantify the effect), not just _whether_ there are associations. \n\n:::sticky\n\n- F tests and Model Comparisons: __does/do__ [predictor(s)] explain variance in [outcome]?  \n- Coefficients: *__how__* __is__ [predictor] associated with [outcome]?  \n\n:::\n\nWe've seen that when we include multiple predictors, we get out multiple coefficients. The intercept in multiple regression model is very similar to the simple regression model - it is the estimated value of $y$ _when all predictors are zero_. The coefficients for each predictor are similar to previously in that they still represent the \"change in $y$ (outcome) for a 1 unit change in $x$ (predictor). However, these are now the estimated associations while _holding all other predictors constant_. \n\nSometimes, this gets phrased as \"the increase in [outcome] for a one unit increase in [predictor] when...\"\n\n- \"holding [other predictors] constant.\"\n- \"holding [other predictors] fixed.\"\n- \"accounting for [other predictors].\"\n- \"adjusting for [other predictors].\"\n- \"controlling for differences in [other predictors].\"\n- \"partialling out the effects of [other predictors].\"\n- \"holding [other predictors] equal.\" \n- \"accounting for effects of [other predictors].\"\n\nWhat exactly do all these mean? If we return to our regression surface, our coefficients are the angles of this surface. We can see that as $x_2$ increases, the surface goes down (the angle of the orange lines in the grid). This decrease is the same no matter where on $x_1$ we are (i.e. the angle doesn't change as we move up $x_1$). The coefficient for $x_2$ is the amount the surface changes on $y$, provided we stay at the same value for $x_1$.^[and vice versa for the coefficient of $x_1$.]  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07a_mlr_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.39138    3.67735  -0.650  0.51867   \nx1           0.17570    0.06435   2.730  0.00888 **\nx2          -0.64756    0.19959  -3.244  0.00217 **\n```\n\n\n:::\n:::\n\n\n\n\nAnother way to think of this is to imagine a person who scores 3 on $x_2$. What is the estimated difference between in $y$ between them and someone who scored 4 instead? The coefficient for $x_2$ tells us their scores on $y$ would differ by -0.65 _provided they don't also differ on $x_1$._ So we are moving along the regression surface in the $x_2$ direction. This makes sense, because if they _also_ differed on $x_1$, then we would expect their score on $y$ to change because of this too (i.e. we would be moving diagonally on the surface).  \n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: F and t\n\nBack to our Venn diagrams, the coefficients from our multiple regression model reflect the areas in each variable that overlap with $y$ and _not_ with other variables, but they are scaled to be in terms of \"change in $y$ associated with a one unit change in [predictor], holding [other predictors] constant\".   \n\nThis means that our interpretation of multiple regression _coefficients_ is analogous to the idea of a model comparison with and without that predictor. \n\n| model comparison                                                            | coefficient for x2                               |\n| --------------------------------------------------------------------------- | ------------------------------------------------ |\n| `model1 <- lm(y~x1,mlr_data)`<br>`model2 <- lm(y~x1+x2,mlr_data)`<br>`anova(model1, model2)` | `model2 <- lm(y~x1+x2,mlr_data)`<br>`coef(model2)['x2']` |\n| \"_does X2_ explain additional variance after accounting for x1?\"            | \"what is the association between x2 and y, after accounting for x1?\"                                                 |\n\nIn fact, the $p-values$ from the model comparison will even match those from the coefficient tests (see below). These are equivalent in this specific case, but when we more to more general forms of the model, it does not hold. The key reason being is that the model comparison in the linear regression world is assessing the reduction in residual sums of squares, which is directly linked to the slope of a regression line (see back to @fig-sstssrssm above). When we move to more general models, the way we assess \"improvements in fit\" is not always about reduction in residual sums of squares, and so this equivalence can break.  \n\n::::panelset\n:::panel\n#### Model comparison\n\nWith and without x2.  \n$$\nF(1,47)=10.527,\\, p = .00217\n$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm(y ~ x1, data = mlr_data)\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nanova(model1, model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ x1\nModel 2: y ~ x1 + x2\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)   \n1     48 2874.8                               \n2     47 2348.8  1    526.06 10.527 0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n:::\n:::panel\n#### Coefficient\n\ncoefficient for x2.  \n\n$$\nt(47)=-3.244,\\, p = .00217\n$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel2 <- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = mlr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.5201  -4.2912  -0.0268   3.3044  16.2154 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.39138    3.67735  -0.650  0.51867   \nx1           0.17570    0.06435   2.730  0.00888 **\nx2          -0.64756    0.19959  -3.244  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.069 on 47 degrees of freedom\nMultiple R-squared:  0.2643,\tAdjusted R-squared:  0.233 \nF-statistic: 8.443 on 2 and 47 DF,  p-value: 0.0007369\n```\n\n\n:::\n:::\n\n\n\n:::\n::::\n\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Model Visualisations\n\nThe associations we get out from our coefficients are conditional upon holding constant other predictors. How are we supposed to visualise this? Three-dimensional plots like the ones above are lovely, but a) they're difficult to make and b) they only work when there is _one_ other predictor variable being controlled for.  \n\nThe typical way to plot these associations is to make a 2-dimensional figure that shows the _model estimated_ increase in outcome $y$ across values of predictor $x$.  \n\nNotice what we are doing here is plotting a \"model estimated\" slope. We can't just plot our data an add `geom_smooth(method=lm)`, because that would give a visualisation of a linear model **with just one predictor** (whichever one is on the x-axis). \n\nReturning to our Happy Commuter example from above, we want a way to present the estimated slope of `commute` from the model which _also_ has `work_flex` in it.  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhappycommute <- read_csv(\"https://uoepsy.github.io/data/happycommute.csv\")\nmodel2 <- lm(jobsat ~ work_flex + commute, data = happycommute)\ncoef(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   work_flex     commute \n-9.06999839  1.36735284 -0.03732698 \n```\n\n\n:::\n:::\n\n\n\nIn our Happy Commuters example, the difference between the coefficients when we did/didn't include the `work_flex` predictor were _very_ different.  \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07a_mlr_files/figure-html/unnamed-chunk-41-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nSo how do we create plots like the on one the right? \nThere are various helpful packages that are designed to make this easy (which we will come to a little later on in the course), but right now it will be very useful to go through the process of making these plots manually.  \n\nWe will need to create a little dataset of the values of the predictors that we wish to predict across. We'll go across all values of `commute` from 3 to 58 (the range of commuting times in our data), and we'll keep `work_flex` as 20.75 (which is the mean of `work_flex` in our data):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotdata <- data.frame(\n  commute = 3:58, # integers from 3 to 58\n  work_flex = 20.75 # every value will be 20.75\n)\n```\n:::\n\n\n\n\nUsing this little dataset we can now form predictions from our model, which we can then plot. Using the **broom** package is a nice way to get the confidence intervals^[these intervals are using the standard errors multiplied by the appropriate value of $t$. We did this more explicitly in Week 5, but the `augment()` function will do it for us here] as well as the predictions, and it returns it all attached to the dataframe:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\naugment(model2, newdata = plotdata, interval=\"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 56 × 5\n   commute work_flex .fitted .lower .upper\n     <int>     <dbl>   <dbl>  <dbl>  <dbl>\n 1       3      20.8    19.2   17.8   20.5\n 2       4      20.8    19.2   17.8   20.5\n 3       5      20.8    19.1   17.8   20.4\n 4       6      20.8    19.1   17.8   20.3\n 5       7      20.8    19.0   17.8   20.2\n 6       8      20.8    19.0   17.8   20.2\n 7       9      20.8    19.0   17.8   20.1\n 8      10      20.8    18.9   17.8   20.0\n 9      11      20.8    18.9   17.8   19.9\n10      12      20.8    18.9   17.8   19.9\n# ℹ 46 more rows\n```\n\n\n:::\n:::\n\n\n\n\nAnd this looks like something we can put into ggplot!! \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(model2, newdata = plotdata, interval=\"confidence\") |>\n  ggplot(aes(x = commute, y = .fitted,\n             ymin = .lower, ymax = .upper)) +\n  geom_line() +\n  geom_ribbon(alpha = .3) # alpha sets the transparency\n```\n\n::: {.cell-output-display}\n![](07a_mlr_files/figure-html/unnamed-chunk-44-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n\n<!-- ::::panelset -->\n<!-- :::panel -->\n<!-- #### sjPlot -->\n\n<!-- The __sjPlot__ package can make it very easy for us to create plots of model estimated effects. We need to give it the model, the type of thing we want plotted (in this case \"eff\" for \"effect\"), and the relevant predictor term (in this case \"commute\"):   -->\n\n<!-- ```{r} -->\n<!-- #| fig-height: 2.5 -->\n<!-- library(sjPlot) -->\n<!-- plot_model(model2, type = \"eff\", terms = \"commute\") -->\n<!-- ``` -->\n\n<!-- ::: -->\n<!-- :::panel -->\n<!-- #### jtools -->\n\n<!-- __jtools__ is an alternative to sjPlot, and it's just as easy:   -->\n<!-- ```{r} -->\n<!-- library(jtools) -->\n<!-- effect_plot(model2, pred=commute, interval=TRUE) -->\n<!-- ``` -->\n\n\n<!-- ::: -->\n<!-- :::panel -->\n<!-- #### effects -->\n\n<!-- The __effects__ package is handy because it can give us a dataframe of estimates with upper and lower intervals to plot. It's a bit more cumbersome than __sjPlot__, but can be handy if we want more customisation:   -->\n\n<!-- ```{r} -->\n<!-- library(effects) -->\n<!-- effect(term = \"commute\", mod = model2) |> -->\n<!--   as.data.frame() -->\n<!-- ``` -->\n<!-- That dataframe we can then use to plot:   -->\n<!-- ```{r} -->\n<!-- # the xlevels tells it to estimate at 50 different values. -->\n<!-- # this makes it a smoother plot: -->\n<!-- effect(term = \"commute\", mod = model2, xlevels=50) |> -->\n<!--   as.data.frame() |> -->\n<!--   ggplot(aes(x=commute,y=fit))+ -->\n<!--   geom_line()+ -->\n<!--   geom_ribbon(aes(ymin=lower,ymax=upper), alpha=.3) -->\n<!-- ``` -->\n\n<!-- ::: -->\n<!-- :::panel -->\n<!-- #### Manually (optional) -->\n\n<!-- We can extract the predicted values, and confidence bounds, using `predict()`. First we create a dataset of the values of the predictors that we wish to predict across. We'll predict `jobsat` for all values of `commute` from 3 to 58 (the range of commuting times in our data), and holding `work_flex` as 20.75 (the mean of `work_flex` in our data):  -->\n\n<!-- ```{r} -->\n<!-- plotdata <- data.frame( -->\n<!--   work_flex = 20.75, -->\n<!--   commute = 3:58 -->\n<!-- ) -->\n<!-- ``` -->\n<!-- Then we add the predictions and the standard error   -->\n<!-- ```{r} -->\n<!-- plotdata <- plotdata |> -->\n<!--   mutate( -->\n<!--     predy = predict(model2, newdata=plotdata), -->\n<!--     se = predict(model2, plotdata, se.fit = TRUE)$se -->\n<!--   ) -->\n<!-- ``` -->\n<!-- And use the standard error to create the confidence intervals. Doing $1.96 \\times SE$ would get us close, but to do it properly we should use the $t$-distribution:  -->\n<!-- ```{r} -->\n<!-- plotdata <- plotdata |>  -->\n<!--   mutate( -->\n<!--     lower = predy - (qt(.975, df=47) * se), -->\n<!--     upper = predy + (qt(.975, df=47) * se) -->\n<!--   ) -->\n<!-- ``` -->\n\n<!-- And finally we can plot!  -->\n\n<!-- ```{r} -->\n<!-- ggplot(plotdata, aes(x = commute, y = predy,  -->\n<!--                      ymin = lower, ymax = upper)) + -->\n<!--   geom_line() +  -->\n<!--   geom_ribbon(alpha = .3) # alpha sets the transparency -->\n<!-- ``` -->\n\n\n<!-- ::: -->\n<!-- :::: -->\n\n\n\n<!-- It might help to think of this as if we are just tilting the our view of the regression surface so that we see it from only one edge:  -->\n\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- mlr_data <- happycommute |> transmute(y=jobsat,x1=work_flex,x2=commute) -->\n<!-- fit<-lm(y~x1+x2, data=mlr_data) -->\n<!-- steps=50 -->\n<!-- x1 <- with(mlr_data, seq(min(x1),max(x1),length=steps)) -->\n<!-- x2 <- with(mlr_data, seq(min(x2),max(x2),length=steps)) -->\n<!-- newdat <- expand.grid(x1=x1, x2=x2) -->\n<!-- y <- matrix(predict(fit, newdat), steps, steps) -->\n<!-- par(mfrow=c(1,2)) -->\n<!-- p <- persp(x1,x2,y, theta = 50,phi=25, col = NA, -->\n<!--            zlim=c(min(mlr_data$y),max(mlr_data$y))) -->\n<!-- obs <- with(mlr_data, trans3d(x1,x2, y, p)) -->\n<!-- pred <- with(mlr_data, trans3d(x1, x2, fitted(fit), p)) -->\n<!-- points(obs, col = \"red\", pch = 16) -->\n<!-- segments(obs$x, obs$y, pred$x, pred$y,lty = \"dashed\") -->\n\n<!-- p <- persp(x1,x2,y, theta = 90,phi=46, col = NA, -->\n<!--            zlim=c(min(mlr_data$y),max(mlr_data$y))) -->\n<!-- obs <- with(mlr_data, trans3d(x1,x2, y, p)) -->\n<!-- pred <- with(mlr_data, trans3d(x1, x2, fitted(fit), p)) -->\n<!-- points(obs, col = \"red\", pch = 16) -->\n<!-- segments(obs$x, obs$y, pred$x, pred$y,lty = \"dashed\") -->\n<!-- par(mfrow=c(1,1)) -->\n<!-- ``` -->\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>\n\n\n",
    "supporting": [
      "07a_mlr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.3.0/panelset.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/panelset-0.3.0/panelset.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}