{
  "hash": "398d794351b4c23e9dea03b24dffbe48",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"7B: Assumptions, Diagnostics, and Troubleshooting\"\nparams: \n    SHOW_SOLS: TRUE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n:::lo\nThis reading:  \n\n- What assumptions do we need to make when we use a linear model to generalise beyond our sample data (i.e. to make statements about the world) \n- How can we investigate how individual observations influence our model? \n- What can we do when it looks like our assumptions are not satisfied? \n\n:::\n\n\nWhen we introduced simple linear regression, we talked about the assumptions that these models rely on (see [5B #a-first-look-at-linear-model-assumptions](05b_slr.html#a-first-look-at-linear-model-assumptions){target=\"_blank\"}). \n\nWe talked about the very high-level view:\n$$\n\\color{red}{outcome} \\color{black}{= }\\color{blue}{model}\\color{black}{ + error}\n$$\nThe idea here is that our _error_, which is represented by the _residuals_ from our model (the deviations from observed values to model predicted values) should look more or less like randomness. If they don't, then it indicates that something might be wrong.  \n\nTo examine our assumptions, we can either rely on plotting, or on conducting statistical tests, or both. \n\n# Plotting Assumptions\n\nWe already saw how to interpret the plots in simple regression models ([5B #first-look-assumptions](05b_slr.html#a-first-look-at-linear-model-assumptions){target=\"_blank\"}), and this doesn't change when we move to multiple regression models.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmydata <- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmymodel <- lm(y ~ x1 + x2, data = mydata)\nplot(mymodel)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07b_assumptdiag_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n- Top Left: For the __Residuals vs Fitted__ plot, we want the red line to be horizontal at close to zero across the plot. We don't want the residuals (the points) to be fanning in/out.  \n- Top Right: For the __Normal Q-Q__ plot, we want the residuals (the points) to follow closely to the diagonal line, indicating that they are relatively normally distributed.^[QQplots plot the values against the associated percentiles of the normal distribution. So if we had ten values, it would order them lowest to highest, then plot them on the y against the 10th, 20th, 30th.. and so on percentiles of the standard normal distribution (mean 0, SD 1)]\n- Bottom Left: For the __Scale-Location__ plot, we want the red line to be horizontal across the plot. These plots allow us to examine the extent to which the variance of the residuals changes accross the fitted values. If it is angled, we are likely to see fanning in/out of the points in the residuals vs fitted plot.\n- Bottom Right: The __Residuals vs Leverage__ plot indicates points that might be of individual interest as they may be unduly influencing the model. There are funnel-shaped lines on this plot (sometimes out of scope of the plotting window). Ideally, we want our residuals inside the funnel - the further the residual is to the right (the more leverage it has), the closer to the 0 we want it to be.  \n\n_(Note, if we have only categorical predictors in our model, many of these will show vertical lines of points. This doesn't indicate that anything is wrong, and the same principles described above continue to apply)_\n\n:::rtip\n__Tip:__  \nrunning `plot(model)` will cycle through these plots (asking us to press enter each time to move to the next plot).  \n\nIf needed, we can extract specific plots using, for instance:\n`plot(model, which = 3)`\nfor the third plot.  \n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Testing Assumptions \n\nWe can perform tests to examine how (un)likely we would be to see some residuals like those we have got, if they really did come from randomness.  \n\n## testing normality \n\nThe Shapiro-Wilk test (we saw this very briefly with our $t$-tests in [3B#assumptions](03b_inference2.html#assumptions){target=\"_blank\"}) is a test against the alternative hypothesis that the residuals were not sampled from a normally distributed population. $p >.05$ indicates that we do _not_ have evidence that the assumption has been violated. We can perform this test quickly in R using `shapiro.test(residuals(modelname))`.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshapiro.test(residuals(mymodel))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(mymodel)\nW = 0.98161, p-value = 0.6215\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(mymodel, which = 2)\n```\n\n::: {.cell-output-display}\n![](07b_assumptdiag_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n\n:::int\nModel residuals showed slight deviation from normality (see QQplot), but this was not deemed problematic as the Shapiro-Wilk test failed to reject the null hypothesis that the residuals were drawn from a normally distributed population ($W = 0.98$, $p = 0.62$)\n:::\n\n## testing constant variance\n\nThe `ncvTest()` function (from the **car** package) performs a test against the alternative hypothesis that the error variance changes with the level of the fitted values (also known as the \"Breusch-Pagan test\"). $p >.05$ indicates that we do _not_ have evidence that the assumption has been violated.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(mymodel, which = 3)\n```\n\n::: {.cell-output-display}\n![](07b_assumptdiag_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=80%}\n:::\n\n```{.r .cell-code}\nlibrary(car)\nncvTest(mymodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 2.50283, Df = 1, p = 0.11364\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::int\nVisual inspection of suggested little sign of non-constant variance, with the Breusch-Pagan test failing to reject the null that error variance does not change across the fitted values ($\\chi^2(1)=2.5$, $p = 0.11$)\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Multicollinearity\n\nFor the linear model with multiple explanatory variables, we need to also think about **multicollinearity** - this is when two (or more) of the predictors in our regression model are moderately or highly correlated.  \nRecall our interpretation of multiple regression coefficients as  \n  \n<center>\"the effect of $x_1$ on $y$ when _holding the values of $x_2$, $x_3$, ... $x_k$ constant_\"</center>  \n  \nThis interpretation falls down if predictors are highly correlated because if, e.g., predictors $x_1$ and $x_2$ are highly correlated, then changing the value of $x_1$ necessarily entails a change the value of $x_2$ meaning that it no longer makes sense to talk about _holding $x_2$ constant._^[For an example, consider two variables: foot length (in cm) and UK shoe size. These will be very strongly correlated. If we used them both to predict some outcome like height, then trying to imagine \"holding shoe size constant, having feet 1cm longer is associated with $b$ increase in height\" doesn't make sense. Because having feet 1cm longer will very likely also mean wearing a bigger shoe size. In cases like this, the regression surface becomes unstable.]  \n\nWe can assess multicollinearity using the **variance inflation factor (VIF)**, which for a given predictor $x_j$ is calculated as:  \n$$\nVIF_j = \\frac{1}{1-R_j^2} \\\\\n$$\nWhere $R_j^2$ is the coefficient of determination (the R-squared) resulting from a regression of $x_j$ on to all the other predictors in the model ($x_j = x_1 + ... x_k + \\epsilon$). The more highly correlated $x_j$ is with other predictors, the bigger $R_j^2$ becomes, and thus the bigger $VIF_j$ becomes.  \n\n  \nThe square root of VIF indicates how much the SE of the coefficient has been inflated due to multicollinearity. For example, if the VIF of a predictor variable were 4.6 ($\\sqrt{4.6} = 2.1$), then the standard error of the coefficient of that predictor is 2.1 times larger than if the predictor had zero correlation with the other predictor variables. Suggested cut-offs for VIF are varied. Some suggest 10, others 5.  \nIt's good practice to define what you will consider an acceptable value _prior_ to calculating it.   \n\nIn R, the `vif()` function from the **car** package will provide VIF values for each predictor in your model (these are called \"GVIF\" if there are categorical predictors). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvif(mymodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      x1       x2 \n1.004384 1.004384 \n```\n\n\n:::\n:::\n\n\n\n\n:::int\nVIF values <5 indicate that multicollinearity is not adversely affecting model estimates. \n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Individual Case Diagnostics\n\nIn linear regression, individual cases in our data can influence our model more than others. There are a variety of measures we can use to evaluate the amount of misfit and influence that single observations have on our model and our model estimates. \n\n:::imp\n__THERE ARE NO HARD RULES FOR WHAT COUNTS AS \"INFLUENTIAL\" AND HOW WE SHOULD DEAL WITH THESE CASES__  \n\nThere are many ways to make a cake. recipes can be useful, but you really need to think about what ingredients you actually have (what data you have). \n\nYou don't __have__ to exclude influential observations. Try to avoid blindly following cut-offs, and try to think carefully about outliers and influential points and whether you want to exclude them, and whether there might be some other model specification that captures this in some estimable way. \n\nYou might also ask \"do these observations change the conclusions i might make?\" (consider conducting a 'sensitivity analysis' by fitting the models with and without these certain cases).\n\n:::\n\nThere are various measures of outlyngness and influence. \nHere are a few. <b style=\"color:red\">You do not need to remember all of these!</b>  \n\n**Regression outliers:**  \nA large residual $\\hat \\epsilon_i$ - i.e., a big discrepancy between their predicted y-value and their observed y-value.  \n\n+ **Standardised residuals:** For residual $\\hat \\epsilon_i$, divide by the estimate of the standard deviation of the residuals. In R, the `rstandard()` function will give you these\n+ **Studentised residuals:** For residual $\\hat \\epsilon_i$, divide by the estimate of the standard deviation of the residuals excluding case $i$. In R, the `rstudent()` function will give you these. Values $>|2|$ (greater in magnitude than two) are considered *potential* outliers.  \n\n**High leverage cases:**  \nThese are cases which have considerable _potential_ to influence the regression model (e.g., cases with an unusual combination of predictor values).  \n\n+ **Hat values:** are used to assess leverage. In R, The `hatvalues()` function will retrieve these.  \nHat values of more than $2 \\bar{h}$ (2 times the average hat value) are often worth looking at. $\\bar{h}$ is calculated as $\\frac{k + 1}{n}$, where $k$ is the number of predictors, and $n$ is the sample size.  \n\n**High influence cases:**  \nWhen a case has high leverage *and* is an outlier, it will have a large influence on the regression model. \n\n+ **Cook's Distance:** combines *leverage* (hatvalues) with *outlying-ness* to capture influence. There are many suggested [Cook's Distance cut-offs](https://en.wikipedia.org/wiki/Cook%27s_distance#Detecting_highly_influential_observations).  \n  - In R, the `cooks.distance()` function will provide these.  \n  - The funnels on the \"residuals vs leverage plot\" (`plot(model, which = 5)`) are a visual representation of where different cook's distance values will fall.  \n  - We can also see the Cook's distance values themselves using `plot(model, which = 4)`.  \n+ **DFFit:** the change in the predicted value at the $i^{th}$ observation with and without the $i^{th}$ observation is included in the regression.  \n+ **DFbeta:**  the change in a specific coefficient with and without the $i^{th}$ observation is included in the regression.  \n+ **DFbetas:**  the change in a specific coefficient divided by the standard error, with and without the $i^{th}$ observation is included in the regression.  \n+ **COVRATIO:** measures the effect of an observation on the covariance matrix of the parameter estimates. In simpler terms, it captures an observation's influence on standard errors. Values which are $>1+\\frac{3(k+1)}{n}$ or $<1-\\frac{3(k+1)}{n}$ are *sometimes* considered as having strong influence.  \n\n:::rtip\nYou can get a whole bucket-load of these measures with the `influence.measures()` function.  \n\n- `influence.measures(mymodel)` will give you out a dataframe of the various measures.  \n- `summary(influence.measures(mymodel))` will provide a nice summary of what R deems to be the influential points. \n\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(influence.measures(mymodel))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPotentially influential observations of\n\t lm(formula = y ~ x1 + x2, data = mydata) :\n\n   dfb.1_ dfb.x1 dfb.x2 dffit   cov.r   cook.d hat  \n2   0.02   0.00  -0.02   0.03    1.22_*  0.00   0.13\n15 -0.05   0.57  -0.38   0.76_*  0.79_*  0.17   0.08\n20  0.00  -0.25   0.13  -0.46    0.75_*  0.06   0.03\n27 -0.02   0.08  -0.04   0.09    1.19_*  0.00   0.11\n32 -0.11  -0.31   0.35  -0.57    0.78_*  0.10   0.05\n36 -0.10   0.06   0.07  -0.10    1.22_*  0.00   0.13\n```\n\n\n:::\n:::\n\n\n\n\n### Sensitivity Analyses  \n\nOnce we identify influential observations, it can often be sensible to re-fit our model with those observations excluded. We then examine whether the pattern of results change, and whether our conclusions (i.e. decisions based on significance tests) change.  \nThis process can then go into our write-up!  \n\nTo refit a model without the $n^{th}$ rows, we can simply remove those from the data that we give the `lm()` function.  \nFor instance, if I want to fit my model to all the data _except_ the 10th and 32nd rows:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmydata_sub <- mydata[-c(10,32), ]\nmymodel_subset <- lm(y ~ x1 + x2, mydata_sub)\n```\n:::\n\n\n\n\nWe can then compare `summary(mymodel)` and `summary(mymodel_subset)` to examine whether our conclusions about the relevant association changes when we exclude these people. If it doesn't change our conclusions, then our eventual findings are robust to the  influence of these people. If it does, then we have something we can discuss!  \n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Troubleshooting\n\nThere are lots of things that can result in assumptions that look problematic. \nAssumptions may be violated if, for instance, there is non-linearity in the relationship being studied, or if relationship between $y$ and $x_1$ _depends on_ the level of $x_2$ (we'll cover this scenario next week), or if there is some important variable that isn't measured or included in the model.   \n\nThe various causes behind having a model that violates assumptions means that there is __no fix-all solution__.  \n\n## Transformations \n\nOne option that may be useful is to consider transforming your outcome variable. Common variables such as neuropsychological tests and scales have a lot of people bunched up near the bottom or top of the scale (referred to as the \"ceiling\" and \"floor\"). Transformations can spread out the bunched-up scores.  \n\nFor an example, consider a scale where most people score fairly low: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(agetest$score, breaks=40)\n```\n\n::: {.cell-output-display}\n![](07b_assumptdiag_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nIf we are interested in how scores are predicted by age, it may well be the case that the amount scores increase/decrease with age is different depending on how far up the scores we are thinking about. It may be that modelling the raw scores predicted by age results in some problematic assumption plots: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod <- lm(score ~ age, data = agetest)\nplot(mod, which = 2)\n```\n\n::: {.cell-output-display}\n![](07b_assumptdiag_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nWe can transform the scores using something such as `log()`^[another useful transformation is `BoxCox()` from the __forecast__ package]. This basically stretches out the differences at the lower end of the scale. So scoring 2 vs scoring 3 is considered to be a bigger difference than scoring 20 vs 21: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(\n  raw = c(2,3,20,21),\n  log = log(raw)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 Ã— 2\n    raw   log\n  <dbl> <dbl>\n1     2 0.693\n2     3 1.10 \n3    20 3.00 \n4    21 3.04 \n```\n\n\n:::\n:::\n\n\n\nThe resulting distribution of transformed scores is closer to normally distributed, and the residuals of modelling `lm(log(score) ~ age)` looking less problematic:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(log(agetest$score), breaks=40)\nmodlog <- lm(log(score) ~ age, data = agetest)\nplot(modlog, which = 2)\n```\n\n::: {.cell-output-display}\n![](07b_assumptdiag_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nThe __big downside__ of transforming an outcome variable is that you are no longer modelling the outcome variable on its original scale. Coefficients now represent \"a 1 unit change in x is associated with a $b$ change in __log y__\". Which makes it a lot harder to interpret.  \n\nIt's best to only transform your outcome variable if you have theoretical justification to do so. \n\n## Back to the Bootstrap?\n\nConducting significance tests requires thinking about the variability that we would expect with \"repeated sampling\". To do this, the majority of our tests have been using the _standard error_ of a statistic (the standard deviation of the distribution of statistics that we would get if we did the study lots of times).  \nWe actually learned the logic of bootstrapping back in [2B#standard-error-in-practice](02b_sampling.html#standard-error-in-practice){target=\"_blank\"}. It all comes down to the construction of the standard error. In the output of `summary(model)`, we are given, alongside each coefficient $b$, a standard error, a $t$-value and a $p$-value. These all rely on using a formula to calculate the standard error, and it is this which requires making assumptions about the distribution of residuals.  \n\nBootstrapping is a way to estimate the standard error without having to rely on a formula - it _acts_ out the process of repeatedly sampling from a population, by repeatedly **re**sampling from our original sample (do check back to [2B#standard-error-in-practice](02b_sampling.html#standard-error-in-practice){target=\"_blank\"} for an example). By doing this lots and lots of times, we actually *construct* a distribution of possible values for coefficient estimates $b$, from which we can calculate a confidence interval. In doing so, we don't have to make as many assumptions - we don't have to assume normality and constant variance of the residuals, because bootstrapping involves fitting 1000s of models to 1000s of resampled datasets, and our residuals are different for each one. \n\nIn practice, this can all be done very easily in R by using the `Boot()` function from the __car__ package. We can ask it to resample 1000 times (getting a distribution of 1000 values for the coefficients): \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmydata <- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmymodel <- lm(y ~ x1 + x2, data = mydata)\n\nlibrary(car)\nbootmymodel <- Boot(mymodel, R = 1000)\n```\n:::\n\n\n\nThe `bootmymodel` object actually contains lots of stuff (it contains all 1000 sets of coefficient estimates!). \nWe can get some confidence intervals easily:  \n_Note: the actual estimates are those from our original model, it is just the bounds of the interval that bootstrapping is providing us with)_  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nConfint(bootmymodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap bca confidence intervals\n\n              Estimate       2.5 %     97.5 %\n(Intercept) -2.3913797 -6.81733162  3.4833958\nx1           0.1757046  0.06226639  0.3014810\nx2          -0.6475619 -1.02709666 -0.2969874\n```\n\n\n:::\n:::\n\n\n\n\n:::imp\n**Bootstrapping is not a panacea!!**  \n\nThe bootstrap may provide us with an alternative way of conducting inference, but our model may still be misspecified. Furthermore, studies show that bootstrap doesn't perform well in small samples, and actually results increased Type 1 errors ([3A#making-mistakes](03a_inference.html#making-mistakes){target=\"_blank\"}).  \n\nIt's very important to remember that we can't get something from nothing. Bootstrapping is entirely reliant on utilising our original sample to pretend that it is a population (and mimick sampling from that population). If our original sample is not representative of the population that we're interested in, bootstrapping doesn't help us at all.  \n\nAs with most things in statistics, the garbage-in-garbage-out principle applies here.  \n\nbootstrapped(&#128169;) = &#128169;&#128169;&#128169;...\n\n\n\n:::\n",
    "supporting": [
      "07b_assumptdiag_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.3.0/panelset.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/panelset-0.3.0/panelset.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}