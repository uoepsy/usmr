{
  "hash": "2206725161fc84b781dbc459614c10a4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3B: Practical Inference\"\nlink-citations: true\nparams: \n    SHOW_SOLS: TRUE\n    TOGGLE: TRUE\n---\n\n\n\n\n\n\n:::lo\nThis reading:  \n\n- How does hypothesis testing work in practice?  \n- How do we do all this in R? \n    - *spoiler: it's easier than you think*\n- What are some basic hypothesis tests that we can conduct?  \n  - Tests of a single continuous variable\n  - Tests of the relationship between a continuous variable and a binary categorical variable  \n\n:::\n\nIn [3A](03a_inference.html){target=\"_blank\"} we learned about the logic of _Null Hypothesis Significance Testing_ (NHST), allowing us to draw perform inferentials tests about parameters in the __population__, based on statistics computed on the __sample__ that we have collected.  \n\n:::statbox\n__NHST Recap__ \n\nWe have a sample ($n=10$):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmysample <- c(1, -4, 6, 4, -2, 3, 2, -5, 6, 8)\n```\n:::\n\n\n\nAnd a sample mean:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(mysample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.9\n```\n\n\n:::\n:::\n\n\n\n\nWe're interested in whether - in the _population_ - the mean of this variable is different from 0. So we are testing between two hypotheses:\n\n- Null Hypothesis $H_0: \\mu = 0$.  \n- Alternative Hypothesis $H_1: \\mu \\neq 0$. \n\nRemember, there are lots of samples of size $n=10$ that we _could_ take, and they all have different means. To quantify the spread of these different means we can use the __standard error__, calculated using $SE = \\frac{\\sigma}{\\sqrt{n}}$:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsd(mysample) / sqrt(length(mysample))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.394035\n```\n\n\n:::\n:::\n\n\n\n\nWe can use this information to express how far away from the null hypothesis (mean = 0) our observed sample is, in terms of standard errors: \n\n$$\nZ \\ = \\ \\frac{\\text{estimate}-\\text{null}}{SE} \\ = \\ \\frac{1.9 - 0}{1.39} \\ = \\ 1.36\n$$\nWe can then calculate, __if__ the mean in the population _is_ 0, what is the probability of obtaining a $Z$-statistic from a sample of this size at least as extreme as the one we _have_ observed?  \n\nThe resulting probability is our __p-value:__  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n2*pnorm(1.36, mean = 0, sd = 1, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1738299\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![2*pnorm gives the two tails](03b_inference2_files/figure-html/fig-pnorm23-1.png){#fig-pnorm23 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nAs our $p$-value is above our threshold of $\\alpha=.05$, we fail to reject the null hypothesis that the mean in the population is zero.  \n\nWe can get to the same conclusion by constructing a 95% confidence interval: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxbar = mean(mysample)\nse = sd(mysample) / sqrt(length(mysample))\nc(xbar - (1.96 * se), xbar + (1.96 * se))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.8323084  4.6323084\n```\n\n\n:::\n:::\n\n\n\nAs this interval includes zero, then at the 5% level we fail to reject the null hypothesis that the population mean is zero.^[Remember that confidence intervals provide a range of plausible values for the population mean. In this case, zero is a plausible value.]\n\n\n:::\n\nWhile in practice NHST follows the logic described above, there is something important that we have been sweeping under the carpet.  \n\nIn our estimation of the __standard error__ we have used the formula that includes $\\sigma$, which refers to the __population__ standard deviation. However, we never know this value (because we don't have data for the population), so we have been using the __sample__ standard deviation $s$ instead. This is an _approximation_, and might be okay when we have a very large $n$ (meaning $s$ provides an accurate estimate of $\\sigma$), but in practice this is not always feasible.\n$$\nSE = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}\n$$\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# $t$ - distributions  \n\nTo resolve the issues with the approximation of using $s$ in place of $\\sigma$ to compute the standard error, we can move from using the normal distribution to referring to the $t$-distribution.  \nThe $t$ distribution is very similar to the normal distribution, but it has slightly _heavier_ tails (see @fig-tnorm). $t$-distributions are always centered on zero, and the precise shape (how heavy the tails are) depends upon a parameter known as the __degrees of freedom.__  \n  \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Normal distribution (black) vs t-distribution with 3 degrees of freedom (red)](03b_inference2_files/figure-html/fig-tnorm-1.png){#fig-tnorm fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n:::sticky\n__Degrees of Freedom - $df$__ \n\n'Degrees of freedom' is a tricky concept. One of the most intuitive ways to understand it is to think of it as the number of independent bits of information that go into calculating an estimate. Put another way, it is the number of datapoints that are _free to vary_.  \n\n::: {.callout-note collapse=\"true\"}\n### Degrees of freedom (df)\n\nSuppose we have four unknown numbers ($a$, $b$, $c$ and $d$) which *must* have a mean of 5.  \n\nDo the following, *in order:*  \n\n1. Choose a value for $a$.\n1. Choose a value for $b$.\n1. Choose a value for $c$.\n1. Can you choose a value for $d$ while ensuring the mean of the four numbers you have chosen is 5?\n\nYou are free to choose anything you like for $a$, $b$ and $c$.  \nBut once those are fixed, you have no freedom to choose $d$.  \n\nExample:  \n\n+ $a$ = 1  \n+ $b$ = 2  \n+ $c$ = 3  \n\nWe know that $\\frac{1+2+3+d}{4} = 5$\nSo there is only one possible value for $d$:  \n$\\frac{1+2+3+d}{4} = 5$  \n$1+2+3+d = 5*4$  \n$1+2+3+d = 20$  \n$d = 20-3-2-1$   \n$d = 14$  \n\n:::\n\n:::\n\n\nWhen we estimate the mean from a sample, we use up one of our degrees of freedom, and so our test of a single mean will require us to use a $t$-distribution with $n-1$ degrees of freedom. For $t$-distributions, as the $df$ increases the distribution becomes closer and closer to a normal distribution (see @fig-tdf) - the use of these $t$-distributions is exactly what we need in order to account for using $s$ in our calculation of the standard error.^[This is because with smaller samples we have less certainty in the estimate of the population standard deviation, and our estimates of mean and standard deviation are more dependent on one another. The bottom part of $\\frac{\\bar x - \\mu}{SE}$ has a greater chance of being smaller than the top part, meaning that our resulting our test statistics will tend to be slightly bigger. To better represent this greater chance of seeing bigger test statistics from small samples, our $t$-distributions have heavier tails.]  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![t distributions with various degrees of freedom.](03b_inference2_files/figure-html/fig-tdf-1.png){#fig-tdf fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nTo make use of the $t$-distribution in hypothesis testing, we need to move to performing $t$-tests! \n\nThe logic remains the same as before, but where we previously were relying on the normal distribution: \n\n- `pnorm()` for our $p$-values\n- `qnorm()` in order to calculate our confidence intervals (`qnorm(0.975)` gives the 1.96 we have been using)\n\nWe can use `pt()` and `qt()` to conduct the same process but in reference to the appropriate $t$-distribution.  \n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# $t$ -test demonstration\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmysample <- c(1, -4, 6, 4, -2, 3, 2, -5, 6, 8)\n```\n:::\n\n\n\n\nLet us then perform a more appropriate test against the null hypothesis that the mean in the population is zero. It is going to look pretty much the same as it did previously, but things like \"Z\" and \"norm\" are going to be replaced with \"t\".  \n\nFor example, our test-statistic is now going to be a $t$-statistic:  \n\n$$\n\\begin{align}\n& t =  \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\\\\n\\ \\\\\n& \\text{where:} \\\\ \n& \\bar x : \\text{sample mean} \\\\\n& \\mu_0 : \\text{hypothesised population mean} \\\\\n& s : \\text{sample standard deviation} \\\\\n& n : \\text{sample size} \\\\\n\\end{align}\n$$\nWhich can be calculated as: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxbar <- mean(mysample)\nse <- sd(mysample) / sqrt(length(mysample))\ntstat <- (xbar - 0) / se\ntstat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.36295\n```\n\n\n:::\n:::\n\n\n\n\nWhile the calculation is just the same as it was previously, we're calling it a $t$-statistic because we are going to compare it to a reference $t$-distribution. \nAs our sample has $n=10$, and as part of the statistic we are estimating a mean, the relevant $t$-distribution for us has 9 ($10-1$) degrees of freedom (we lose one by calculating the mean). \n  \nOur p-value can be found with the `pt()` function:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n2*pt(tstat, df = 9, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2060213\n```\n\n\n:::\n:::\n\n\n\n\nAnd our confidence interval can be constructed using: \n$$\n\\text{CI} = \\bar{x} \\pm t^* \\times SE \\\\\n$$\nNote that $t^*$ has replaced the 1.96 we saw in previous chapters, because we obtained that using the normal distribution. The code `qnorm(c(0.025, 0.975))` showed us that 95% of __normal distribution__ is beyond 1.96 from the mean. But what we actually want to know is where 95% of the __$t$-distribution__ with $df=9$ lies:  \nSo instead we can use:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqt(c(0.025, 0.975), df = 9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.262157  2.262157\n```\n\n\n:::\n:::\n\n\n\nAnd our confidence interval is:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxbar <- mean(mysample)\nse <- sd(mysample) / sqrt(length(mysample))\nxbar - (2.262157 * se)\nxbar + (2.262157 * se)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.253526\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.053526\n```\n\n\n:::\n:::\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Make life easier with R\n\nAll of the above is crucial for understanding how this all works, but in practice we can avoid all of the rigmarole of ever calculating the standard error or using functions like `pt()`, `qt()`. This is where R starts to become far more powerful - there are functions that do all this sort of stuff for us - in just _one single line of code!_ \n\nTake a look at the output of the function below. I have given it the sample, and specified that we want it to test against the null hypothesis that $\\mu=0$.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(mysample, mu = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  mysample\nt = 1.363, df = 9, p-value = 0.206\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.253526  5.053526\nsample estimates:\nmean of x \n      1.9 \n```\n\n\n:::\n:::\n\n\n\n\nThe `t.test()` function here gives us the $t$-statistic, the $df$, the $p$-value, the 95% CI, the mean $\\bar x$, and it even tells us the alternative hypothesis (that the true mean is $\\neq 0$).  \n  \nAll of these numbers will match those that we calculated above (there may be a small bit of rounding error).  \n\nIt's __that__ easy!  \n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Assumptions  \n\nStatistical tests often require us to meet a set of conditions in order for our inferences to be valid. When we perform tests like these that involve estimating a mean, a common requirement is that the deviations from that mean are close to normally distributed. For t-tests, this assumption can be relaxed somewhat in cases where our sample size is larger and there is not too much skew.^[This is because, practically speaking, what we really _need_ in order to make useful, defensible conclusions, is not that the _population_ itself is normally distributed, but that the _sampling distribution of the statistic_ is close enough to the $t$-distribution. This can often be the case when we have a large sample without much skew.]  \n\n:::frame\n__Assumption Plots__  \n\nWe can evaluate how close to normal a distribution is by visualising it via histograms and density plots and making a judgment call, but this can sometimes be hard:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata <- tibble(mysample = mysample)\nggplot(data,aes(x=mysample))+geom_histogram(bins=14) +\nggplot(data,aes(x=mysample))+geom_density()\n```\n\n::: {.cell-output-display}\n![Am I normal?](03b_inference2_files/figure-html/fig-normalquestion-1.png){#fig-normalquestion fig-align='center' width=100%}\n:::\n:::\n\n\n\nAnother useful visualisation tool is the __QQplot__. The closer to the diagonal line, the closer our data is to being normally distributed: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqqnorm(data$mysample) # make the plot\nqqline(data$mysample) # add the line\n```\n\n::: {.cell-output-display}\n![A QQplot](03b_inference2_files/figure-html/fig-normalqq-1.png){#fig-normalqq fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n::: {.callout-caution collapse=\"true\"}\n### optional: what are the 'theoretical quantiles?\n\nThe theoretical quantiles are the equivalent quantiles of the _standard normal distribution_ (see [#2B standard-normal-distribution](02b_sampling.html#the-standard-normal-distribution){target=\"_blank\"}).  \nIn the `data$mysample` example above, we have 10 datapoints. If we cut the standard normal distribution into 10 sections of equal area (see @fig-qqnormcurve), it is the points on the x-axis at the center of each area that we are plotting our data against.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![10 quantiles of the normal distribution](03b_inference2_files/figure-html/fig-qqnormcurve-1.png){#fig-qqnormcurve fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n:::frame\n__Assumption Tests__  \n\nThere are also, if we wish to make use of them, specific hypothesis tests that assess normality, such as the 'Shapiro-Wilks' Test. The null hypothesis for this test is that the data we give it are drawn from a normal distribution. This means that __we want a p-value *greater* than .05__. So in the example below, we have no reason to reject the hypothesis that our data are drawn from a normal distribution. This means we can continue to conduct a t-test.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshapiro.test(mysample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  mysample\nW = 0.94722, p-value = 0.6358\n```\n\n\n:::\n:::\n\n\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Basic Tests  \n\nNow that we've gone through all the nitty-gritty bits of how hypothesis testing works, the heavy lifting is done.  \nwe're going to start to look at some of the different basic hypothesis tests that we can perform.  \n\nFor each test below we show an example conducted the quick way (e.g. `t.test()` function), and also the manually computations (for those of you who are interested!). We've already seen the one sample $t$-test in the example above, so you might want to skim over that section.\n\n:::imp\n__something to bear in mind__  \n\nThese tests are the simple hypothesis tests that were developed in the 19th and 20th centuries, and provide a good basis of understanding the _null hypothesis significance testing_ framework.  \n\nIn the latter half of this course, we move to focus on a modelling based approach for analysing data. We will start to see how many of these simple tests that we are learning now are actually special cases of a more general statistical model. \n\n:::\n\n<!-- For each test, we'll detail:   -->\n\n<!-- - Purpose -->\n<!-- - Hypotheses -->\n<!-- - Test statistic -->\n<!-- - P-value -->\n<!-- - Assumptions -->\n\n<!-- And then we will show an example conducted the quick way (e.g. `t.test()` function), and also the manually computations (for those of you who are interested!).   -->\n<!-- We've already seen the one sample $t$-test in the example above, so you might want to skim over that section.   -->\n\n\n## One sample t-test  \n\n:::statbox\n__Purpose__  \n\nThe one sample t-test is what we have already seen above. We use it to test whether the mean is different from/greater than/less than some hypothesised value.  \n\n- __Examples:__\n  - Is the mean age of USMR students different from 20? \n  - Is the mean IQ different from 100?  \n  - Do people read more than 250 words per minute?  \n  \n__Assumptions:__\n\n- The data are continuous (not discrete)\n- The data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)\n- The data are normally distributed (can be relaxed somewhat if the sample size is \"large enough\" (rule-of-thumb n = 30) and the data are not strongly skewed)\n  \n:::\n\n<!-- :::statbox -->\n<!-- __Hypotheses__  -->\n\n<!-- Our hypotheses will take one of the combinations below, depending on if we want to perform the test of whether the mean $\\mu$ is less than/different from/greater than some hypothesised value $\\mu_0$.   -->\n\n<!-- | Null Hypothesis   | Alternative Hypothesis | -->\n<!-- | ----------- | ----------- | -->\n<!-- | $H_0: \\mu = \\ \\mu_0$  | $H_1: \\mu \\neq \\ \\mu_0$ | -->\n<!-- | $H_0: \\mu \\leq \\ \\mu_0$  | $H_1: \\mu > \\ \\mu_0$ | -->\n<!-- | $H_0: \\mu \\geq \\ \\mu_0$  | $H_1: \\mu < \\ \\mu_0$ | -->\n\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __Test-statistic__  -->\n\n<!-- $$ -->\n<!-- \\begin{align} -->\n<!-- & t =  \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\\\ -->\n<!-- \\ \\\\ -->\n<!-- & \\text{where:} \\\\  -->\n<!-- & \\bar x : \\text{sample mean} \\\\ -->\n<!-- & \\mu_0 : \\text{hypothesised population mean} \\\\ -->\n<!-- & s : \\text{sample standard deviation} \\\\ -->\n<!-- & n : \\text{sample size} \\\\ -->\n<!-- \\end{align} -->\n<!-- $$ -->\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __P-value:__  -->\n\n<!-- Our $p$-value will be computed from the $t$-distribution with $n-1$ degrees of freedom.   -->\n\n<!-- If the alternative hypothesis is $H_1: \\mu \\neq \\mu_0$, then we are asking about the probability of a test statistic at least as extreme _in either direction._   -->\n<!-- So we would use:   -->\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- # abs() takes the absolute value, ensuring we get the smaller tail -->\n<!-- 2 * pt(abs(tstatistic), df = n-1, lower.tail = FALSE) -->\n<!-- ``` -->\n\n<!-- If $H_1: \\mu > \\mu_0$, then this would be  -->\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- pt(tstatistic, df = n-1, lower.tail = FALSE) -->\n<!-- ``` -->\n<!-- And if $H_1: \\mu < \\mu_0$:   -->\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- pt(tstatistic, df = n-1, lower.tail = TRUE) -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __Assumptions:__  -->\n\n<!-- - The data are continuous (not discrete) -->\n<!-- - The data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way) -->\n<!-- - The data are normally distributed _OR_ the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed -->\n\n<!-- ::: -->\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Research Question & Data\n\n> **Research Question:**  Do people read more than 250 words per minute? \n\nFifty participants were recruited and tasked with reading a passage of text that was 2000 words long. Their reading times (in words per minute) was recorded, and these are accessible at [https://uoepsy.github.io/data/usmr_tread.csv](https://uoepsy.github.io/data/usmr_tread.csv).  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwpmtime <- read_csv(\"https://uoepsy.github.io/data/usmr_tread.csv\")\nhead(wpmtime)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  id      wpm\n  <chr> <dbl>\n1 ppt_1   307\n2 ppt_2   265\n3 ppt_3   205\n4 ppt_4   300\n5 ppt_5   207\n6 ppt_6   300\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Descriptives and Assumptions\n\nBelow are some quick descriptives.  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(wpmtime$wpm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 258.36\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(wpmtime$wpm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 32.08646\n```\n\n\n:::\n\n```{.r .cell-code}\nhist(wpmtime$wpm)\n```\n\n::: {.cell-output-display}\n![](03b_inference2_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nOur histogram looks roughly normally distributed. We can (if we like), test this using the Shapiro-Wilk test.  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshapiro.test(wpmtime$wpm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  wpmtime$wpm\nW = 0.9636, p-value = 0.1258\n```\n\n\n:::\n:::\n\n\n\n\nThe $p$-value of 0.126 is $>.05$, so we fail to reject the null hypothesis that the data come from a normal distribution. In other words, we have no reason to consider our assumption to be violated.  \n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Quick and easy `t.test()`\n\nPaying careful attention to the research question (\"Do people read more than 250 words per minute?\"), our null hypothesis here is that reading time is $\\leq 250$ words per minute (wpm), and our alternative hypothesis is that it is $>250$ wpm.  \n\nThis means that we will reject our null hypothesis if we get a test statistic indicating the mean is $>250$. We won't reject it if the mean is $<250$.  \n\nWe specify the direction of the alternative in the `t.test()` function:  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(wpmtime$wpm, mu = 250, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  wpmtime$wpm\nt = 1.8423, df = 49, p-value = 0.03574\nalternative hypothesis: true mean is greater than 250\n95 percent confidence interval:\n 250.7523      Inf\nsample estimates:\nmean of x \n   258.36 \n```\n\n\n:::\n:::\n\n\n\n\n:::\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nOur test-statistic is calculated as \n$$\nt =  \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n$$\n\nThere's a lot of brackets in the code below, so go through it piece by piece if you are unsure of how it matches to the formula above  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(mean(wpmtime$wpm) - 250 ) / (sd(wpmtime$wpm) / sqrt(nrow(wpmtime)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.842338\n```\n\n\n:::\n:::\n\n\n\n\nThe test we are performing is against the null hypothesis that $\\mu_0 \\leq 250$. So we will only reject the null hypothesis if we get a test statistic indicating the mean is $>250$. This means that our p-value will be just the one tail of the $t$-distribution:  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npt(1.842338, df = 49, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0357404\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Write-up\n\n:::int\nA one-sample t-test was conducted in order to determine if the average reading time was significantly ($\\alpha=.05$) higher than 250 words per minute (wpm).  \nThe sample of 50 participants read on average at 258 words per minute (Mean=258, SD=32). This was significantly above 250 ($t(49)=1.84, p = .036$, one-tailed).\n:::\n\n:::\n\n## Two sample t-test  \n\n:::statbox\n__Purpose__  \nThe two sample t-test is used to test whether the mean of one group is different from/greater than/less than the mean of another.  \n\n\n- __Examples:__\n  - Is the mean age of cat people different from the mean age of dog people? \n  - Do people who identify as \"morning people\" have a higher average rating of sleep quality than those who identify as \"evening people\"?\n  - Is the average reaction time different between people who do and don't drink caffeinated drinks?  \n\n__Assumptions:__  \n\n- The data are continuous (not discrete)\n- The data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)\n- The data are normally distributed _for each group_ (can be relaxed somewhat if the sample size is \"large enough\" (rule-of-thumb n = 30) and the data are not strongly skewed)\n- The variance is equal across groups*. \n\n*We can relax this assumption by using an adjusted test called the \"Welch $t$-test\", which calculates the standard error slightly differently, and estimates the degrees of freedom differently too. This is actually the default in R, and we change this easily in R using `t.test(...., var.equal = FALSE/TRUE)`  \n\n:::\n\n<!-- :::statbox -->\n<!-- __Hypotheses:__    -->\n\n<!-- Our hypotheses can be stated in terms of 2 population means, $\\mu_1$ and $\\mu_2$, representing the mean of each group.  -->\n<!-- The null hypothesis is typically that the difference between the two means is zero:   -->\n\n<!-- $H_0: \\mu_1 - \\mu_2 = 0$  -->\n\n<!-- And our alternative hypothesis again can take on the following forms:     -->\n\n<!-- $H_1: \\mu_1 - \\mu_2 \\neq 0$   -->\n<!-- $H_1: \\mu_1 - \\mu_2 > 0$   -->\n<!-- $H_1: \\mu_1 - \\mu_2 < 0$   -->\n\n\n<!-- ::: -->\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Research Question & Data\n\n> **Research Question:**  Is the average reaction time different between people who do and don't drink caffeinated drinks? \n\nOne hundred participants were recruited and completed a simple reaction time task. They were also surveyed on whether they regularly drank caffeine in any form. The data are accessible at [https://uoepsy.github.io/data/usmr_tcaff.csv](https://uoepsy.github.io/data/usmr_tcaff.csv).  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntcaff <- read_csv(\"https://uoepsy.github.io/data/usmr_tcaff.csv\")\nhead(tcaff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n     rt caff \n  <dbl> <chr>\n1  482. yes  \n2  389. yes  \n3  484. no   \n4  601. no   \n5  409. yes  \n6  368. no   \n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Descriptives and Assumptions\n\nFirst some quick descriptive stats. We'll calculate the mean and standard deviation of reaction times for each group:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntcaff |> \n  group_by(caff) |>\n  summarise(\n    m = mean(rt),\n    s = sd(rt)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  caff      m     s\n  <chr> <dbl> <dbl>\n1 no     408.  88.9\n2 yes    465. 109. \n```\n\n\n:::\n:::\n\n\n\nAnd we can make a plot here:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(tcaff, aes(x = rt)) +\n  geom_histogram() + \n  facet_wrap(~caff)\n```\n\n::: {.cell-output-display}\n![](03b_inference2_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nThe data look fairly close to normally distributed for each group here. One thing to note is that the variances look like they may be different between the two groups. The caffeine drinkers' reaction time's have a standard deviation of 109ms, and the non-caffeine drinkers have an sd of only 89ms. \n\nAs before, we can (if we are so inclined) rely on specific tests of these assumptions, such as using `shapiro.test()` for the distribution _in each group separately_.  \n\nSimilarly, the `var.test()` function performs a test to compare two variances (the null hypothesis of this test being that they are equal). However, it is more common to simply perform the Welch test straight away, and thus not have to worry about this assumption.  \n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Quick and easy `t.test()`\n\nWe can give R the two sets of data in two ways. \nEither by extracting the relevant entries:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(x = tcaff$rt[tcaff$caff==\"no\"], \n       y = tcaff$rt[tcaff$caff==\"yes\"])\n```\n:::\n\n\n\nOr using the **formula** notation, with the `~` (\"tilde\") symbol. In R, you can interpret `y ~ x` as \"y is modeled as a function of x\". By splitting the numeric values (`rt` variable) by the categories of the `caff` variable, we can conduct a $t$-test using:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(rt ~ caff, data = tcaff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rt by caff\nt = -2.8497, df = 93.971, p-value = 0.005377\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -96.20205 -17.19423\nsample estimates:\n mean in group no mean in group yes \n         408.0505          464.7486 \n```\n\n\n:::\n:::\n\n\n\n\nNote that the default behaviour of `t.test()` is to perform the Welch test - so we don't have to assume equal variances. If we want to override this, we can use `t.test(rt ~ caff, data = tcaff, var.equal = TRUE)`.  \n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nOur test statistic here is:^[The formula here is for the Welch test.  \nFor a standard two sample t-test that assumes equal variances, we first calculate the \"pooled standard deviation\" - $s_p = \\sqrt\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}$.  \nWe then use this to calculate the standard error - $SE_{(\\bar{x}_1 - \\bar{x}_2)} = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}$]  \n$$\n\\begin{align}\n& t =  \\frac{\\bar x_1 - \\bar x_2}{SE}\\\\\n\\ \\\\\n& \\text{where:} \\\\\n& \\bar x_1 : \\text{sample mean group 1} \\\\\n& \\bar x_2 : \\text{sample mean group 2} \\\\\n& SE : \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\\\\n& s_1 : \\text{sample standard deviation of group 1} \\\\\n& s_2 : \\text{sample standard deviation of group 2} \\\\\n& n_1 : \\text{sample size group 1} \\\\\n& n_2 : \\text{sample size group 2} \\\\\n\\end{align}\n$$\n\n\nWe can calculate each part:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntcaff |>\n  group_by(caff) |>\n  summarise(\n    xbar = mean(rt),\n    s = sd(rt),\n    s2 = var(rt),\n    n = n()\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  caff   xbar     s     s2     n\n  <chr> <dbl> <dbl>  <dbl> <int>\n1 no     408.  88.9  7906.    40\n2 yes    465. 109.  11892.    60\n```\n\n\n:::\n:::\n\n\n\nplugging these bits in gives us: \n$$\n\\begin{align}\nSE & = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} = \\sqrt{\\frac{7906}{40} + \\frac{11892}{60}} = \\sqrt{395.85} \\\\\n\\qquad \\\\\n& = 19.9\n\\end{align}\n$$\nand\n$$\n\\begin{align}\nt & =  \\frac{\\bar x_1 - \\bar x_2}{SE} = \\frac{408.1 - 464.8}{19.9} \\\\\n\\qquad \\\\\n& = -2.849 \\\\\n\\end{align}\n$$\n\nOur $p$-value is determined against a $t$-distribution with a specific number of degrees of freedom. We are estimating two means here, the standard two-sample t-test uses $df = n-2$. However, the Welch t-test, which we performed quickly with `t.test()`, where we don't assume equal variances, makes the calculation of the degrees of freedom much more complicated.^[If you _really_ want it, the formula is: $\\text{df}=\\frac{\\left(\\dfrac{s_1^2}{n_1}+\\dfrac{s_2^2}{n_2}\\right)^2}{\\dfrac{\\left(\\dfrac{s_1^2}{n_1}\\right)^2}{n_1-1}+\\dfrac{\\left(\\dfrac{s_2^2}{n_2}\\right)^2}{n_2-1}}$\n] \n\nUsing the same degrees of freedom as was used in the quick use of `t.test()` above, we get out our same p-value (or thereabouts - we have some rounding error):  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n2*pt(abs(-2.849), df = 93.971, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.005388563\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Write-up  \n\n\n\n\n\n\n\n\n\n:::int\n\nA Welch two sample t-test was used to assess whether the mean reaction time of people who regularly drink caffeine ($n = 60$) was different to that of people who do not ($n=40$). There was a significant difference in average reaction time between the caffeine (Mean=465; SD=109) and\nnon-caffeine (Mean=408; SD=89) groups ($t(93.97)=-2.85, p = 0.005$, two-tailed). Therefore, we reject the null hypothesis that there is no difference in reaction times between caffeine drinkers and non-caffeine drinkers.   \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(tcaff, aes(x = caff, y = rt)) +\n  geom_boxplot()+\n  labs(x=\"drinks caffeine\",y=\"reaction time (ms)\")\n```\n\n::: {.cell-output-display}\n![](03b_inference2_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n:::\n\n:::\n\n\n## Paired sample t-test  \n\n:::statbox\n__Purpose__  \n\nThe paired sample t-test is used to test whether the mean difference between two sets of _paired_ observations is different from 0. \n\n- __Examples:__\n  - Is the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?  \n  - Are scores on test 1 different on average from scores on test 2 (with participants completing both tests).\n  \n__Assumptions:__\n\n- The data are continuous (not discrete)\n- The _differences_ are independent (i.e. the value of a the difference for one pair does not depend on the values of another pair in any way)\n- The _differences_ are normally distributed _OR_ the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed\n  \n:::\n\n::: {.callout-note collapse=\"true\"}\n### Research Question & Data\n\n> **Research Question:** Is the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?  \n\nAddenbrooke’s Cognitive Examination-III (ACE-III) is a brief cognitive test that assesses five cognitive domains: attention, memory, verbal fluency, language and visuospatial abilities. The total score is 100 with higher scores indicating better cognitive functioning. A research project is examining changes in cognitive functioning with age, and administers the ACE-III to a set of participants at age 60, then again at age 70. The data is accessible at [https://uoepsy.github.io/data/usmr_tcaff.csv](https://uoepsy.github.io/data/usmr_tcaff.csv).  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nacedata <- read_csv(\"https://uoepsy.github.io/data/acedata.csv\")\nhead(acedata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  participant ace_60 ace_70\n  <chr>        <dbl>  <dbl>\n1 sub1            93     85\n2 sub2            95     92\n3 sub3            93     90\n4 sub4            93     95\n5 sub5            96     88\n6 sub6            91     85\n```\n\n\n:::\n:::\n\n\n\n\n:::\n::: {.callout-note collapse=\"true\"}\n### The paired t test is the one sample t test in disguise\n\nWe can either perform this with the data exactly as it is: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(x = acedata$ace_60, y = acedata$ace_70, \n       paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  acedata$ace_60 and acedata$ace_70\nt = 2.2542, df = 24, p-value = 0.03359\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.2093364 4.7506636\nsample estimates:\nmean difference \n           2.48 \n```\n\n\n:::\n:::\n\n\n\n\nOr we can compute the differences, and perform a one sample test on the mean of those differences being different from 0.  \nIt's just the same result:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nacedata <- acedata |>\n  mutate(diff_score = ace_60 - ace_70)\n\nt.test(acedata$diff_score, mu = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  acedata$diff_score\nt = 2.2542, df = 24, p-value = 0.03359\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.2093364 4.7506636\nsample estimates:\nmean of x \n     2.48 \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n",
    "supporting": [
      "03b_inference2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.3.0/panelset.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/panelset-0.3.0/panelset.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}