{
  "hash": "43755711c97e7f10fda7748b7194ff8e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"4A: Binomial & Chi-Square Tests\"\nparams: \n    SHOW_SOLS: TRUE\n    TOGGLE: TRUE\n---\n\n\n\n\n\n\n:::lo\nThis reading:  \n\n- What are the basic hypothesis tests that we can conduct when we are interested in variables that have categories instead of numbers?  \n  - Tests of a proportion\n  - Tests of the distribution of a single categorical variable\n  - Tests of the relationship between two categorical variables\n\n\n:::\n\n\n\n\n\n\n\n\n\nJust like we did with the various types of $t$-test, we're going to continue with some more brief explainers of different basic statistical tests. The past few weeks have focused on tests for __numeric__ outcome variables, where we have been concerned with the mean of that variable (e.g. whether that mean is different from some specific value, or whether it is different between two groups). \nWe now turn to investigate tests for __categorical__ outcome variables. \n\nWhen studying categorical variables, we tend to be interested in __counts__ (or \"frequencies\"), and these can be presented in tables:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntimehands <- read_csv(\"https://uoepsy.github.io/data/timehands.csv\") |>\n  mutate(\n    isLeft = ifelse(handed==\"left\", \"left\", \"other\")\n  )\n\ntable(timehands$isLeft) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n left other \n   12    88 \n```\n\n\n:::\n:::\n\n\n\n\n# Binomials\n\nIf we just have two categories, then our frequency table really can be summarised as a proportion. E.g., \"the proportion who are left-handed\". We know that the other number (the proportion who are *not* left-handed) *has* to be 1-the proportion who _are_:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(timehands$isLeft) |>\n  prop.table() # turn into proportions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n left other \n 0.12  0.88 \n```\n\n\n:::\n:::\n\n\n\n\nReally, these proportions are \"the number of $x$'s out of $n$ things\", and we often see this written as \"$k$ successes in $n$ trials\". The reason for this is because it is built upon counting up a series of independent binary observations. The distribution of \"$k$ successes in $n$ trials\" is known as the **Binomial distribution**. \n\nAs an example, if we think of what probability distribution governs a single coin flip, it would look something like the Left hand plot in @fig-bernbin. But if we are instead considering what governs the behaviour of \"number of tails in 5 coin flips\" then our probabilities of each possible outcome look like the Right hand panel.   \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Left: A 'Bernoulli distribution' - the probability for a single observation (e.g. a single coin flip) with a binary outcome, where the probability of 'success' (i.e. Tails) is 0.5. Right: A 'Binomial distribution' - the probability of k success in n trials, each with a binary outcome (e.g., number of tails in 5 coin flips).](04a_chisq_files/figure-html/fig-bernbin-1.png){#fig-bernbin fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: Bernoulli and Binomial\n\nThe distribution of something that can just take one of two discrete values (i.e. a single binary outcome like one coin flip) is referred to as a \"Bernoulli\" probability distribution. A \"distribution\" here is just a way of expressing the probability of seeing different values, and we can write it mathematically if we are that way inclined:  \n\n$$\nP(k) = p^k(1-p)^{1-k} \\qquad \\textrm{for } k \\in \\{0,1\\}\n$$\nThis looks a bit nonsensical at first, but it's actually just a complicated way of saying \"the probability of seeing $k = 1$ is the probability that $k = 1$\".  \nFor instance, if we had a coin-flip, and we said that Tails is our \"success\" (and we give it the value 1) then the probability of seeing Heads and of seeing Tails is: \n$$\n\\begin{align}\n\\textrm{Probability of failure (Heads)} = P(0) &= 0.5^0(1-0.5)^{1-0} \\\\\n&= 0.5^0(0.5)^{1} \\\\\n&= 0.5 \\\\\n\\textrm{Probability of success (Tails)} = P(1) &= 0.5^1(1-0.5)^{1-1} \\\\\n&= 0.5^1(0.5)^{0} \\\\\n&= 0.5\n\\end{align}\n$$\nWhen we move to thinking about the number of success in a set of trials, there we have to factor in the different ways in which we could see that number of success. For instance, if we had 3 coin flips, we could either see 0 Tails, 1 Tails, 2 Tails, or 3 Tails. There is only _one_ way we could see \"0 Tails\", but there are 3 different ways we could see \"1 Tails\" - we could have THH, HTH, or HHT!  \n\nThe probability of $k$ successes in $n$ trials is the Binomial distribution, and is written as:\n \n$$\nP(k) = {n \\choose k}p^k(1-p)^{n-k}\n$$\nWhich can be read as: the probability of seeing $k$ successes is \"the number of ways in which we could see k successes in n trials\" (this is the ${n \\choose k}$ bit) multiplied by the probability of success in a single trial to the power of $k$, multiplied by the probability of failure in a single trial to the power of $n-k$.  \n\nTo make more sense, let's put it in the context of our \"1 Tails in 3 coin flips\". The coin is fair, so the probability of a single Tails is 0.5 (so below we replace $p$ with 0.5), and we saw just above that there are 3 ways that we can get 1 success in 3 trials (so the calculation ${3 \\choose 1}$ becomes 3)  \n$$\n\\begin{align}\n\\text{Probabiltiy of 1 Tails in 3 flips} = P(1) &= {3 \\choose 1}0.5^1(1-0.5)^{3-1} \\\\\n&= {3 \\choose 1}0.5^1(0.5)^{2} \\\\\n&= {3 \\choose 1}0.5 \\times 0.25 \\\\\n&= {3 \\choose 1} 0.125 \\\\\n&= 3 \\times 0.125 \\\\\n&= 0.375 \\\\\n\\end{align}\n$$\n\nWe can actually check this in R using a handy function: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndbinom(x = 1, size = 3, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.375\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n\nReturning to \"the proportion of people who are left-handed\", the internet (and everything on there is true, of course) tells me that 10% of the population is left-handed. As we have 100 people in our dataset, our expectation of what we are likely to see from samples of 100 people (if the probability of being left-handed is 0.10) therefore looks something like @fig-exp100.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Expected distribution of 'number of left-handed people out of 100', assuming the true probability of being left-handed is 0.1](04a_chisq_files/figure-html/fig-exp100-1.png){#fig-exp100 fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nMuch like the null hypothesis testing we have seen already concerning whether our observed mean is different from some hypothesised value (see [3B #one-sample-t-test](03b_inference2.html#one-sample-t-test){target=\"_blank\"}), we can conduct a test of whether our observed sample proportion (12/100 = 0.12) is likely to have been drawn from a population where the true probability is 0.1:  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(timehands$isLeft)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n left other \n   12    88 \n```\n\n\n:::\n:::\n\n\n\n\nEither of these will work, and both will produce the same output:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbinom.test(x = 12, n = 100, p = 0.1)\ntable(timehands$isLeft) |> binom.test(p = 0.1)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact binomial test\n\ndata:  12 and 100\nnumber of successes = 12, number of trials = 100, p-value = 0.503\nalternative hypothesis: true probability of success is not equal to 0.1\n95 percent confidence interval:\n 0.0635689 0.2002357\nsample estimates:\nprobability of success \n                  0.12 \n```\n\n\n:::\n:::\n\n\n\n\nIn this case, the p-value is >.05^[the conventional $\\alpha$ level used in psychology], so we fail to reject the null hypothesis that the probability of being left-handed is 0.1.  \n\n# More than 2 categories\n\nWhile the binomials capture the distribution of a series of binary trials, what happens when we are interested in something that _isn't_ binary? There are lots of categorical variables that have more than two possible values. \nFor example, handedness isn't as simple as \"Left vs Others\", we might have 3 categories: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(timehands$handed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n ambi  left right \n    4    12    84 \n```\n\n\n:::\n:::\n\n\n\n\nFurthermore, we might have a slightly more nuanced question that involves two variables, rather than just one! For instance, we might be interested in whether left/right/ambidextrous proportions are different across people who prefer the morning or night.  \nIn this case, with 2 variables, we have a 2-dimensional table (also referred to as a 'contingency table'):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(timehands$handed, timehands$ampm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        morning night\n  ambi        1     3\n  left        9     3\n  right      42    42\n```\n\n\n:::\n:::\n\n\n\n\nSo we want to be able to perform tests to examine things such as:  \n\nb) how likely we are to see our sample frequencies in a single categorical variable, if some some hypothesised null distribution were true (e.g. how likely are we to see the numbers of left/right/ambi people in our sample if in bigger population we expect a 1/3 chance of each?)  \nc) how likely we are to see our sample frequencies across two categorical variables, if these variables are _independent_ in the population. (e.g. how likely are we to see the counts in the 2x2 table above, if being left/right/ambi has _nothing to do with_ whether you are a morning or night person).  \n\nThe test-statistics for these sort of tests (denoted $\\chi^2$, spelled *chi-square*, pronounced \"kai-square\") are obtained by adding up the standardized squared deviations in each cell of a table of frequencies:  \n\n$$\n\\chi^2 = \\sum_{all\\ cells} \\frac{(\\text{Observed} - \\text{Expected})^2}{\\text{Expected}}\n$$\nwhere:\n\n- $\\text{Observed}$ = observed count for a cell\n- $\\text{Expected}$ = expected count for a cell\n\n\nJust like the $t$-statistics we calculate follow $t$-distributions, the $\\chi^2$-statistics follow $\\chi^2$ distributions! If you look carefully at the formula above, it can never be negative (because the value on top of the fraction is squared, and so is always positive). \nFor a given cell of the table, if we observe exactly what we expect, then $\\text{(Observed - Expected)}^2$ becomes zero. The further away the observed count is from the expected count, the larger it becomes.  \n\nThis means that under the null hypothesis, larger values are less likely. And the shape of our $\\chi^2$ distributions follow this logic. They have higher probability for small values, getting progressively less likely for large values. $\\chi^2$-distributions _also_ have a degrees of freedom, because with more cells in a table, there are more chances for random deviations between \"observed and expected\" to come in, meaning we are more likely to see higher test statistics when we have more cells in the table (and therefore more degrees of freedom). You can see the distribution of $\\chi^2$ statistics with different degrees of freedom in @fig-chidist1 below.  \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Chi-Square Distributions](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/1200px-Chi-square_pdf.svg.png){#fig-chidist1 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# $\\chi^2$ Goodness of Fit Test\n\n\n:::statbox\n\n__Purpose__\n\nThe $\\chi^2$ Goodness of Fit Test is typically used to investigate whether observed sample proportions are consistent with an hypothesis about the proportional breakdown of the various categories in the population.  \n\n- __Examples:__\n  - Do 20% of the adult population suffer from some form of depression? \n  - Are people equally likely to be born on any of the seven days of the week?  \n  - Are 25% of [Smarties](https://g.co/kgs/wr3MCd){target=\"_blank\"} brown?  \n  - Are 2/3 of people 'dog people' and 1/3 of people 'cat people'?  \n  \n__Assumptions__\n\n1. Data should be randomly sampled from the population. \n2. Data should be at the categorical or nominal level - goodness-of-fit test is not appropriate for continuous level data. \n3. Expected counts should be *at least* 5. \n\n:::\n\n<!-- :::statbox -->\n<!-- __Hypotheses:__ -->\n\n<!-- The null hypothesis for a $\\chi^2$ Goodness of Fit Test involves specifying the hypothesised proportions for each category.  If the null hypothesis is that all categories are equally distributed, then this is just saying that all proportions are equal to $\\frac{1}{\\text{number of categories}}$.   -->\n<!-- The alternative hypothesis is simply that at least one of those proportions is not equal to its hypothesised value.   -->\n\n<!-- - __Null Hypothesis ($H_0$):__   -->\n<!-- $$ -->\n<!-- p_{1} = \\ ?, \\ p_{2} = \\ ?, \\ ... \\ , \\ p_{k} = \\ ?  -->\n<!-- $$ -->\n<!-- - __Alternative Hypothesis ($H_1$):__   -->\n<!-- $$ -->\n<!-- p_{1} \\neq \\ ? \\ \\text{or} \\ p_{2} \\neq \\ ? \\ \\text{or} \\ \\ ... \\ \\ \\text{or} \\ p_{k} \\neq \\ ?  -->\n<!-- $$ -->\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __Test Statistic__   -->\n\n<!-- The test-statistic (denoted $\\chi^2$, spelled *chi-square*, pronounced \"kai-square\") is obtained by adding up the standardized squared deviations in each category:   -->\n\n<!-- $$ -->\n<!-- \\chi^2 = \\sum_{i} \\frac{(\\text{Observed}_i - \\text{Expected}_i)^2}{\\text{Expected}_i} -->\n<!-- $$ -->\n<!-- where: -->\n\n<!-- - $\\text{Observed}_i$ = observed counts for category $i$ -->\n<!-- - $\\text{Expected}_i$ = expected counts for category $i$ -->\n<!--   - calculated as $\\text{Expected}_i = n \\times p_{i,null}$, where $n$ = sample size, $p_{i,null}$ is the hypothesised population proportion for that category under the null hypothesis.   -->\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __p-value__ -->\n\n<!-- The p-value for a $\\chi^2$ Goodness of Fit Test is computed using a $\\chi^2$ distribution with $df = \\text{nr categories} - 1$.  -->\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __Assumptions__ -->\n\n<!-- 1. Data should be randomly sampled from the population.  -->\n<!-- 2. Data should be at the categorical or nominal level - goodness-of-fit test is not appropriate for continuous level data.  -->\n<!-- 3. Expected counts should be *at least* 5.  -->\n\n<!-- ::: -->\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Research Question & Data\n\n> **Research Question:** Have proportions of adults suffering no/mild/moderate/severe depression changed from 2019?  \n\nIn 2019, it was reported that 80% of  adults (18+) experienced no symptoms of depression, 12% experienced mild symptoms, 4% experienced moderate symptoms, and 4% experienced severe symptoms.  \nThe dataset is accessible at [https://uoepsy.github.io/data/usmr_chisqdep.csv](https://uoepsy.github.io/data/usmr_chisqdep.csv) contains data from 1000 people to whom the PHQ-9 depression scale was administered in 2022.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndepdata <- read_csv(\"https://uoepsy.github.io/data/usmr_chisqdep.csv\") \nhead(depdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  id    dep    fam_hist\n  <chr> <chr>  <chr>   \n1 ID1   severe n       \n2 ID2   mild   n       \n3 ID3   no     n       \n4 ID4   no     n       \n5 ID5   no     n       \n6 ID6   no     n       \n```\n\n\n:::\n:::\n\n\n\nWe can see our table of observed counts with the `table()` function:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(depdata$dep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    mild moderate       no   severe \n     143       34      771       52 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Quick and easy `chisq.test()`\n\nWe can perform the $\\chi^2$ test very easily, by simply passing the table to the `chisq.test()` function, and passing it the hypothesised proportions. If we don't give it any, it will assume they are equal. \n\n:::imp\n__Note:__ the proportions must be in the correct order as the entries in the table!  \n:::\n\nThis will give us the test statistic, degrees of freedom, and the p-value:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# note the order of the table is mild, moderate, no, severe. \n# so we put the proportions in that order\nchisq.test(table(depdata$dep), p = c(.12, .04, .8, .04))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  table(depdata$dep)\nX-squared = 9.9596, df = 3, p-value = 0.01891\n```\n\n\n:::\n:::\n\n\n\nIf the distribution of no/mild/moderate/severe depression were as suggested (80%/12%/4%/4%), then the probability that we would obtain a test statistic this large (or larger) by random chance alone is .019. With an $\\alpha = 0.05$, we reject the null hypothesis that the proportion of people suffering from different levels of depression are the same as those indicated previously in 2019.\n\n:::int\n$\\chi^2$ goodness of fit test indicated that the observed proportions of people suffering from no/mild/moderate/severe depression were significantly different ($\\chi^2(3)=9.96, p = .019$) from those expected under the distribution suggested from a 2019 study (80%/12%/4%/4%).\n:::\n\nWe can examine where the biggest deviations from the hypothesised distribution are by examining the 'residuals': \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchisq.test(table(depdata$dep), p = c(.12, .04, .8, .04))$residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n      mild   moderate         no     severe \n 2.0996031 -0.9486833 -1.0253048  1.8973666 \n```\n\n\n:::\n:::\n\n\n\nThis matches with what we see when we look at the table of counts. With $n=1000$, under our 2019 distribution, we would expect 800 to have no depression, 120 mild, 40 moderate, and 40 severe.   \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(depdata$dep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    mild moderate       no   severe \n     143       34      771       52 \n```\n\n\n:::\n:::\n\n\n\n\nThe difference in the moderate \"observed - expected\" is 6, and the difference in the \"no depression\" is 29. But these are not comparable, because really the 6 is a much bigger amount of the expected for that category than 29 is for the no depression category. The residuals are a way of standardising these.  \n\nThey are calculated as:\n$$\n\\text{residual} = \\frac{\\text{observed} - \\text{expected}}{\\sqrt{expected}}\n$$\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nFirst we calculate the observed counts: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndepdata |> \n  count(dep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  dep          n\n  <chr>    <int>\n1 mild       143\n2 moderate    34\n3 no         771\n4 severe      52\n```\n\n\n:::\n:::\n\n\n\nLet's add to this the expected counts:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndepdata |> \n  count(dep) |>\n  mutate(\n    expected = c(.12, .04, .8, .04)*1000\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  dep          n expected\n  <chr>    <int>    <dbl>\n1 mild       143      120\n2 moderate    34       40\n3 no         771      800\n4 severe      52       40\n```\n\n\n:::\n:::\n\n\n\n\nHow do we measure how far the observed counts are from the expected counts under the null? If we simply subtracted the expected counts from the observed counts and then add them up, you will get 0. Instead, we will square the differences between the observed and expected counts, and then add them up.\n\nOne issue, however, remains to be solved. A squared difference between observed and expected counts of 100 has a different weight in these two scenarios:\n\n__Scenario 1:__ $O = 30$ and $E = 20$ leads to a squared difference $(O - E)^2 = 10^2 = 100$.  \n__Scenario 2:__ $O = 3000$ and $E = 2990$ leads to a squared difference $(O - E)^2 = 10^2 = 100$\n\nHowever, it is clear that a squared difference of 100 in Scenario 1 is much more substantial than a squared difference of 100 in Scenario 2. It is for this reason that we divide the squared differences by the the expected counts to \"standardize\" the squared deviation.\n\n$$\n\\chi^2 = \\sum_{i} \\frac{(\\text{Observed}_i - \\text{Expected}_i)^2}{\\text{Expected}_i}\n$$\n\nWe can calculate each part of the equation:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndepdata |> \n  count(dep) |>\n  mutate(\n    expected = c(.12, .04, .8, .04)*1000,\n    sq_diff = (n - expected)^2,\n    std_sq_diff = sq_diff/expected\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  dep          n expected sq_diff std_sq_diff\n  <chr>    <int>    <dbl>   <dbl>       <dbl>\n1 mild       143      120     529        4.41\n2 moderate    34       40      36        0.9 \n3 no         771      800     841        1.05\n4 severe      52       40     144        3.6 \n```\n\n\n:::\n:::\n\n\n\n\nThe test-statistic $\\chi^2$ is obtained by adding up all the standardized squared deviations:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndepdata |> \n  count(dep) |>\n  mutate(\n    expected = c(.12, .04, .8, .04)*1000,\n    sq_diff = (n - expected)^2,\n    std_sq_diff = sq_diff/expected\n  ) |> \n  summarise(\n    chi = sum(std_sq_diff)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n    chi\n  <dbl>\n1  9.96\n```\n\n\n:::\n:::\n\n\n\n\nThe p-value for a $\\chi^2$ Goodness of Fit Test is computed using a $\\chi^2$ distribution with $df = \\text{nr categories} - 1$.  \nWe calculate our p-value by using `pchisq()` and we have 4 levels of depression, so $df = 4-1 = 3$.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npchisq(9.959583, df=3, lower.tail=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01891284\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# $\\chi^2$ Test of Independence  \n\n:::statbox\n__Purpose__\n\nThe $\\chi^2$ Test of Independence is used to determine whether or not there is a significant association between two categorical variables. To examine the independence of two categorical variables, we have a contingency table:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Family History of Depression\nDepression Severity   n   y\n           mild      93  50\n           moderate  23  11\n           no       532 239\n           severe    37  15\n```\n\n\n:::\n:::\n\n\n\n\n- __Examples:__\n  - Is depression severity associated with having a family history of depression?  \n  - Are people with blue eyes more likely to be over 6 foot tall?  \n  - Are people who carry the APOE-4 gene more likely to have mild cognitive impairment?  \n\n__Assumptions__\n\n1. Two or more categories (groups) for each variable. \n2. Independence of observations \n    + there is no relationship between the subjects in each group \n3. Large enough sample size, such that:\n    + expected frequencies for each cell are at least 1 \n    + expected frequencies should be at least 5 for the majority (80%) of cells \n    \n\n:::\n\n<!-- :::statbox -->\n<!-- __Hypotheses:__ -->\n\n<!-- The hypotheses for the $\\chi^2$ Test of Independence follow the same logic as the $\\chi^2$ goodness of fit test. We have some hypothesised breakdown of proportions that we would _expect_.  However, it's less easy to compute these expected breakdowns and so we typically state the hypotheses as:   -->\n\n<!-- - __Null Hypothesis ($H_0:$)__ Variable A is not associated with variable B   -->\n<!-- - __Alternative Hypothesis ($H_1:$)__ Variable A is associated with variable B -->\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __Test Statistic__   -->\n\n<!-- Our test statistic looks much the same as before, but extends to all cells of the contingency table:   -->\n\n<!-- $$ -->\n<!-- \\chi^2 = \\sum_{all\\ cells} \\frac{(\\text{Observed} - \\text{Expected})^2}{\\text{Expected}} -->\n<!-- $$ -->\n<!-- where: -->\n\n<!-- - $Observed$ = observed count for a cell -->\n<!-- - $Expected$ = expected count for a cell -->\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __p-value__ -->\n\n<!-- The p-value is computed using a $\\chi^2$ distribution with $df = (\\text{nr rows} - 1) \\times (\\text{nr columns} - 1)$.  -->\n<!-- Why is this? Well, remember that the degrees of freedom is the number of values that are *free to vary* as we estimate parameters. In a table such as the one below, where we have 4 rows and 2 columns, the degrees of freedom is the number of cells in the table that can vary before we can simply calculate the values of the other cells (where we're constrained by the need to sum to our row/column totals).    -->\n\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- table(depdata$dep,depdata$fam_hist,  dnn=c(\"Depression Severity\",\"Family History of Depression\")) -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n<!-- :::statbox -->\n<!-- __Assumptions__ -->\n\n<!-- 1. Two or more categories (groups) for each variable.  -->\n<!-- 2. Independence of observations  -->\n<!--     + there is not relationship between the subjects in each group  -->\n<!-- 3. Large enough sample size, such that: -->\n<!--     + expected frequencies for each cell are at least 1  -->\n<!--     + expected frequencies should be at least 5 for the majority (80%) of cells  -->\n\n<!-- ::: -->\n\n::: {.callout-note collapse=\"true\"}\n### Research Question & Data\n\n> **Research Question:** Is severity of depression associated with having a family history of depression?  \n\nThe dataset accessible at [https://uoepsy.github.io/data/usmr_chisqdep.csv](https://uoepsy.github.io/data/usmr_chisqdep.csv) contains data from 1000 people to whom the PHQ-9 depression scale was administered in 2022, and for which respondents were asked a brief family history questionnaire to establish whether they had a family history of depression.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndepdata <- read_csv(\"https://uoepsy.github.io/data/usmr_chisqdep.csv\")\nhead(depdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  id    dep    fam_hist\n  <chr> <chr>  <chr>   \n1 ID1   severe n       \n2 ID2   mild   n       \n3 ID3   no     n       \n4 ID4   no     n       \n5 ID5   no     n       \n6 ID6   no     n       \n```\n\n\n:::\n:::\n\n\n\nWe can create our contingency table: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(depdata$dep, depdata$fam_hist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          \n             n   y\n  mild      93  50\n  moderate  23  11\n  no       532 239\n  severe    37  15\n```\n\n\n:::\n:::\n\n\n\nAnd even create a quick and dirty visualisation of this too: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(table(depdata$dep, depdata$fam_hist))\n```\n\n::: {.cell-output-display}\n![](04a_chisq_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Quick and easy `chisq.test()`\n\nAgain, we can perform this test very easily by passing the table to the `chisq.test()` function. We don't need to give it any hypothesised proportions here - it will work them out based on the null hypothesis that the two variables are independent.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchisq.test(table(depdata$dep, depdata$fam_hist))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(depdata$dep, depdata$fam_hist)\nX-squared = 1.0667, df = 3, p-value = 0.7851\n```\n\n\n:::\n:::\n\n\n\nIf there was no association between depression severity and having a family history of depression, then the probability that we would obtain a test statistic this large (or larger) by random chance alone is 0.79. With an $\\alpha=.05$, we fail to reject the null hypothesis that there is no association between depression severity and family history of depression.  \n\n\n:::int\nA $\\chi^2$ test of independence indicated no significant association between severity and family history ($\\chi^2(3)=1.07, p=.785$), suggesting that a participants' severity of depression was not dependent on whether or not they had a family history of depression.  \n:::\n\nWe can see the expected and observed counts:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchisq.test(table(depdata$dep, depdata$fam_hist))$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          \n                 n       y\n  mild      97.955  45.045\n  moderate  23.290  10.710\n  no       528.135 242.865\n  severe    35.620  16.380\n```\n\n\n:::\n\n```{.r .cell-code}\nchisq.test(table(depdata$dep, depdata$fam_hist))$observed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          \n             n   y\n  mild      93  50\n  moderate  23  11\n  no       532 239\n  severe    37  15\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Step-by-step calculations\n\nWe have our observed table:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(depdata$dep, depdata$fam_hist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          \n             n   y\n  mild      93  50\n  moderate  23  11\n  no       532 239\n  severe    37  15\n```\n\n\n:::\n:::\n\n\n\nTo work out our _expected_ counts, we have to do something a bit tricky. \nLet's look at the variables independently:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(depdata$fam_hist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  n   y \n685 315 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(depdata$dep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    mild moderate       no   severe \n     143       34      771       52 \n```\n\n\n:::\n:::\n\n\n\nWith $\\frac{315}{315+685} = 0.315$ of the sample having a family history, then _if depression severity is independent of family history,_ we would expect that 0.315 of each severity group to have a family history of depression. For example, for the mild depression, with 143 people, we would expect $143 \\times 0.315 = 45.045$ people in that group to have a family history of depression.  \n\nFor a given cell of the table we can calculate the expected count as $\\text{row total} \\times \\frac{\\text{column total}}{\\text{samplesize}}$.  \nOr, quickly in R:  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nobs <- table(depdata$dep, depdata$fam_hist)\nexp <- rowSums(obs) %o% colSums(obs) / sum(obs)\nexp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               n       y\nmild      97.955  45.045\nmoderate  23.290  10.710\nno       528.135 242.865\nsevere    35.620  16.380\n```\n\n\n:::\n:::\n\n\n\nNow that we have our table of observed counts, and our table of expected counts, we can actually fit these into our formula to calculate the test statistic: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum ( (obs - exp)^2 / exp )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.066686\n```\n\n\n:::\n:::\n\n\n\nThe p-value is computed using a $\\chi^2$ distribution with $df = (\\text{nr rows} - 1) \\times (\\text{nr columns} - 1)$.  \n\nWhy is this? Well, remember that the degrees of freedom is the number of values that are *free to vary* as we estimate parameters. In a table such as the one below, where we have 4 rows and 2 columns, the degrees of freedom is the number of cells in the table that can vary before we can simply calculate the values of the other cells (where we're constrained by the need to sum to our row/column totals).  \n\nWe have 4 rows, and 2 columns, so $df = (4-1) \\times (2-1) = 3$.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npchisq(1.066686, df = 3, lower.tail=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7851217\n```\n\n\n:::\n:::\n\n\n\n\n:::",
    "supporting": [
      "04a_chisq_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}