{"entries":[{"order":{"section":[1,0,0,0,0,0,0],"number":1},"key":"fig-simplelm","caption":"Simple linear regression model. one continuous predictor."},{"order":{"section":[4,0,0,0,0,0,0],"number":8},"key":"fig-plotprobab","caption":"As x increases, probability of y gets closer and closer (but never meets) 1"},{"order":{"section":[4,0,0,0,0,0,0],"number":10},"key":"fig-clcnt","caption":"counts and log(counts)"},{"order":{"section":[1,0,0,0,0,0,0],"number":2},"key":"fig-multiplelm","caption":"multiple linear regression model with two continuous predictors."},{"order":{"section":[3,0,0,0,0,0,0],"number":7},"key":"fig-outcomeplots","caption":"Three different outcomes: continuous (top), binary (middle), count (bottom)"},{"order":{"section":[1,0,0,0,0,0,0],"number":5},"key":"fig-catintglm","caption":"Continuous*Binary interaction"},{"order":{"section":[12,0,0,0,0,0,0],"number":16},"key":"fig-summarylman","caption":"lm summary, annotated. click the image to zoom in, click the image to zoom out"},{"order":{"section":[5,0,0,0,0,0,0],"number":11},"key":"fig-resslineplot","caption":"linear regression minimises the sum of squared residuals"},{"order":{"section":[5,0,0,0,0,0,0],"number":15},"key":"fig-mleprob2","caption":"Note - the dotted lines are not residuals, but just to show the projections of observations down to the line"},{"order":{"section":[1,0,0,0,0,0,0],"number":3},"key":"fig-intlm","caption":"multiple linear regression model with two continuous predictors that interact"},{"order":{"section":[1,0,0,0,0,0,0],"number":4},"key":"fig-catlm","caption":"A categorical predictor in a regression model in R will by default set one level as a reference (e.g. Group1 here), and compare each level to that reference"},{"order":{"section":[2,0,0,0,0,0,0],"number":6},"key":"fig-mmsezoom","caption":"Our lm(mmse ~ air_pol * apoe4) model is a continuous line extending infinitely"},{"order":{"section":[5,0,0,0,0,0,0],"number":14},"key":"fig-mleprob","caption":"Note - the dotted lines are not residuals, but just to show the projections of observations down to the line"},{"order":{"section":[4,0,0,0,0,0,0],"number":9},"key":"fig-plodd","caption":"Probability, Odds, and Log-Odds"},{"order":{"section":[12,0,0,0,0,0,0],"number":17},"key":"fig-summaryglman","caption":"glm summary, annotated. click the image to zoom in, click the image to zoom out"},{"order":{"section":[5,0,0,0,0,0,0],"number":12},"key":"fig-loglineplot","caption":"Modelling the probability of Y across values of X requires fitting a squiggle"},{"order":{"section":[5,0,0,0,0,0,0],"number":13},"key":"fig-mlelogodd","caption":"In the log-odds world, comparing our predicted values to observed values of -Inf and Inf isn’t very useful, because all those distances will be infinite (there is no way of measuring the dotted lines, so we can’t really use them)"}],"headings":["quick-recaps","continuous-outcomes","other-outcomes","introducing-the-glm","fitting-glm-in-r","marshmallows","comparing-models","coefficient-interpretation","coefficient-tests-intervals","visualising","assumptions","summary.lm-and-summary.glm"]}