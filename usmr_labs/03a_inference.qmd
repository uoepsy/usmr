---
title: "3A: Foundations of Inference"
link-citations: true
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
```

:::lo
This reading:  

- How do we quantify uncertainty due to sampling?  
- How can we make decisions (what to believe/how to act, etc) that take uncertainty into account?  
- How likely are we to make the wrong decision?  

:::

We use statistics primarily to estimate parameters in a population. Whether we are polling people to make predictions about the proportion of people who will vote for a certain party in the next election, or conducting a medical trial and assessing the change in blood pressure for patients given drug X vs those given a placebo in order to decide whether to put the drug into circulation in health service. 

We have seen this already last week:
We _observed_ a sample of peoples' life satisfaction ratings (scale 0-100), and we wanted to use these to make some statement about the wider population, such as "the average life-satisfaction rating is ?? out of 100". So we use the mean of our sample, as an _estimate_ of the mean of the population.     

# Uncertainty due to sampling

A sample estimate is not going to be spot-on. By taking measurements on only a subset of the population that we are interested in, we introduce sampling variability - we have uncertainty in the accuracy of our estimate. 
We saw [previously (2B)](02b_sampling.html) how to make a __confidence interval__ as a means of capturing this uncertainty, providing a range of plausible values. 

Let's look at this with a different example.

:::frame
__Stroop Data__ 

The data we are going to use here come from an experiment using one of the best known tasks in psychology, the "Stroop task".  

In our dataset, we have information from 131 participants who completed an online task in which they saw two sets of coloured words. Participants spoke out loud the colour of each word, and timed how long it took to complete each set. In the one set of words, the words _matched_ the colours they were presented in (e.g., word "blue" was coloured blue). In the other set of words, the words _mismatched_ the colours (e.g., the word "blue" was coloured red (see @fig-stroop1). The order of matching/mismatching sets was randomly allocated for each participant. Participants' recorded their times for each set (*matching* and *mismatching*).^[You can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html]      

The data are available at [https://uoepsy.github.io/data/stroop.csv](https://uoepsy.github.io/data/stroop.csv).  

```{r}
#| label: fig-stroop1
#| fig-cap: "Stroop Task - Color word interference. Images from https://faculty.washington.edu/chudler/java/ready.html"
#| echo: false
knitr::include_graphics("images/numeric/stroop1.png")
```

:::

First, we read in the data and take a look at it. We should note how many rows (130), how many columns (4), and so on.  
```{r}
library(tidyverse)
stroopdata <- read_csv("https://uoepsy.github.io/data/stroop.csv")
head(stroopdata)
```

What we are interested in is the differences between the matching and mismatching times. For someone who took 10 seconds for the matching set, and 30 seconds for the mismatching set, we want to record their score as a difference of 20 seconds.   

So we can add this as a new variable to our data: 
```{r}
#| code-fold: true
stroopdata <- 
  stroopdata |> 
  mutate(
    diff = mismatching - matching
  )
head(stroopdata) # take a look to check it works
```

What we're interested in is the distribution of these difference-scores. Are people, on average, _slower_? This would correspond to a positive value of the `diff` variable. 
Let's get some summary statistics and a visualisation of out observed distribution.  

```{r}
#| code-fold: true
# mean(stroopdata$diff) and sd(stroopdata$diff) work just as well
stroopdata |> 
  summarise(
    meandiff = mean(diff),
    sddiff = sd(diff)
  )
```
```{r}
#| fig-height: 3
#| code-fold: true
ggplot(stroopdata, aes(x = diff)) +
  geom_histogram()
```

What we're really interested in talking about is the average 'mismatching - matching' score for __everybody__, not just our sample of 131. But we only have those 131 people, so we'll have to make do and use their data to provide us with an __estimate__.  

Our __estimate__ is that people are `r round(mean(stroopdata$diff),1)` seconds slower, on average, when reading mismatched colour words than reading matched colour words.  

Remember that there are many many different samples of $n=131$ that we _might have_ taken. And if we had taken a different sample, then our mean 'mismatching - matching' score (the `mean(stroopdata$diff)` value) would be different.  

Previously we learned about how the distribution of all possible sample means we _might_ take is known as a __sampling distribution__. We also learned that these tend to be normally distributed (regardless of the underlying population distribution) and so we can use the standard deviation of the sampling distribution (known as the __standard error__) to quantify the variation due to sampling.  

To actually get at the standard error, one of our options is to simulate the act of 'taking many samples of size $n$' by taking lots of samples _with replacement_ from our original sample (this is known as "bootstrapping").  

As our analyses become more advanced, then this will become more complex. In this example, because we are just interested in estimating a single mean value, we can use `replicate` to do calculate the means from 1000 resamples from our original sample. We can then simply calculate the standard deviation of all these means:   
```{r}
stroop_bootstrapdist <- 
  replicate(1000, mean(sample(stroopdata$diff, size = 131, replace = TRUE)))

sd(stroop_bootstrapdist)
```

Alternatively (and more conventionally), we use a formula of $\frac{\sigma}{\sqrt{n}}$, which we can calculate much more easily.  
The symbol "$\sigma$" is the standard deviation of the population, but we are going to approximate this with "$s$", the standard deviation of our sample. The "$n$" is the size of our sample:   

```{r}
stroopdata |> 
  summarise(
    s = sd(diff), # standard deviation of sample
    n = n(), # number of sample
    SE = s/sqrt(n) # standard error of mean
)
```

What the standard error gives us is an idea of how much uncertainty there is going to be in our estimated mean from our sample of this size:  
```{r}
#| label: fig-sampdist
#| echo: false
#| fig-cap: "Sampling distribution for mean of sample size 131, with SE = 5.01 / sqrt(131) = 0.44"
#| fig-width: 6
#| fig-height: 2.5
set.seed(2394)
samplemeans <- replicate(2000, mean(rnorm(n=131, mean=0, sd=5.015774)))
g <- ggplot(data=tibble(samplemeans),aes(x=samplemeans))+
  #geom_histogram(alpha=.3)+
  stat_function(geom="line",fun=~dnorm(.x, mean=0,sd=sd(samplemeans))*270,lwd=1)

ld <- layer_data(g) %>% filter(x <= sd(samplemeans) & x >= (-sd(samplemeans)))
ld2 <- layer_data(g) %>% filter(x <= 2*sd(samplemeans) & x >= (-2*sd(samplemeans)))
g + geom_area(data=ld,aes(x=x,y=y),fill="grey30",alpha=.3) + 
  geom_area(data=ld2,aes(x=x,y=y),fill="grey30",alpha=.1) +
  geom_segment(aes(x=0,xend=0,y=0,yend=dnorm(0,0,sd=sd(samplemeans))*270), lty="dashed")+
  labs(x = "mean 'mismatching-matching' score")+
  scale_y_continuous(NULL, breaks=NULL)+
  scale_x_continuous(NULL, breaks=NULL)+
  theme_minimal()+
  annotate("text",x=1.2, y=220, label="What we would expect for the mean\n'mismatching-matching' score from\nsamples of size 131", col="grey30")+
  geom_curve(aes(x=1.3, xend=0.5, y=180, yend=150), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))+
  annotate("text",x=-1.1, y=150, label="0.438\n(standard error)", col="grey30")+
  geom_curve(aes(x=-0.8, xend=-0.2, y=150, yend=100), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))

```

This enables us to, for instance, construct a range of plausible values for our estimated 'mismatching - matching' score. 
The range is centered around our __point estimate__ (the mean score in our sample), and we widen it to include X% of the possible means we might also see from a sample of the same size. This is achieved by multiplying the standard error by a value that corresponds to our confidence level.  

$$
\begin{align}
\text{95\% CI} & \qquad = \bar{x} \pm 1.96 \times SE & \\
& \qquad = 2.40 \pm 1.96 \times 0.438 \\
& \qquad = 2.40 \pm 0.858 \\
& \qquad = [1.542, 3.258] \\
\end{align}
$$
The 1.96 used here comes from our rules-of-thumb about the normal distribution (see [2B #normal-distributions](02b_sampling.html#normal-distributions){target="_blank"}): 95% of the data fall within 1.96 standard deviations of the mean:^[
For other intervals, such as a 90% interval, we need to know the point at which 5% is either side of a normal distribution (i.e., giving us the middle 90%). `qnorm(c(0.05,0.95))` will give us 1.64, which we then put into our construction of the interval: $90\%\, CI = \bar{x} \pm 1.64 \times SE$.
]
```{r}
qnorm(c(0.025, 0.975))
```

<div class="divider div-transparent div-dot"></div>

# Null Hypothesis Significance Testing (NHST)

Let's suppose we are interested in asking a question.  

> __is there__ an effect of the mismatching/matching colour-words?  

Our objective here has changed slightly: rather than being concerned with parameter estimation ("what is the estimated score"), our question is now about making a statement about two competing hypotheses: 

- $H_0 \,\, (\text{The Null Hypothesis}):$ There is _no_ difference between matching and mismatching conditions. 
- $H_1 \,\, (\text{The Alternative Hypothesis}):$ There is _some_ difference between matching and mismatching conditions. 

If we use $\mu$ to denote the average 'mismatching - matching' score _in the population_, then we can state these as: 

- $H_0: \mu = 0$.  
- $H_1: \mu \neq 0$.


With the confidence interval that we have created above, we can already make a statement about these. Our 95% CI does not contain zero, meaning that we can, with that same level of confidence, reject $H_0$.  

However, there are instances where it is not feasible for us to create a confidence interval.^[Think about an example where our question is about whether there is a difference in variable $Y$ between groups A, B, C and D. Around what should we construct our interval? Around the difference $\bar{Y}_A - \bar{Y}_B$ (difference between A and B's average scores on $Y$), or $\bar{Y}_A - \bar{Y}_C$, or $\bar{Y}_B - \bar{Y}_D$?] 
This is where the other primary tool for null hypothesis significance testing comes in, the p-value.  

## Test-statistics & p-values

The p-value is a formal way of testing a statistic against a null hypothesis. To introduce the p-value, instead of thinking first about what we _have_ observed in our sample, we need to think about what we would expect to observe _if our null hypothesis is true._  

With our Stroop Task example, our null hypothesis is that there is no difference between matching and mismatching conditions ($H_0: \mu = 0$). Under $H_0$, the average 'mismatching-matching' score in the population is zero, and we would expect most of the samples we _might_ take to have a mean score of close to this (not _exactly_ 0, but centered around 0). We saw above that we could express the sampling distribution of means taken from samples of size $n=131$ using the __standard error__. Under $H_0$ we would expect the samples of $n=131$ we _might_ take to have means that follow something like the distribution in @fig-hyp. We can think of this as the sampling distribution of $\bar{x}$, but centered on our null hypothesis (in this case, $\mu = 0$). We call this the 'null distribution'.  

```{r}
#| label: fig-hyp
#| echo: false
#| fig-cap: "Sampling distribution for mean of sample size 131, assuming population mean = 0. Observed sample mean shown in red"
#| fig-width: 6
#| fig-height: 3.5
g + geom_area(data=ld,aes(x=x,y=y),fill="grey30",alpha=.3) + 
  geom_area(data=ld2,aes(x=x,y=y),fill="grey30",alpha=.1) +
  geom_segment(aes(x=0,xend=0,y=0,yend=dnorm(0,0,sd=sd(samplemeans))*270), lty="dashed")+
  geom_vline(aes(xintercept=mean(stroopdata$diff)),lty="dashed",col="tomato1")+
  labs(x = "mean 'mismatching-matching' score")+
  scale_y_continuous(NULL, breaks=NULL)+
  theme_minimal()+
  annotate("text",x=1.3, y=220, label="What we would expect for the mean\n'mismatching-matching' score from\nsamples of size 131 if the\npopulation mean is 0", col="grey30")+
  annotate("text",x=1.5, y=100, label="What we observed for the mean\nin our actual sample", col="tomato1")+
  geom_curve(aes(x=1.3, xend=0.5, y=180, yend=150), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=1.5, xend=2.35, y=75, yend=30), col="tomato1", size=0.5, curvature = 0.2, arrow = arrow(length = unit(0.03, "npc")))+
  annotate("text",x=-1.1, y=150, label="0.438\n(standard error)", col="grey30")+
  geom_curve(aes(x=-0.8, xend=-0.2, y=150, yend=100), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))
```



:::statbox
__Test-statistic__ 

The first step now is to create a test-statistic. That is, a statistic that tell us, in some _standardised units_, how big our observed effect is from the null hypothesis (i.e. in this case, how far from $\mu=0$ our sample mean is).  

The straightforward way to do this is to express how far away from $\mu=0$ our sample mean is _in terms of standard errors._ We'll call our test statistic $Z$:  

$$
Z = \frac{\text{estimate}-\text{null}}{SE}
$$

Our mean and standard error are:
```{r}
mean(stroopdata$diff)
sd(stroopdata$diff) / sqrt(nrow(stroopdata))
```

So our test-statistic is 
$$
Z = \frac{2.40 - 0}{0.438} = 5.479
$$
:::

:::statbox
__p-value__ 

We can now calculate how likely it is to see values _at least as extreme_ as our observed test-statistic, _if the null is true_.  

If the null hypothesis is true (there was no 'mismatching-matching' difference) then we would expect Z-statistics to be normally distributed with a mean of 0 and a standard deviation of 1.  

We have seen the process of how we might calculate a probability from a distribution like this already: the `pnorm()` function gives us the area of a distribution to the one side of a given value:  
```{r}
#| eval: false
pnorm(??, mean = 0, sd = 1, lower.tail = FALSE)
```

```{r}
#| label: fig-pnormstroop
#| echo: false
#| fig-cap: "pnorm() provides us with a p-value for a z-statistic"
#| fig-height: 2.5
new_theme_empty <- theme_bw()
new_theme_empty$line <- element_blank()
new_theme_empty$rect <- element_blank()
new_theme_empty$axis.text.y <- element_blank()
new_theme_empty$plot.title <- element_blank()
new_theme_empty$axis.title.y <- element_blank()

df <- tibble(x=c(-4,4))
g <- df %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1) +
  labs(x = "Z statistics \n (normally distributed with mean of 0, sd of 1)")
ld <- layer_data(g) %>% filter(x>= 2)
g + geom_area(data=ld,aes(x=x,y=y),fill="red") +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1)+
  scale_x_continuous(breaks=c(0,2),label=c(0,"??"))+
  new_theme_empty
```
Remember, our Z-statistic we calculated above is 5.479. If the null hypothesis were true then the probability that we would see a sample ($n=131$) with a Z-statistic _at least_ that large is:
```{r}
pnorm(5.479, lower.tail = FALSE)
```
which is R's way of printing `r format(pnorm(5.479, lower.tail = FALSE), scientific=F)`.  

There is one last thing, and that the _direction_ of our hypotheses. Recall from earlier that we stated $H_0: \mu = 0$ and $H_1: \mu \neq 0$. This means that we are interested in the probability of getting results this far away from 0 _in either direction._  
We are interested in both tails:  

```{r}
#| label: fig-pnormstroop2
#| echo: false
#| fig-cap: "2*pnorm gives the two tails"
#| fig-height: 2.5
df <- tibble(x=c(-4,4))
g <- df %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1) +
  labs(x = "Z statistics \n (normally distributed with mean of 0, sd of 1)")
ld <- layer_data(g) %>% filter(x>= 2)
ld2 <- layer_data(g) %>% filter(x<= -2)
g = g + geom_area(data=ld,aes(x=x,y=y),fill="red") +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1)
g + geom_area(data=ld2,aes(x=x,y=y),fill="red") +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1)+
  scale_x_continuous(breaks=c(-2,0,2),label=c("-??",0,"??"))+
  new_theme_empty
```


```{r}
2 * pnorm(5.479, lower.tail = FALSE)
```

or $p =$ `r format(2*pnorm(5.479, lower.tail = FALSE), scientific=F)`. 

:::sticky
__p-value__  

The p-value is the probability^[What we have been seeing is that probabilities in NHST are defined as the relative frequency of an event *over many trials* (as "many" $\to \infty$). This requires assuming some features of the data generating process which guides what the "many trials" would look like (e.g., that there is no effect). The $p$-value is the probability of observing results as or more extreme than the data, *if the data were really generated by a hypothesised chance process*.] that we observe a test statistic at least as extreme as the one we observed, _assuming the null hypothesis $H_0$ to be true_.  


:::

:::

## Making Decisions

Now that we have our p-value of `r format(2 * pnorm(5.479, lower.tail = FALSE), scientific=F)`, we need to use it to make a decision about our hypotheses.  

Typically, we pre-specify the probability level at which we will consider results to be so unlikely to have arisen from the null distribution that we will take them as evidence to reject the null hypothesis. 
This pre-specified level is commonly referred to as $\alpha$ ("alpha"). Setting $\alpha = 0.05$ means that we will reject $H_0$ when we get a result which is extreme enough to only occur 0.05 (5%) of the time or less if the $H_0$ is true.  

In our case, `r format(2 * pnorm(5.479, lower.tail = FALSE), scientific=F)` $< 0.05$, so we reject the null hypothesis that there is no difference in the mismatching/matching conditions of the Stroop Task.  


:::sticky
There's a lot of convention to how we talk about NHST, but the typical process is as follows: 

1. _Clearly_ specify the null and alternative hypotheses.  
2. Specify $\alpha$
3. Calculate statistic
4. Compute p-value  
    - If $p<\alpha$, then reject the null hypothesis.
    - If $p\geq\alpha$, then fail to reject* the null hypothesis. 
    
*Note, we don't "accept" anything, we just "reject" or "fail to reject" the null hypothesis. Think of it like a criminal court, and we are trying the null hypothesis - $H_0$ is "innocent until proven guilty".  
:::

## Making Mistakes  

Whether our eventual decision is a) reject the null hypothesis, or b) fail to reject the null hypothesis, there's always a chance that we might be making a mistake. There are actually two different types of mistakes we might make. An often used analogy (@fig-httrial) is the idea of criminal trials in which an innocent person can be wrongfully convicted, or a guilty person can be set free. 

```{r}
#| label: fig-httrial
#| fig-cap: "Making errors in NHST is like a criminal court making errors in its decision on the defendent"
#| echo: false
knitr::include_graphics("images/hypothesis/ht-errors-trial.png")
```



We can actually quantify the chance that we're making errors in our different decisions. Thinking back to the definition of a p-value, it is the probability of seeing our results _if_ the null hypothesis is true. If we make a decision to reject the null hypothesis based on whether $p<\alpha$, then the probability that this decision is a mistake is $\alpha$. 

The probability that we the other sort of error (failing to reject the null hypothesis when the null hypothesis is actually false), we denote with $\beta$. 

Doing statistics is partly a matter of balancing these possibilities. If we used a very low $\alpha$-level (e.g. we reject when $p<.0001$ rather than $p<.05$) then we _increase_ the probability of making a type II error. 

:::sticky
__Types of Errors in NHST__  

```{r}
#| label: fig-httrial2
#| fig-cap: "Probabilities of making different errors in NHST"
#| echo: false
knitr::include_graphics("images/hypothesis/ht-errors-table-2.png")
```
:::

:::statbox
__Power ($1-\beta$)__  

A key notion in conducting studies is "statistical power". Studies want to increase the probability of correctly rejecting the null hypothesis (i.e. correctly identifying that there _is_ something more than chance going on).  

This is the bottom right cell of the tables in @fig-httrial and @fig-httrial2. We know that this will depend on the $\alpha$-level that we choose, but there are other important factors that influence $1-\beta$: 

- power increases as sample size increases
  - *e.g. it's easier to determine that cats weigh less than dogs if we measure 100 animals vs if we measure only 10 animals*
- power increases the farther away the true value is from the null hypothesis value 
  - *e.g. it's easier to determine that cats weigh less than elephants than it is to determine that cats weigh less than dogs*  

:::

<div class="divider div-transparent div-dot"></div>
