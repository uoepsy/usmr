---
title: "test"
---




<div class="divider div-transparent div-dot"></div>


# Correlation vs Causation, Again!

It's very important to remember that all of this stuff we're doing with regression models does __not__ allow us to simply start talking about causal effects. The coefficients we get from a regression model are _still_ just associations (i.e. correlations). It is the same with the multiple regression model, in which they are associations that are _conditional_ upon holding constant some other variable.  
To make the point, we could fit a model such as:

```{r}
#| eval: false
lm(birthweight ~ IQ_age11 + bilingual, data = ...)
```
And get some coefficients:  
```
Coefficients:
            Estimate    ...   ...
(Intercept)  600.000    ...   ...
IQ_age11     10.0000    ...   ...
bilingualYes 5.00000    ...   ...
```

Now imagine that you have a newborn baby who weighs 700 grams. Are we to say that "If I raise this child to be bilingual, her birthweight will increase by 5 grams (assuming her IQ at age 11 remains the same)"?  
This is obviously nonsense - the baby weighs 700 grams and that's not something that will change.  

To talk about causal effects we need a lot of careful thought about our theoretical model of the world (i.e. what causes what) combined with a model that isolates the relevant effect of interest by controlling for the appropriate possible confounds (either through statistical control or control by design).  

`r optbegin("Optional: Good Controls, Bad Controls", olabel=FALSE)`

When you first learn about multiple regression and the idea of "controlling for ....", it's tempting to shove _EVERY_ variable that you have into your model. That way you've controlled for everything, right?  

Unfortunately, it doesn't really work like that. Instead, we need to think carefully about our theory of the world.  

Let's suppose that what we are interested in is "the effect of caffeine consumption on resting heart rate". We think that age influences both of these things, so we could draw something like @fig-confounddag.  

```{r}
#| fig-cap: Age is a confounder of Caffeine and RHR
#| label: fig-confounddag
#| echo: false
knitr::include_graphics("images/mlr/dagconfound.png")
```

If we are correct in our thinking, then we _do_ want to control for Age. If we don't control for Age, then the effect of Age on RHR has to go via Caffeine consumption (i.e. because we don't let our model have the arrow **b**, Age influences RHR by going through arrows **a** and **c**. But this means the our estimate of arrow **c** includes _both_ age effects _and_ caffeine effects). This is what we have just seen earlier in this reading.  

Let's now think about a different example. Suppose we are interested in "the influence of Age on consumption of Alcohol", and we are trying to decide on whether or not we should control for 'Sleep Quality' (@fig-collidedag). In this example, if we include Sleep Quality in our model, then some of the association between Age and Alcohol will go via paths **a** and **b**, when we don't actually want it to. So in this case we might _**not** want_ to control for Sleep Quality. 
One thing to learn here is that in our model, information flow doesn't care about which end we have drawn an arrow - the associations we are using our model to look at don't actually have directions - it is only our theory that adds the arrows.  

```{r}
#| fig-cap: "Sleep Quality as a 'collider'"
#| label: fig-collidedag
#| echo: false
knitr::include_graphics("images/mlr/dagcollider.png")
```

`r optend()`


<div class="divider div-transparent div-dot"></div>





# Analysis of Variance


## What's the point?  

It might feel pointless to bother asking "__does__ [predictor(s)] explain variance in y?" when we can look at the coefficients to ask "*__how__* __does__ [predictor] influence y?"  

The main utility in this approach comes in when we want to assess _overall group differences_, rather than testing differences between specific groups.  

:::statbox

__Example__  

> Question A: Do species have different brain mass? 

If we fit our linear model, we can examine our coefficients (remember, the reference level for species here is "Humans":  
```{r}
#| eval: false
braindata <- read_csv("https://uoepsy.github.io/data/usmr_braindata.csv")
fit <- lm(mass_brain ~ species, data = braindata)
summary(fit)
```
```{r}
#| echo: false
.pp(summary(lm(mass_brain ~ species, data = braindata)), l=list(3,9:13))
```

But these actually answer the questions:  

- Question B: Do Potar monkeys have different brain mass from Humans? 
- Question C: Do Rhesus monkeys have different brain mass from Humans?  

And these are not the same as our original question. However, we _can_ answer it with an analysis of variance, which tests the variance explained by `species` grouping as a whole!  
```{r}
anova(fit)
```

:::

## Types of Sums of Squares

The logic seen in @fig-vennss1 is to think of the "incremental addition" of each predictor. We don't have to use this approach, however. We can, for instance, choose to analyse the variance explained by each predictor _as if it were the last one_ entered in the model. This is equivalent to examining the effect of all predictors __after__ considering all others (see @fig-vennss3). The area $C$ is the same in both @fig-vennss1 and @fig-vennss3, demonstrating that this approach is like considering each predictor as if it were the "last one in" in the incremental approach.  

```{r}
#| label: fig-vennss3
#| fig-cap: "Venn Diagram showing partial ('last one in') sums of squares"
#| echo: false
knitr::include_graphics("images/mlr/venn_ss_type3.png")
```

The `drop1()` function allows us to conduct an analysis of variance using the “last one in” approach:

```{r}
eg_model2 <- lm(y ~ x1 + x2 + x3, mydata)
drop1(eg_model2, test = "F")
```


