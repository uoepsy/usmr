---
title: "test"
---




# Multiple Categories = Multiple Regression

We saw last week how to interpret simple regression models when there is a binary predictor (see [5B #binary-predictors](05b_slr.html#binary-predictors)). The addition of binary predictors in multiple regression models is pretty much the same - the coefficient will give us the estimated change in $y$ when moving from one level to the other^[and the intercept will be the estimated $y$ when all predictors are zero, where "zero" is the reference category of the binary predictor], _holding other predictors constant._  

If you want a visual intuition to this, it is like a shift between two lines, or between two surfaces (depending on how many other predictors there are). It's actually just another dimension to the model, but a dimension that is on a __discrete__ scale - observations fall on 0 or 1, not on the continuum in between.  

```{r}
#| echo: false

par(mfrow=c(1,2))

set.seed(03)
df <- tibble(
  x1 = rnorm(50),
  x2 = sample(0:1,50,T),
  y = .3*x1 + 2*x2 + rnorm(50)
)
fit<-lm(y~x1+x2,df %>% mutate(x2=factor(x2)))

library(scatterplot3d)
plt <- with(df,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,x.ticklabs = c(0,NA,NA,NA,NA,1), main="y~x1+x2\n(x2 is categorical)"))

pp <- expand_grid(x1=seq(-4,4,.1), x2=factor(0:1))
pp$y <- predict(fit, pp)
pp$x2 = as.numeric(pp$x2)-1
pp1 <- pp[pp$x2==0,]
pp2 <- pp[pp$x2==1,]
plt$points(pp1$x1,pp1$x2,pp1$y,type="l")
plt$points(pp2$x1,pp2$x2,pp2$y,type="l")


df <- tibble(
  x1 = rnorm(50),
  x2 = rnorm(50),
  x3 = sample(1:2,50,T),
  y = 2*x1 + 4*x2 + 7*x3 + rnorm(50)
)

fit<-lm(y~x1+x2+x3,df)
steps=20
x1 <- with(df, seq(min(x1),max(x2),length=steps))
x2 <- with(df, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2, x3=1)
y <- matrix(predict(fit, newdat), steps, steps)
p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(-10,30), main="y~x1+x2+x3\n(x3 is categorical)")

newdat <- expand.grid(x1=x1,x2=x2,x3=2)
y <- matrix(predict(fit, newdat), steps, steps)
par(new=TRUE)
persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(-10,30),axes=F)

par(mfrow=c(1,1))

```

What about when we have a predictor with more than two categories? We might have lots of different conditions in our experiment, or we might have observations from lots of different distinct groups of people. 

Consider an example where we are investigating the brain mass of different species of animals. We might have a datset which looks like this:  

```{r}
#| include: false
#| eval: false
set.seed(50)
mm <- MASS::Animals
mm <- mm[c("Rhesus monkey","Potar monkey","Chimpanzee"), ]
mm$n <- rdunif(3,8,14)
mm$sbody <- c(2,7,6)
mm$sbrain <- rep(100,3)

mm <- 
  mm %>% 
  mutate(
    species = row.names(mm),
    mass_body = pmap(list(n,body,sbody),~round(rnorm(..1,..2,..3))),
    mass_brain = pmap(list(n,brain,sbrain),~round(rnorm(..1,..2,..3))/1000)
  ) %>%
  select(species,mass_body,mass_brain) %>%
  unnest() %>% 
  sample_n(n())

mm$mass_body = mm$mass_body + 10
mm$mass_brain = mm$mass_brain + .2
mm$species[mm$species=="Chimpanzee"]<-"Human"
mm$mass_brain[mm$species=="Rhesus monkey"]<-mm$mass_brain[mm$species=="Rhesus monkey"]+.06
# write_csv(mm, "../../data/usmr_braindata.csv")
```

```{r}
#| eval: false
braindata <- read_csv("https://uoepsy.github.io/data/usmr_braindata.csv")
head(braindata)
```

```{r}
#| echo: false
braindata <- read_csv("https://uoepsy.github.io/data/usmr_braindata.csv")
knitr::kable(head(braindata) %>% rbind(.,c("...","...","...")))
```


When we consider a model in which brain mass is predicted by species, the `species` variable contains more than just two categories. In our example it has 3: "Potar monkey", "Rhesus Monkey" and "Human".  

When we fit the model `lm(mass_brain ~ species)`, the default way in which the `species` predictor is included in the model is by setting one category as the "reference level", and comparing each level to that reference level. So if the reference level is "Human", the coefficients we get out include the intercept (which is the estimated brain mass of humans); the estimated difference in brain mass when we move from humans to potar monkeys; and from humans to rhesus monkeys:   


```{r}
#| echo: false
.pp(summary(lm(mass_brain~species, braindata)),l=list(3,9:13))
```


```{r}
#| include: false
fit <- lm(mass_brain~species, braindata)
ggplot(braindata, aes(x=species,y=mass_brain))+
  geom_jitter(width=.3)+
  geom_boxplot(alpha=.5)+
  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col="blue",lwd=1) + 
  geom_segment(aes(x=1,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1,3)])),col="blue",lwd=1)
```


Under the hood, what really gets inputted into our model is a set of variables that are all 0s and 1s (much like it did for a binary predictor). In the table below, the left column shows the original `species` variable, and the remaining columns are the variables that R actually inputs to the model when we give it `species` as a predictor. We can see that one category ("Human") is where all these are zeros. 

```{r}
#| echo: false
cc = contrasts(factor(braindata$species))
dimnames(cc)[[2]]<-paste0("species",dimnames(cc)[[2]])
knitr::kable(rbind(cc,rep("...",3)))
```

For a categorical variable with $k$ levels, this is the same as adding $k-1$ predictors into our model. Each of $k-1$ predictors is actually just another dimension to the model:  

```{r}
#| echo: false
fit<-lm(mass_brain ~ species,braindata)
braindata2 <- as.data.frame(model.matrix(fit)[,2:3]) 
braindata2$mass_brain = braindata$mass_brain


library(scatterplot3d)
plt <- with(braindata2,scatterplot3d(`speciesRhesus monkey`,`speciesPotar monkey`,mass_brain, scale.y=1.,angle=30,ylab="",
                                     y.ticklabs = c(0,NA,NA,NA,NA,1),
                                     x.ticklabs = c(0,NA,NA,NA,NA,1),
                                     main = "mass_brain ~ species(human/potar monkey/rhesus monkey)"))
text(x = 7.5, y = 0.5, "speciesPotar monkey", srt =15)


pp1 <- tibble(
  `speciesRhesus monkey`=0,
  `speciesPotar monkey`=seq(0,1,.1),
  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[2],length.out=11)
)
pp2 <- tibble(
  `speciesRhesus monkey`=seq(0,1,.1),
  `speciesPotar monkey`=0,
  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[3],length.out=11)
)

plt$points(pp1$`speciesRhesus monkey`,pp1$`speciesPotar monkey`,pp1$y,type="l",col="blue")
plt$points(pp2$`speciesRhesus monkey`,pp2$`speciesPotar monkey`,pp2$y,type="l",col="blue")
```


:::rtip
R will default to using alphabetical ordering, hence the reference level being set as "Human". We could override this by making it a factor with an ordering to it's levels (see the use of `factor()` and `levels()` in [2A#categorical](02a_measurement.html#categorical){target="_blank"}). Functions like `fct_relevel()` might be handy too.  

:::

<div class="divider div-transparent div-dot"></div>


<div class="divider div-transparent div-dot"></div>


# Correlation vs Causation, Again!

It's very important to remember that all of this stuff we're doing with regression models does __not__ allow us to simply start talking about causal effects. The coefficients we get from a regression model are _still_ just associations (i.e. correlations). It is the same with the multiple regression model, in which they are associations that are _conditional_ upon holding constant some other variable.  
To make the point, we could fit a model such as:

```{r}
#| eval: false
lm(birthweight ~ IQ_age11 + bilingual, data = ...)
```
And get some coefficients:  
```
Coefficients:
            Estimate    ...   ...
(Intercept)  600.000    ...   ...
IQ_age11     10.0000    ...   ...
bilingualYes 5.00000    ...   ...
```

Now imagine that you have a newborn baby who weighs 700 grams. Are we to say that "If I raise this child to be bilingual, her birthweight will increase by 5 grams (assuming her IQ at age 11 remains the same)"?  
This is obviously nonsense - the baby weighs 700 grams and that's not something that will change.  

To talk about causal effects we need a lot of careful thought about our theoretical model of the world (i.e. what causes what) combined with a model that isolates the relevant effect of interest by controlling for the appropriate possible confounds (either through statistical control or control by design).  

`r optbegin("Optional: Good Controls, Bad Controls", olabel=FALSE)`

When you first learn about multiple regression and the idea of "controlling for ....", it's tempting to shove _EVERY_ variable that you have into your model. That way you've controlled for everything, right?  

Unfortunately, it doesn't really work like that. Instead, we need to think carefully about our theory of the world.  

Let's suppose that what we are interested in is "the effect of caffeine consumption on resting heart rate". We think that age influences both of these things, so we could draw something like @fig-confounddag.  

```{r}
#| fig-cap: Age is a confounder of Caffeine and RHR
#| label: fig-confounddag
#| echo: false
knitr::include_graphics("images/mlr/dagconfound.png")
```

If we are correct in our thinking, then we _do_ want to control for Age. If we don't control for Age, then the effect of Age on RHR has to go via Caffeine consumption (i.e. because we don't let our model have the arrow **b**, Age influences RHR by going through arrows **a** and **c**. But this means the our estimate of arrow **c** includes _both_ age effects _and_ caffeine effects). This is what we have just seen earlier in this reading.  

Let's now think about a different example. Suppose we are interested in "the influence of Age on consumption of Alcohol", and we are trying to decide on whether or not we should control for 'Sleep Quality' (@fig-collidedag). In this example, if we include Sleep Quality in our model, then some of the association between Age and Alcohol will go via paths **a** and **b**, when we don't actually want it to. So in this case we might _**not** want_ to control for Sleep Quality. 
One thing to learn here is that in our model, information flow doesn't care about which end we have drawn an arrow - the associations we are using our model to look at don't actually have directions - it is only our theory that adds the arrows.  

```{r}
#| fig-cap: "Sleep Quality as a 'collider'"
#| label: fig-collidedag
#| echo: false
knitr::include_graphics("images/mlr/dagcollider.png")
```

`r optend()`


<div class="divider div-transparent div-dot"></div>





# Analysis of Variance

What we are actually doing when we conduct a model comparison is asking "is there a reduction in residual sums of squares?".  

:::statbox
__Refresher: Residual Sums of Squares__ 

The residuals are the distances from the actual observed values to the model predicted values. If we square those distances (to make them all positive), and sum them up, we get the "residual sums of squares".  

:::

Another way of phrasing "is there are reduction in residual sums of squares?" is to say "is more variance explained?", and this is matches with the fact that we are using the function `anova()` - it stands for "analysis of variance".  

There's a subtle distinction here: 

:::sticky

- Analysis of variance: __does__ [predictor(s)] explain variance in y?  
- Coefficient: *__how__* __does__ [predictor] influence y?  

:::

We can apply the idea of 'analysing variance explained' to a single model, partitioning out variance explained by the _**incremental addition** of each predictor._ This is done by simply passing a single model (built with `lm()`) to the `anova()` function. It's very important to note that the __order matters__ here, because it will assess the improvement in model fit due to each predictor in turn: 

1. $x_1$ vs no predictors
2. the addition of $x_2$ to the model with _just_ $x_1$
3. the addition of $x_3$ to the model with _just_ $x_1$ and $x_2$  

```{r}
eg_model2 <- lm(y ~ x1 + x2 + x3, data = mydata)
anova(eg_model2)
```

If it helps, we might think of this again in terms of a Venn diagram. Each line of the Analysis of Variance Table above corresponds to the area of one of the coloured areas in @fig-vennss1 (relative to the size of the white area labelled "E").  
```{r}
#| label: fig-vennss1
#| fig-cap: "Venn Diagram showing incremental sums of squares"
#| echo: false
knitr::include_graphics("images/mlr/venn_ss_type1.png")
```

And we can really just think of this as a big set of model comparisons:
```{r}
#| eval: false
eg_model0 <- lm(y ~ 1, data = mydata)
eg_model1 <- lm(y ~ x1, data = mydata)
eg_model2 <- lm(y ~ x1 + x2, data = mydata)
eg_model3 <- lm(y ~ x1 + x2 + x3, data = mydata)
anova(eg_model0, eg_model1, eg_model2, eg_model3)
```

## What's the point?  

It might feel pointless to bother asking "__does__ [predictor(s)] explain variance in y?" when we can look at the coefficients to ask "*__how__* __does__ [predictor] influence y?"  

The main utility in this approach comes in when we want to assess _overall group differences_, rather than testing differences between specific groups.  

:::statbox

__Example__  

> Question A: Do species have different brain mass? 

If we fit our linear model, we can examine our coefficients (remember, the reference level for species here is "Humans":  
```{r}
#| eval: false
braindata <- read_csv("https://uoepsy.github.io/data/usmr_braindata.csv")
fit <- lm(mass_brain ~ species, data = braindata)
summary(fit)
```
```{r}
#| echo: false
.pp(summary(lm(mass_brain ~ species, data = braindata)), l=list(3,9:13))
```

But these actually answer the questions:  

- Question B: Do Potar monkeys have different brain mass from Humans? 
- Question C: Do Rhesus monkeys have different brain mass from Humans?  

And these are not the same as our original question. However, we _can_ answer it with an analysis of variance, which tests the variance explained by `species` grouping as a whole!  
```{r}
anova(fit)
```

:::

## Types of Sums of Squares

The logic seen in @fig-vennss1 is to think of the "incremental addition" of each predictor. We don't have to use this approach, however. We can, for instance, choose to analyse the variance explained by each predictor _as if it were the last one_ entered in the model. This is equivalent to examining the effect of all predictors __after__ considering all others (see @fig-vennss3). The area $C$ is the same in both @fig-vennss1 and @fig-vennss3, demonstrating that this approach is like considering each predictor as if it were the "last one in" in the incremental approach.  

```{r}
#| label: fig-vennss3
#| fig-cap: "Venn Diagram showing partial ('last one in') sums of squares"
#| echo: false
knitr::include_graphics("images/mlr/venn_ss_type3.png")
```

The `drop1()` function allows us to conduct an analysis of variance using the “last one in” approach:

```{r}
eg_model2 <- lm(y ~ x1 + x2 + x3, mydata)
drop1(eg_model2, test = "F")
```


