---
title: "Week 4 Exercises: Chi-Square Tests"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
set.seed(017)
survey_data <- read_csv("https://uoepsy.github.io/data/surveydata_allcourse.csv") 
```

# $\chi^2$ Goodness of Fit Test 

:::frame

TODO update on new survey data 

As you may remember, in the survey we asked students to complete in welcome week, one of the questions concerned the month in which you were born. You can download the data from https://uoepsy.github.io/data/surveydata_allcourse.csv.  

We need to make sure we know how many observations we have. While there are currently `r nrow(survey_data)` observations in our sample, we may have missing data in our variable of interest! 

We can use filter to remove any which are missing (or `NA`s).  
Take a bit of time to look at what each bit of this does. To help figure it out, try seeing what `is.na(c(NA,1,2))` does. Then try `!is.na(c(NA,1,2))`.  
```{r}
survey_data <- read_csv("https://uoepsy.github.io/data/surveydata_allcourse.csv") 
survey_data <-
  survey_data %>% filter(!is.na(birthmonth))
```
:::

`r qbegin(1)`
What is your intuition about the distribution of all students' birth-months?  
Do you think they will be spread uniformly across all months of the year (like a fair 12-sided dice), or do you think people are more likely to be born in certain months more than others?  

Plot the distribution and get an initial idea of how things are looking.  

:::hints
You can do this quickly with `barplot()` and `table()`, or you could create try using `ggplot()` and looking into `geom_bar()`.  
:::
`r qend()`

`r solbegin(show=TRUE, toggle=params$TOGGLE)`
The quick and dirty way:
```{r}
barplot(table(survey_data$birthmonth))
```

A ggplot option:
```{r}
ggplot(data = survey_data, aes(x = birthmonth)) +
    geom_bar() +
    labs(x = "- Birth Month -")
```
To order the months correctly, we need to make the "birthmonth" variable a *factor* (a categorical variable), and order the levels:
```{r}
# we can make use of month.abb which is the abbreviated months in R
month.abb
# but we want them all lower case:
tolower(month.abb)

#This is one way to do it
survey_data$birthmonth <- factor(survey_data$birthmonth, levels = tolower(month.abb), ordered=T)

# and this is another (the tidyverse way):
survey_data <-
  survey_data %>%
    mutate(
      birthmonth = fct_relevel(factor(birthmonth), tolower(month.abb))
    )
```
Now when we plot it, they're in the right order!
```{r}
ggplot(data = survey_data, aes(x = birthmonth)) +
    geom_bar() +
    labs(x = "- Birth Month -")
```
`r solend()`

`r qbegin(2)`
We're going to perform a statistical test to assess the extent to which our data conforms to the hypothesis that people are no more likely to be born on one month than another.  

Under this hypothesis, what would be the proportional breakdown of observed births in each of the months? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
If people are no more likely to be born in one month than another, then we would expect the same proportion of observed births in each month.  
There are 12 months, so we would expect $\frac{1}{12}$ observations in each month.  

We can write these as: 
$$
\begin{align}
& p_{jan} = 1/12 \\
& p_{feb} = 1/12 \\
& ... \\
& p_{dec} = 1/12 \\
\end{align}
$$
`r solend()`

`r qbegin(3)`
Given that there are there are `r nrow(survey_data)` people who entered their birth-month, how many observations would we *expect* to find with a birthday in January? And in February? ... and so on? 
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We would expect $\frac{1}{12} \times$ `r nrow(survey_data)` = `r round(nrow(survey_data)/12,2)` observations in each month. 
`r solend()`

:::sticky
The **chi-square goodness-of-fit test** applies to a categorical variable.  
The null hypothesis asserts specific values for the population proportion in each category. The alternative hypothesis simply states that at least one of the population proportions is not as specified in the null hypothesis.  

As always, the test statistic measures how far the observed sample results deviate from what is expected if the null hypothesis is true.  

With a chi-square test, we construct the test statistic by comparing the **observed sample counts** in each category to the **expected counts** under the null hypothesis.  

The test-statistic (denoted $\chi^2$, spelled *chi-square*, pronounced "kai-square") is obtained by adding up the standardized squared deviations in each category:  
$$
\chi^2 = \sum_{i} \frac{(\text{Observed}_i - \text{Expected}_i)^2}{\text{Expected}_i}
$$
:::

`r qbegin(4)`
The code below creates counts for each month: 
```{r}
#| eval: false
survey_data %>%
    group_by(birthmonth) %>%
    summarise(
        observed = n()
    )
```
(A shortcut for this would be `survey_data %>% count(birthmonth)`)  

Add to the code above to create columns showing:

- the expected counts $E_i$
- observed-expected ($O_i - E_i$)
- the squared differences $(O_i - E_i)^2$
- the standardised square differences $\frac{(O_i - E_i)^2}{E_i}$  

Then calculate the $\chi^2$ statistic (the sum of the standardised squared differences).  
If your observed counts matched the expected counts *perfectly*, what would the $\chi^2$ statistic be? 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
# the nrow() function tells us how many rows there are in the dataset!
totaln <- nrow(survey_data)

chi_table <- 
    survey_data %>%
    group_by(birthmonth) %>%
    summarise(
        observed = n(),
        expected = totaln/12,
        diff = observed-expected,
        sq_diff = diff^2,
        std_sq_diff = sq_diff / expected
    )
chi_table
```

And we can calculate our $\chi^2$ test statistic by simply summing the values in the last column we created:
```{r}
sum(chi_table$std_sq_diff)
```

If all our observed counts are equal to our expected counts, then the `diff` column above will be all $0$, and $0^2=0$, and $\frac{0}{E_i}$ will be $0$. So $\chi^2$ will be $0$. 
`r solend()`


`r qbegin(5)`
You can see the distribution of $\chi^2$ distributions with different degrees of freedom below.  
```{r}
#| label: fig-chidist
#| echo: false
#| fig.cap: "Chi-Square Distributions"
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/1200px-Chi-square_pdf.svg.png")
```

We can find out the proportion of the distribution which falls to either side of a given value of $\chi^2$ using `pchisq()`. We need to give it our calculated $\chi^2$ statistic, our degrees of freedom (`df`), which is equal to the number of categories minus 1. We also need to specify whether we want the proportion to the left (`lower.tail=TRUE`) or to the right (`lower.tail=FALSE`).  

1. Using `pchisq()`, calculate the probability of observing a $\chi^2$ statistic as least as extreme as the one we have calculated.  
2. Check that these results match with those provided by R's built-in function: `chisq.test(table(survey_data$birthmonth))`.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
sum(chi_table$std_sq_diff)
pchisq(sum(chi_table$std_sq_diff), df = 11, lower.tail = FALSE)
```

```{r}
chisq.test(table(survey_data$birthmonth))
```
`r solend()`

`r qbegin(6)`
Which months of year had the highest contributions to the chi-square test statistic?  

:::hints
Think about your standardised squared deviations. 
:::
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
**Standardized squared deviations**  

One possible way to answer this question is to look at the individual contribution of each category to the $\chi^2$ statistic. We computed these values in an earlier question.  
```{r}
chi_table %>%
  select(birthmonth, std_sq_diff)
```

From the barplot we created earlier on, we can see which months make up higher/lower proportions than expected:
```{r}
ggplot(chi_table, aes(x = birthmonth, y = observed/nrow(survey_data))) +
  geom_col(fill = 'lightblue') +
  geom_hline(yintercept = 1/12, color = 'red') +
  theme_classic(base_size = 15)
```

**Pearson residuals**

Equivalently, you could answer by looking at Pearson residuals:
```{r}
chisq.test(table(survey_data$birthmonth))$residuals
```

```{r}
#| echo: false
presids <- chisq.test(table(survey_data$birthmonth))$residuals
presids <- presids[order(abs(presids),decreasing = TRUE)]
```

The greatest *absolute* values are for `r names(presids)[1]` and `r names(presids)[2]`, showing that for these months the deviations from expected to observed were the greatest. 

`r solend()`

`r qbegin(7)`
According to the internet (that reliable source of information!), 76% of people in the world have brown eyes, 10% have blue, 5% hazel, 5% amber, 2% green, 1% grey, and 1% have some other eye colouring (red/violet/heterochromia).

Perform a $\chi^2$ goodness of fit test to assess the extent to which our sample of students conform to this theorised distribution of eye-colours.  

:::hints
Try using `chisq.test(..., p = c(?,?,?,...) )`.  
:::
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Let's look at the observed counts:
```{r}
table(survey_data$eyecolour)
```
Our theoretical probabilities of different eye colours must match the order in the table which we give `chisq.test()`. They must also always sum to 1.  
```{r}
chisq.test(table(survey_data$eyecolour), p = c(.05,.1,.76,.02,.01,.05,.01))
```
`r solend()`

`r qbegin(8)`
What are the *observed* proportions of our sample with each eyecolour?  
Can you figure out how to use the `prop.table()` function?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
From the help documentation (`?prop.table()`), we see that we can pass `prop.table()` the argument `x`, which needs to be a table. 
```{r}
prop.table(table(survey_data$eyecolour))*100
```

```{r}
barplot(prop.table(table(survey_data$eyecolour))*100)
```
`r solend()`

<div class="divider div-transparent div-dot"></div>

# $\chi^2$ Test of Independence  

:::frame
__Data: TipJokes__  

> **Research Question:** Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?  

A [study](https://doi.org/10.1111/j.1559-1816.2002.tb00266.x) published in the Journal of Applied Social Psychology^[Gueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. _Journal of Applied Social Psychology, 32_(9), 1955-1963.] investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. 
The waiter randomly assigned coffee-ordering customers to one of three groups. 
When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all. 

The data are available at https://uoepsy.github.io/data/TipJoke.csv.  
The dataset contains the variables:

- `Card`: None, Joke, Ad.
- `Tip`: 1 = The customer left a tip, 0 = The customer did not leave tip. 

:::


`r qbegin(9)`
Produce a plot and a table to display the relationship between whether or not the customer left a tip, and what (if any) card they received alongside the bill.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
tipjoke <- read_csv('https://uoepsy.github.io/data/TipJoke.csv')

table(tipjoke$Card, tipjoke$Tip)

plot(table(tipjoke$Card, tipjoke$Tip))
```
`r solend()`

`r qbegin(10)`
What would you *expect* the cell counts to look like if there were no relationship between what the waiter left and whether or not the customer tipped?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
In total, 60 customers tipped (14+30+16), and 151 did not. So *overall*, 0.28 ($\frac{60}{(60+151)}$) of customers tip.  

74 customers got an Ad card, 72 customers got a Joke, and 65 got None. If this were independent of whether or not they left a tip, we would expect equal proportions of tippers in each group.  
So we would expect 0.28 of each group to leave a tip.  

You can think about observed vs expected by looking at the two-way table along with the marginal row and column totals given:   
```{r}
#| echo: false
table(tipjoke$Card, tipjoke$Tip) %>%
  rbind(colSums(.)) %>% 
  cbind(rowSums(.)) -> conttable
conttable[1:3,1:2]<-"  "
conttable %>% knitr::kable(.) %>%
  kableExtra::kable_styling()
```
For a given cell of the table we can calculate the expected count as $\text{row total} \times \frac{\text{column total}}{\text{samplesize}}$:

**Expected:**
```{r}
#| echo: false
table(tipjoke$Card, tipjoke$Tip) %>%
  rbind(colSums(.)) %>% 
  cbind(rowSums(.)) -> conttable
conttable[1:3,1]<-round(conttable[1:3,3]*(conttable[4,1])/conttable[4,3],2)
conttable[1:3,2]<-round(conttable[1:3,3]*(conttable[4,2])/conttable[4,3],2)
conttable %>% knitr::kable(.) %>%
  kableExtra::kable_styling()
```

(If you're wondering how we do this in R, we saw in the lectures briefly a complicated bit of code using `%o%` which could do this for us):
```{r}
t <- tipjoke %>%
  select(Card, Tip) %>% table()

e <- rowSums(t) %o% colSums(t) / sum(t)
e
```
`r solend()`

`r qbegin(11)`
Just like we gave the `chisq.test()` function a table of observed frequencies when we conducted a goodness of fit test in earlier exercises, we can give it a two-way table of observed frequencies to conduct a test of independence.  

Try it now.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
chisq.test(table(tipjoke$Card, tipjoke$Tip))
```
`r solend()`

:::statbox
Note that for the test we just performed (`chisq.test(table(tipjoke$Card, tipjoke$Tip))`), the degrees of freedom is given as 2.  
  
This is because the degrees of freedom for a $\chi^2$ test of independence is calculated as:  
$$
\begin{align}
& df = (r - 1)(c - 1) \\
& \text{Where:} \\
& r = \text{number of rows} \\
& c = \text{number of columns} \\
\end{align}
$$
  
Why is this?  
Well, remember that the degrees of freedom is the number of values that are *free to vary* as we estimate parameters. In a $3 \times 2$ table like the one we have for Cards $\times$ Tips, the degrees of freedom is the number of cells in the table that can vary before we can simply calculate the values of the other cells (where we're constrained by the need to sum to our row/column totals). 

:::

<div class="divider div-transparent div-dot"></div>

# Some RMarkdown

`r qbegin(12)`
Can you create an RMarkdown document which:

1. Reads in the https://uoepsy.github.io/data/TipJoke.csv data.
2. Conducts and reports a $\chi^2$ test of independence examining whether telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer.
3. Successfully compiles ("knits") into an **.html** file. 

`r qend()`

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>



