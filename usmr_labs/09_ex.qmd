---
title: "Week 9 Exercises: Interactions and Categorical Predictors"
link-citations: yes
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
set.seed(017)

```


# Aging Cognition

:::frame
__Data: cogapoe4.csv__  

Ninety adult participants were recruited for a study investigating how cognitive functioning varies by age, and whether this is different depending on whether people carry an APOE-4 gene.  

> **Research Question:** Does the relationship between age and cognitive functioning differ between those with and without the APOE-4 genotype?  

The data are available at [https://uoepsy.github.io/data/cogapoe4.csv](https://uoepsy.github.io/data/cogapoe4.csv).  

```{r}
#| echo: false
library(tidyverse)
cogapoe <- read_csv("https://uoepsy.github.io/data/cogapoe4.csv")
tibble(
  variables = names(cogapoe),
  description = c("Participant ID","Age (in Years)","Years of Education","Birthweight (in Kg)","APOE-4 Gene expansion ('none', 'apoe4a', 'apoe4b', apoe4c')","Score on Addenbrooke's Cognitive Examination")
) %>% knitr::kable() %>% kableExtra::kable_styling(.,full_width=T)
```

:::


`r qbegin(1)`
> Does the relationship between age and cognitive functioning differ between those with and without the APOE-4 genotype?  

Read in the data and explore the variables which you think you will use to answer the research question above (create some plots, some descriptive stats etc.) 

:::hints
__handy functions:__  

- The `pairs.panels()` function from the __psych__ package is quite a nice way to plot a scatterplot matrix of a dataset.  
- The `describe()` function is also quite nice (from the __psych__ package too).  

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
cogapoe <- read_csv("https://uoepsy.github.io/data/cogapoe4.csv")
summary(cogapoe)
```

Judging by the research question, we're going to be interested in participants' ages, whether they carry the APOE4 gene, and their cognitive functioning.    
```{r}
library(psych)

cogapoe %>% 
  select(age, apoe4, acer) %>%
  pairs.panels()

cogapoe %>% 
  select(age, apoe4, acer) %>%
  describe()
```

`r solend()`

`r qbegin(2)`
Check the `apoe4` variable. It currently has four levels ("none"/"apoe4a"/"apoe4b"/"apoe4c"), but the research question is actually interested in two ("none" vs "apoe4").  

Make a new variable that indicates whether or not someone has the APOE4 genotype.  

:::hints
__Hints:__  

- One way to do this would be to use `ifelse()` to define a variable which takes one value (e.g., "NO") if the observation meets from some condition, or another value (e.g., "YES") if it does not. You can use it to add a new variable either inside `mutate()`, or using `data$new_variable_name <- ifelse(test, x, y)` syntax. 

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Create a new variable for APOE4 Yes or No
```{r}
cogapoe <- 
  cogapoe %>% 
  mutate(
    isAPOE4 = ifelse(apoe4 == "none", "No", "Yes")
  )
```
`r solend()`

`r qbegin(3)`
> Does the relationship between age and cognitive functioning differ between those with and without the APOE-4 genotype? 

To answer this question, do you need an interaction? If so, between what kind of variables (continuous x continuous, continuous x categorical, or categorical x categorical)?  

:::hints
__Hint:__

- See [9A#it-depends](09a_interactions.html#it-depends){target="_blank"}

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Does the relationship between [continuous predictor] and [outcome] differ between [categorical predictor]?   

It's continuous x categorical!   

`r solend()`


`r qbegin(4)`
Especially for this type of interactions, we can start to get a sense of things by plotting the data and having a separate _facet_ for each group.  

Produce a visualisation of the relationship between age and cognitive functioning, with separate _facets_ for people with and without the APOE4 gene.  

:::hints
__Hint:__ 

- remember `facet_wrap()`?  

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(data = cogapoe, aes(x = age, y = acer)) + 
  geom_point() + 
  facet_wrap(~isAPOE4)
```
`r solend()`


`r qbegin(5)`
> Does the relationship between age and cognitive functioning differ between those with and without the APOE-4 genotype?  

Fit a model to answer this research question.  

:::hints
__Hint:__

- for how to fit interactions, see [9A#fitting-interactions-in-r](09a_interactions.html#fitting-interactions-in-r){target="_blank"}

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
apoe4mod <- lm(acer ~ 1 + age * isAPOE4, data = cogapoe)
```
`r solend()`

`r qbegin(6)`  
Looking at the coefficient estimates from your model, write a description of what each one corresponds to on the plot shown in @fig-plotannotateint (it may help to sketch out the plot yourself and annotate it).  

```{r}
#| echo: false
#| label: fig-plotannotateint
#| fig-cap: "Multiple regression model: ACER ~ Age * isAPOE4<br>The dotted lines show the extension back to where the x-axis is zero"  
nd = expand_grid(age=0:100,isAPOE4=c("No","Yes"))
nd = nd %>% mutate(acer = predict(apoe4mod, newdata = .))
sjPlot::plot_model(apoe4mod, type="int")+
  scale_fill_manual(NULL, values=c(NA,NA))+
  scale_x_continuous(expand = c(0, 0),limits=c(0,100)) + 
  scale_y_continuous(expand = c(0, 1))+
  geom_line(inherit.aes=FALSE,data=nd,aes(x=age,y=acer,col=factor(isAPOE4)), lty="longdash")+
  NULL
```

:::hints

__Some options for you to choose from:__  

+ The point at which the blue line cuts the y-axis (where age = 0)
+ The point at which the red line cuts the y-axis (where age = 0)
+ The average vertical distance between the red and blue lines. 
+ The vertical distance from the blue to the red line _at the y-axis_ (where age = 0)
+ The vertical distance from the red to the blue line _at the y-axis_ (where age = 0)
+ The vertical distance from the blue to the red line _at the center of the plot_
+ The vertical distance from the red to the blue line _at the center of the plot_
+ The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line
+ The slope of the red line
+ The adjustment to the slope when you move from the blue to the red line
+ The adjustment to the slope when you move from the red to the blue line

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

As always, we can obtain our coefficient estimates using various functions such as `summary(apoe4mod)`,`coef(apoe4mod)`, `coefficients(apoe4mod)` etc. 

```{r}
coefficients(apoe4mod)
```


+ `(Intercept)` = `r round(coef(apoe4mod)[1],2)`: The point at which the red line cuts the y-axis (where age = 0).  
+ `age` = `r round(coef(apoe4mod)[2],2)`: The slope (vertical change on the y-axis associated with a 1 unit change on the x-axis) of the red line.
+ `isAPOE4Yes` = `r round(coef(apoe4mod)[3],2)`: The vertical distance from the red to the blue line _at the y-axis_ (where age = 0).  
+ `age:isAPOE4Yes` = `r round(coef(apoe4mod)[4],2)`: How the slope of the line changes when you move from the red to the blue line.   

`r solend()`

`r qbegin(7)`
Produce a visualisation of the estimated interaction.  

:::hints
__Hints:__  

- e.g. plot your model! 
- see [9A#visualisation](09a_interactions.html#visualisation){target="_blank"}

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
If we use **sjPlot**:  
```{r}
library(sjPlot)
plot_model(apoe4mod, type="int")
```

Or **interactions**:  
```{r}
library(interactions)
interact_plot(apoe4mod, pred = "age", modx="isAPOE4", interval = TRUE)
```

`r solend()`

`r qbegin(8)`
Write a short paragraph explaining the pattern of results, including coefficient estimates as appropriate.   

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
#| echo: false
res = summary(apoe4mod)$coefficients
res[,1:3] = apply(res[,1:3],2,function(x) round(x,2))
dimnames(res)[[2]] = c("est","se","t","p")
res = as.data.frame(res)
res[,4] = format.pval(res[,4], digits=3, eps=.05)
res$df = apoe4mod$df.residual
```

```{r}
summary(apoe4mod)
```

For those without the APOE4 gene, the estimated average score on the ACER was found to decrease by `r abs(res[2,1])` points with every year of age ($\beta = `r res[2,1]`$, $t(`r res[2,5]`) = `r res[2,3]`$, $p `r res[2,4]`$). A significant interaction between age and APOE4 status indicates that those _with_ the APOE4 gene have an decline by `r abs(res[4,1])` more for every year of age than those without the gene ($\beta = `r res[4,1]`$, $t(`r res[4,5]`) = `r res[4,3]`$, $p `r res[4,4]`$). This is visualised in @fig-intplotwrite.  

```{r}
#| label: fig-intplotwrite
#| fig-cap: "Decreases in cognition depend upon APOE4 genotype"
#| echo: false
interact_plot(apoe4mod, pred="age",modx="isAPOE4", interval=TRUE) + 
  labs(x = "- Age (years) - ", y = "ACER score")
```

`r solend()`

`r qbegin(9)`
It's important to emphasise that models are structures that _we_ design, then estimate from our data, and assess how well they fit.  

Here are three models:  
```{r}
cogapoe$isAPOE4 = ifelse(cogapoe$apoe4 == "none", "No", "Yes")
apoe4mod1 <- lm(acer ~ 1 + age, data = cogapoe)
apoe4mod2 <- lm(acer ~ 1 + age + isAPOE4, data = cogapoe)
apoe4mod3 <- lm(acer ~ 1 + age + isAPOE4 + age:isAPOE4, data = cogapoe)
```

And below are those three models plotted, with the data added.    
Which model corresponds to which plot?  

```{r}
#| echo: false
p1 <- ggplot(cogapoe,aes(x=age,y=acer))+
  geom_point(aes(fill=isAPOE4,col=isAPOE4), size=2.5, alpha=.4, shape=21) +
  geom_smooth(method=lm, lty="solid",
              col=sjplot_pal("hero")[1],fill=sjplot_pal("hero")[1]) +
  geom_smooth(method=lm, lty="dashed",
              col=sjplot_pal("hero")[2],fill=sjplot_pal("hero")[2]) +
  scale_fill_manual(values=sjplot_pal("hero")[1:2])+
  scale_color_manual(values=sjplot_pal("hero")[1:2])+
  labs(title="Predicted values of acer")+
  guides(col="none",fill="none")

p2 <- plot_model(apoe4mod2, type = "eff", terms = c("age", "isAPOE4 [No, Yes]"), show.data = TRUE)
p3 <- plot_model(apoe4mod3, type = "eff", terms = c("age", "isAPOE4 [No, Yes]"), show.data = TRUE)

((p3 | p1 )/ p2) + 
  plot_layout(guides="collect") & ylim(70,105) & theme_bw(base_size=11)
```

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The top left shows _non-parallel_ lines. It's an interaction! 
This is the model `acer ~ 1 + age + isAPOE4 + age:isAPOE4`.  

The top right shows no estimated differences in ACER scores between APOE4 positive and negative. This is probably the model `acer ~ 1 + age`, where APOE4 isn't in the model at all.  

The bottom shows two parallel lines. They differ in height, so there _is_ an estimated association between APOE4 and ACER scores. The lines being parallel means this will be the one _without_ an interaction `acer ~ 1 + age + isAPOE4`.  

:::imp
If you fit a model with an interaction in it, you don't always get non-parallel lines. If the interaction coefficient is 0 (and so non-significant), then the model would look like two parallel lines.    
:::



`r solend()`


# Personality and Social Rank

:::frame
__Data: scs_study.csv__  

Data from 656 participants containing information on scores on each trait of a Big 5 personality measure, their perception of their own social rank, and their scores on a measure of depression. Previous research has identified an association between an individual's perception of their social rank and symptoms of depression, anxiety and stress. We are interested in the individual differences in this relationship. 

> **Research Question:** After accounting for other personality traits, does the effect of social comparison on symptoms of depression, anxiety and stress vary depending on level of neuroticism?

The data is available at [https://uoepsy.github.io/data/scs_study.csv](https://uoepsy.github.io/data/scs_study.csv).  

The data in `scs_study.csv` contain seven attributes collected from a random sample of $n=656$ participants: 

- `zo`: Openness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `zc`: Conscientiousness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `ze`: Extraversion (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `za`: Agreeableness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `zn`: Neuroticism (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `scs`: Social Comparison Scale - An 11-item scale that measures an individual’s perception of their social rank, attractiveness and belonging relative to others. The scale is scored as a sum of the 11 items (each measured on a 5-point scale), with higher scores indicating more favourable perceptions of social rank. Scores range from 11 to 55.  
- `dass`: Depression Anxiety and Stress Scale - The DASS-21 includes 21 items, each measured on a 4-point scale. The score is derived from the sum of all 21 items, with higher scores indicating higher a severity of symptoms. Scores range from 21 to 84.  

`r optbegin("What does 'Z-scored' mean?", olabel=FALSE)`  
"Z-scoring" is a method of standardising a variable so that it has a mean of zero and a standard deviation of one. We actually saw this back in [2B#the-standard-normal-distribution](02b_sampling.html#the-standard-normal-distribution){target="_blank"}.  

To transform a given value $x_i$ into a __z-score__ $z_i$, we simply calculate the distance from $x_i$ to the mean, $\bar{x}$, and divide this by the standard deviation, $s$:    
$$
z_i = \frac{x_i - \bar{x}}{s}
$$

A Z-score of a value is the number of standard deviations below/above the mean that the value falls.  

`r optend()`
:::

`r qbegin(10)`
Read in the Social Comparison Study data and explore the relevant distributions and relationships between the variables of interest to the research question below.  

> After accounting for other personality traits, does the effect of social comparison on symptoms of depression, anxiety and stress vary depending on level of neuroticism?

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
summary(scs_study)
```

```{r}
p1 <- ggplot(data = scs_study, aes(x=dass)) + 
  geom_density() + 
  geom_boxplot(width = 1/50) +
  labs(title="Marginal distribution of DASS-21 Scores", 
       x = "Depression Anxiety and Stress Scale", y = "Probability density")

p2 <- ggplot(data = scs_study, aes(x=scs)) + 
  geom_density() + 
  geom_boxplot(width = 1/50) +
  labs(title="Marginal distribution of Social Comparison Scale (SCS) scores", 
       x = "Social Comparison Scale Score", y = "Probability density")

p3 <- ggplot(data = scs_study, aes(x=zn)) + 
  geom_density() + 
  geom_boxplot(width = 1/50) +
  labs(title="Marginal distribution of Neuroticism (Z-Scored)", 
       x = "Neuroticism (Z-Scored)", y = "Probability density")
library(patchwork)
p1 / (p2 + p3)
```

:::int 
The marginal distribution of scores on the Depression, Anxiety and Stress Scale (DASS-21) is unimodal with a mean of approximately `r round(mean(scs_study$dass))` and a standard deviation of `r round(sd(scs_study$dass))`. 

The marginal distribution of scores on the Social Comparison Scale (SCS) is unimodal with a mean of approximately `r round(mean(scs_study$scs))` and a standard deviation of `r round(sd(scs_study$scs))`. There look to be a number of outliers at the upper end of the scale. 

The marginal distribution of Neuroticism (Z-scored) is positively skewed, with the 25\% of scores falling below `r round(quantile(scs_study$zn, .25),2)`, 75\% of scores falling below `r round(quantile(scs_study$zn, .75),2)`.
:::


```{r}
p1 <- ggplot(data = scs_study, aes(x=scs, y=dass)) + 
  geom_point()+
  labs(x = "SCS", y = "DASS-21")

p2 <- ggplot(data = scs_study, aes(x=zn, y=dass)) + 
  geom_point()+
  labs(x = "Neuroticism", y = "DASS-21")

p1 / p2

library(knitr) # for making tables look nice
# the kable() function from the knitr package can make table outputs print nicely into html.
scs_study %>%
  select(dass, scs, zn) %>%
  cor %>% 
  kable
```

:::int
There is a weak, negative, linear relationship between scores on the Social Comparison Scale and scores on the Depression Anxiety and Stress Scale for the participants in the sample. Severity of symptoms measured on the DASS-21 tend to decrease, on average, the more favourably participants view their social rank.  
There is a weak, positive, linear relationship between the levels of Neuroticism and scores on the DASS-21. Participants who are more neurotic tend to, on average, display a higher severity of symptoms of depression, anxiety and stress.  
:::

`r solend()`

`r qbegin(11)`
Fit your model using `lm()`. 
Examine the coefficients, plot the relevant estimated associations to visually address the research question.  

:::hints
__Hints:__  

- there are sort of two bits to this:  
    - "after accounting for" (i.e., the stuff we learned in [8A#multiple-regression](08a_mlr.html){target="_blank"})
    - "the effect of this depends on that" (the interaction stuff from [9a#interactions](09a_interactions.html){target="_blank"}). 


:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
dass_mdl <- lm(dass ~ 1 + zo + zc + ze + za + scs*zn, data = scs_study)
summary(dass_mdl)
```

Recall that the coefficient `zn` from our model now reflects the estimated change in the outcome associated with an increase of 1 in neuroticism, _when the other variable (SCS) is zero_ (and holding constant other personality traits). 


```{r}
plot_model(dass_mdl, type = "int")
```

`r solend()`

`r qbegin(12)`
Think - what does "0" represent in each variable? And what does an increase of 1 represent? Are these meaningful?  

The `zn` variable is standardised, meaning that "0" is the mean, and moving from "0" to "1" is moving 1 standard deviation. 

I would argue that because "0" on `scs` variable is not a possible score that anyone can get, we may want to re-center this variable.  

Recenter the `scs` variable on either: 

- the minimum possible SCS score
- the mean SCS score

:::hints
__Hint:__  

- You can choose either min or mean. It won't change your model, it will just change what you get out of it.  
- You can see an example of mean-centering in [9A#getting-more-from-your-model](09a_interactions.html#getting-more-from-your-model){target="_blank"}.  

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
scs_study <-
  scs_study %>%
  mutate(
    scs_Cmean = scs - mean(scs),
    scs_Cmin = scs - min(scs)
  )
```

`r solend()`

`r qbegin(13)`
Re-fit your model using your re-centered SCS scores instead of the original variable.  

Fill in the blanks in the statements below. 

+ For those at the mean on all personality traits and scoring **???** on the SCS, the estimated DASS-21 Score is **???**  
+ For those who score **???** on the SCS, an increase of **???** in neuroticism is associated with a change of **???** in DASS-21 Scores, holding other personality traits constant. 
+ For those of average neuroticism, an increase of **???** on the SCS is associated with a change of **???** in DASS-21 Scores, holding other personality traits constant.   
+ For every increase of **???** in neuroticism, the change in DASS-21 associated with an increase of **???** on the SCS is asjusted by **???**
+ For every increase of **???** in SCS, the change in DASS-21 associated with an increase of **???** in neuroticism is asjusted by **???**
  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`


:::panelset

:::panel
#### Mean Centered

```{r}
dass_mdl2 <- lm(dass ~ 1 + scs_Cmean * zn, data = scs_study)
# pull out the coefficients from the summary():
summary(dass_mdl2)$coefficients
```


+ For those at the mean on all personality traits and scoring **average** on the SCS, the estimated DASS-21 Score is **`r round(coef(dass_mdl2)[1],2)`**  
+ For those who score **average (mean)** on the SCS, an increase of **1 standard deviation** in neuroticism is associated with a change of `r round(coef(dass_mdl2)[3],2)` in DASS-21 Scores, holding other personality traits constant. 
+ For those of average neuroticism, an increase of **1 point** on the SCS is associated with a change of `r round(coef(dass_mdl2)[2],2)` in DASS-21 Scores, holding other personality traits constant.   
+ For every increase of **1 standard deviation** in neuroticism, the change in DASS-21 associated with an increase of **1 point** on the SCS is asjusted by **`r round(coef(dass_mdl2)[4],2)`**
+ For every increase of **1 point** in SCS, the change in DASS-21 associated with an increase of **1 standard deviation** in neuroticism is asjusted by **`r round(coef(dass_mdl2)[4],2)`**
  
:::
:::panel
#### Minimum-centered

```{r}
dass_mdl2a <- lm(dass ~ 1 + scs_Cmin * zn, data = scs_study)
# pull out the coefficients from the summary():
summary(dass_mdl2a)$coefficients
```

+ For those at the mean on all personality traits and scoring **minimum** on the SCS, the estimated DASS-21 Score is **`r round(coef(dass_mdl2a)[1],2)`**  
+ For those who score **the minimum** on the SCS, an increase of **1 standard deviation** in neuroticism is associated with a change of `r round(coef(dass_mdl2a)[3],2)` in DASS-21 Scores, holding other personality traits constant. 
+ For those of average neuroticism, an increase of **1 point** on the SCS is associated with a change of `r round(coef(dass_mdl2a)[2],2)` in DASS-21 Scores, holding other personality traits constant.   
+ For every increase of **1 standard deviation** in neuroticism, the change in DASS-21 associated with an increase of **1 point** on the SCS is asjusted by **`r round(coef(dass_mdl2a)[4],2)`**
+ For every increase of **1 point** in SCS, the change in DASS-21 associated with an increase of **1 standard deviation** in neuroticism is asjusted by **`r round(coef(dass_mdl2a)[4],2)`**

:::

:::

`r solend()`


# Seasonal Extraversion

:::frame
__USMR 2022 Data__  

The data from the USMR 2022 survey (now closed) can be found at [https://uoepsy.github.io/data/usmr2022.csv](https://uoepsy.github.io/data/usmr2022.csv).  

_note, this is the survey data just from USMR **this** year, not other students on other courses or in previous years_

:::

`r qbegin(14)`
We want to make a variable that indicates what season each person was born in.  

We have seen the `ifelse()` command a few of times now, but something we are yet to see is that we can have _nested_ statements. So we could turn everybody's birth-months into a season by doing something like the below, where we ask "if it's january then it's winter, else if it's february then it's winter, else if it's march..." and so on.  
```{r}
#| eval: false
usmrdata <- read_csv("https://uoepsy.github.io/data/usmr2022.csv")
usmrdata %>% mutate(
  season = ifelse(birthmonth == "jan", "winter",
                  ifelse(birthmonth == "feb", "winter",
                         ifelse(birthmonth == "mar", "spring"
                                ifelse( ..... 
)
```

We can make this a little more efficient by use of the `%in%` operator, which allows us to ask "if it's _one of_ [december, january, february] then it's winter, else... ".  
```{r}
#| eval: false
usmrdata %>% mutate(
  season = ifelse(birthmonth %in% c("dec","jan", "feb"), "winter",
                  ifelse(birthmonth %in% c("mar","apr","may"), "spring",
                         ...
                         ...
)
```

Complete the code above in order to make a column that assigns the correct birth-season based on their month of birth.  

:::hints
__Hints:__  

- you don't have to write out the last "if", because you can capture those in the final "else" bit.  
- if you want a more tidyverse way to do this then look up the help docs for the `case_when()` function. 

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
usmrdata <- read_csv("https://uoepsy.github.io/data/usmr2022.csv")

usmrdata <- 
  usmrdata %>% 
  mutate(
    season = ifelse(birthmonth %in% c("dec","jan", "feb"), "winter",
                  ifelse(birthmonth %in% c("mar","apr","may"), "spring",
                         ifelse(birthmonth %in% c("jun","jul","aug"), "summer",
                                "autumn")))
  )
```

The tidyverse way:  
```{r}
usmrdata <- 
  usmrdata %>% 
  mutate(
    season = case_when(
      birthmonth %in% c("dec","jan", "feb") ~ "winter",
      birthmonth %in% c("mar","apr","may") ~ "spring",
      birthmonth %in% c("jun","jul","aug") ~ "summer",
      TRUE ~ "autumn"
    )
  )
```


`r solend()`

`r qbegin(15)`
Make the season variable that you just created a factor.  

We would like you to do the following, in order: 

1. fit a model `lm(extraversion ~ season)`, and assign it the name `seasonmod1`.  
2. relevel the season variable to have "spring" as the reference level. 
3. fit the same model, this time assigning it the name `seasonmod2`
4. set the contrasts for the season variable to be "sum contrasts"
5. fit the same model, this time assigning it the name `seasonmod3`

:::hints
__Hints:__  

- for relevelling factors, we've seen this in [9A#getting-more-from-your-model](09a_interactions.html#getting-more-from-your-model){target="_blank"}. 
- for "sum contrasts" and how to set them, see [9B#sum-contrasts](09b_contrasts.html#sum-contrasts){target="_blank"}. 


:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Make it a factor:
```{r}
usmrdata$season <- factor(usmrdata$season)
```

fit model 1
```{r}
seasonmod1 <- lm(extraversion ~ season, data = usmrdata)
```

relevel to have spring as the reference:
```{r}
usmrdata$season <- fct_relevel(usmrdata$season, "spring")
```

fit model again (model 2)
```{r}
seasonmod2 <- lm(extraversion ~ season, data = usmrdata)
```

here are the current contrasts:
```{r}
contrasts(usmrdata$season)
```
set them to sum contrasts:
```{r}
contrasts(usmrdata$season) <- "contr.sum"
```
and take a look
```{r}
contrasts(usmrdata$season)
```

fit the model _again_ (model 3)
```{r}
seasonmod3 <- lm(extraversion ~ season, data = usmrdata)
```

`r solend()`

`r qbegin(16)`
Calculate the mean extraversion of people born in each season. From these, also calculate the mean of those four means.  

:::hints
__Hints:__  


- you can use the `data %>% group_by %>% summarise` logic here.  
- to calculate a mean of means, how about `data %>% group_by %>% summarise %>% summarise`! 


:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
usmrdata %>%
  group_by(season) %>%
  summarise(meanEx = mean(extraversion))

usmrdata %>%
  group_by(season) %>%
  summarise(meanEx = mean(extraversion)) %>%
  summarise(grandmeanEx = mean(meanEx))
```


`r solend()`

`r qbegin(17)`
Because there aren't any other predictors in our model, our coefficients from all three models we fitted in question 15 are going to be various comparisons between the numbers we calculated in question 16

For every coefficient estimate in the three models, write down what difference it is estimating, and check it against the numbers you calculated in question 16

```{r}
coef(seasonmod1)
coef(seasonmod2)
coef(seasonmod3)
```

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

:::panelset

:::panel
#### seasonmod1
```{r}
coef(seasonmod1)
```

For the first model, the ordering of the levels is alphabetical, and so "autumn" is the reference level. This means the intercept is the extraversion of autumn-babies, and each coefficient is the other months compared to autumn:  

```{r}
usmrdata %>%
  group_by(season) %>%
  summarise(meanEx = mean(extraversion)) %>%
  mutate(compare = meanEx - meanEx[season=="autumn"])
```

:::

:::panel
#### seasonmod2 
```{r}
coef(seasonmod2)
```

For the second model, we changed it so that spring is the reference level, which means spring is the intercept, and the coefficients are each level compared to spring:  

```{r}
usmrdata %>%
  group_by(season) %>%
  summarise(meanEx = mean(extraversion)) %>%
  mutate(compare = meanEx - meanEx[season=="spring"])
```

:::

:::panel
#### seasonmod3
```{r}
coef(seasonmod3)
```

For the final model it is a little more difficult. We're using sum contrasts, so the intercept is going to be the 'grand mean' (mean of season means), which we calculated in question 16 was `r usmrdata %>% group_by(season) %>% summarise(m = mean(extraversion)) %>% pull(m) %>% mean %>% round(2)`.  

We know that the coefficients are going to be comparisons between each season and the grand mean. But (annoyingly) they no longer have useful names - the coefficients are just `season1` and `season2` (whatever they are).  

To see it more clearly, we can look at our contrast matrix: 
```{r}
contrasts(usmrdata$season)
```

We can see from the 1s that our coefficients are going to be spring vs the grand mean, autumn vs the grand mean and summer vs the grand mean.  

```{r}
usmrdata %>%
  group_by(season) %>%
  summarise(meanEx = mean(extraversion)) %>%
  mutate(compare = meanEx - 29.85)
```

:::

:::

`r solend()`

