---
title: "Exercises: Chi-Square"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(xaringanExtra)
xaringanExtra::use_panelset()
set.seed(017)
```

# Chi-Square Tests

## Birth-Months

:::frame

> **Research Question:** Are people more likely to be born in certain months than others?  

As you may remember, in the survey we asked students to complete in welcome week, one of the questions concerned the month in which you were born. You can download the data from [https://uoepsy.github.io/data/surveydata_historical.csv](https://uoepsy.github.io/data/surveydata_historical.csv).  

```{r}
survey_data <- 
  read_csv("https://uoepsy.github.io/data/surveydata_historical.csv")
```
:::

`r qbegin(1)`
What is your intuition about the distribution of all students' birth-months?  
Do you think they will be spread uniformly across all months of the year (like a fair 12-sided dice), or do you think people are more likely to be born in certain months more than others?  

Plot the distribution and get an initial idea of how things are looking.  

::: {.callout-tip collapse="true"}
#### Hints  
You can do this quickly with `barplot()` and `table()`, or you could create try using `ggplot()` and looking into `geom_bar()`.  
:::
`r qend()`

`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`
The quick and dirty way to plot:
```{r}
barplot(table(survey_data$birthmonth))
```

A ggplot option:
```{r}
ggplot(data = survey_data, aes(x = birthmonth)) +
    geom_bar() +
    labs(x = "- Birth Month -")
```
`r solend()`

`r qbegin(2)`
We're going to perform a statistical test to assess the extent to which our data conforms to the hypothesis that people are no more likely to be born on one month than another.  

Under this hypothesis, what would be the proportional breakdown of observed births in each of the months?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
If people are no more likely to be born in one month than another, then we would expect the same proportion of observed births in each month.  
There are 12 months, so we would expect $\frac{1}{12}$ observations in each month.  

We can write these as: 
$$
\begin{align}
& p_{jan} = 1/12 \\
& p_{feb} = 1/12 \\
& ... \\
& p_{dec} = 1/12 \\
\end{align}
$$
`r solend()`

`r qbegin(3)`

How many observations in our sample would we *expect* to find with a birthday in January? And in February? ... and so on?  

::: {.callout-tip collapse="true"}
#### Hints

How many responses (i.e. not missing values) do we have for this question?  

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
There are `r sum(!is.na(survey_data$birthmonth))` people who have non-NA values (`sum(!is.na(survey_data$birthmonth))`).  

Under the null hypothesis, we would expect $\frac{1}{12} \times$ `r sum(!is.na(survey_data$birthmonth))` = `r round(sum(!is.na(survey_data$birthmonth))/12,2)` observations born in each month. 
`r solend()`

`r qbegin(4)`
The code below creates counts for each month. Before doing that, it removes the rows which have an NA in them for birthmonth: 
```{r}
#| eval: false
survey_data %>%
  filter(!is.na(birthmonth)) %>%
  group_by(birthmonth) %>%
  summarise(
      observed = n()
  )
```
(A shortcut for this would be `survey_data %>% filter(!is.na(birthmonth)) %>% count(birthmonth)`)  

Add to the code above to create columns showing:

- the expected counts $E_i$
- observed-expected ($O_i - E_i$)
- the squared differences $(O_i - E_i)^2$
- the standardised square differences $\frac{(O_i - E_i)^2}{E_i}$  

Then calculate the $\chi^2$ statistic (the sum of the standardised squared differences).  
If your observed counts matched the expected counts *perfectly*, what would the $\chi^2$ statistic be? 

::: {.callout-tip collapse="true"}
#### Hints  
This was all done in the step-by-step example of a $\chi^2$ test in [4A #chi2-goodness-of-fit-test](04a_chisq.html#chi2-goodness-of-fit-test){target="_blank"}
:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
chi_table <- 
    survey_data %>%
    filter(!is.na(birthmonth)) %>%
    group_by(birthmonth) %>%
    summarise(
        observed = n(),
        expected = sum(!is.na(survey_data$birthmonth))/12,
        diff = observed-expected,
        sq_diff = diff^2,
        std_sq_diff = sq_diff / expected
    )
chi_table
```

And we can calculate our $\chi^2$ test statistic by simply summing the values in the last column we created:
```{r}
sum(chi_table$std_sq_diff)
```

If all our observed counts are equal to our expected counts, then the `diff` column above will be all $0$, and $0^2=0$, and $\frac{0}{E_i}$ will be $0$. So $\chi^2$ will be $0$. 
`r solend()`

`r qbegin(5)`
You can see the distribution of $\chi^2$ statistics with different degrees of freedom below.  
```{r}
#| label: fig-chidist
#| echo: false
#| fig.cap: "Chi-Square Distributions"
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/1200px-Chi-square_pdf.svg.png")
```

We can find out the proportion of the distribution which falls to either side of a given value of $\chi^2$ using `pchisq()`. We need to give it our calculated $\chi^2$ statistic, our degrees of freedom (`df`), which is equal to the number of categories minus 1. We also need to specify whether we want the proportion to the left (`lower.tail=TRUE`) or to the right (`lower.tail=FALSE`).  

1. Using `pchisq()`, calculate the probability of observing a $\chi^2$ statistic as least as extreme as the one we have calculated.  
2. Check that these results match with those provided by R's built-in function: `chisq.test(table(survey_data$birthmonth))` (the `table` function will ignore NAs by default, so we don't need to do anything extra for this).    

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
sum(chi_table$std_sq_diff)
pchisq(sum(chi_table$std_sq_diff), df = 11, lower.tail = FALSE)
```

```{r}
chisq.test(table(survey_data$birthmonth))
```
`r solend()`

`r qbegin(6)`
Which months of year had the highest contributions to the chi-square test statistic?  

::: {.callout-tip collapse="true"}
#### Hints  
Think about your standardised squared deviations. 
:::
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
**Standardized squared deviations**  

One possible way to answer this question is to look at the individual contribution of each category to the $\chi^2$ statistic. We computed these values in an earlier question.  
```{r}
chi_table %>%
  select(birthmonth, std_sq_diff)
```

From the barplot we created earlier on, we can see which months make up higher/lower proportions than expected:
```{r}
ggplot(chi_table, aes(x = birthmonth, y = observed/nrow(survey_data))) +
  geom_col(fill = 'lightblue') +
  geom_hline(yintercept = 1/12, color = 'red') +
  theme_classic(base_size = 15)
```

**Pearson residuals**

Equivalently, you could answer by looking at Pearson residuals:
```{r}
chisq.test(table(survey_data$birthmonth))$residuals
```

```{r}
#| echo: false
presids <- chisq.test(table(survey_data$birthmonth))$residuals
presids <- presids[order(abs(presids),decreasing = TRUE)]
```

The greatest *absolute* values are for `r names(presids)[1]` and `r names(presids)[2]`, showing that for these months the deviations from expected to observed were the greatest. 

`r solend()`

## Eye-Colours

:::frame
> **Research Question:** Do the proportions of people with different eye-colours correspond to those suggested by the internet? 

According one part of the internet (that reliable source of information!), 76% of people in the world have brown eyes, 10% have blue, 5% hazel, 5% amber, 2% green, 1% grey, and 1% have some other eye colouring (red/violet/heterochromia).

We'll use the same data from the course survey's here:
```{r}
survey_data <- 
  read_csv("https://uoepsy.github.io/data/surveydata_historical.csv")
```
:::


`r qbegin(7)`
Perform a $\chi^2$ goodness of fit test to assess the extent to which our sample of students conform to this theorised distribution of eye-colours.  

No need to do this manually - once is enough. Just go straight to using the `chisq.test()` function.  

::: {.callout-tip collapse="true"}
#### Hints  
Try using `chisq.test(..., p = c(?,?,?,...) )`.  
We saw this in the example goodness of fit test, [4A #example](04a_chisq.html#example){target="_blank"}  
:::
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Let's look at the observed counts:
```{r}
table(survey_data$eyecolour)
```
Our theoretical probabilities of different eye colours must match the order in the table which we give `chisq.test()`. They must also always sum to 1.  
```{r}
#| warning: true
#| message: true
chisq.test(table(survey_data$eyecolour), p = c(.05,.1,.76,.02,.01,.05,.01))
```

Note, we get a warning here of <span style="color:blue">"Chi-squared approximation may be incorrect"</span>. This is because some of the expected cell counts are <5.  
```{r}
#| warning: false
#| message: false
chisq.test(table(survey_data$eyecolour), 
           p = c(.05,.1,.76,.02,.01,.05,.01))$expected
```

There are a couple of options here, but the easiest is to use the functionality of `chisq.test()` that allows us to compute the p-value by using a simulation (similar to the idea we saw in [2B#sampling-&-sampling-distributions](02b_sampling.html#sampling-sampling-distributions){target="_blank"}), rather than by comparing it to a theoretical $\chi^2$ distribution.  We can do this by using:  

```{r}
chisq.test(table(survey_data$eyecolour), p = c(.05,.1,.76,.02,.01,.05,.01),
           simulate.p.value = TRUE)
```

`r solend()`

`r qbegin(8)`
What are the *observed* proportions of our sample with each eyecolour?  


::: {.callout-tip collapse="true"}
#### Hints

Look up the `prop.table()` function?  

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
From the help documentation (`?prop.table()`), we see that we can pass `prop.table()` the argument `x`, which needs to be a table. 
```{r}
prop.table(table(survey_data$eyecolour))*100
```

```{r}
barplot(prop.table(table(survey_data$eyecolour))*100)
```
`r solend()`

<div class="divider div-transparent div-dot"></div>

## Jokes and Tips


:::frame
__Data: TipJokes__  

> **Research Question:** Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?  

A [study](https://doi.org/10.1111/j.1559-1816.2002.tb00266.x) published in the Journal of Applied Social Psychology^[Gueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. _Journal of Applied Social Psychology, 32_(9), 1955-1963.] investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. 
The waiter randomly assigned coffee-ordering customers to one of three groups. 
When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all. 

The data are available at https://uoepsy.github.io/data/TipJoke.csv.  
The dataset contains the variables:

- `Card`: None, Joke, Ad.
- `Tip`: 1 = The customer left a tip, 0 = The customer did not leave tip. 

:::


`r qbegin(9)`
Produce a plot and a table to display the relationship between whether or not the customer left a tip, and what (if any) card they received alongside the bill.  

Don't worry about making it all pretty. Mosaic plots in R are a bit difficult. 

::: {.callout-tip collapse="true"}
#### Hints  
`plot(table(...))` will give you something. You can see one in the example $\chi^2$ test of independence,[4A #example-1](04a_chisq.html#example-1){target="_blank"}.  
:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
tipjoke <- read_csv('https://uoepsy.github.io/data/TipJoke.csv')

table(tipjoke$Card, tipjoke$Tip)

plot(table(tipjoke$Card, tipjoke$Tip))
```
`r solend()`

`r qbegin(10)`
What would you *expect* the cell counts to look like if there were no relationship between what the waiter left and whether or not the customer tipped?  


::: {.callout-tip collapse="true"}
#### Hints

Think about what proportion of customers tipped. 
Then work out how many customers got each type of card. If there were no relationship, then the proportions would be the same in each group. 

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
In total, 60 customers tipped (14+30+16), and 151 did not. So *overall*, 0.28 ($\frac{60}{(60+151)}$) of customers tip.  

74 customers got an Ad card, 72 customers got a Joke, and 65 got None. If this were independent of whether or not they left a tip, we would expect equal proportions of tippers in each group.  
So we would expect 0.28 of each group to leave a tip.  

You can think about observed vs expected by looking at the two-way table along with the marginal row and column totals given:   
```{r}
#| echo: false
table(tipjoke$Card, tipjoke$Tip) %>%
  rbind(colSums(.)) %>% 
  cbind(rowSums(.)) -> conttable
conttable[1:3,1:2]<-"  "
conttable %>% knitr::kable(.) %>%
  kableExtra::kable_styling()
```
For a given cell of the table we can calculate the expected count as $\text{row total} \times \frac{\text{column total}}{\text{samplesize}}$:

**Expected:**
```{r}
#| echo: false
table(tipjoke$Card, tipjoke$Tip) %>%
  rbind(colSums(.)) %>% 
  cbind(rowSums(.)) -> conttable
conttable[1:3,1]<-round(conttable[1:3,3]*(conttable[4,1])/conttable[4,3],2)
conttable[1:3,2]<-round(conttable[1:3,3]*(conttable[4,2])/conttable[4,3],2)
conttable %>% knitr::kable(.) %>%
  kableExtra::kable_styling()
```


If you're wondering how we do this in R.. 
here's our table:  
```{r}
t <- tipjoke %>%
  select(Card, Tip) %>% table()
t
```
Here are the row totals:  
```{r}
rowSums(t)
```
and column totals divided by total:
```{r}
colSums(t) / sum(t)
```

there's a complicated bit of code using `%o%` which could do this for us. You don't need to remember %o%, it's very rarely used): 
```{r}
e <- rowSums(t) %o% colSums(t) / sum(t)
e
```

Or, alternatively, do it one by one:  
```{r}
rowSums(t) * (colSums(t) / sum(t))[1]
rowSums(t) * (colSums(t) / sum(t))[2]
```

`r solend()`

`r qbegin(11)`
Just like we gave the `chisq.test()` function a table of observed frequencies when we conducted a goodness of fit test in earlier exercises, we can give it a two-way table of observed frequencies to conduct a test of independence.  

Try it now.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
chisq.test(table(tipjoke$Card, tipjoke$Tip))
```
`r solend()`


<div class="divider div-transparent div-dot"></div>

# Some RMarkdown

```{r}
#| include: false
pass_scores <- read_csv("https://uoepsy.github.io/data/pass_scores.csv")
res2<-t.test(pass_scores$PASS, mu = 33, alternative = "less")
```


Using one of the $t$-tests we saw in the previous week's exercises, we can use an RMarkdown document in which we write our results so that they get compiled to look nice and pretty:  

::: {.panelset}

::: {.panel}
#### Writing this

```{r echo=FALSE, out.width="1000px"}
knitr::include_graphics("images/hypothesis/rmarkdownbacktick.png")
```

:::
::: {.panel}
#### Compiles to this

:::frame
A one-sided one-sample t-test was conducted in order to determine if the average score on the Procrastination Assessment Scale for Students (PASS) for a sample of `r nrow(pass_scores)` students at Edinburgh University was significantly lower ($\alpha = .05$) than the average score obtained during development of the PASS. 

Edinburgh University students scored lower (Mean = `r mean(pass_scores$PASS) %>% round(2)`, SD = `r sd(pass_scores$PASS) %>% round(2)`) than the score reported by the authors of the PASS (Mean = 33). This difference was statistically significant (t(`r nrow(pass_scores)-1`)=`r res2$statistic %>% round(2)`, p < .05, one-tailed).  
:::

:::

:::

This is one of the huge benefits of RMarkdown. Imagine we collected more data - we wouldn't have to edit all the results, we could simply recompile and they would update for us!  
Note how it works:  

  - the code chunk saves the results of the `t.test()` function as a named object `res2`.
  - in text, the backticks <code>&grave;r ... ... ... &grave;</code> are used to execute small bits of R code, and include the output within the text. For instance, the line <code>&grave;r res2\$statistic %>% round(2)&grave;</code> gets the t-statistic from the results, and rounds it to 2 decimal places, which get's printed out as `r res2[["statistic"]] %>% round(2)`. 
  - the bits between the dollar signs, e.g. \$\\alpha\$ will get printed as mathematical symbols such as $\alpha$. 



::: {.callout-important collapse="true"}
#### RMarkdown documents are self-contained.

You need to to put **everything** that is needed to reproduce your analysis **in the correct order**.  

For instance, if you have used the console (bottom left window) to define an object `peppapig <- 30`, you will have an object in your environment (top right window) called "peppapig" which has the value 30.  

If you were to refer to that object in your RMarkdown document, you will be able to run a line of code such as `peppapig/10` because it will find the "peppapig" object in **your** environment. __BUT__ you won't be able to compile the document because it "starts fresh" (i.e., compiles within its own environment). In order for it to compile, you would need to *define* what "peppapig" is **inside** your document, and **before** the document then refers to it. 

The same applies with using functions in from packages. The RMarkdown document needs to know what packages to load before it uses functions from them. Just because *you* yourself have loaded a package in your session, it does not mean the compilation process for your RMarkdown has access to it. 

:::

If you want some extra explanations on these aspects of RMarkdown, then please see Lessons 0-3 of our [Rmd-bootcamp](https://uoepsy.github.io/scs/rmd-bootcamp/){target="_blank"}.  

`r qbegin(12)`
Can you create an RMarkdown document which:

1. Reads in the [https://uoepsy.github.io/data/TipJoke.csv](https://uoepsy.github.io/data/TipJoke.csv) data.
2. Conducts and reports a $\chi^2$ test of independence examining whether telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer.
3. Successfully compiles ("knits") into an **.html** file. 

`r qend()`


<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>