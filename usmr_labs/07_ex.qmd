---
title: "Week 7 Exercises: Linear Regression"
link-citations: yes
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---


```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
set.seed(017)
survey_data <- read_csv("https://uoepsy.github.io/data/surveydata_allcourse.csv") 
```


# Income and Education

:::frame
> **Research Question:** is there an association between the level of education and an employee's income? 

__Data: riverview.csv__  

Our data for these exercises is from an hypothetical study into income disparity for employees in a local authority. The riverview data, which come from [Lewis-Beck, 2015](https://methods.sagepub.com/book/applied-regression-an-introduction-second-edition), contain five attributes collected from a random sample of $n=32$ employees working for the city of Riverview, a hypothetical midwestern city in the US. The attributes include:  

- `education`: Years of formal education
- `income`: Annual income (in thousands of U.S. dollars)
- `seniority`: Years of seniority
- `gender`: Employee's gender
- `male`: Dummy coded gender variable (0 = Female, 1 = Male)
- `party`: Political party affiliation

The first six rows of the data are:

```{r echo=FALSE}
library(tidyverse)
library(kableExtra)

riverview <- read_csv('https://uoepsy.github.io/data/riverview.csv')
kable(head(riverview), align='c') %>% kable_styling(full_width = FALSE)
```

The data is available at [https://uoepsy.github.io/data/riverview.csv.](https://uoepsy.github.io/data/riverview.csv){target="_blank"}

:::


`r qbegin(1)`  
Load the required libraries (probably just __tidyverse__ for now), and read in the riverview data to your R session.   

Let us first visualise and describe the *marginal distributions* of those variables which are of interest to us. 
These are the distribution of each variable (employee incomes and education levels) *without* reference to the values of the other variables.

:::hints
__Hints:__  

- You could use, for example, `geom_density()` for a density plot or `geom_histogram()` for a histogram.
- Look at the shape, center and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal? 
- Do you notice any extreme observations?

:::
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

```{r, warning=FALSE, message=FALSE}
library(tidyverse)

riverview <- read_csv("https://uoepsy.github.io/data/riverview.csv")
head(riverview)
```


We can plot the marginal distribution of employee incomes as a density curve, and add a boxplot underneath to check for the presence of outliers. The `width` of the geom_boxplot() is always quite wide, so I want to make it narrower so that we can see it at the same time as the density plot. Deciding on the exact value for the width here is just trial and error:

```{r}
#| label: fig-incomeplot
#| fig.cap: "Density plot and boxplot of employee incomes and education levels"

library(patchwork)
# the patchwork library allows us to combine plots together
ggplot(data = riverview, aes(x = income)) +
  geom_density() +
  geom_boxplot(width = 1/300) +
  labs(x = "Income (in thousands of U.S. dollars)", 
       y = "Probability density") +

ggplot(data = riverview, aes(x = education)) +
  geom_density() +
  geom_boxplot(width = 1/100) +
  labs(x = "Education (in years)", 
       y = "Probability density")

```

The plots suggests that the distributions of employee incomes and education levels are both unimodal. Most of the incomes are between roughly \$45,000 and \$70,000, and most people have between 12 and 20 years of education. Furthermore, the boxplot does not highlight any outliers in either variable.  

To further summarize a distribution, it is typical to compute and report numerical summary statistics such as the mean and standard deviation.  

As we have seen, in earlier weeks, one way to compute these values is to use the `summarise()/summarize()` function from the `tidyverse` library:

```{r}
riverview %>% 
  summarize(
    mean_income = mean(income), 
    sd_income = sd(income),
    mean_edu = mean(education),
    sd_edu = sd(education)
    )
```

:::int
The marginal distribution of income is unimodal with a mean of approximately \$53,700. There is variation in employees' salaries (SD = \$14,553).  
The marginal distribution of education is unimodal with a mean of 16 years. There is variation in employees' level of education (SD = 4.4 years).  
:::
`r solend()`

`r qbegin(2)`  
After we've looked at the marginal distributions of the variables of interest in the analysis, we typically move on to examining *relationships* between the variables.  
  
Visualise and describe the relationship between income and level of education among the employees in the sample.  


:::hints
__Hints:__  
Think about:  

- *Direction* of association
- *Form* of association (can it be summarised well with a straight line?)  
- *Strength* of association (how closely do points fall to a recognizable pattern such as a line?)
- *Unusual observations* that do not fit the pattern of the rest of the observations and which are worth examining in more detail.  
:::
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
Because we are investigating how income varies when varying years of formal education, income here is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).

```{r}
#| label: fig-riverview-scatterplot
#| fig.cap: 'The relationship between employees education level and income.'
ggplot(data = riverview, aes(x = education, y = income)) +
  geom_point(alpha = 0.5) +
  labs(x = "Education (in years)", 
       y = "Income (in thousands of U.S. dollars)")
```

There appears to be a strong positive linear relationship between education level and income for the employees in the sample.
High incomes tend to be observed, on average, with more years of formal education.
The scatterplot does not highlight any outliers.

To comment numerically on the strength of the linear association we might compute the correlation coefficient that we were introduced to in [5A: Covariance & Correlation](05a_covcor.html)
```{r}
riverview %>%
  select(education, income) %>%
  cor()
```

that is, $r_{\text{education, income}} = 0.79$

`r solend()`

`r qbegin(3)`
Using the `lm()` function, fit a linear model to the sample data, in which employee income is explained by education level. Assign it to a name to store it in your environment.  

:::hints
__Hints:__  
You can see how to fit linear models in R using `lm()` in [7A #fitting-linear-models-in-r](07a_slr.html#fitting-linear-models-in-r){target="_blank"}
:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
As the variables are in the `riverview` dataframe, we would write:  
```{r}
model1 <- lm(income ~ 1 + education, data = riverview)
```
`r solend()`

`r qbegin(4)`
Interpret the estimated intercept and slope in the context of the question of interest.  

:::hints
__Hints:__  
We saw how to extract lots of information on our model using `summary()` (see [7A #model-summary](07a_slr.html#model-summary){target="_blank"}), but there are lots of other functions too.  

If we called our linear model object "model1" in the environment, then we can use:  

- type `model1`, i.e. simply invoke the name of the fitted model;
- type `model1$coefficients`;
- use the `coef(model1)` function;
- use the `coefficients(model1)` function;
- use the `summary(model1)$coefficients` to extract just that part of the summary.

:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
coef(model1)
```
From this, we get that the fitted line is:
$$
\widehat{Income} = 11.32 + 2.65 \ Education \\
$$

We can interpret the estimated intercept as:

:::int
The estimated average income associated with zero years of formal education is \$11,321.
:::

For the estimated slope we get:

:::int
The estimated increase in average income associated with a one year increase in education is \$2,651.
:::
`r solend()`


`r qbegin(5)`
Test the hypothesis that the population slope is zero --- that is, that there is no linear association between income and education level in the population.  

:::hints
__Hints:__  
You don't need to *do* anything for this, you can find all the necessary information in `summary()` of your model.  
See [7A #inference-for-regression-coefficients](07a_slr.html#inference-for-regression-coefficients){target="_blank"}.
:::

`r qend()`
`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The information is already contained in the row corresponding to the variable "education" in the output of `summary()`, which reports the t-statistic under `t value` and the p-value under `Pr(>|t|)`:
```{r}
summary(model1)
```

Recall that the p-value `5.56e-08` in the `Pr(>|t|)` column simply means $5.56 \times 10^{-8}$. This is a very small value, hence we will report it as <.001 following the APA guidelines.

:::int
A significant association was found between level of education (in years) and income, with income increasing by on average \$2,651 for every additional year of education ($b = 2.65$, $SE = 0.37$, $t(30)=7.17$, $p<.001$). 
:::

`r solend()`



`r qbegin(6)`
What is the proportion of the total variability in incomes explained by the linear relationship with education level?

:::hints
**Hints:**  

- The question asks to compute the value of $R^2$, but you might be able to find it already computed somewhere (so much stuff is already in `summary()` of the model.
- See [7A #r2](07a_slr.html#r2){target="_blank"} if you're unsure about what $R^2$ is.    

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
For the moment, ignore "Adjusted R-squared". We will come back to this later on. 
```{r}
summary(model1)
```

:::int
Approximately 63\% of the total variability in employee incomes is explained by the linear association with education level.
:::

`r solend()`

`r qbegin(7)`
Look at the output of `summary()` of your model. Identify the relevant information to conduct an F-test against the null hypothesis that the model is ineffective at predicting income using education level.  

:::hints
__Hints:__  
[7A #the-f-statistic](http://localhost:3288/07a_slr.html#the-f-statistic){target="_blank"} will help.  
:::

`r qend()`
`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
summary(model1)
```

From the `summary(model1`), the relevant row is just below the $R^2$, where it states: 

```
F-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08
```

:::int
The overall test of model utility was significant $F(1, 30) = 51.45, p < .001$, indicating evidence against the null hypothesis that the model is ineffective (that education is not a useful predictor of income). 
:::

`r solend()`


`r qbegin(8)`
Create a visualisation of the estimated association between income and education.  


:::hints
__Hints:__
Check [7A#example](07a_slr.html#example){target="_blank"}, which shows an example of using the __sjPlot__ package to create a plot.  
:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
#| message: false
#| warning: false
library(sjPlot)
plot_model(model1, type="eff")
```

`r solend()`


`r qbegin("Optional: Question 9A", qlabel = FALSE)`
Consider the following:  

1. In fitting a linear regression model, we make the assumption that the errors around the line are normally distributed around zero (this is the $\epsilon \sim N(0, \sigma)$ bit.)  
2. About 95\% of values from a normal distribution fall within two standard deviations of the centre.  

We can obtain the estimated standard deviation of the errors ($\hat \sigma$) from the fitted model using `sigma()` and giving it the name of our model.  
What does this tell us?  

:::hints
__Hints:__  
See [7A #the-error](07a_slr.html#the-error){target="_blank"}.
:::

`r qend()`
`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The estimated standard deviation of the errors can be obtained by:

```{r}
sigma(model1)
```

:::int
For any particular level of education, employee incomes should be distributed above and below the regression line with standard deviation estimated to be $\hat \sigma = 8.98$. 
Since $2 \hat \sigma = 2 (8.98) = 17.96$, we expect most (about 95\%) of the employee incomes to be within about \$18,000 from the regression line.
:::  

`r solend()`

`r qbegin("Optional: Question 9B", qlabel=FALSE)`
Compute the model-predicted income for someone with 1 year of education.  

:::hints
__Hints:__   
Given that you know the intercept and slope, you can calculate this algebraically. However, try to also use the `predict()` function (see [7A#model-predictions](07a_slr.html#model-predictions){target="_blank}). 
:::

`r qend()`
`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
Using `predict()`, we need to give it our model, plus some new data which contains a person with only 1 year of education.  First we make a new dataframe with an education variable, with one entry which has the value 1, and then we give that to `predict()`:  
```{r}
education_query <- data.frame(education = c(1))
predict(model1, newdata = education_query)
```

Given that our fitted model takes the form: 

$$
Income = 11.32 + 2.65\cdot Education
$$

We are asking what the predicted income is for someone with 1 year of education. So we can substitute in "1" for the Education variable:
$$
\begin{align}
Income &= 11.32 + 2.65\cdot Education \\
Income &= 11.32 + 2.65\cdot 1 \\
Income &= 11.32 + 2.65\\
Income &= 13.97 \\
\end{align}
$$

`r solend()`

<div class="divider div-transparent div-dot"></div>


# Income and Degree Status

Let's suppose instead of having measured education in years, researchers simply asked "have you had more than 18 years of education?", assuming this to be indicative of having a degree.  
The code below creates a this new variable for us:  
```{r}
riverview <- riverview %>%
  mutate(
    degree = ifelse(education >= 18, "yes","no")
  )
```

`r qbegin(10)`

Fit the following model, and interpret the coefficients.  

$$
Income = b_0 + b_1 \ Degree + \varepsilon
$$

:::hints
__Hints:__   
For help interpreting the coefficients, see [7A #binary-predictors](07a_slr.html#binary-predictors){target="_blank"}.  
:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
model2 <- lm(income ~ degree, data = riverview)
summary(model2)
```
- `(Intercept)` = the estimated income of employees without a degree ($46,083$)
- `degreeyes` = the estimated change in income from employees without a degree to employees with a degree 

```{r}
ggplot(riverview, aes(x = degree, y = income)) +
  geom_boxplot()
```
`r solend()`

`r qbegin(11)`
We've actually already seen a way to analyse questions of this sort ("is the average income different between those with and those without a degree?")  

Run the following t-test, and consider the statistic, p value etc. How does it compare to the model in the previous question?  
```{r}
t.test(income ~ degree, data = riverview, var.equal = TRUE)
```
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
It is identical! the $t$-statistics are the same, the p-values are the same, the degrees of freedom. Everything!  

The two sample t-test is actually just a special case of the linear model, where we have a numeric outcome variable and a binary predictor! 

__And...__ the one-sample t-test is the linear model without any predictors, so just with an intercept.  
```{r}
t.test(riverview$income, mu = 0)
interceptonly_model <- lm(income ~ 1, data = riverview)
summary(interceptonly_model)$coefficients
```


`r solend()`

<div class="divider div-transparent div-dot"></div>

# Student Sleep Quality

:::frame
In the survey that many of you completed at the start of term (thank you to those that did!), we had a long set of items that were from various personality and psychometric scales. 

One of these was a set of items that measure a persons' "locus of control" (belief that you have control over your life, as opposed to it being decided by external forces such as fate/luck/destiny/god/the flying spaghetti monster). We also asked you to rate your average sleep quality from 0 to 100.  

My pet theory is that people who believe they have more control over life are more inclined to have a better sleep routine, and so will rate their sleep quality as being better than those who believe that life is out of their control.  

You can read in the data for your 2022 USMR cohort using the following code:  
```{r}
usmrdata <- 
  read_csv("https://uoepsy.github.io/data/surveydata_allcourse22.csv") %>%
  filter(course == "usmr", year == 2022)
```

The relevant variables are:  

```{r}
#| echo: false
tibble(
  variable = c("pseudonym","loc","sleeprating","...","..."),
  description = c("User provided pseudonym to enable lookup",
                  "Locus of Control Score. Minimum score = 6, maximum score = 30. Higher scores indicate more perceived self-control over life", "Rating of average quality of sleep. Higher values indicate better sleep quality.", "...","...")
) %>% knitr::kable() %>%
  kableExtra::row_spec(2:3,bold=TRUE)
```

:::

`r qbegin(12)`
Fit a linear regression model to test the association. 

:::hints
__Hints:__  
Try to do the following steps: 

1. Explore the data (plot and describe)
2. Fit the model
3. Take a look at the assumption plots (see [7A#assumptions](07a_slr.html#assumptions){target="_blank"}).  
    - The trick to looking at assumption plots is to look for "things that don't look random". 
    - As well as looking for patterns, these plots can also higlight individual datapoints that might be skewing the results. Can you figure out if there is [someone weird](https://ianhajnosz.com/wp-content/uploads/2018/08/IMGP4662-3-724x1024.jpg){target="_blank"} in our dataset? Can you re-fit the model without that person? When you do so, do your conclusions change?

:::
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
usmrdata <- 
  read_csv("https://uoepsy.github.io/data/surveydata_allcourse22.csv") %>%
  filter(course == "usmr", year == 2022)
```


Let's just take a look at the relevant variables and relationship:  
```{r}
p1 <- ggplot(usmrdata, aes(x=loc))+
  geom_histogram()
p2 <- ggplot(usmrdata, aes(x=sleeprating))+
  geom_histogram() 
p3 <- ggplot(usmrdata, aes(x=loc, y=sleeprating))+
  geom_point()
# remember, to make this plot arranging work,
# we need to load the patchwork package
p3 / (p1 + p2)
```

And go straight ahead and fit our model
```{r}
sleepmod <- lm(sleeprating ~ loc, usmrdata)
summary(sleepmod)
```

Wooh, looks like we were right! People with greater perceived control tend to report better sleep quality.  

Our assumptions don't look all that great though.. 
```{r echo=2}
par(mfrow=c(2,2))
plot(sleepmod)
par(mfrow=c(1,1))
```

From the bottom right plot above, we can see that the 65th observation is looking a bit influential. There is also one datapoint that is looking weird in both of the left-hand plots - this may well be the same 65th person.   
I wonder who that is..  
```{r}
usmrdata[65, c("pseudonym","sleeprating","loc")]
```

Ah, it's Ian!  
Ian is a guy who sleeps very badly (low sleeprating score) and feels like he has no control over his life (low loc score).  
The thing is - Ian is a valuable datapoint - he's a real person and we shouldn't just get rid of him because he's "a bit odd".  

However, it would be nice to know how much Ian's "oddness" is affecting our conclusions, so let's re-fit the model on everybody except Ian: 

```{r}
sleepmod_noian <- lm(sleeprating ~ loc, usmrdata[-65, ])
summary(sleepmod_noian)
```

Our conclusions haven't changed - we still have a significant association.  

What we have just done is called a "sensitivity analysis" - we've asked if our analyses are sensitive to including/excluding Ian.  

```{r}
#| fig-height: 3.5
#| code-fold: true
ggplot(usmrdata, aes(x=loc,y=sleeprating))+
  geom_point()+
  geom_smooth(method=lm, fullrange=TRUE)+
  xlim(5,30)+
  labs(title="With Ian") +

ggplot(usmrdata[-65, ], aes(x=loc,y=sleeprating))+
  geom_point()+
  geom_smooth(method=lm, fullrange=TRUE)+
  xlim(5,30)+
  labs(title="Without Ian")
```

We now have a decision to make. Do we continue with Ian removed, or do we keep him in? There's not really a right answer here, but it's worth noting a practical issue - our assumption plots look considerably better^[Still not perfect, but don't worry about that right now] for our model without Ian.  

Personally, I would probably remove Ian, and when writing up the analysis, mention clearly _why_ we exclude him, and _how_ that decision has/hasn't influenced our conclusions. 

`r solend()`



<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
