---
title: "LM Individual Case Diagnostics"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: FALSE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(patchwork)
```

:::lo 

individual case diagnostics 

:::

# Setup

`r optbegin("Data", olabel=F, show=T, toggle=F)`

```{r}
library(tidyverse)
# Read in data
mwdata = read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
```

Recall the form of our model which we fitted and stored as `wb_mdl1`:  

$$ 
\text{Wellbeing} = \beta_0 + \beta_1 \cdot \text{Outdoor Time} + \beta_2 \cdot \text{Social Interactions} + \epsilon
$$

Which we fitted in R using:
```{r}
wbmodel <- lm(wellbeing ~ outdoor_time + social_int, data = mwdata)
```

**Note:** We have have forgone writing the `1` in `lm(y ~ 1 + x...`. The 1 just tells R that we want to estimate the Intercept, and it will do this by default even if we leave it out. 

`r optend()`


## Individual Case Diagnostics

:::statbox
In linear regression, individual cases in our data can influence our model more than others. There are a variety of measures we can use to evaluate the amount of misfit and influence that single observations have on our model and our model estimates. 

+ **Regression outliers:** A large residual $\hat \epsilon_i$ - i.e., a big discrepancy between their predicted y-value and their observed y-value.  
    + **Standardised residuals:** For residual $\hat \epsilon_i$, divide by the estimate of the standard deviation of the residuals. In R, the `rstandard()` function will give you these
    + **Studentised residuals:** For residual $\hat \epsilon_i$, divide by the estimate of the standard deviation of the residuals excluding case $i$. In R, the `rstudent()` function will give you these. Values $>|2|$ (greater in magnitude than two) are considered potential outliers.  
+ **High leverage cases:** These are cases which have considerable _potential_ to influence the regression model (e.g., cases with an unusual combination of predictor values). 
    + **Hat values:** are used to assess leverage. In R, The `hatvalues()` function will retrieve these.  
    Hat values of more than $2 \bar{h}$ (2 times the average hat value) are considered high leverage. $\bar{h}$ is calculated as $\frac{k + 1}{n}$, where $k$ is the number of predictors, and $n$ is the sample size.  
+ **High influence cases:** When a case has high leverage *and* is an outlier, it will have a large influence on the regression model. 
    + **Cook's Distance:** combines *leverage* (hatvalues) with *outlying-ness* to capture influence. In R, the `cooks.distance()` function will provide these.  
    A suggested Cook's Distance cut-off is $\frac{4}{n-k-1}$, where $k$ is the number of predictors, and $n$ is the sample size.   

:::

`r qbegin("C8")`
Create a new tibble which contains:  

1. The original variables from the model (Hint, what does `<fitted model>$model` give you?)
2. The fitted values from the model $\hat y$  
3. The residuals $\hat epsilon$
4. The studentised residuals
5. The hat values
6. The Cook's Distance values. 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mdl_diagnost <- 
  tibble(
  wbmodel$model,
  fitted = fitted(wbmodel),
  resid = residuals(wbmodel),
  studres = rstudent(wbmodel),
  hats = hatvalues(wbmodel),
  cooksd = cooks.distance(wbmodel)
)
```

`r solend()`

`r qbegin("C9")`
Looking at the studentised residuals, are there any outliers?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Recall from the lectures, studentised residuals of $>2$ or $< -2$ indicate potential outlyingness.  

We can ask R whether the *absolute* values are $>2$:
```{r}
abs(mdl_diagnost$studres) > 2
```

We could *filter* our newly created tibble to these observations:
```{r}
mdl_diagnost %>% 
  filter(abs(studres)>2)
```
There are zero rows. 

`r solend()`

`r qbegin("C10")`
Looking at the hat values, are there any observations with high leverage?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
For our model, the average hat value $\bar h$ is:  
$$
\bar h = \frac{k+1}{n} = \frac{2+1}{32} = \frac{3}{32} = 0.094
$$

We can ask whether any of observations have hat values which are greater than $2 \times \bar h$:

```{r}
mdl_diagnost %>%
  filter(hats > (2*0.094))
```

Note that `r sum(mdl_diagnost$hats > (2*0.094))` observations have high leverage. 

`r solend()`
  
`r qbegin("C11")`  
Looking at the Cook's Distance values, are there any highly influential points?  
(You can also display these graphically using `plot(model, which = 4)` and `plot(model, which = 5)`). 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
For our model, a proposed cut-off for Cook's Distance is:
$$
D_{cutoff} = \frac{4}{n-k-1} = \frac{4}{32 - 2 - 1} = \frac{4}{29} = 0.138
$$

There are no observations which have a high influence on our model estimates:
```{r}
mdl_diagnost %>%
  filter(cooksd > 0.138)
```

`r solend()`

:::statbox
__Other influence.measures()__  

Alongside Cook's Distance, we can examine the extent to which model estimates and predictions are affected when an entire case is dropped from the dataset and the model is refitted.  

+ **DFFit:** the change in the predicted value at the $i^{th}$ observation with and without the $i^{th}$ observation is included in the regression.  
+ **DFbeta:**  the change in a specific coefficient with and without the $i^{th}$ observation is included in the regression.  
+ **DFbetas:**  the change in a specific coefficient divided by the standard error, with and without the $i^{th}$ observation is included in the regression.  
+ **COVRATIO:** measures the effect of an observation on the covariance matrix of the parameter estimates. In simpler terms, it captures an observation's influence on standard errors. Values which are $>1+\frac{3(k+1)}{n}$ or $<1-\frac{3(k+1)}{n}$ are considered as having strong influence.  

:::


`r qbegin("C12")`

Use the function `influence.measures()` to extract these delete-1 measures of influence.  

Try plotting the distributions of some of these measures.  

**Tip:** the function `influence.measures()` returns an `infl`-type object. To plot this, we need to find a way to extract the actual numbers from it.  
What do you think `names(influence.measures(<fitted model>))` shows you? How can we use `influence.measures(<fitted model>)$ ???? ` to extract the matrix of numbers?  


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
influence.measures(wbmodel)
```
  
Let's plot the distribution of COVRATIO statistics.  
Recall that values which are $>1+\frac{3(k+1)}{n}$ or $<1-\frac{3(k+1)}{n}$ are considered as having strong influence.  
For our model:
$$
1 \pm \frac{3(k+1)}{n} \quad = \quad 1 \pm\frac{3(2+1)}{32} \quad = \quad 1\pm \frac{9}{32} \quad = \quad 1\pm0.28
$$

The "infmat" bit of an `infl`-type object contains the numbers. To use it with ggplot, we will need to turn it into a dataframe (`as.data.frame()`), or a tibble (`as_tibble()`):   
```{r}
infdata <- influence.measures(wbmodel)$infmat %>%
  as_tibble()

ggplot(data = infdata, aes(x = cov.r)) + 
  geom_histogram() +
  geom_vline(aes(xintercept = c(1-0.28)))+
  geom_vline(aes(xintercept = c(1+0.28)))
```

It looks like a few observations may be having quite a high influence here. This is perhaps not that surprising as we only have 32 datapoints. 

`r solend()`


<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
