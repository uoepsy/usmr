---
title: "Sums of Squares in R"
author: "uoepsy.github.io"
date: "11/11/2021"
output: pdf_document
---

# Some Data  

```{r setup, message=F,warning=F}
library(tidyverse)
usmrsurv2 <- read_csv("https://uoepsy.github.io/data/usmrsurvey2.csv")
names(usmrsurv2)
names(usmrsurv2)[9:14]<-c("E","A","C","ES","I","LOC")
```

# A model 

Here is a model with two predictors, with the order of the predictors differing between the two models:  
```{r}
mymod1 <- lm(LOC ~ ES + optimism, data = usmrsurv2)
mymod2 <- lm(LOC ~ optimism + ES, data = usmrsurv2)
```

\pagebreak 
# Type 1 of Sums of Squares

Type 1 Sums of Squares is the "incremental" or "sequential" sums of squares.  
If we have a model $Y \sim A + B$, this method tests:  

- the main effect of A
- the main effect of B after the main effect of A
- _Interactions (which come in Week 9 of the course) are tested after the main effects_^[So a model $Y \sim A + B + A:B$ will (in addition to the two above) test 1) the main effect of A 2) the main effect of B after the main effect of A and 3) the effect of the interaction A:B *after* the main effects of A and B.]. 

Because this is sequential, the order matters.  


We can get the Type 1 SS in R using the function `anova()`.  
As you will see, the order in which the predictors are entered in the model influences the results.  
This is because:  

- for `mymod1` (`lm(LOC ~ ES + optimism)`) we are testing the main effect of `ES`, followed by the main effect of `optimism` _after_ accounting for effects of `ES`.  
- for `mymod2` (`lm(LOC ~ optimism + ES)`) it is the other way around: we test the main effect of `optimism`, followed by the main effect of `ES` _after_ accounting for effects of `optimism`.  

```{r}
# mymod1 <- lm(LOC ~ ES + optimism, data = usmrsurv2)
anova(mymod1)
# mymod2 <- lm(LOC ~ optimism + ES, data = usmrsurv2)
anova(mymod2)
```

\pagebreak 
# Type 3 of Sums of Squares

Type 3 Sums of Squares is the "partial" sums of squares.  
If we have a model $Y \sim A + B$, this method tests:  

- the main effect of A after the main effect of B
- the main effect of B after the main effect of A

So the Type 3 will be equivalent to Type 1 _only for the final predictor in the model_. 

Martin showed us one approach in the Live R session, using the `drop1()` function. We can also get the same using the `Anova()` function (capital __A__) from the __car__ package.  
```{r}
# mymod1 <- lm(LOC ~ ES + optimism, data = usmrsurv2)
drop1(mymod1, test = "F")
```

Note that these results are the same as the Type 3 for `optimism` in the model `lm(LOC ~ ES + optimism)`, and the Type 3 for `ES` in the model `lm(LOC ~ optimism + ES)`. Take a look at the previous page for confirmation of this.  
  
Note that Type 3 SS are __invariant to the order of predictors:__
We get the same when we switch around our predictors:
```{r}
# mymod2 <- lm(LOC ~ optimism + ES, data = usmrsurv2)
drop1(mymod2, test = "F")
```
\pagebreak
# The summary() function  

Remember that we mentioned in the Week 8 lab that in the simple regression model (one predictor), the $t$-statistic for the coefficient test is the square root of the $F$-statistic for the test of the overall reduction in the residual sums of squares?  

Well this does still hold for the multiple regression model, but it is a little more complicated.  
For a given coefficient $t$-statistic in a multiple regression model, the associated $F$-statistic is the one corresponding to the reduction in residual sums of squares that is attributable to that predictor only. Or, in other words, the Type 3 $F$-statistic.  

Here are our model coefficients and $t$-statistics:
```{r}
# mymod1 <- lm(LOC ~ optimism + ES, data = usmrsurv2)
summary(mymod1)$coefficients
```

Here are our Type 3 SS $F$-statistics:
```{r}
drop1(mymod1, test = "F")
```

We can square-root them to get back to the $t$-statistic:  
```{r}
sqrt(drop1(mymod1, test = "F")$`F value`)
```

What this means is that just like the `drop1()` $F$ test for reduction in residual sums of squares uses Type 3 SS, the $t$ tests for the coefficients produced in `summary()` for a linear model are also _invariant to the order of predictors._


