---
title: "7A: Multiple Linear Regression"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
```

:::lo
This reading:  

- "Variance explained" in a linear model
- Multiple regression: Building models where more than one thing explains variance
- Comparing models
- Associations 'while holding other things constant'

:::


# Variance Explained in Simple Regression

TODO intro.  

```{r}
slr_data <- read_csv("https://uoepsy.github.io/data/usmr_slr.csv")
my_model <- lm(y ~ x, data = slr_data)
summary(my_model)
```

We might ask ourselves if the model is useful in explaining the variance in our outcome variable $y$. To quantify and assess model utility, we split the total variability of the outcome variable into two terms: the variability explained by the model plus the variability left unexplained in the residuals.

$$
\begin{align}
& \qquad \qquad \qquad \qquad \text{total variability in response } =  \\
& \text{variability explained by model } + \text{unexplained variability in residuals}
\end{align}
$$

The illustration in @fig-sstssrssm gets at the intuition behind this: the top panel shows the total variability in the outcome variable $y$ - for each datapoint we see the distance from the mean of y. These distances can be split into the bit from the mean to the model predicted value (seen in the bottom left panel of @fig-sstssrssm), and the bit from that value to the actual value (bottom right panel). 
```{r}
#| label: fig-sstssrssm
#| fig-cap: "Total Sums of Squares = Model Sums of Squares + Residual Sums of Squares"
#| echo: false
df <- slr_data %>% rename(x1=x)
plt_sst = 
  ggplot(df, aes(x=x1,y=y))+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=mean(df$y)), lty="dashed",col="black")+
  geom_point(col="tomato1", size=3, alpha=.3)+
  geom_hline(yintercept=mean(df$y), lty="solid")+
  geom_text(x=1,y=6,label="bar(y)",parse=T, size=6)+
  geom_curve(x=1.1,xend=2,y=6,yend=mean(df$y),curvature=-.2)+
  labs(title="SS Total")

plt_ssr = ggplot(df, aes(x=x1,y=y))+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=fitted(lm(y~x1,df))), lty="dashed",col="black")+
  geom_point(col="tomato1", size=3, alpha=.3)+
  geom_hline(yintercept=mean(df$y), lty="solid")+
  geom_smooth(method=lm,se=F)+
  labs(title="SS Residual")

plt_ssm = ggplot(df, aes(x=x1,y=y))+
  geom_point(col="tomato1", size=3, alpha=.2)+
  geom_hline(yintercept=mean(df$y), lty="solid")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,yend=mean(df$y),y=fitted(lm(y~x1,df))), lty="dashed",col="blue")+
  labs(title="SS Model")

plt_sst / (plt_ssm + plt_ssr) & scale_x_continuous("x",breaks=NULL) & scale_y_continuous("y",breaks=NULL)
```

Each term can be quantified by a 'sum of squares' (i.e. summing the squared differences)^[e.g. for the 'total sums of squares', this is the sum of squared differences from each observation to the mean. For the 'model sums of squares', this is the sum of the squared differences from each model-predicted value to the mean. For the 'residual sums of squares', it is the sum of the squared differences from each observation to the relative model-predicted value.]  

$$
\begin{aligned}
SS_{Total} &= SS_{Model} + SS_{Residual} \\
\sum_{i=1}^n (y_i - \bar y)^2 &= \sum_{i=1}^n (\hat y_i - \bar y)^2 + \sum_{i=1}^n (y_i - \hat y_i)^2 \\
\quad \\
\text{Where:} \\
& y_i = \text{observed value} \\
&\bar{y} = \text{mean} \\
& \hat{y}_i = \text{model predicted value} \\
\end{aligned}
$$

## $R^2$: proportion of variance explained

A useful statistic is the $R^2$, which shows us the proportion of the total variability in the outcome (`y`) that is explained by the linear relationship with the predictor (`x`).

:::sticky
The $R^2$ coefficient is defined as the proportion of the total variability in the outcome variable which is explained by our model:  
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$
:::

We can find the $R^2$ easily in the `summary()` of the model! 

The output of `summary()` displays the R-squared value in the following line:
```
Multiple R-squared:  0.3847
```
:::column-margin
For the moment, ignore "Adjusted R-squared". We will come back to this in a little bit.  
:::

:::int
Approximately 38\% of the total variability in `y` is explained by the linear association with `x`.
:::

::: {.callout-caution collapse="true"}
#### optional: manual calculation of R-Squared   

```{r}
slr_data <- read_csv("https://uoepsy.github.io/data/usmr_slr.csv")
my_model <- lm(y ~ x, data = slr_data)

my_model_fitted <- slr_data |>
  mutate(
    y_hat = predict(my_model),
    resid = y - y_hat
  )
head(my_model_fitted)

my_model_fitted |>
  summarise(
    SSModel = sum( (y_hat - mean(y))^2 ),
    SSTotal = sum( (y - mean(y))^2 )
  ) |>
  summarise(
    RSquared = SSModel / SSTotal
  )
```

:::


## The $F$ Statistic

We can also perform a test to investigate if the model is 'useful' --- that is, a test to see if our explanatory variable explains more variance in our outcome than we would expect by just some random chance variable.  

We test the following hypotheses:

$$
\begin{aligned}
H_0 &: \text{the model is ineffective, } b_1 = 0 \\
H_1 &: \text{the model is effective, } b_1 \neq 0
\end{aligned}
$$
The relevant test-statistic is the F-statistic, which uses "Mean Squares" (these are Sums of Squares divided by the relevant degrees of freedom).  

:::sticky
**$F$-statistic for simple linear regression**

$$
\begin{split}
F = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}
\end{split}
$$

This is a comparison between amount of variation in the response explained by the model and the amount of variation explained by the residuals.

The sample F-statistic is compared to an F-distribution with $df_{1} = 1$ and $df_{2} = n - 2$ degrees of freedom.^[
$SS_{Total}$ has $n - 1$ degrees of freedom as one degree of freedom is lost in estimating the population mean with the sample mean $\bar{y}$.
$SS_{Residual}$ has $n - 2$ degrees of freedom. There are $n$ residuals, but two degrees of freedom are lost in estimating the intercept and slope of the line used to obtain the $\hat y_i$s.
Hence, by difference, $SS_{Model}$ has $n - 1 - (n - 2) = 1$ degree of freedom.
]

:::


Like the $R^2$, the `summary()` of our model prints out the $F$-statistic, degrees of freedom, and p-value. These are right at the bottom of the summary output, printed as:  

```
F-statistic: 61.26 on 1 and 98 DF,  p-value: 5.918e-12
```

:::int
The F-test of model utility was significant ($R^2=0.38, F(1,98) = 61.26,\ p <.001$), suggesting that predictor $x$ is effective in explaining variance in the outcome.  
:::

Note that the p-value here is exactly the same as the one for the coefficient. This is because in testing "the model is (in)effective", the "model" is really _only_ the relationship between the outcome and our **one** predictor. We're about to start adding more explanatory variables into our model, which mean's our hypotheses for the $F$-test will be about a *set* of $b$'s.   

::: {.callout-caution collapse="true"}
#### optional: with only one predictor variable, the F-test is equivalent to the t-test of the slope  

**In simple linear regression only** (where we have just __one__ predictor), the F-statistic for overall model significance is equal to the square of the t-statistic for $H_0: b_1 = 0$.

You can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:
```{r}
slr_data <- read_csv("https://uoepsy.github.io/data/usmr_slr.csv")
my_model <- lm(y ~ x, data = slr_data)

summary(my_model)$fstatistic['value']
summary(my_model)$coefficients['x','t value']
```
$$
\begin{align}
& t^2 = F \\
& 7.827194^2 = 61.26497
\end{align}
$$

We can also show the equivalence of the F-test for model effectiveness and t-test for the slope through their respecive formulae. 

Recall the formula of the sum of squares due to the model. We are going to re-express this in an equivalent form below:
$$
\begin{aligned}
SS_{Model} &= \sum_i (\hat y_i - \bar y)^2 \\
&= \sum_i (\hat b_0 + \hat b_1 x_i - \bar y)^2 \\
&= \sum_i (\bar y - \hat b_1 \bar x + \hat b_1 x_i - \bar y)^2 \\
&= \sum_i (\hat b_1 (x_i - \bar x))^2 \\
&= \hat b_1^2 \sum_i (x_i - \bar x)^2
\end{aligned}
$$

The F-statistic is given by:
$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\hat b_1^2 \sum_i (x_i - \bar x)^2}{\hat \sigma^2} 
= \frac{\hat b_1^2 }{\hat \sigma^2 / \sum_i (x_i - \bar x)^2}
\end{aligned}
$$

Now recall the formula of the t-statistic,
$$
t = \frac{\hat b_1}{SE(\hat b_1)} = \frac{\hat b_1}{\hat \sigma / \sqrt{\sum_i (x_i - \bar x)^2}}
$$

It is evident that the latter is obtained as the square root of the former.

:::


::: {.callout-caution collapse="true"}
#### optional: another formula for the F-test  

With some algebra we can also show that:
$$
F = \frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }
$$

Proof:

$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\frac{SS_{Model}}{SS_{Total}}}{\frac{SS_{Residual}}{SS_{Total}} \cdot \frac{1}{(n - 2)}} 
= \frac{R^2 / 1}{(1 - R^2) / (n - 2)}
\end{aligned}
$$

:::

<div class="divider div-transparent div-dot"></div>

# The Multiple Regression Model

The real power of regression models comes into effect when we start to concern ourselves with more than just "one outcome explained by one predictor".  
  
Enter... $x_2$!  

We're going to start with looking at the case of "one outcome, two predictors", but the beauty of this is that the logic scales up to however many predictor variables we want to include in our model. 

```
lm(y ~ x1 + x2 + ... + xp)
```

:::statbox
#### More predictors = More Dimensions  

When we fitted the simple regression model with __one__ predictor: 
$$
y = b_0 + b_1(x) + \epsilon
$$
we were fitting a _line_ to a scatterplot of points that we plotted in _2 dimensions_ (an x-axis and a y-axis). 

When we fit a multiple regression model with __two__ predictors:  
$$
y = b_0 + b_1(x_1) + b_2(x_2) + \epsilon
$$
we are fitting a __surface__ (or "plane") to a 3-dimensional cloud of datapoints (@fig-regsurf). There are three dimensions: x1, x2, and y.    
```{r}
#| label: fig-regsurf
#| echo: false
#| fig-cap: "Regression surface for y~x1+x2, from two different angles"
#| message: false
#| warning: false

mwdata = read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
mwdata %>% rename(y=wellbeing,x1=outdoor_time,x2=social_int) -> mwdata
fit<-lm(y~x1+x2, data=mwdata)
steps=50
x1 <- with(mwdata, seq(min(x1),max(x1),length=steps))
x2 <- with(mwdata, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2)
y <- matrix(predict(fit, newdat), steps, steps)

par(mfrow=c(1,2))

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA)
obs <- with(mwdata, trans3d(x1,x2, y, p))
pred <- with(mwdata, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y,lty = "dashed")

p <- persp(x1,x2,y, theta = -35,phi=10, col = NA)
obs <- with(mwdata, trans3d(x1,x2, y, p))
pred <- with(mwdata, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y,lty = "dashed")

par(mfrow=c(1,1))
```

Don't worry about trying to figure out how to visualise it if we had more predictors! We can only conceive of 3 spatial dimensions.^[One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it's not worth trying!] However, the logic stays the same when we increase this to having $p$ predictors, but we have a model that is a $p$-dimensional surface, and each coefficient is the angle of that surface with respect to each predictor.  

When we have two predictors, our model is now determined by three numbers:  

- the __intercept__, denoted $b_0$.  
This is the point at which the plane hits the y-axis (i.e. where $x_1=0$ __and__ $x_2=0$)
- the __slope of x1__, in this case denoted $b_1$.  
This is the angle of the regression plane with respect to the axis of $x_1$. It is the amount which the plane increases for every 1 increase in $x_1$.  
- the __slope of x2__, in this case denoted $b_2$.  
This is the angle of the regression plane with respect to the axis of $x_2$. It is the amount which the plane increases for every 1 increase in $x_2$.  

:::


## Fitting Multiple Regression Models in R  

As we did for simple linear regression, we can fit our multiple regression model using the `lm()` function. We can add as many explanatory variables as we like, separating them with a `+`.  

```
model_name <- lm(y ~ 1 + x1 + x2 + ... + xp, data = dataframe)
```

And we can use all the same functions that we have already seen such as `summary()`, `predict()`, `fitted()`, `coef()` etc.  

```{r}
#| include: false
set.seed(85)
mydata <- tibble(
  x1 = rdunif(50,18,70),
  x2 = round(rnorm(50,14,5)),
  x3 = round(rnorm(50,0,1),2),
  y = .15*x1 - .8*x2 + rnorm(50,0,7)
)
mydata$x3<-cut(mydata$x3,3,labels = c("level1","level2","level3"))
#write_csv(mydata, "../../data/usmr_mlr.csv")
```
```{r}
mlr_data <- read_csv("https://uoepsy.github.io/data/usmr_mlr.csv")
eg_model <- lm(y ~ x1 + x2, data = mlr_data)
summary(eg_model)
```

Just like we saw for the regression model with one predictor, the `summary()` output of a multiple regression model shows us all the same information: residuals, coefficients, $R^2$ and and $F$-test. We'll get to the coefficients a little later on, but first we're going to take a look at the overall model evaluation.  


<div class="divider div-transparent div-dot"></div>


# Variance Explained in Multiple Regression  



## The Adjusted $R^2$

We know from above that in simple linear regression the R-squared can be obtained as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$

In multiple regression, the "multiple $R^2$" uses this exact same formula. However, when we add more and more predictors into a multiple regression model, $SS_{Residual}$ _cannot_ increase. In fact, it will _always_ decrease, regardless of how useful our new predictors are. This means that $R^2$ will _always increase_ (because $SS_{Total}$ is constant, so $1-\frac{SS_{Residual}}{SS_{Total}}$ will increase as $SS_{Residual}$ decreases).  

If we added randomly generated 1000 new predictors (completely random, so they have nothing to do with the outcome), then by chance alone they will explain _some_ variance in the outcome $y$, and the multiple $R^2$ will always increase.  
  
An alternative, the "Adjusted-$R^2$", does not necessarily increase with the addition of more explanatory variables, by the inclusion of a penalty according to the number of explanatory variables in the model. The number by itself isn't directly meaningful, but can be useful in determining the amount of additional variance explained by adding predictor(s) into a model.  

:::sticky
The **Adjusted** $R^2$ is a measure of the proportion of variability in the outcome that is explained by our model, *adjusted for the number of predictors in the model.*  

$$
\begin{align}
& Adjusted{-}R^2=1-\frac{(1-R^2)(n-1)}{n-k-1} \\
& \quad \\
& \text{Where:} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
\end{align}
$$

:::
  

**In R,** we can view both the mutiple and adjusted $R^2$ at the bottom of the output of `summary(<modelname>)`:

```{r}
#| eval: false
eg_model <- lm(y ~ x1 + x2, data = mlr_data)
summary(eg_model)
```
```{r}
#| echo: false
knitr::include_graphics("images/mlr/output_mlr_rsq.png")
```


## The $F$-statistic: a joint test

We saw just above that with one predictor, the F-statistic is used to test the null hypothesis that the regression slope for that predictor is zero. In multiple regression, the logic is the same, but we are now testing against the null hypothesis that _all_ regression slopes are zero (now that we have multiple predictors, "all" is more than 1).  

$$
\begin{aligned}
H_0: & \text{the model is ineffective, } \\
& b_1, ..., b_k = 0 \\
H_1: &\text{the model is effective, } \\
& \text{any of }b_1, ..., b_k \neq 0
\end{aligned}
$$

The $F$-statistic is sometimes called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per parameter) versus how much of the variation is unexplained (per remaining degrees of freedom). We can generalise the formula for the $F$-statistic in simple regression that we saw above, to encompass situations where there are more predictors:    

:::sticky
**$F$-ratio**

$$
\begin{align}
& F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
& \quad \\
& \text{Where:} \\
& df_{model} = k \\
& df_{residual} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$
:::


**In R,** at the bottom of the output of `summary(<modelname>)`, you can view the F ratio, along with an hypothesis test against the alternative hypothesis that the at least one of the coefficients $\neq 0$:^[under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1]

```{r}
#| eval: false
eg_model <- lm(y ~ x1 + x2, data = mlr_data)
summary(eg_model)
```
```{r}
#| echo: false
knitr::include_graphics("images/mlr/output_mlr_f.png")
```

:::int
the linear model with $x1$ and $x2$ explained a significant amount of variance in $y$ beyond what we would expect by chance ($R^2=0.26, F(2, 47) = 8.44,\ p <.001$). 
:::
  

<div class="divider div-transparent div-dot"></div>

# Model Comparisons

Now we have seen that we can add in more predictors to a linear regression model, what are we going to _do_ with our modelling? Before we look in more depth at the coefficients, we're going to frame things in terms of model comparisons.  

An easy starting point would be to compare how the $R^2$ values change when we add predictors - showing the extent to which different models 'explain variance' in our outcome:  

```{r}
#| eval: false
eg_model1 <- lm(y ~ x1, data = mlr_data)
eg_model2 <- lm(y ~ x1 + x2, data = mlr_data)
summary(eg_model1)
summary(eg_model2)
```
```{r}
#| echo: false
#| out-width: "100%"
knitr::include_graphics("images/mlr/output_mlr_rsq2.png")
```

But this comparison is _descriptive_, in that we are not performing any _test_ of whether the differences between the models' explanatory power is more than we might just expect by chance. To do this, we need to utilise our $F$-tests again!  

As it happens, the $F$-statistic we see at the bottom of `summary(model)` is actually a comparison between two models: our model (with some explanatory variables in predicting $y$) and __the null model.__ 

:::sticky
**the null model** can be thought of as the model in which all explanatory variables have zero regression coefficients. It is also referred to as the __intercept-only model__, because if all predictor variable coefficients are zero, then the only we are only estimating $y$ via an intercept (which will be the mean: $\bar y$).  
:::

We aren't limited to comparing our model to the null model. We can compare all the intermediate models which vary in the complexity, from the null model to our full model.  


:::sticky
__Incremental F-test__  

If (*and only if*) two models are __nested__ (one model contains all the predictors of the other and is fitted to the same data), we can compare them using an __incremental F-test.__  

This is a formal test of whether the __additional predictors__ provide a better fitting model.  
Formally this is the test of:  

+ $H_0:$ coefficients for the added/ommitted variables are all zero.
+ $H_1:$ at least one of the added/ommitted variables has a coefficient that is not zero. 

:::

::: {.callout-caution collapse="true"}
#### optional: F-ratio written for model comparison  

The F-ratio for comparing the residual sums of squares between two models can be written as:

$$
\begin{align}
& F_{(df_R-df_F),df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F} \\
& \quad \\
& \text{Where:} \\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$
:::

:::rtip

**In R,** we can conduct an incremental F-test by constructing two models, and passing them to the `anova()` function: 
```{r}
#| eval: false
model1 <- lm( ... )
model2 <- lm( ... )
anova(model1, model2)
```

:::

If we wanted to, for example, compare a model with just one predictor, $x_1$, to a model with 3 predictors: $x_1,\ x_2,\ x_3$, we can assess the extent to which the variables $x_2$ and $x_3$ *jointly* improve model fit:  

```{r}
eg_model1 <- lm(y ~ x1, data = mlr_data)
eg_model3 <- lm(y ~ x1 + x2 + x3, data = mlr_data)
anova(eg_model1, eg_model3)
```

:::int
$x2$ and $x3$ explained a significant amount of additional variance in $y$ beyond $x1$ ($F(3,45) = 3.8,\ p =.017$). 
:::

<div class="divider div-transparent div-dot"></div>


# Associations in multiple regression

Okay, so we can build multiple regression models, adding in as many predictors as we like. But __why__ is this a useful thing that we may want to do?  

One important reason is that it allows us to exercise _statistical control_. We often conduct research where we are interested mainly in one relationship, but we know that there are other things also at play - there are other variables that will probably strongly influence results if they aren't held constant.  

Statistical control allows us to examine the relationship of interest _after accounting for variance explained by other predictors._ Including additional predictors also often has the benefit of improving the predictive accuracy of our model (although this is not often the primary goal in Psychological studies, which are less concerned with 'the best fitting model' and more geared towards understanding and estimating the relevant relationship while exercising appropriate control).

Multiple regression allows to do this because when we have multiple predictor variables in our model, we can examine the association between the outcome $y$ and **the bit of our focal predictor variable that is unique from the other predictors** (i.e. "after accounting for" those other predictors).  

:::statbox
__Terminology__ 

As with all areas of statistics, people seem to use lots of different terms here. It can be confusing!    

- **outcome/response/dependent variable**: variable on the left hand side of the model equation 
*(thing that varies and we're trying to explain how/why)*
- **predictor:** any variable on the right hand side of the model equation 
*(things that we thing explain differences in the outcome)*
- **focal predictor/independent variable:** the predictor of interest
- **covariates/confounders/control variables:** other variables that we are less interested in but believe to relevant to how the data comes about, and that may influence both the outcome and the focal predictor.  
:::


TODO EXAMPLE


One way to build this intuition is to consider a Venn diagram with a circle showing the variance in each variable. @fig-vennslr shows a simple linear regression with one predictor (i.e. `lm(y ~ x1)`). The circle for $y$ shows the total variance in $y$ (the same for the $x_1$ circle). The overlap between circles (labelled "A") shows the variance in $y$ that is explained by $x_1$ (i.e. the covariance).  

```{r}
#| label: fig-vennslr
#| fig-cap: "Venn Diagram for Simple Regression y ~ x1"
#| echo: false
knitr::include_graphics("images/mlr/venn_slr.png")
```

When we add in a new predictor, $x_2$, where do we add it? If $x_1$ and $x_2$ are _completely_ uncorrelated with one another, then it would look something like @fig-vennmlr1, where there is no overlap between the $x_1$ and $x_2$ circles. The total variance explained in $y$ by both predictors is $A + B$, and in this case, nothing changes in our estimate of the relationship between $y$ and $x_1$. It's just the same as before (the area labelled "A" is the same in both @fig-vennslr and @fig-vennmlr1). 

```{r}
#| label: fig-vennmlr1
#| fig-cap: "Venn Diagram for Multiple Regression y ~ x1 + x2 where x1 and x2 are completely uncorrelated"
#| echo: false
knitr::include_graphics("images/mlr/venn_mlr1.png")
```

However, in practice the predictors in our regression model are likely to overlap a bit (it's hard to make sure our focal predictor is completely unrelated to other predictors). In this case, our Venn diagram is going to look like @fig-vennmlr2. The correlation between $x_1$ and $x_2$ is shown by the overlap of those two circles (the area $C + D$ in the diagram). 
The total variance explained in $y$ is now separated into the areas $A + B + C$ (and $E$ is the _unexplained_ variance - the residuals).  

Areas $A$ and $B$ are no longer the same as in the previous diagrams - there's a little bit (area $C$) that we don't want to double count in its explanatory power as it can't be attributable to specifically one variable or the other.  

```{r}
#| label: fig-vennmlr2
#| fig-cap: "Venn Diagram for Multiple Regression y ~ x1 + x2 where x1 and x2 are somewhat correlated"
#| echo: false
knitr::include_graphics("images/mlr/venn_mlr2.png")
```

- $A$ is the variance in $y$ _uniquely_ explained by $x_1$  
- $B$ is the variance in $y$ _uniquely_ explained by $x_2$
- $C$ is the variance in $y$ that is explained by both $x_1$ and $x_2$ but not attributable to either one uniquely.  

We can map the model comparisons we talked about above to this diagram too. The comparison below would allow us to ask whether the area $B$ explains significantly more than the area $A+C$.  
```{r}
#| eval: false
model1 <- lm(y ~ x1, data = data)
model2 <- lm(y ~ x1 + x2, data = data)
anova(model1, model2)
```


::: {.callout-note collapse="true"}
## Example: Caffeinated Heart Rates  

We have a sample of 100 people, and we measure their resting heart rate and their caffeine consumption. We're interested in estimating how caffeine consumption is associated with differences in resting heart rate. However, we also know that heart rate increases with age _and_ we think that older people tend to drink less caffeine. So we want to isolate the differences in heart rate due to caffeine from those due to age.  

```{r}
#| include: false
set.seed(38)
hrcaff <- tibble(
  age = rdunif(100,40,80),
  caffeine = 150 + age*-.8 + rnorm(100,0,5),
  rhr = 56 + age*.3 + 0*caffeine + rnorm(100,0,5)
)
# write_csv(hrcaff, "../../data/usmr_hrcaff.csv")
# summary(lm(rhr ~ caffeine, data = hrcaff))$coefficients
# summary(lm(rhr ~ age + caffeine, data = hrcaff))$coefficients
```

The toy dataset for our heart rate and caffeine example is at [https://uoepsy.github.io/data/usmr_hrcaff.csv](https://uoepsy.github.io/data/usmr_hrcaff.csv).  
We can see plots of the different relationships in @fig-caffplot. It looks from these like heart rate _decreases_ with caffeine, and _increases_ with age. But note also that caffeine decreases with age. 
```{r}
#| label: fig-caffplot
#| fig-cap: "bi-variate relationships between each of resting heart rate, caffeine consumption, and age"
#| out-width: "100%"
#| fig-height: 3.5
#| code-fold: true
hrcaff <- read_csv("https://uoepsy.github.io/data/usmr_hrcaff.csv")
library(patchwork)
ggplot(hrcaff, aes(x=caffeine,y=rhr))+
  geom_point() +
ggplot(hrcaff, aes(x=age,y=rhr))+
  geom_point() +
ggplot(hrcaff, aes(x=age,y=caffeine))+
  geom_point()

```

If we fit a simple regression $heartrate \sim b_0 + b_1(caffeine)$, we get a nice line, and our model is significant. The coefficient is negative suggesting to us that drinking more caffeine is associated with lower heart rate! Good news for me, I'm on my 6th coffee today!  

```{r}
#| fig-height: 3
#| code-fold: true
ggplot(hrcaff, aes(x=caffeine,y=rhr))+
  geom_point() +
  geom_smooth(method=lm)
```
```{r}
#| echo: false
.pp(summary(lm(rhr ~ caffeine, data = hrcaff)), l=list(3, 9:12,18))
```

But... what if the reason that people in our sample who drink more caffeine have lower heart rates _not_ because they drink more caffeine, but because they are older (and older people have lower heart rates).  

When we compare two models, one with heart rate predicted by age, and one with HR predicted by both age _and_ caffeine, we might conclude that caffeine is **not** a useful predictor.. 

```{r}
model1 <- lm(rhr ~ age, data = hrcaff)
model2 <- lm(rhr ~ age + caffeine, data = hrcaff)
anova(model1, model2)
```

Why? Because after we take into account how old people are, knowing their caffeine consumption doesn't actually provide any information about their heart rate.  

If it helps, we might think of this model as the diagram in @fig-vennhrcaff. When we don't have age in our model, then the estimated effect of caffeine on heart rate is the areas $B + C$. When we _do_ have age in the model, the _additional_ variance in heart rate explained _uniquely_ by caffeine is just the tiny area $B$ (not a useful amount). 

```{r}
#| label: fig-vennhrcaff
#| fig-cap: "lm(rhr ~ age + caffeine)"
#| echo: false
knitr::include_graphics("images/mlr/venn_hrcaff.png")
```


:::frame
This example is a very extreme one where the relationship completely disappears. in real data associations tend to be more subtle/less clear cut. Including $x_2$ may increase or decrease the association between $y$ and $x_1$, depending on the extent to which $x_1$ and $x_2$ are correlated. 
:::

:::







::: {.callout-caution collapse="true"}
#### optional: control on the front-end

If we haven't collected the data yet, one good option is to __control by design__. This would involve trying to collect our data such that the predictor of interest is _independent_ from other possibly confounding variables.  

We could do this by __randomisation__, where we randomly allocate people to different levels of our focal predictor, meaning that other variables will not be related to the focal predictor. This is what a "randomized control trial" does, randomly allocating people to take drug X or a placebo means that the two groups should be similar in aspects such as age.  

Alternatively, we could do achieve it by __"case-matching"__. This involves finding people at different levels of the focal predictor who match on possible confounders. For example, for every 60 year who takes drug X, we also collect data from a 60 year old who does not.  

:::

<div class="divider div-transparent div-dot"></div>

# Interpreting Multiple Regression Coefficients  

:::sticky

- F tests and Model Comparisons: __does/do__ [predictor(s)] explain variance in y?  
- Coefficient Tests: *__how__* __is__ [predictor] associated with y?  

:::

The coefficients (one for each predictor) from our multiple regression model `lm(y ~ x1 + x2)` reflect the areas $A$ and $B$, scaled to be the "change in $y$ associated with a one unit change in [predictor], holding [other predictors] constant".   


The parameters of a multiple regression model are:

+ $b_0$ (The intercept);
+ $b_1$ (The slope across values of $x_1$);
+ ...  
+ ...
+ $b_k$ (The slope across values of $x_k$);
+ $\sigma$ (The standard deviation of the errors).

<br>
You'll hear a lot of different ways that people explain multiple regression coefficients. For the model $y = b_0 + b_1(x_1) + b_2 (x_2) + \epsilon$, we might hear $b_1$ (the coefficient for $x_1$), described as:
  
the increase in $y$ for a one unit increase in $x_1$ when...

- holding $x_2$ constant.
- controlling for differences in $x_2$.
- partialling out the effects of $x_2$.
- holding $x_2$ equal. 
- accounting for effects of $x_2$.

What exactly do all these mean? If we return to our regression surface, our coefficients are the angles of this surface. We can see that as $x_1$ increases, the surface goes up. This increase is the same no matter where on $x_2$ we are (i.e. the angle doesn't change as we move up $x_2$). The coefficient for $x_1$ is the amount the surface changes on $y$, provided we stay at the same value for $x_2$.  

```{r}
#| echo: false
#| message: false
#| warning: false
mwdata = read_csv("https://uoepsy.github.io/data/usmr_mlr.csv")
fit<-lm(y~x1+x2, data=mwdata)
steps=50
x1 <- with(mwdata, seq(min(x1),max(x1),length=steps))
x2 <- with(mwdata, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2)
y <- matrix(predict(fit, newdat), steps, steps)
p <- persp(x1,x2,y, theta = -140,phi=10, col = NA,zlim=c(min(mwdata$y),max(mwdata$y)))
obs <- with(mwdata, trans3d(x1,x2, y, p))
pred <- with(mwdata, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y,lty = "dashed")
```

```{r}
#| eval: false
mlr_data <- read_csv("https://uoepsy.github.io/data/usmr_mlr.csv")
eg_model <- lm(y ~ x1 + x2, data = mlr_data)
summary(eg_model)
```
```{r}
#| echo: false
.pp(summary(lm(y ~ x1 + x2, data = mlr_data)),l=list(9:13))
```

Imagine a person who scores 3 on $x_1$. what is the estimated change in $y$ if they scored 4 instead? The coefficient for $x_1$ tells us how much their score on $y$ would increase by 0.176 _provided they don't also change on $x_2$._ So we are moving along the regression surface in the $x_1$ direction. This makes sense, because if they _also_ changed on $x_2$, then we would expect their score on $y$ to change because of this too (i.e. we would be moving diagonally on the surface).  


::: {.callout-caution collapse="true"}
#### optional: F and t

Our interpretation of multiple regression _coefficients_ is analogous to the idea of a model comparison with and without that predictor - i.e. 

TODO 
table
model comp = "_does X2_ explain additional variance after accounting for x1?"
coef = "what is the association between x2 and y, after accounting for x1?" 

In fact, the $p-values$ from the model comparison will even match those from the coefficient tests: 

```{r}
#| eval: false
summary(eg_model2)$coefficients
```

These are equivalent in this specific case, but when we more to more general forms of the model, it does not hold. The key reason being is that the model comparison in the linear regression world is assessing the reduction in residual sums of squares, which is directly linked to the slope of the regression line (see back to @fig-sstssrssm above). When we move to more general models, the way we assess "improvements in fit" is not always about reduction in residual sums of squares, and so this equivalence can break.  

:::


## Visualising Associations  

The associations we get out from our coefficients are conditional upon holding constant other predictors. How are we supposed to visualise this?  

Three-dimensional plots like the ones above are lovely, but a) they're difficult to make and b) they only work when there is _one_ other predictor variable being controlled for.  

The typical way to plot these associations is to make a 2-dimensional figure that shows the _model estimated_ increase in $y$ across values of $x$. Notice the use of "model estimated" - we are visualising the model, _not_ the data.  

Luckily, the __sjPlot__ package can make it very easy for us to create plots of model estimated effects. We need to give it the model, the type of thing we want plotted (in this case "eff" for "effect"), and the relevant predictor term (in this case "x1"):  

```{r}
#| fig-height: 2.5
mlr_data <- read_csv("https://uoepsy.github.io/data/usmr_mlr.csv")
eg_model <- lm(y ~ x1 + x2, data = mlr_data)

library(sjPlot)
plot_model(eg_model, type = "eff", terms = "x1")
```

It might help to think of this as if we are just tilting the our view of the regression surface so that we see it from only one edge: 

```{r}
#| echo: false
mwdata = read_csv("https://uoepsy.github.io/data/usmr_mlr.csv")
fit<-lm(y~x1+x2, data=mwdata)
steps=50
x1 <- with(mwdata, seq(min(x1),max(x1),length=steps))
x2 <- with(mwdata, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2)
y <- matrix(predict(fit, newdat), steps, steps)
par(mfrow=c(1,2))
p <- persp(x1,x2,y, theta = 140,phi=10, col = NA,
           zlim=c(min(mwdata$y),max(mwdata$y)))
obs <- with(mwdata, trans3d(x1,x2, y, p))
pred <- with(mwdata, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y,lty = "dashed")

p <- persp(x1,x2,y, theta = 0,phi=18, col = NA,
           zlim=c(min(mwdata$y),max(mwdata$y)))
obs <- with(mwdata, trans3d(x1,x2, y, p))
pred <- with(mwdata, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y,lty = "dashed")
par(mfrow=c(1,1))
```


::: {.callout-caution collapse="true"}
#### optional: manually making the plot  

We can extract the predicted values, and confidence bounds, using `predict()`. First we create a dataset of the values of the predictors that we wish to predict across. We'll predict $y$ for all values of $x1$ from 18 to 70 (the range of x1 in our data), and holding $x2$ as 12.6 (the mean of x2 in our data): 

```{r}
plotdata <- data.frame(
  x1 = 18:70,
  x2 = 12.6
)
```
Then we add the predictions and the standard error  
```{r}
plotdata <- plotdata %>%
  mutate(
    predy = predict(eg_model, newdata=plotdata),
    se = predict(eg_model, plotdata, se.fit = TRUE)$se
  )
```
And use the standard error to create the confidence intervals. Doing $1.96 \times SE$ would get us close, but to do it properly we should use the $t$-distribution: 
```{r}
plotdata <- plotdata %>% 
  mutate(
    lower = predy - (qt(.975, df=47) * se),
    upper = predy + (qt(.975, df=47) * se)
  )
```

And finally we can plot! 

```{r}
ggplot(plotdata, aes(x = x1, y = predy, 
                     ymin = lower, ymax = upper)) +
  geom_line() + 
  geom_ribbon(alpha = .3) # alpha sets the transparency
```

:::





<div class="divider div-transparent div-dot"></div>



# more model comparisons  

## series of incremental comparisons

TODO 

```{r}
eg_modelA <- lm(y ~ 1, data = mlr_data)
eg_modelB <- lm(y ~ 1 + x1, data = mlr_data)
eg_modelC <- lm(y ~ 1 + x1 + x2, data = mlr_data)
eg_modelD <- lm(y ~ 1 + x1 + x2 + x3, data = mlr_data)
anova(eg_modelA, eg_modelB, eg_modelC, eg_modelD)
```

:::imp
THE ORDER MATTERS
:::

```{r}
anova(eg_modelD)
```

## quickly comparing with models that drop just 1 predictor


note that this differs

```{r}
drop1(eg_modelD, test="F")
```

these test each predictor as if it were the "last one in"
(you'll note that the x3 test is the same as above).  

```{r}
eg_modelD_1 <- lm(y ~ 1 + x2 + x3, data = mlr_data)
eg_modelD_2 <- lm(y ~ 1 + x1 + x3, data = mlr_data)
eg_modelD_3 <- lm(y ~ 1 + x1 + x2, data = mlr_data)

eg_modelD <- lm(y ~ 1 + x1 + x2 + x3, data = mlr_data)

anova(eg_modelD_1, eg_modelD)
anova(eg_modelD_2, eg_modelD)
anova(eg_modelD_3, eg_modelD)
```

:::imp
WHEN WE MOVE TO MORE COMPLICATED MODELS, THIS IS SENSITIVE TO HOW WE CODE OUR CATEGORICAL PREDICTORS.  
:::







<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>


