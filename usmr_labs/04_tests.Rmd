---
title: "More tests"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(patchwork)
set.seed(017)
survey_data <- read_csv("https://uoepsy.github.io/data/surveydata_allcourse.csv") 
```

<!-- :::red -->
<!-- **Preliminaries**   -->

<!-- - Be sure to check the [**solutions to last week's exercises**](03_nhst.html). You can still ask any questions about previous weeks' materials if things aren't clear!      -->

<!-- 1. Open Rstudio, make sure you have the USMR project open, and create a new RMarkdown document (giving it a title for this topic).  -->

<!-- ::: -->

# $\chi^2$ Goodness of Fit Test 

:::frame
As you may remember, in the survey we asked students to complete in welcome week, one of the questions concerned the month in which you were born. You can download the data from https://uoepsy.github.io/data/surveydata_allcourse.csv.  

We need to make sure we know how many observations we have. While there are currently `r nrow(survey_data)` observations in our sample, we may have missing data in our variable of interest! 

We can use filter to remove any which are missing (or `NA`s).  
Take a bit of time to look at what each bit of this does. To help figure it out, try seeing what `is.na(c(NA,1,2))` does. Then try `!is.na(c(NA,1,2))`.  
```{r}
survey_data <- read_csv("https://uoepsy.github.io/data/surveydata_allcourse.csv") 
survey_data <-
  survey_data %>% filter(!is.na(birthmonth))
```
:::

`r qbegin("A1")`
What is your intuition about the distribution of all students' birth-months?  
Do you think they will be spread uniformly across all months of the year (like a fair 12-sided dice), or do you think people are more likely to be born in certain months more than others?  

Plot the distribution and get an initial idea of how things are looking.  

*Tip:* You can do this quickly with `barplot()` and `table()`, or you could create try using `ggplot()` and looking into `geom_bar()`.  
  
*(We've made the solutions visible immediately for this question, because they contain some useful little bits of code for re-ordering the months)*
`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)`
The quick and dirty way:
```{r}
barplot(table(survey_data$birthmonth))
```

A ggplot option:
```{r}
ggplot(data = survey_data, aes(x = birthmonth)) +
    geom_bar() +
    labs(x = "- Birth Month -")
```
To order the months correctly, we need to make the "birthmonth" variable a *factor* (a categorical variable), and order the levels:
```{r}
# we can make use of month.abb which is the abbreviated months in R
month.abb
# but we want them all lower case:
tolower(month.abb)

#This is one way to do it
survey_data$birthmonth <- factor(survey_data$birthmonth, levels = tolower(month.abb), ordered=T)

# and this is another (the tidyverse way):
survey_data <-
  survey_data %>%
    mutate(
      birthmonth = fct_relevel(factor(birthmonth), tolower(month.abb))
    )
```
Now when we plot it, they're in the right order!
```{r}
ggplot(data = survey_data, aes(x = birthmonth)) +
    geom_bar() +
    labs(x = "- Birth Month -")
```
`r solend()`


`r qbegin("A2")`
We're going to perform a statistical test to assess the extent to which our data conforms to the hypothesis that people are no more likely to be born on one month than another.  

Under this hypothesis, what would be the proportional breakdown of observed births in each of the months? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
If people are no more likely to be born in one month than another, then we would expect the same proportion of observed births in each month.  
There are 12 months, so we would expect $\frac{1}{12}$ observations in each month.  

We can write these as: 
$$
\begin{align}
& p_{jan} = 1/12 \\
& p_{feb} = 1/12 \\
& ... \\
& p_{dec} = 1/12 \\
\end{align}
$$
`r solend()`

`r qbegin("A3")`
Given that there are there are `r nrow(survey_data)` people who entered their birth-month, how many observations would we *expect* to find with a birthday in January? And in February? ... and so on? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We would expect $\frac{1}{12} \times$ `r nrow(survey_data)` = `r round(nrow(survey_data)/12,2)` observations in each month. 
`r solend()`

:::statbox
The test procedure we are about to apply is called a **chi-square goodness-of-fit test**.

It applies to a categorical variable, and the null hypothesis asserts specific values for the population proportion in each category. The alternative hypothesis simply states that at least one of the population proportions is not as specified in the null hypothesis.  

As always, the test statistic measures how far the observed sample results deviate from what is expected if the null hypothesis is true. With a chi-square test, you construct the test statistic by comparing the **observed sample counts** in each category to the **expected counts** under the null hypothesis.  

The test-statistic (denoted $\chi^2$, spelled *chi-square*, pronounced "kai-square") is obtained by adding up the standardized squared deviations in each category:  
$$
\chi^2 = \sum_{i} \frac{(\text{Observed}_i - \text{Expected}_i)^2}{\text{Expected}_i}
$$
:::

`r qbegin("A4")`
The code below creates counts for each month: 
```{r eval=FALSE}
survey_data %>%
    group_by(birthmonth) %>%
    summarise(
        observed = n()
    )
```
(A shortcut for this would be `survey_data %>% count(birthmonth)`)  

Add to the code above to create columns showing:

- the expected counts $E_i$
- observed-expected ($O_i - E_i$)
- the squared differences $(O_i - E_i)^2$
- the standardised square differences $\frac{(O_i - E_i)^2}{E_i}$  

Then calculate the $\chi^2$ statistic (the sum of the standardised squared differences).  
If your observed counts matched the expected counts *perfectly*, what would the $\chi^2$ statistic be? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
# the nrow() function tells us how many rows there are in the dataset!
totaln <- nrow(survey_data)

chi_table <- 
    survey_data %>%
    group_by(birthmonth) %>%
    summarise(
        observed = n(),
        expected = totaln/12,
        diff = observed-expected,
        sq_diff = diff^2,
        std_sq_diff = sq_diff / expected
    )
chi_table
```

And we can calculate our $\chi^2$ test statistic by simply summing the values in the last column we created:
```{r}
sum(chi_table$std_sq_diff)
```

If all our observed counts are equal to our expected counts, then the `diff` column above will be all $0$, and $0^2=0$, and $\frac{0}{E_i}$ will be $0$. So $\chi^2$ will be $0$. 
`r solend()`

`r qbegin("A5")`
You can see the distribution of $\chi^2$ distributions with different degrees of freedom below.  
```{r chidist, echo=FALSE, fig.cap="Chi-Square Distributions"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/1200px-Chi-square_pdf.svg.png")
```

We can find out the proportion of the distribution which falls to either side of a given value of $\chi^2$ using `pchisq()`. We need to give it our calculated $\chi^2$ statistic, our degrees of freedom (`df`), which is equal to the number of categories minus 1. We also need to specify whether we want the proportion to the left (`lower.tail=TRUE`) or to the right (`lower.tail=FALSE`).  

1. Using `pchisq()`, calculate the probability of observing a $\chi^2$ statistic as least as extreme as the one we have calculated.  
2. Check that these results match with those provided by R's built-in function: `chisq.test(table(survey_data$birthmonth))`.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
sum(chi_table$std_sq_diff)
pchisq(sum(chi_table$std_sq_diff), df = 11, lower.tail = FALSE)
```

```{r}
chisq.test(table(survey_data$birthmonth))
```
`r solend()`

`r qbegin("A6")`
Which months of year had the highest contributions to the chi-square test statistic?  

*Hint:* think about your standardised squared deviations. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
**Standardized squared deviations**

One possible way to answer this question is to look at the individual contribution of each category to the $\chi^2$ statistic. We computed these values in Question A4.  
```{r}
chi_table %>%
  select(birthmonth, std_sq_diff)
```

From the barplot we created earlier on, we can see which months make up higher/lower proportions than expected:
```{r}
ggplot(chi_table, aes(x = birthmonth, y = observed/nrow(survey_data))) +
  geom_col(fill = 'lightblue') +
  geom_hline(yintercept = 1/12, color = 'red') +
  theme_classic(base_size = 15)
```

**Pearson residuals**

Equivalently, you could answer by looking at Pearson residuals:
```{r}
chisq.test(table(survey_data$birthmonth))$residuals
```

```{r echo=FALSE}
presids <- chisq.test(table(survey_data$birthmonth))$residuals
presids <- presids[order(abs(presids),decreasing = TRUE)]
```

The greatest *absolute* values are for `r names(presids)[1]` and `r names(presids)[2]`, showing that for these months the deviations from expected to observed were the greatest. 

`r solend()`


`r qbegin("A7")`
According to the internet (that reliable source of information!), 76% of people in the world have brown eyes, 10% have blue, 5% hazel, 5% amber, 2% green, 1% grey, and 1% have some other eye colouring (red/violet/heterochromia).

Perform a $\chi^2$ goodness of fit test to assess the extent to which our sample of students conform to this theorised distribution of eye-colours.  
  
- *Hint:* `chisq.test(..., p = c(?,?,?,...) )`.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Let's look at the observed counts:
```{r}
table(survey_data$eyecolour)
```
Our theoretical probabilities of different eye colours must match the order in the table which we give `chisq.test()`. They must also always sum to 1.  
```{r}
chisq.test(table(survey_data$eyecolour), p = c(.05,.1,.76,.02,.01,.05,.01))
```
`r solend()`

`r qbegin("A8")`
What are the *observed* proportions of our sample with each eyecolour?  
Can you figure out how to use the `prop.table()` function?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
From the help documentation (`?prop.table()`), we see that we can pass `prop.table()` the argument `x`, which needs to be a table. 
```{r}
prop.table(table(survey_data$eyecolour))*100
```

```{r}
barplot(prop.table(table(survey_data$eyecolour))*100)
```

`r solend()`

# $\chi^2$ Test of Independence  

:::frame
> **Research Question:** Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?  
> A [study](https://doi.org/10.1111/j.1559-1816.2002.tb00266.x) published in the Journal of Applied Social Psychology^[Gueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. _Journal of Applied Social Psychology, 32_(9), 1955-1963.] investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. 
The waiter randomly assigned coffee-ordering customers to one of three groups. 
When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all. 

The data are available at https://uoepsy.github.io/data/TipJoke.csv.  
The dataset contains the variables:

- `Card`: None, Joke, Ad.
- `Tip`: 1 = The customer left a tip, 0 = The customer did not leave tip. 

:::

`r qbegin("B1")`
Produce a plot and a table to display the relationship between whether or not the customer left a tip, and what (if any) card they received alongside the bill.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
tipjoke <- read_csv('https://uoepsy.github.io/data/TipJoke.csv')

table(tipjoke$Card, tipjoke$Tip)

plot(table(tipjoke$Card, tipjoke$Tip))
```

`r solend()`

`r qbegin("B2")`
What would you *expect* the cell counts to look like if there were no relationship between what the waiter left and whether or not the customer tipped?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
In total, 60 customers tipped (14+30+16), and 151 did not. So *overall*, 0.28 ($\frac{60}{(60+151)}$) of customers tip.  

74 customers got an Ad card, 72 customers got a Joke, and 65 got None. If this were independent of whether or not they left a tip, we would expect equal proportions of tippers in each group.  
So we would expect 0.28 of each group to leave a tip.  

You can think about observed vs expected by looking at the two-way table along with the marginal row and column totals given:   
```{r echo=FALSE}
table(tipjoke$Card, tipjoke$Tip) %>%
  rbind(colSums(.)) %>% 
  cbind(rowSums(.)) -> conttable
conttable[1:3,1:2]<-"  "
conttable %>% knitr::kable(.) %>%
  kableExtra::kable_styling()
```
For a given cell of the table we can calculate the expected count as $\text{row total} \times \frac{\text{column total}}{\text{samplesize}}$:

**Expected:**
```{r echo=FALSE}
table(tipjoke$Card, tipjoke$Tip) %>%
  rbind(colSums(.)) %>% 
  cbind(rowSums(.)) -> conttable
conttable[1:3,1]<-round(conttable[1:3,3]*(conttable[4,1])/conttable[4,3],2)
conttable[1:3,2]<-round(conttable[1:3,3]*(conttable[4,2])/conttable[4,3],2)
conttable %>% knitr::kable(.) %>%
  kableExtra::kable_styling()
```


(If you're wondering how we do this in R, we saw in the lectures briefly a complicated bit of code using `%o%` which could do this for us):
```{r}
t <- tipjoke %>%
  select(Card, Tip) %>% table()

e <- rowSums(t) %o% colSums(t) / sum(t)
e
```
`r solend()`

`r qbegin("B3")`
Just like we gave the `chisq.test()` function a table of observed frequencies when we conducted a goodness of fit test in earlier exercises, we can give it a two-way table of observed frequencies to conduct a test of independence.  

Try it now.  

> **Why is the degrees of freedom 2 (df = 2)?**  
Degrees of freedom for a $\chi^2$ test of independence is calculated as:  
$$
\begin{align}
& df = (r - 1)(c - 1) \\
& \text{Where:} \\
& r = \text{number of rows} \\
& c = \text{number of columns} \\
\end{align}
$$
> Why? Well, remember that the degrees of freedom is the number of values that are *free to vary* as we estimate parameters. In a $3 \times 2$ table like the one we have for Cards $\times$ Tips, the degrees of freedom is the number of cells in the table that can vary before we can simply calculate the values of the other cells (where we're constrained by the need to sum to our row/column totals). 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
chisq.test(table(tipjoke$Card, tipjoke$Tip))
```
`r solend()`

# More RMarkdown

`r qbegin()`
Can you create an RMarkdown document which:

1. Reads in the https://uoepsy.github.io/data/TipJoke.csv data.
2. Conducts and reports a $\chi^2$ test of independence examining  whether telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer.
3. Successfully compiles ("knits") into an **.html** file. 

`r qend()`

# Revisiting NHST  

Now that you have performed a number of different types of statistical hypothesis test, it is worth revisiting the general concept in order to consolidate what you've been doing. 

- Step 1. We have been starting by considering what a given statistic is *likely to be* if a given hypothesis (the null) were true.
    - For the $t$-tests, if the null hypothesis is true (there is no difference between group means/between our observed mean and some value), then our $t$-statistics (if we could do our study loads of times) will mainly fall around 0, and follow a $t$-distribution. The precise $t$-distribution depends on the degrees of freedom, which in turn depends on how much data we have.  
    - For the $\chi^2$ tests, if the null hypothesis is true and there is no difference between the observed and expected frequencies, then our $\chi^2$-statistics will follow the $\chi^2$ distribution (i.e., with 2 categories, most of them will be between 0 and 2, with fewer falling >2, see the yellow line in Figure \@ref(fig:chidist)).   
- Step 2. We calculate our statistic from our *observed* data. 
- Step 3. We ask what the probability is of getting a statistic at least as extreme as we get from Step 2, assuming the null hypothesis we stated in Step 1.  

:::lo
**If you're finding the programming easy, but the statistical concepts difficult**

Another way which *might* help to think about this is that if we can make a computer do something over and over again, we can do stats! You may already be familiar with this idea from exercises with the function `replicate()`!  

<div style="width:60%; display: inline-block">

1. make the computer generate random data, based on some null hypothesis. Do it lots of times.  
  
</div>
<div style="width:35%; display: inline-block; vertical-align:top">
```{r echo=FALSE}
knitr::include_graphics("images/moretests/playmo_investors.jpg")
```
</div>

2. what proportion of the simulations produce results similar to the observed data (i.e., as extreme or more extreme)? This is $p$. The only difference between this and "statistics" is that we calculate $p$ using math, rather than having to generate random data. 

:::



# Statistical vs Practical Significance

`r qbegin("C1")`
An agricultural company is testing out a new fertiliser they have developed to improve tomato growth.  

They plant 1000 seeds (taken from the same tomato plant) in the same compost and place them in positions with the same amount of sunlight. 500 of the plants receive 100ml of water daily, and the other 500 receive a 100ml of the fertiliser mixed with water. After 100 days, they measure the height of all the tomato plants (in cm).  

You can find the data at https://uoepsy.github.io/data/tomatogrowth.csv. 

Conduct the appropriate test to determine whether the fertiliser provides a statistically significant improvement to tomato plant growth.   

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("images/moretests/playmotom.jpg")
```


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
tomato <- read_csv("https://uoepsy.github.io/data/tomatogrowth.csv")
summary(tomato)
```

Our alternative hypothesis is that the difference in means $(treatment - control)$ is greater than 0 (i.e., it improves growth). The `t.test()` function will use alphabetical ordering of the `group` variable, so if we say `alternative="less"` then it is the direction we want $(control - treatment < 0)$:
```{r}
t.test(tomato$height ~ tomato$group, alternative = "less")
```

Or, we can set the order by making it a factor and setting the `levels`:  
```{r}
tomato %>% mutate(
  group = factor(group, levels=c("treatment","control"))
) -> tomato

t.test(tomato$height ~ tomato$group, alternative = "greater")
```
`r solend()`

`r qbegin("C2")`
```{r echo=FALSE, fig.cap="Relationship between tomato plant height and crop yield"}
tibble(
  height = rep(50:200,each=5),
  number_tomatoes = height*0.2 + rnorm(755,0,10)
) %>% ggplot(.,aes(x=height, number_tomatoes))+
  geom_point()+
  geom_smooth(method="lm",se=F)
```

Consider the above plot. For every 5cm taller a tomato plant is, it tends to provide 1 more tomato.  
Now consider the agricultural company's situation in the previous question. Given that the fertiliser is comparitively pricey for them to manufacture, is it worth putting into production?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
While the fertiliser does improve plant growth to a statistically significant (at $\alpha=0.05$) degree, the improvement is minimal. The difference in means is only 1.2737, which would result in about 1/4 of a tomato per plant.   

Furthermore, if we take a look at the *confidence interval* provided by the `t.test()` function, we can see that a plausible value for the *true difference* in means is 0.23cm, which is tiny!  
`r solend()`



## Things to think about

The above exercises are aimed to demonstrate that whether or not our p-value is below some set criteria (e.g., .05, .01, .001) is only a small part of the picture. There are many things which are good to remember about p-values:

1. With a big enough sample size, even a tiny tiny effect is detectable at <.05. For example, you might be interested in testing if the difference in population means across two groups is 0 ($\mu_1 - \mu_2 = 0$. Your calculated sample difference could be $\bar{x}_1 - \bar{x}_2 = 0.00002$ but with a very small p-value of 0.00000001. This would tell you that there is strong evidence that the observed difference in means (0.00002) is significantly different from 0. However, the practical difference, that is - the magnitude of the distance between 0.00002 and 0 - is negligible and of pretty much no interest to practitioners. This is the idea we saw in the tomato-plant example. 
  
2. The criteria ($\alpha$) which we set (at .05, .01, etc.), is *arbitrary*.  
Two things need to be kept in mind: there is the true status of the world (which is unknown to us) and the collected data (which are available and reveal the truth only in part).  
An observed p-value smaller than the chosen alpha does not imply the true presence of an effect. The observed difference might be due to sampling variability.  
```{r echo=FALSE, fig.cap="Two possible samples (blue dots) drawn from two populations with same mean. On the left, the selected sample shows a big difference. On the right, the sample shows no difference. Samples such as that on the left are very unlikely to happen (e.g., 5% of the time). It is for these unlikely samples that we would reject the null hypothesis incorrectly 5% of the time."}
knitr::include_graphics("images/moretests/samples.png")
```
  
3. Even if a null hypothesis about the population is actually true, then 5% (if $\alpha$ = 0.05) of the test-statistics computed on different samples from that population would result in a p-value <.05. If you were to obtain 100 random samples from that population, five out of the 100 p-values are likely to be <.05 *even if the null hypothesis about the population was actually true*.  

4. If you have a single dataset, and you perform several tests of hypotheses on those data, each test comes with a probability of incorrectly rejecting the null (making a *type I error*) of 5%. Hence, considering the entire family of tests computed, your overall type I error probability will be larger than 5%. In simple words, this means that if you perform enough tests on the same data, you’re almost sure to reject one of the null hypotheses by mistake. This concept is known as **multiple comparisons**.  

# Further reading

There are many different competing approaches to doing statistical analyses.  
In this course we are learning about what is known as the *frequentist* framework. Roughly speaking, this is where probabilities are defined as "long-run frequencies" (i.e., the probability of $x$ happening over many many trials^[For those of you who are interested in what alternative definitions there are, do a google search for "frequentist vs bayesian". Be prepared that this will open a big can of worms!]). Even within the *frequentist* approach, there are different views as to how to how this definition of probability is best utilised.  
  
The following links provide some introductory readings to some of the different schools of thought:  

1. [Perezgonzalez, J. D. (2015). Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing. Frontiers in Psychology, 6, 223.](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00223/full)  
  
2. [Calin-Jageman, R. J., & Cumming, G. (2019). The new statistics for better science: ask how much, how uncertain, and what else is known. The American Statistician, 73(sup1), 271-280.](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518266)  
  
3. [The correctly-used p value needs an effect size and CI](https://thenewstatistics.com/itns/2019/05/20/reply-to-lakens-the-correctly-used-p-value-needs-an-effect-size-and-ci/) - don't worry too much about the background of this blog, but it offers some useful visualisations to show how important it is to remember about the uncertainty in our estimates.  

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>



