---
title: "10A: The generalised linear model"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(xaringanExtra)
xaringanExtra::use_panelset()
```

# Quick recaps

`r optbegin("Simple linear regression", olabel=FALSE, show=TRUE, toggle=params$TOGGLE)`
Things have moved pretty quickly in the last couple of weeks.   
In the simple linear regression, we defined our model through a number of components:  

- $b_0$: the "Intercept". This is the predicted value of $y$ where $x = 0$. In the plot below, it is the predicted value of wellbeing where outdoor_time is 0 (where the regression line cuts the y-axis) 
- $b_1$: the "Coefficient for $x$". This is the predicted *increase* in $y$ for every unit increase of $x$. In the plot below, this is the amount the regression line goes up for every one unit we move across.  
- $\varepsilon$: the "Residuals" or "Error term". These are the distances from each of our observed values to the model predicted values. The standard deviation of all these gets denoted $\sigma_\varepsilon$. In the plot below, these are the vertical distances from each red dot to the black line.  
    
```{r}
#| echo: false
mwdata = read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
mwdata %>% mutate(x=outdoor_time,y=wellbeing) -> mwdata
with(mwdata, plot(x,y, col="red",pch=16))
abline(lm(y~x, mwdata))
```
`r optend()`

`r optbegin("Multiple linear regression", olabel=FALSE, show=TRUE, toggle=params$TOGGLE)`
When we extended this to multiple regression, we essentially just added more explanatory variables, meaning we fitted more coefficients.  

The values of $y$ are now modelled as some relationship of more than just one $x$ variable.  
   
In the plot below with two predictors, we would have a coefficient for each one, showing the slope of the regression surface along each dimension.  

The residuals continue to be the distances from the observed values to our model predicted values. In the plot below, these are the vertical distances from each red dot to the surface. 

```{r}
#| echo: false
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=50
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)


p <- persp(outdoor_time,social_int,wellbeing, theta = 40,phi=10, col = NA,xlab="x1",ylab="x2",zlab="y")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)
```
`r optend()`

`r optbegin("Interactions", olabel=FALSE, show=TRUE, toggle=params$TOGGLE)`
We then saw how we can let the relationships between the outcome $y$ and an explanatory variable $x1$ vary *as a function of* some other explanatory variable $x2$.  
  
In the plot below, if someone asks you "what is the relationship between $x1$ and $x$, your reply should (hopefully) be  'it depends on what the value of $x2$ is'  
  
This is captured in the curvature of the regression surface below, and we estimate it with a combination of 3 coefficients:  
  
1.  the association between $x1$ and $y$ where $x2=0$  
(this is the slope of the surface where it meets the **front** face of the cube)  
2. the association between $x2$ and $y$ where $x1=0$  
(this is the slope of the surface where it meets the **left** of face of the cube)  
3. the amount to which **1.** and **2.** are adjusted when there is an increase in the other variable. 

```{r}
#| echo: false
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")

fit<-lm(dass ~ scs*zn, data = scs_study)
steps=50
scs <- with(scs_study, seq(min(scs),max(scs),length=steps))
zn <- with(scs_study, seq(min(zn),max(zn),length=steps))
newdat <- expand.grid(scs=scs, zn=zn)
dass <- matrix(predict(fit, newdat), steps, steps)
p <- persp(scs,zn,dass, theta = 10,phi=15, col = NA,
           xlab = "x1",zlab="y",ylab="x2")

```
`r optend()`  


`r optbegin("Categorical explanatory variables", olabel=FALSE, show=TRUE, toggle=params$TOGGLE)`
Throughout these, we also saw how this applied when we had explanatory variables that were **categorical**.  

They were entered in to the model as a set of "dummy variables" (zeroes and ones), representing differences between groups. If we had a categorical variable which contained $k$ levels (or "groups", or "categories"), then we R will put $k-1$ dummy variables into the model for us, and we will get out $k-1$ coefficients.  

The default behaviour in R is that, depending on what we set as our **reference level**, the coefficients represent the difference from this group to each of the others.  
In the plots below, if "Group1" is the reference, then the coefficient `groupGroup2` would represent the vertical distance from the red line to the green line, and the coefficient `groupGroup3` would be the vertical distance from the red line to the blue line.  

```{r}
#| echo: false
library(patchwork)
mwdata <- mwdata %>% mutate(group=fct_recode(factor(location), Group1="City",Group2="Rural",Group3="Suburb"))
    ggplot(mwdata, aes(x=group, y=wellbeing,col=group))+
    geom_jitter(width = .1, height = 0)+
    stat_summary(fun = mean, geom="errorbar", aes(ymax=..y..,ymin=..y..,col=group),lwd=2) + 
        labs(title="y ~ group",subtitle="separated",y="y") + 

    ggplot(mwdata, aes(x=0, y=wellbeing,col=group))+
    geom_jitter(width = .1, height = 0)+
    stat_summary(fun = mean, geom="errorbar", aes(x=0,ymax=..y..,ymin=..y..,col=group),lwd=2) + 
    scale_x_continuous(NULL, breaks=NULL) +
        labs(title="y ~ group",subtitle="(collapsed)",y="y")+
        
    plot_layout(guides="collect")

```

*Note - because categorical predictors are coded as various dummy variables of zeroes and ones, the assumption of linearity is intrinsically there. The only possible way to get from the red line to the green line is via a straight line.*  

We also saw how this same logic applies when categorical explanatory variables are involved in an interaction, as in @fig-catintglm. 

```{r}
#| label: fig-catintglm
#| fig-cap: "https://uoepsy.github.io/usmr/2223/lectures/lecture_9.html#62"
#| echo: false
knitr::include_graphics("images/glm/catint.png")
```

The coefficients provide:    

1. coefficient for practice: the increase in 'reading age' associated with an increase of 1 unit of 'practice', when the dummy variables for 'method' are all 0  
    + (Where the reference level is *phonics*, this is the slope of the red line)
2. the coefficient for methodWord: the change in 'reading age' associated with the move from the reference level *phonics* to *word*, when 'practice' = 0.  
    + (This is the distance between the two lines where they meet the y-axis).  
3. the coefficient for the interaction: the adjustment to 1 and 2 associated with an increase of 1 in other variable (which for the categorical variable means the move to the other group).  

`r optend()`  

# Continuous outcomes!  

```{r}
#| include: false
set.seed(2)
dd <- tibble(
  ppt = paste0("fisherperson ", 1:120),
  hours = round(rchisq(120, 8)*.5, 1),
  lp = -2 + .6*hours,
  n_fish = rpois(120, lambda=exp(lp)),
  tiredness = round(56 + scale(lp + rnorm(120))[,1]*16.47),
  caught_any = rbinom(120, size = 1, prob = plogis(lp)),
) %>% mutate(n_fish = ifelse(caught_any==0, 0, n_fish)) %>%
  select(-lp)
# write_csv(dd, "../../data/fishing_exped.csv")
#with(dd,plot(n_fish~hours))
#with(dd,plot(caught_any~hours))
#with(dd,plot(tiredness~hours))
```

In all of the models thus far, we have been dealing with outcomes variables that are *continuous*. These are variables for which an observation can take _any_ real value - i.e. 3.4, or 1.50365, or 0.000532. We might not have have measured these variables with any degree of accuracy, but the models we have been fitting have fundamentally assumed this. 

For instance, even if our outcome is the "sleep quality rating" from our class survey, measured in whole numbers from 0 to 100, our model predictions will be values that are much more specific, and my model predicted sleep quality might be 36.54374. This is because we are modelling a continuous line through the sleep quality ratings. This also means that the model extends to scores of from negative to positive infinity ($- \infty < y < \infty$), even though we know our scale is bounded at 0 and 100 (see @fig-sleepqualzoom).  

Often, this is a perfectly acceptable thing to do, especially when we consider the outcome variable to be measuring a continuous thing (e.g. sleep quality), just hindered by the precision of measurement, and when the bounds of our measurement tool are not preventing us from capturing variation (e.g. if everyone rates sleep as 100, there is no variance to explain).  
  
```{r}
#| code-fold: true
#| label: fig-sleepqualzoom
#| fig-cap: "Our sleep quality ~ loc model is a continuous line extending infinitely"  
library(tidyverse)
library(patchwork)
usmr <- read_csv("https://uoepsy.github.io/data/usmr2022.csv")
ggplot(usmr, aes(x = loc, y = sleeprating)) +
  geom_point() + 
  geom_smooth(method=lm) + 
  labs(title="our model") +
  
ggplot(usmr, aes(x = loc, y = sleeprating)) +
  geom_point() + 
  geom_smooth(method=lm) + 
  geom_smooth(method=lm, se = FALSE, fullrange = TRUE) + xlim(-50,50) + 
  labs(title = "our model (zoomed out)")
```


# Other outcomes 

There are lots of variables that we collect that don't fit with this way of thinking. For instance, what if we want to model something that takes the form of a __binary variable:__^[we have seen this type of variable already when looking at categorical predictors in our models, but we haven't seen how we use it as the *outcome*]. 

- being Y vs not being Y  
- Y happening vs Y not happening  

For binary variables, values can only take one of two values, which we often encode as 0 or 1. Values can't be 0.5, -2, or 1.3, they can _only_ be 0 or 1.

Another type of outcome that we can't really model as continuous is __count variable__:  

- The number of Y's
- How many times Y happened

For count data, values can't take *any* real number, they can only take integers (0, 1, 2, 3, ...).  


:::frame
__An example__  

At a lake, I survey 120 people who have just finished fishing for the day. I'm interested in how the length of time they spent fishing is associated with: 

1. how tired they are from 0 to 100 (continuous)
2. catching *any* fish (binary, 1 = yes, 0 = no)
3. the number of fish caught (count)

We can try at first to visualise all these relationships as scatterplots (@fig-outcomeplots). 

We can happily model our first outcome, tiredness, with a linear model. In @fig-outcomeplots (top panel), we can see that the relationship looks linear, and the residuals are spread fairly consistently above and below the fitted line.  

For the binary outcome however (middle panel), there are a couple of things that are problematic. For someone who spends 8 hours fishing, the model estimates approximately **1.1** "catching any fish". But what does that 1.1 really mean? Does it mean that if I fish 8 hours I will have 1.1 fish? no, because it's not a count. Does it mean there is a 110% probability of catching fish? This is closer, but again it doesn't make much sense - how can we have a probability of 115%?  

For the count variable (bottom panel), we can also see that the standard `lm` model looks problematic. Again it estimates values we simply _cannot_ have (it looks like if i spend 1 hour fishing I will catch about negative 3 fish?). Another thing to note is that the variance in the points on the y axis tends to increase as we get higher. This is typical of count data - we're going to see lots of people catching 0, 1 or 2 fish, and so the "people catching few fish" are all quite close (in number of fish) to one another, but because fewer people catch 20 fish, 30 fish etc, there is a lot more variability in "number of fish" when we get higher up. This is a problem for a linear model, which assumes _constant variance_ across the fitted model.  

```{r}
#| code-fold: true
#| label: fig-outcomeplots
#| fig-cap: "Three different outcomes: continuous (top), binary (middle), count (bottom)"
#| fig-height: 8
fish <- read_csv("https://uoepsy.github.io/data/fishing_exped.csv")

p1 <- ggplot(fish, aes(x = hours, y = tiredness)) + 
  geom_point() +
  labs(title="How tired are you (0 - 100)") +
  geom_smooth(method=lm, se=FALSE)

p2 <- ggplot(fish, aes(x = hours, y = caught_any)) + 
  geom_point() +
  labs(title="Did you catch any fish?") +
  geom_smooth(method=lm, se=FALSE) + 
  scale_y_continuous(limits=c(0,1.2),breaks=seq(0,1.2,.2))
  
p3 <- ggplot(fish, aes(x = hours, y = n_fish)) + 
  geom_point() +
  labs(title="How many fish did you catch?") + 
  geom_smooth(method=lm, se=FALSE)
  
p1 / p2 / p3
```

:::

# Introducing the GLM

To deal with these sorts of outcome variable, we need to learn about the **Generalised Linear Model (GLM)**. 

The GLM involves a little bit of trickery, in which we translate something that is inherently not linear (e.g. the _probability_ of Y, or the increase in numbers of Y) into something which is both linear and unbounded, and model that instead.  

The problem, fundamentally, is that probability is not linear. We think of it as getting ever and ever closer, but never reaching - 0 and 1 (@fig-plotprobab)

```{r}
#| echo: false
#| label: fig-plotprobab
#| fig-cap: "As x increases, probability of y gets closer and closer (but never meets) 1"
#| fig-height: 3.5
pt <- tibble(x=seq(-4.5,4.5,length=49),lo=x,o=exp(x),p=o/(1+o))
pt %>% ggplot(aes(x=x,y=p)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  ylab("probability of y")+xlab("x")
```



To use our linear toolbox for probability, we can't think in terms of modelling how $x$ influences the probability of $y$ _directly_, but we can model it in terms of how it influences the "log-odds of $y$". The same is true for count data: instead of modelling counts directly, we can model the log of the counts.  

:::statbox
**Probability, odds, and log-odds**  

If we let $p$ denote the probability of a given event, then:  

- $\frac{p}{(1-p)}$ are the odds of the event happening.    
For example, the odds of rolling a 6 with a normal die is 1/5 (sometimes this is expressed '1:5', and in gambling the order is sometimes flipped and you'll see '5/1' or 'odds of five to one').  
If we ever need to convert between the two:  
    + $odds = \frac{p}{(1-p)}$  
    + $p = \frac{odds}{1 + odds}$  
- $ln(\frac{p}{(1-p)})$ are the log-odds of the event.  

```{r}
#| echo: false
#| label: fig-plodd
#| out-width: "100%"
#| fig-height: 4
#| fig-cap: "Probability, Odds, and Log-Odds"
library(patchwork)
pt <- tibble(x=seq(-4.5,4.5,length=49),lo=x,o=exp(x),p=o/(1+o))

p1 <- pt %>% ggplot(aes(x=x,y=p)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 0,linetype="dashed") +
  ylab("probability p") +
  ggtitle("probability") +
  annotate("text",0,.75,label="p=.5",size=4, vjust=-0.5, angle=90)

p2 <- pt %>% ggplot(aes(x=x,y=o)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 0,linetype="dashed") +
  ylab("odds p/(1-p)") +
  ggtitle("odds") +
  annotate("text",0,70,label="odds=1",size=4, vjust=-0.5, angle=90)

p3 <- pt %>% ggplot(aes(x=x,y=lo)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  geom_vline(xintercept = 0,linetype="dashed") +
  ylab("log(odds)") +
  ggtitle("log-odds (logits)")

p4 <- p3 + annotate("text",0,2,label="log(odds)=0",size=4, vjust=-0.5, angle=90) +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_segment(x=1,xend=2,y=1,yend=1, lty="dotted",col="blue", lwd=1) + 
  geom_segment(x=2,xend=2,y=1,yend=2, col="blue",lwd=1) +
  annotate("text", x=1.5,y=1,vjust=1, label="1",col="blue")+
  annotate("text", x=2,y=1.5,hjust=-1, label="b",col="blue")

p1+p2+p4 & theme(plot.title = element_text(size=14))
```

:::sticky
__Conversions__  

Between probabilities, odds and log-odds ("logit"):    
$$
\begin{align}
&\text{for probability of Y for observation i:}\\
\qquad \\
odds_i & = \frac{p_i}{1-p_i} \\
\qquad \\
logit_i &= log(odds_i) = log(\frac{p_i}{1-p_i}) \\
\qquad \\
p_i & = \frac{odds_i}{1 + odds_i} = \frac{e^{logit_i}}{(1+e^{logit_i})}

\end{align}
$$
:::



__Counts and log-counts__  
We can apply the same log transformation to count data in order to think about how we might consider an association on a linear 
```{r}
#| echo: false
#| label: fig-clcnt
#| fig-height: 4
#| fig-cap: "counts and log(counts)"
pt <- tibble(x=seq(1,4,length=49),c=exp(x))

p1 <- pt %>% ggplot(aes(x=x,y=c)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 1,linetype="dashed") +
  ylab("count") +
  ggtitle("count")

p2 <- pt %>% ggplot(aes(x=x,y=x)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 1,linetype="dashed") +
  ylab("log(count)") +
  ggtitle("log(count)") +
  geom_segment(x=2,xend=3,y=2,yend=2, lty="dotted",col="blue", lwd=1) + 
  geom_segment(x=3,xend=3,y=2,yend=3, col="blue",lwd=1) +
  annotate("text", x=2.5,y=2,vjust=1, label="1",col="blue")+
  annotate("text", x=3,y=2.5,hjust=-1, label="b",col="blue")

p1 + p2
```


:::

`r optbegin("Optional: What are logarithms?",olabel=FALSE)`

:::sticky
__Essentials of logarithms__

A logarithm is a mathematical way to write any positive number as a power of a "base number". 

:::

:::frame
__Log base 10__

Consider the base number 10. The R function implementing the logarithm base 10 is `log10()`.

- To what power do I need to raise the base number 10 in order to obtain the desired number 100? 


```{r}
x <- 100  # 100 = 10^2
x

pow <- log10(x)
pow
```

log10(100) = 2 as we need to raise 10 to the power of 2 in order to obtain the desired number 100.


- To what power do I need to raise the base number 10 in order to obtain the desired number 1000? 

```{r}
x <- 1000  # 1000 = 10^3
x

pow <- log10(x)
pow
```


log10(1000) = 3 as we need to raise 10 to the power of 3 in order to obtain the desired number 1000.

Note that you can use this to write any desired number as a power of 10. For example, to which power do we raise 10 in order to obtain 13.5?

```{r}
x <- 13.5
x

pow <- log10(x)
pow
```

In fact, 10^1.130334 = 13.5 (up to rounding error)

```{r}
10^pow
```


An important property is that:

$$10^{\log_{10}(x)} = x$$

For example, $10^{\log_{10}(100)} = 10^{2} = 100$.

```{r}
10^log10(100)
```

:::
:::frame
__Log base e (aka natural logarithm)__  

A special number in mathematics is Euler's (or Napier's) number $e = 2.718282$. As well as a logarithm in base 10, telling us which power to raise the base 10 to, in order to obtain a desired number, we can also use any other bases such as $e$.

In R, you obtain Euler's number as follows:

```{r}
e <- exp(1)  # obtain the e number
e
```

The logarithm with base number $e$ is implemented in the R function `log()`.

- To which number do I need to raise $e$ in order to obtain 8?

```{r}
x <- 8
x

pow <- log(8)
pow
```

```{r}
e^pow
```

Up to rounding error, you see that you get 8 back.

To not have rounding errors, save the full result:

```{r}
pow <- log(8)
e^pow
```

An important property is that:

$$e^{\log(x)} = x$$

```{r}
e^log(8)
```
:::

`r optend()`

<br> 

This little trick allows us to be able to talk about _linear_ associations between predictors and the _log-odds of an event_, or between predictors and _log-counts_. In @fig-plodd, and @fig-clcnt above, we can see that by talking in terms of log-odds, or log-counts, we can model something that is linear and unbounded. So when we think about these models, what we are really have in mind is that we are capturing the associations seen in the right hand plots of @fig-plodd and @fig-clcnt.  

:::sticky
__The Generalised Linear Model__  

In its general form, the GLM looks pretty similar to the standard LM.  
we can write it as:  

$$
\color{green}{g(\color{red}{y})} = \mathbf{\color{blue}{b_0 + b_1(x_1) + ... + b_k(x_k)} }
$$
The difference is that $\color{green}{g}$ is a function (like log, or logit) that links the expected value of $\color{red}{y}$ to the linear prediction of $\color{blue}{b_0 + b_1(x_1) + ... + b_k(x_k)}$.  

:::

<br>

# Fitting GLM in R

In order to fit these models in R, we need to use the `glm()` function. This is (unexpectedly) a generalised form of the `lm()` function we have been using. The only difference is that we need to also tell the function the `family` of the response variable, and the "link function" (the mapping we use to get from the thing we want to model to the thing we _actually_ model, e.g. `"log"`, or `"logit"` (log-odds)).  

:::sticky
__Linear Regression__  
Outcome $y$ is continuous $y \in (-\infty, \infty)$  
$$
\color{red}{y} = \mathbf{\color{blue}{b_0 + b_1(x_1) + ... + b_k(x_k)}} 
$$
```{r}
#| eval: false
#| echo: true
linear_model <- lm(continuous_y ~ 1 + x1 + ... xk, data = df)
```

_As it happens, `lm()` is just a special case of this, where we use the "gaussian" family (Gaussian is another term for 'normal'), with the "identity" link function (the identity function is just saying 'model y directly'):_

```{r}
#| eval: false
glm(continuous_y ~ 1 + x1 + ... xk, data = df,
   family = gaussian(link = "identity"))
```
:::

:::sticky
__Logistic Regression__  
Outcome $y$ is binary, $y \in [0,1]$  
$$
\begin{align}
\color{red}{ln \left( \frac{p}{1-p} \right) } &= \mathbf{\color{blue}{b_0 + b_1(x_1) + ... + b_k(x_k)}} \\
\qquad \\
\text{where } \color{red}{p} &= \text{probability of event }y\\
\end{align}
$$
```{r}
#| eval: false
#| echo: true
logistic_model <- glm(binary_y ~  1 + x1 + ... xk, data = df,
                      family=binomial(link="logit"))
```

- `family = binomial` and `family = "binomial"` will also work. 
- the outcome can either be 0s and 1s, or coded as a `factor` with 2 levels.  

`r optbegin("Binomial? Binary?", olabel=FALSE)`
The "binomial" distribution, which is used to represent the number of successes in a sequence of $n$ independent trials (e.g. number of heads in 20 coin-flips follows a binomial distribution, see [Lecture 4!](https://uoepsy.github.io/usmr/2223/lectures/lecture_4.html#10){target="_blank"}).  
A binary response is a special case of this, where $n = 1$.  

The logistic regression model, more generally, can be written as: 

$$
\begin{align}
\color{red}{ln \left( \frac{p}{1-p} \right) } &= \mathbf{\color{blue}{b_0 + b_1(x_1) + ... + b_k(x_k)}} \\
\qquad \\
\text{where } Y &\sim Binomial(n, \color{red}{p})\\
\end{align}
$$

`r optend()`

:::

:::sticky
__Poisson Regression__  
Outcome $y$ is positive $y \in  (0, \infty)$
$$
\color{red}{ln (y) } = \mathbf{\color{blue}{b_0 + b_1(x_1) + ... + b_k(x_k)}}
$$
```{r}
#| eval: false
#| echo: true
poisson_model <- glm(count_y ~  1 + x1 + ... xk, data = df,
                     family=poisson(link="log"))
```

- `family = poisson` and `family = "poisson"` will also work

::: 


`r optbegin("Optional: A tangent: Model estimation", olabel=FALSE, show=TRUE, toggle=params$TOGGLE)`
**Maximum Likelihood Estimation**  

For a linear regression, R has been fitting our regression models by "minimising the residual sums of squares" (i.e., we rotate the line to find the point at which $\sum{(y - \hat{y})^2}$ is smallest. This gave us the "best fitting line" (@fig-resslineplot).
```{r}
#| label: fig-resslineplot
#| echo: false
#| out-width: "60%"
#| fig-cap: "https://uoepsy.github.io/usmr/2223/lectures/lecture_7.html#34"
knitr::include_graphics("images/glm/ss.png")
```

For the generalized linear models we can't do this. For instance, in logistic regression model, what we're really wanting is the "best fitting *squiggle*" (@fig-loglineplot), and to get to this we must do something else other than looking at our residuals $y - \hat{y}$.  
```{r}
#| label: fig-loglineplot
#| fig-cap: "Probability of Y across values of X"
#| fig-height: 3.5
#| echo: false
fish <- read_csv("https://uoepsy.github.io/data/fishing_exped.csv")

fish %>%
ggplot(aes(x=hours,y=caught_any)) +
  ylab("P(y)") +
  geom_jitter(size=3,height=.02,alpha=.1) +
  geom_smooth(method="glm",method.args=list(family=binomial),se=F) +
  scale_y_continuous(breaks=seq(0,1,by=.2))+
  scale_x_continuous("x", breaks=NULL)
```
The reason we have to do something different is because for our actual observations, the event has either happened or it hasn't. So we can't take the raw data as "probabilities" which we can translate into log-odds. We can try, but it doesn't make sense, and we would just be trying to fit some line between infinity and negative infinity (@fig-mlelogodd). This would mean the residuals would also all be infinity, and so it becomes impossible to work out anything! 

```{r}
#| echo: false
fish %>% transmute(
  observed = caught_any,
  odds = observed/(1-observed),
  log_odds = log(odds)
) %>% head(2L) %>% rbind(.,"...")
```

```{r}
#| echo: false
#| label: fig-mlelogodd
#| fig-cap: "In the log-odds world, comparing our predicted values to observed values of -Inf and Inf isn't very useful, because all those distances will be infinite (there is no way of measuring the dotted lines, so we can't really use them)"
f2 <- fish %>%
  mutate(fittedp = fitted(glm(caught_any~hours,., family="binomial")),
         fitted = log(fittedp/(1-fittedp)),
         y = ifelse(caught_any==0,min(fitted)-1,max(fitted)+1)
  )

p1 <- ggplot(f2, aes(x=hours))+
  geom_jitter(aes(y=y,col=caught_any==0),size=3,width=0,height=.04,alpha=.2) + 
  #geom_point(aes(y=fitted,col=caught_any==0),size=3,alpha=.8) +
  geom_path(aes(y=fitted),size=1)+
  geom_segment(aes(x=hours,xend=hours,y=fitted,yend=y,col=caught_any==0),lty="dotted", alpha=.5) +
  scale_y_continuous("Log-odds of Y", breaks=c(min(f2$fitted)-1,.5,max(f2$fitted)+1), labels=c("-Inf","0","Inf"))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_hline(aes(yintercept=.5))+
  guides(col=FALSE)+scale_x_continuous("x", breaks=NULL)
p1
```

Instead, **maximum likelihood estimation** is used to find the set of coefficients which best reproduce the observed data.  
To do this, the computer considers loads of different lines, trying to slowly converge on the one which _increases the likelihood of the data_.    

To understand this a bit better, let's take consider one possible line in the probability world. The left-hand plot of @fig-mleprob is the same line as the one shown in @fig-mlelogodd, and we can see it translated back into probabilities in the right-hand plot, so that we have a predicted probability of the outcome for each datapoint. From the right-hand plot, we can then ask "what is the likelihood of the data, given this line?". To get this, we multiply up all the probabilities that observed 1s are predicted as 1s and that observed 0s predicted as 0s. 

```{r}
#| echo: false
#| label: fig-mleprob
#| out-width: "100%"
#| fig-cap: "Note - the dotted lines are not residuals, but just to show the projections of observations down to the line"
p2 <- ggplot(f2, aes(x=hours))+
  geom_point(aes(y=caught_any,col=caught_any==0),size=3,alpha=.2) + 
  #geom_point(aes(y=fittedp,col=caught_any==0),size=3,alpha=.8) +
  geom_line(aes(y=fittedp),size=1)+
  geom_segment(aes(x=hours,xend=hours,y=fittedp,yend=caught_any,col=caught_any==0),lty="dotted", alpha=.4) +
  scale_y_continuous("Probability of Y", breaks=c(0,.5,1))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_hline(aes(yintercept=.5))+
  guides(col=FALSE)+
  scale_x_continuous("x", breaks=NULL)
p1 + p2
```

As an example @fig-mleprob2 shows the predicted probabilities for a couple of datapoints. The furthest left point is an observed 0, and so the likelihood of this data given the predicted probability is 1-0.12, or 0.88. The furthest right is an observed 1, so the likelihood of this is 0.98. This means the probability of seeing those two datapoints, given the line we have, is 0.98 * 0.88. But what we want is the probability of seeing _all_ our data, given this line. So we need to do this for _all_ of the datapoints.^[This means we will end up with a lot of multiplying (computationally expensive), and a tiny tiny number. So instead we typically talk about the log-likelihood, and use summation instead of multiplication. The log-likelihood of the two data points highlighted in @fig-mleprob, given the line is `log(0.98)+log(0.88)`.] Once we have this, we can compare the likelihood of all of seeing this data for a whole range of different lines.  

The cool thing is that computers do all this for us. *Maximum likelihood estimation* is just a method to find out which parameters maximise the likelihood of the data. In our context, it tells us what values of coefficients in $\color{red}{ln\left(\frac{p}{1-p} \right)} = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} X_1 }$ maximise the likelihood of reproducing the data we observed.

```{r}
#| echo: false
#| label: fig-mleprob2
#| fig-cap: "Note - the dotted lines are not residuals, but just to show the projections of observations down to the line"
ll <- f2 %>% arrange(hours) %>% slice(1,nrow(.)) %>% 
  mutate(lab = paste0("p(y) = ",round(fittedp,2)))

p2 +
  geom_segment(data=ll, aes(x=hours,xend=hours,y=fittedp,yend=caught_any,col=caught_any==0),lty="solid", alpha=1) + 
  geom_segment(
    x=ll$hours[1]+.3,xend=ll$hours[1],
    y=ll$fittedp[1]+.1,yend=ll$fittedp[1]
  ) + 
  geom_label(x=ll$hours[1],
             y=ll$fittedp[1]+.1,
             label=ll$lab[1], hjust=0) +
  geom_segment(
    x=ll$hours[2]-.3,xend=ll$hours[2],
    y=ll$fittedp[2]-.1,yend=ll$fittedp[2]
  ) + 
  geom_label(x=ll$hours[2],
             y=ll$fittedp[2]-.1,
             label=ll$lab[2], hjust=1)

```


```{r}
#| include: false

# basic model
glm(caught_any~1,fish,family=binomial)  %>% coef()
# odds to p
exp(0.2006707)/(1+exp(0.2006707))
# likelihood
0.55^sum(fish$caught_any) * 0.45^(sum(fish$caught_any==0))
# log likelihood 
log(0.55^sum(fish$caught_any)) + 
  log(0.45^(sum(fish$caught_any==0)))

glm(caught_any~1,fish,family=binomial)  %>% logLik()


tibble(
  y = fish$caught_any,
  p = fitted(glm(caught_any~hours,fish,family=binomial)),
  l = ifelse(y==1,p,1-p),
  ll = log(l)
) %>% 
  #pull(l) %>% prod()
  pull(ll) %>% sum()
glm(caught_any~hours,fish,family=binomial) %>% logLik()
```

`r optend()`

<br><br>

# Logistic Regression

:::lo
For the remainder of this reading, we're going to leave discussions of Poisson regression and focus on working with binary outcomes using logistic regression, which you are much more likely to encounter in your reading/research. A lot of the logic of these models transfers across to when we work with count data, but __you will *not* need poisson regression for any assignment on this course.__  
:::

Let's return to our fishing example, and fit a model in which we predict "having caught some fish" by the number of hours spent fishing.  

Practically, speaking, this quite straightforward. We have a binary outcome (1 = caught fish, 0 is didn't catch fish), so we'll fit a logistic regression. We have a single predictor, which is "hours spent fishing":  

```{r}
fish <- read_csv("https://uoepsy.github.io/data/fishing_exped.csv")
fishmod <- glm(caught_any ~ hours, data = fish,
               family = binomial(link="logit"))
summary(fishmod)
```

## Interpretation of Coefficients   

An important result of our model capturing the linear associations between the predictors and the **log-odds of outcome Y**, is that our coefficients are all in these units. 
```{r}
#| eval: false
summary(fishmod)
```
```{r}
#| echo: false
.pp(summary(fishmod), l=list(3,10:13))
```

- `(Intercept)`: the log-odds of having caught some fish when spending zero hours fishing are `r round(coef(fishmod)[1],2)`.  
- `hours` coefficient: for every hour someone has spent fishing, the log-odds of them having caught some fish increase by `r round(coef(fishmod)[2],2)`.  

If you're anything like me, these are not the most useful interpretations. I have no idea what increasing `r round(coef(fishmod)[2],2)` log-odds means in real terms. What we can do instead is translate these back into odds, which might make things easier. The opposite of the natural **logarithm** is the **exponential** (see [here](https://mathonweb.com/help_ebook/functions.htm#functions_3){target="_blank"} if you're interested). We can turn $log(x)$ back into $x$ by raising $e$ to the power of it: $e^{\log(x)} = x$. In R, this is just the two functions `log()` and `exp()`:  `exp(log(2)) == 2`.  

This means we can turn our *log-odds* back into *odds* by using `exp()` on our coefficients! A crucial thing to note is that _addition_ on the log-odds scale is the same as _multiplication_ on the odds scale. A coefficient on the log-odds scale is interpreted the same way as we interpret normal linear regression coefficients: _"if you go up 1 on X you add `b` on log-odds(Y)"_. When we convert these coefficients to odds using `exp()`, the interpretation becomes _"if you go up 1 on X, your odds are **multiplied** by `exp(b)`"._  

We can see our exponentiated coefficients here:  

```{r}
exp(coef(fishmod))
```

The intercept is now the odds of having caught a fish when spending zero hours fishing: `r round(exp(coef(fishmod))[1],2)`.  

Because our intercept is at a single point (it's not an _association_), we can actually convert this to a probability. Remember that $odds = \frac{p}{1-p}$, which means that $p = \frac{odds}{1 + odds}$. So the probability of catching a fish if I spend zero hours fishing is $\frac{`r round(exp(coef(fishmod))[1],2)`}{1 + `r round(exp(coef(fishmod))[1],2)`} = `r round(round(exp(coef(fishmod))[1],2)/(1+round(exp(coef(fishmod))[1],2)),2)`$.  

Unfortunately, we can't do the same for any of the slope coefficients. This is because changes in odds are not the same for different levels of probability.  
`r optbegin("quick demonstration",olabel=FALSE)`
Consider how when we multiply odds by 2, the increase in probability is not constant:  

| Odds     | Probability |
| ----------- | ----------- |
| 0.5   | $\frac{1}{1+0.5} = 0.33$  |
| 1   | $\frac{1}{1+1} = 0.5$  |
| 2   | $\frac{2}{1+2} = 0.66$  |
| 4   | $\frac{4}{1+4} = 0.8$  |
| 8   | $\frac{8}{1+8} = 0.88$  |

`r optend()`
Our best interpretation of our coefficient for `hours` is simply that for every hour someone has spent fishing, the odds of them having caught some fish are _multiplied_ by `r round(exp(coef(fishmod)[2]),2)`.  

:::sticky
__Odds Ratios__  

When we exponentiate coefficients from a model fitted to the log-odds, the resulting association is referred to as an "odds ratio" (OR).  

For every 1 unit increase in $x$: 

  - "the odds of $y$ change by _a ratio_ of `exp(b)`"   
  - "the odds of $y$ are multiplied by `exp(b)`"  
  - "there are `exp(b)` increased/decreased odds of $y$"  


Instead of thinking of a coefficient of 0 as indicating "no association", in odds ratios this when the OR = 1. 

- __OR = 1__ : equal odds ($1 \times odds = odds \text{ don't change}$). 
- __OR < 1__ : decreased odds ($0.5 \times odds = odds \text{ are halved}$)
- __OR > 1__ : increased odds ($2 \times odds = odds \text{ are doubled}$)

<img src="images/danger.svg" style="width:30px;"/>
<small><small>
Often you will hear people interpreting OR as "$y$ is `exp(b)` times as likely to occur".  
Although it is true that increased odds is increased likelihood of $y$ occurring. Double the odds does not mean you will see _twice_ as many occurrences of $y$ (see the "I still don't really get odds" box below).
</small></small>

:::


`r optbegin("I still don't really get odds.. ",olabel=FALSE)`
Even talking in odds and multiplying odds can be frustratingly confusing, so here's a little more step-by-step explanation to try and help:  

```{r}
#| echo: false
tibble(term=c("(Intercept)","hours"),b=round(coef(fishmod),2),`exp(b)`=round(exp(coef(fishmod)),2)) %>% gt::gt()
```


1. For people spending just 1 hour fishing, the log-odds of them having caught fish are `r paste0(round(coef(fishmod),2),collapse=" + ")` = `r round(sum(coef(fishmod)),2)`.  
2. Translating this to odds, we exponentiate it, so the odds of them having caught fish are $e^{(`r paste0(round(coef(fishmod),2),collapse=" + ")`)} = e^{`r round(sum(coef(fishmod)),2)`} = `r round(exp(sum(coef(fishmod))),2)`$.   
(This is the same^[$e^{a+b} = e^a \times e^b$. For example: $2^2 \times 2^3 = 4 \times 8 = 32 = 2^5 = 2^{2+3}$] as $e^{`r round(coef(fishmod)[1],2)`} \times e^{`r round(coef(fishmod)[2],2)`}$)   
3. These odds of `r round(exp(sum(coef(fishmod))),2)` equate to approximately $\frac{1}{5}$, which means that if we take 6 people who have each spent 1 hour fishing, we expect 1 of them to have caught fish, and 5 of them to have not caught anything.  
4. If we consider how the odds increase for every hour spent fishing (i.e. for those spending **2** hours fishing as opposed to just 1):  
    - the log-odds of catching fish increase by `r round(coef(fishmod)[2],2)`.  
    - the odds of catching fish are multiplied by `r round(exp(coef(fishmod)[2]),2)`.  
    - so for 2 hours of fishing, the odds are $`r round(exp(sum(coef(fishmod))),2)` \times `r round(exp(coef(fishmod)[2]),2)` = `r round(exp(sum(coef(fishmod)[c(1,2,2)])),2)`$.  
    (And we can also calculate this as $e^{`r round(sum(coef(fishmod)),2)` + `r round(coef(fishmod)[2],2)`}$)  
5. So these new odds of 0.36 for 2 hours fished are 1.96 times greater than the odds of 0.19 for 1 hour fished. 
For ease, let's just round 1.96 to 2 and say that the odds have doubled.  
What does this mean? It means we've gone from odds of $\frac{1}{5}$ to $\frac{2}{5}$. This doesn't mean we'll now see 2 out of 6 people catching fish. It means we'll see 2 out of 7 (@fig-fishing).  

```{r}
#| label: fig-fishing
#| fig-cap: "Increasing from odds of 1/5 to 2/5"  
#| echo: false
#| out-width: "100%"
knitr::include_graphics("images/glm/fishing.png")
```

`r optend()`

## Inference

:::statbox
__Coefficient Tests__  

You might notice that the summary output of our logistic regression model has $z$ statistics instead of $t$ statistics. For logistic regression, the distribution of the estimated coefficients gets close and closer to a Normal distribution as the sample size gets larger. All of the tests are based on the hypothesis that the sample size is large enough^[whatever 'large enough' is!] to justify a normal approximation.  

```{r}
#| eval: false
summary(fishmod)
```
```{r}
#| echo: false
.pp(summary(fishmod),l=list(9:13))
```

The logic of the tests remains the same as it was for the linear model, where it is a test against the null hypothesis that the coefficient is zero. Or, less formally, we can think of the corresponding p-value as "in the null universe where there is no relationship between $x$ and the occurrence of event $y$, there is a `p-value` chance of seeing a sample of this size with an association as strong as the one we have got"  

:::int
For every hour more someone spent fishing, the odds of them catching a fish increased by `r round(exp(coef(fishmod)[2]),2)` ($z = `r round(coefficients(summary(fishmod))[2,3],2)`$, $p < .001$).  
:::

:::

:::statbox
__Confidence Intervals__  
As previously, we can obtain confidence intervals using the `confint()` function. Because confidence intervals are just lower and upper bounds for a range of plausible values for our estimates, we can turn these into the confidence intervals around our odds ratios!  

```{r}
exp(confint(fishmod))
```

:::int
For every hour more someone spent fishing, the odds of them catching a fish increased by `r round(exp(coef(fishmod)[2]),2)` (95% CI [`r paste0(round(exp(confint(fishmod)[2,]),2),collapse=" - ")`]).  
:::

:::

### Visualising 

While interpreting coefficients leaves us in speaking in the realm of odds-ratios, we can do more with visualisations! This is because we can simply display the predicted probability across values of some predictor. We don't have to try and rely solely on a numerical description of a relationship - a picture speaks a thousand words (and it's a lot easier!)  

```{r}
library(sjPlot)
plot_model(fishmod, type = "eff")
```

`r optbegin("Optional: making plots manually",olabel=FALSE)`

to make plots manually, we need to give our model some new data to predict values for, and then plot those predictions!  
For these models, we can actually predict on various different scales (e.g. do we want to predict log-odds values (`predict(mod, type="link")`) or probabilities (`predict(mod, type="response")`)? 
```{r}
plotdata <- tibble(
  # make hours a sequence of length 50, from 0 to 10
  hours = seq(0, 10, length.out = 50)
)

# add the predcted probabilities
plotdata <- 
  plotdata %>% mutate(
    prob = predict(fishmod, newdata = plotdata, type = "response"),
  )

# plot the predicted probabilities
ggplot(plotdata, aes(x = hours, y = prob)) +
  geom_line()


# Getting the intervals is much trickier, but essentially requires computing the upper and lower bounds on the log-odds scale and then converting them back:
plotdata <- 
  plotdata %>% 
  mutate(
    logodd = predict(fishmod, newdata = plotdata, type = "link"),
    se = predict(fishmod, newdata = plotdata, type = "link", se.fit = TRUE)$se,
    logoddlower = logodd - (1.96*se),
    logoddupper = logodd + (1.96*se),
    prob = fishmod$family$linkinv(logodd),
    lower = fishmod$family$linkinv(logoddlower),
    upper = fishmod$family$linkinv(logoddupper)
  )

ggplot(plotdata, aes(x = hours, y = prob)) +
  geom_line() + 
  geom_ribbon(aes(ymin=lower,ymax=upper), alpha=.2)
```

`r optend()`

# Comparing models

When moving from linear regression to more advanced and flexible models, testing of goodness of fit is more often done by comparing a model of interest to a simpler one.
The only caveat is that the two models need to be __nested__, i.e. one model needs to be a simplification of the other, and all predictors of one model needs to be within the other.  

To compare models, we test the difference in __deviance__.  

:::sticky
__Deviance__  

Deviance is a measure of deviation/discrepancy/mismatch between the data and the model. You can think of it as a generalisation of the terms making up the residual sum of squares in simple linear regression, in that it measures the misfit, or _badness of fit_. This means (as it was for the residual sum of squares) that smaller deviance is better!  

More technically, deviance is calculated as -2 times the log-likelihood.  

- Likelihood = $P(data | model)$ (the probability of seeing the sample data given our model). If we had a _perfect_ model, this would be 1.  
    - if you want a (slightly) more in depth look at "likelihood", see [here](lvp.html){target="_blank"}.
- Log-likelihood = the log of the likelihood. The log transformation means that where likelihood ranges from 0 to 1, log-likelihood ranges from negative infinity to 0 (0 would be the perfect model)
- -2Log-likelihood = turns it from negative to positive, and allows us to test it against a $\chi^2$ distribution.  

:::

For instance, we can compare our model to the model where all slopes are 0. The null hypothesis will be that the simpler model is a good fit, while the alternative is that the more complex model is needed.  

In R we do this comparison as follows:
```{r}
fishmod0 <- glm(caught_any ~ 1, data = fish,
               family = binomial(link="logit"))

fishmod1 <- glm(caught_any ~ hours, data = fish,
               family = binomial(link="logit"))

anova(fishmod0, fishmod1, test = 'Chisq')
```

The above code shows the two fitted models, amd reports the _Residual Deviance_ of each model, 165.1 and 127.7 respectively. So by adding the predictor `hours` to the model, we reduce our deviance from 165.1 to 127.7, i.e. we reduce it by 37.4 (remember, deviance is like badness-of-fit, so smaller is better).   
Is this reduction sufficient to be attributed solely to the contribution of the predictor, or could it just be due to random sampling variation? This is what the chi-squared test tells us!  


<!-- Above was Drop-in-deviance GOF test -->
<!-- # Deviance GOF test -->
<!-- mdl_sat = glm(senility ~ factor(1:nrow(sen)), family = binomial, data = sen) -->
<!-- anova(mdl1, mdl_sat, test = 'Chisq') -->


`r optbegin("Akaike and Bayesian Information Criteria")`
Deviance measures lack of fit, and it can be reduced to zero by making the model more and more complex, effectively estimating the value at each single data point. 
However, this involves adding more and more predictors, which makes the model more complex (and less interpretable). 

Typically, simpler models are preferred when they still explain the data almost as well. This is why information criteria were devised, exactly to account for both the model misfit but also its complexity.

$$
\text{Information Criterion} = \text{Deviance} + \text{Penalty for model complexity}
$$

Depending on the chosen penalty, you get different criteria. Two common ones are the Akaike and Bayesian Information Criteria, AIC and BIC respectively:
$$
\begin{aligned}
\text{AIC} &= \text{Deviance} + 2 p \\
\text{BIC} &= \text{Deviance} + p \log(n)
\end{aligned}
$$

where $n$ is the sample size and $p$ is the number of regression coefficients in the model. __Models that produce smaller values of these fitting criteria should be preferred.__ 

AIC and BIC differ in their degrees of penalization for number of regression coefficients, with BIC usually favouring models with fewer terms. 

```{r}
AIC(fishmod0, fishmod1)
BIC(fishmod0, fishmod1)
```

According to both AIC and BIC the model with hours as a predictor has a lower score, meaning it's preferable to the other.

`r optend()`

### Assumptions

Because these models don't have the same expected error distribution (we don't expect residuals to be normally distributed around the mean, with constant variance), checking the assumptions of logistic regression is a little different.  

Typically, we look at the "deviance residuals". But we __don't__ examine plots for patterns, we simply examine them for potentially outlying observations. If we use a standardised residual, it makes it easier to explore extreme values as we expect most residuals to be within -2, 2 or -3, 3 (depending on how strict we feel).  

:::statbox
__Deviance Residuals__  

There are three ways we can get out deviance residuals, each scaled differently 

- $i$th residual = measure of deviance contributed from the $i$th observation
- $i$th standardized residual = residual / SD(residual)
- $i$th studentized residual = residual / SD(residual from model fitted without observation $i$)

We get these in R using:  
```{r}
#| eval: false
# deviance residuals
residuals(fishmod, type = 'deviance')
# studentised residuals
rstudent(fishmod, type = 'deviance')
# standardised residuals
rstandard(fishmod, type = 'deviance')
```

We can check whether any residuals are larger than 2 or 3 in absolute value:
```{r}
plot(rstudent(fishmod, type = 'deviance'), 
     ylab = 'Studentized Deviance Residuals')
```
_*Warning:* Don't inspect this plot for patterns!!!_  

There appears to be 1 residual with a value slightly larger than 2 in absolute value. We will keep these in mind and check later if they are also influential points (using `cooks.distance`)

Sometimes a __binned plot__^[Gelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511790942] can be more informative, but not always! It works by combining together all responses for people having the same covariate $x_i$ value, and taking the average studentized Pearson residual for those.

Before using this function, make sure you have installed the **arm** package!

```{r}
arm::binnedplot(fitted(fishmod), rstudent(fishmod, type = 'deviance'), 
                xlab = 'Prob. of Catching fish', ylab = 'Studentized Deviance Residuals')
```

There doesn't appear to be any extreme residuals.

:::

<!-- `r optbegin("Optional: Pearson Residuals", olabel=FALSE)` -->
<!-- __Pearson Residuals__   -->

<!-- There is another form of residual we can use, called "Pearson residuals". These are similar to the Pearson residuals you might remember from the [chi-squared test (4A)](04a_chisq.html){targt="_blank"}, which compared Observed and Expected values, but in this case Expected means Predicted from the model: -->
<!-- $$ -->
<!-- Pres_i = \frac{Observed - Expected}{\sqrt{Expected}} -->
<!-- $$ -->

<!-- In logistic regression, this is  -->
<!-- $$ -->
<!-- Pres_i = \frac{y_i - \hat p_i}{\sqrt{\hat p_i (1 - \hat p_i)}} -->
<!-- $$ -->
<!-- where  -->

<!-- - $y_i$ is the observed response for unit $i$, either 0 (failure) or 1 (success) -->
<!-- - $\hat p_i$ is the model-predicted probability of success -->

<!-- We can get these out from R, and use them to create similar plots to the one we made for deviance residuals.   -->
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- # pearson residuals -->
<!-- residuals(fishmod, type = 'pearson') -->
<!-- # standardised pearson residuals -->
<!-- rstandard(fishmod, type = 'pearson') -->
<!-- # studentised pearson residuals -->
<!-- rstudent(fishmod, type = 'pearson') -->
<!-- ``` -->
<!-- `r optend()` -->
