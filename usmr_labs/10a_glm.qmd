---
title: "10A: The Generalized Linear Model"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  
  <style>
  .zoomDiv {
    opacity: 0;
    position:fixed;
    top: 50%;
    left: 50%;
    z-index: 50;
    transform: translate(-50%, -50%);
    box-shadow: 0px 0px 50px #888888;
    max-height:100%; 
    overflow: scroll;
  }

.zoomImg {
  width: 150%;
}
</style>
  <script type="text/javascript">
  $(document).ready(function() {
    $('body').prepend("<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>");
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src'));
      $('.zoomDiv').css({opacity: '1', width: '60%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'});
    });
  });
</script>


```{r}
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(xaringanExtra)
xaringanExtra::use_panelset()
```

# Quick recaps

Things have moved pretty quickly in the last couple of readings, so it's worth taking a little bit of time on recaps to see how it all fits together.  


::: {.callout-note collapse="true"}
#### Simple linear regression
In the simple linear regression, we were modelling a continuous outcome variable $y$ as a function of some predictor $x$.  
Our model of $y$ was defined by a number of components:  

- $b_0$: the "Intercept". This is the predicted value of $y$ where $x = 0$. In the plot below, it is where the regression line cuts the y-axis.   
- $b_1$: the "Coefficient for $x$". This is the predicted *increase* in $y$ for every unit increase of $x$. In the plot below, this is the amount the regression line goes up for every one unit we move across.  
- $\varepsilon$: the "Residuals" or "Error term". These are the distances from each of our observed values to the model predicted values. In the plot below, these are the vertical distances from each red dot to the black line. The standard deviation of all these gets denoted $\sigma_\varepsilon$.

**Model:**  

$$
\begin{align}
y &= b_0 + b_1 \cdot x_1 + \varepsilon\\
\qquad \\
&\varepsilon \sim N(0, \sigma_\varepsilon)
\end{align}
$$

**In R:**
<center>
```
lm(y ~ x1, data = mydata)
```  
</center>
    
```{r}
#| label: fig-simplelm
#| fig-cap: "Simple linear regression model. one continuous predictor." 
#| echo: false
mwdata = read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
mwdata %>% mutate(x=outdoor_time,y=wellbeing) -> mwdata
with(mwdata, plot(x,y, col="red",pch=16))
abline(lm(y~x, mwdata))
```

:::

::: {.callout-note collapse="true"}
#### Multiple linear regression  

When we extended this to multiple regression, we essentially just added more explanatory variables (more 'predictors' of the outcome $y$).

In these models, the values of $y$ are now modelled as a set of relationship with more than just one $x$ variable. So our models are defined by more coefficients.  

- $b_0$: the "Intercept". This is the predicted value of $y$ when _all predictors are zero_.  
- $b_1$: the "Coefficient for $x_1$". This is the predicted *increase* in $y$ for every unit increase of $x_1$ _holding constant_ all other predictors.  
- ...
- ...
- $b_p$: the "Coefficient for $x_p$". This is the predicted *increase* in $y$ for every unit increase of $x_p$ _holding constant_ all other predictors.  
- $\varepsilon$: the "Residuals" or "Error term". These are the distances from each of our observed values to the model predicted values. The standard deviation of all these gets denoted $\sigma_\varepsilon$.
   
**Model:**  
$$
\begin{align}
y &= b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 \,\, ... \,\, + b_p \cdot x_p + \varepsilon\\
\qquad \\
&\varepsilon \sim N(0, \sigma_\varepsilon)
\end{align}
$$
  
**In R:**  
<center>
```
lm(y ~ x1 + x2 ... + xp, data = mydata)
```  
</center>   
   
In the plot below with **two** predictors, we would have a coefficient for each one, showing the slope of the regression surface along each dimension. The residuals continue to be the distances from the observed values to our model predicted values, so in this they are the vertical distances from each red dot to the surface. 

```{r}
#| echo: false
#| label: fig-multiplelm
#| fig-cap: "multiple linear regression model with two continuous predictors." 
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=50
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)


p <- persp(outdoor_time,social_int,wellbeing, theta = 40,phi=10, col = NA,xlab="x1",ylab="x2",zlab="y")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)
```

:::

::: {.callout-note collapse="true"}
#### Interactions  

We then saw how we can let the relationships between the outcome $y$ and an explanatory variable $x1$ vary *as a function of* some other explanatory variable $x2$.  
  
In the plot below, if someone asks you "what is the relationship between $x_1$ and $y$, your reply should (hopefully) be "it depends on what the value of $x_2$ is"  

As we can see from the plot, for low values of $x_2$, there is a steep downward slope between $y$ and $x_1$ (the red line in the plot). For high values of $x_2$, the slope between $y$ and $x_1$ is much flatter (the blue line).  
  
```{r}
#| echo: false
#| label: fig-intlm
#| fig-cap: "multiple linear regression model with two continuous predictors that interact" 
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
scs_study$zn=max(scs_study$zn)-scs_study$zn
fit<-lm(dass ~ scs*zn, data = scs_study)
steps=50
scs <- with(scs_study, seq(min(scs),max(scs),length=steps))
zn <- with(scs_study, seq(min(zn),max(zn),length=steps))
newdat <- expand.grid(scs=scs, zn=zn)
dass <- matrix(predict(fit, newdat), steps, steps)
p <- persp(scs,zn,dass, theta = 45,phi=15, col = NA,
           xlab = "x1",zlab="y",ylab="x2")
df1 = data.frame(scs=27:54,zn=min(scs_study$zn))
df1$y = predict(fit,df1)
df2 = data.frame(scs=27:54,zn=max(scs_study$zn))
df2$y = predict(fit,df2)
pred1 <- with(df1, trans3d(scs ,zn, y, p))
pred2 <- with(df2, trans3d(scs ,zn, y, p))
points(pred1, col = "red", pch = 16,type="l",lwd=3)
points(pred2, col = "blue", pch = 16,type="l",lwd=3)
```

We can capture this by using an additional coefficient in our linear model. To express an interaction, we use the product of the two predictors ($x_1 \cdot x_2$). 

**Model:**  
$$
\begin{align}
y &= b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 + b_3 \cdot x_1 \cdot x_2 + \,\, ... \,\, + b_p \cdot x_p + \varepsilon\\
\qquad \\
&\varepsilon \sim N(0, \sigma_\varepsilon)
\end{align}
$$
  
**In R:**  
<center>
```
lm(y ~ x1 + x2 + x1:x2 + ..., data = mydata)  
  OR  
lm(y ~ x1*x2 + ..., data = mydata)  
```  
</center>
  
The resulting coefficients ($b_1$, $b_2$ and $b_3$ in the equation above) provide us with:  

1. the association between $x_1$ and $y$ when $x_2 = 0$  
2. the association between $x2$ and $y$ when $x_1 = 0$  
3. the amount to which 1. and 2. are adjusted when there is an increase in the other variable  

:::


::: {.callout-note collapse="true"}
#### Categorical explanatory variables   

Throughout the models we've been fitting we have also seen how they behave with **categorical** predictors.  

When we enter a categorical variable in as a predictor in a linear model, it gets entered as a set of "dummy variables" (typically zeroes or ones, but we it _is_ possible to do more clever things). These represent differences between groups.  

If we had a categorical variable which contained $k$ levels (or "groups", or "categories"), then R will put $k-1$ variables into the model for us, and we will get out $k-1$ coefficients. The default behaviour in R is that, depending on what we set as our **reference level**, the coefficients represent the difference from this level to each of the other levels.  

In the plots below, if "Group1" is the reference, then the coefficient `groupGroup2` would represent the vertical distance from the red line to the green line, and the coefficient `groupGroup3` would be the vertical distance from the red line to the blue line.  

```{r}
#| echo: false
#| label: fig-catlm
#| fig-cap: "A categorical predictor in a regression model in R will by default set one level as a reference (e.g. Group1 here), and compare each level to that reference" 
library(patchwork)

mwdata <- mwdata %>% mutate(group=fct_recode(factor(location), Group1="City",Group2="Rural",Group3="Suburb"))

gms = mwdata |> group_by(group) |>
  summarise(y=mean(wellbeing)) |>
  mutate(
    y1 = y[group=="Group1"],
    x1 = "Group1"
  )

ggplot(mwdata, aes(x=group, y=wellbeing,col=group))+
    geom_jitter(width = .1, height = 0)+
    stat_summary(fun = mean, geom="errorbar", aes(ymax=..y..,ymin=..y..,col=group),lwd=2) + 
        labs(title="y ~ group",y="y") + 
  geom_segment(data=gms, aes(x=group,xend=x1,y=y,yend=y1),lwd=1,col="black",lty="dashed")
  
      

    # ggplot(mwdata, aes(x=0, y=wellbeing,col=group))+
    # geom_jitter(width = .1, height = 0)+
    # stat_summary(fun = mean, geom="errorbar", aes(x=0,ymax=..y..,ymin=..y..,col=group),lwd=2) + 
    # scale_x_continuous(NULL, breaks=NULL) +
    #     labs(title="y ~ group",subtitle="(collapsed)",y="y")+
    #     
    # plot_layout(guides="collect")

```

*Note - because categorical predictors is all looking at the difference between groups - this is all still intrinsically "linear" - the only possible way to get from the red line to the green line is via a straight line.*  

We also saw how this same logic applies when categorical explanatory variables are involved in an interaction - e.g., a binary (two-level) predictor interacting with a continuous predictor allows two lines to be different slopes (as in @fig-catintglm). 

```{r}
#| label: fig-catintglm
#| fig-cap: "Continuous*Binary interaction"
#| echo: false
# data
airpol <- read_csv("https://uoepsy.github.io/data/usmr_airpol.csv")
# model
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)

# plot data
plotdat <- expand_grid(
  aqpi = 0:500,
  apoe4 = c("neg","pos")
)
# plot
broom::augment(eg2mod, newdata = plotdat, interval="confidence") |>
  ggplot(aes(x=aqpi, y = .fitted, 
             col = apoe4, fill = apoe4)) + 
  geom_line() +
  geom_point(data=airpol,aes(y=mmse),alpha=.3,size=3)+
  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)
```



:::

# Continuous outcomes!  

```{r}
#| include: false
set.seed(2)
dd <- tibble(
  ppt = paste0("fisherperson ", 1:120),
  hours = round(rchisq(120, 8)*.5, 1),
  lp = -2 + .6*hours,
  n_fish = rpois(120, lambda=exp(lp)),
  tiredness = round(56 + scale(lp + rnorm(120))[,1]*16.47),
  caught_any = rbinom(120, size = 1, prob = plogis(lp)),
) %>% mutate(n_fish = ifelse(caught_any==0, 0, n_fish)) %>%
  select(-lp)
# write_csv(dd, "../../data/fishing_exped.csv")
#with(dd,plot(n_fish~hours))
#with(dd,plot(caught_any~hours))
#with(dd,plot(tiredness~hours))
```

In all of the models thus far, we have been dealing with outcomes variables that are *continuous*. These are variables for which an observation can take _any_ real value - i.e. 3.4, or 1.50365, or 0.000532. We might not have have measured these variables with any degree of accuracy, but the models we have been fitting have fundamentally assumed this. 

For instance, even if our outcome is a score on some neuropsychological test of cognition where we can only score whole numbers, our model predictions will be values that are much more specific. A model predicted score on the test could be 18.220483724...... This is because we are modelling a continuous line through the observed scores. This also means that the model extends to scores from negative to positive infinity ($- \infty < y < \infty$), even if we know that people can't score less than 0 or more than 30 (see @fig-mmsezoom ).  

Often, this is a perfectly acceptable thing to do, especially when we consider the outcome variable to be measuring a continuous thing (e.g. cognitive functioning), just hindered by the precision of measurement, and when the bounds of our measurement tool are not preventing us from capturing variation (e.g. if everyone scores maximum on the test, then there is no variance to explain).  
  
```{r}
#| label: fig-mmsezoom
#| fig-cap: "Our lm(mmse ~ air_pol * apoe4) model is a continuous line extending infinitely"  
#| out-height: "350px"
#| echo: false

# plot data
plotdat <- expand_grid(
  aqpi = 0:500,
  apoe4 = c("neg","pos")
)
broom::augment(eg2mod, newdata = plotdat, interval="confidence") |>
  ggplot(aes(x=aqpi, y = .fitted, 
             col = apoe4, fill = apoe4)) + 
  geom_line() +
  geom_point(data=airpol,aes(y=mmse),alpha=.3,size=3)+
  geom_smooth(data=airpol,aes(y=mmse),method=lm,fullrange=T,
              se=F)+
  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)+
  xlim(-1000,2000)+
  ylim(-50,100) +
  geom_rect(aes(xmin=-2,xmax=500,ymin=0,ymax=32), fill=NA,col="grey30") +
  geom_segment(aes(x=500,xend=600,y=32,yend=36),col="grey30")+
  geom_text(label="what we look at",x=600,y=36,col="grey30",vjust=0,hjust=0)
```


# Other outcomes 

There are lots of variables that we collect that don't fit with this way of thinking. For instance, what if we want to model something that takes the form of a __binary variable:__^[we have seen this type of variable already when looking at categorical predictors in our models, but we haven't seen how we use it as the *outcome*.] 

- being Y vs not being Y  
- Y happening vs Y not happening  

For binary variables, values can only take one of two values, which we often encode as 0 or 1. Values can't be 0.5, -2, or 1.3, they can _only_ be 0 or 1.

Another type of outcome that we can't really model as continuous is a __count variable__:  

- The number of Y's
- How many times Y happened

For count data, values can't take *any* real number, they can only take integers (0, 1, 2, 3, ...).  


:::frame
__An example__  

At a lake, I survey 120 people who have just finished fishing for the day. I'm interested in how the length of time they spent fishing is associated with: 

1. how tired they are from 0 to 100 (continuous)
2. catching *any* fish (binary, 1 = yes, 0 = no)
3. the number of fish caught (count)

We can try at first to visualise all these relationships as scatterplots (@fig-outcomeplots). 


```{r}
#| code-fold: true
#| label: fig-outcomeplots
#| fig-cap: "Three different outcomes: continuous (top), binary (middle), count (bottom)"
#| fig-height: 8
fish <- read_csv("https://uoepsy.github.io/data/fishing_exped.csv")

p1 <- ggplot(fish, aes(x = hours, y = tiredness)) + 
  geom_point() +
  labs(title="How tired are you (0 - 100)?") +
  geom_smooth(method=lm, se=FALSE)

p2 <- ggplot(fish, aes(x = hours, y = caught_any)) + 
  geom_point() +
  labs(title="Did you catch any fish?") +
  geom_smooth(method=lm, se=FALSE) + 
  scale_y_continuous(limits=c(0,1.2),breaks=seq(0,1.2,.2))
  
p3 <- ggplot(fish, aes(x = hours, y = n_fish)) + 
  geom_point() +
  labs(title="How many fish did you catch?") + 
  geom_smooth(method=lm, se=FALSE)
  
p1 / p2 / p3
```

We can happily model our first outcome, tiredness, with a linear model. In @fig-outcomeplots (top panel), we can see that the relationship looks linear, and the residuals are spread fairly consistently above and below the fitted line.  

For the binary outcome however @fig-outcomeplots (middle panel), there are a couple of things that are problematic. For someone who spends 8 hours fishing, the model estimates approximately **1.1** "catching any fish". But what does that 1.1 really mean? Does it mean that if I fish 8 hours I will have 1.1 fish? no, because it's not a count. Does it mean there is a 110% probability of catching fish? This is closer, but again it doesn't make much sense - how can we have a probability of 110%?  

For the count variable (bottom panel), we can also see that the standard linear model looks problematic. Again it estimates values we simply _cannot_ have (it looks like if I spend 1 hour fishing I will catch about negative 3 fish?). Another thing to note is that the variance in the points on the y axis tends to increase as we get higher. This is typical of count data - we're going to see lots of people catching 0, 1 or 2 fish, and so the "people catching few fish" are all quite close (in number of fish) to one another, but because fewer people catch 20 fish, 30 fish etc, there is a lot more variability in "number of fish" when we get higher up. This is a problem for a linear model, which assumes _constant variance_ across the fitted model.  

:::

# Introducing the GLM

To deal with these sorts of outcome variable, we need to learn about the **Generalized Linear Model (GLM)**. 

The GLM involves a little bit of trickery, in which we translate something that is inherently not linear (e.g. "the _probability_ of Y", or for count variables "the _rate_ of occurrence of Y") into something which is both linear and unbounded, and model that thing instead.  

The problem, fundamentally, is that things like probability is not linear. We think of it as getting ever and ever closer, but never reaching, 0 and 1. For instance, if the probability of $y$ ocurring increases with $x$, it makes most sense to think of it looking something like @fig-plotprobab.  

```{r}
#| echo: false
#| label: fig-plotprobab
#| fig-cap: "As x increases, probability of y gets closer and closer (but never meets) 1"
#| fig-height: 3.5
pt <- tibble(x=seq(-4.5,4.5,length=49),lo=x,o=exp(x),p=o/(1+o))
pt %>% ggplot(aes(x=x,y=p)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  ylab("probability of y")+xlab("x")
```


To allow us to use our linear toolbox for things like "probability", we can't think in terms of modelling how $x$ is associated with the the probability of $y$ _directly_ (because we only ever observe $y$ as 0 or 1). What we do instead is model how $x$ is associated with the "log-odds of $y$".  

We use a similar trick for modelling an outcome that is a count - instead of modelling counts directly, we can model the log of the counts.  

:::statbox
**Probability, odds, and log-odds**  

If we let $p$ denote the probability of a given event, then $\frac{p}{(1-p)}$ are the odds of the event happening.    

For example, the odds of rolling a 6 with a normal die is 1/5 (sometimes this is expressed '1:5', and in gambling the order is sometimes flipped and you'll see '5/1' or 'odds of five to one').  

The "log-odds" (sometimes called "logits") are denoted by $ln(\frac{p}{(1-p)})$, where $ln$ refers to the "natural logarithm". Don't worry about exactly what $ln$ means - the important thing is that it converts "odds" (which can go from 0 to infinity but do not increase linearly) in to a straight line (@fig-plodd).  



::: {.callout-caution collapse="true"}
#### optional: what are logarithms?

:::sticky
__Essentials of logarithms__

A logarithm is a mathematical way to write any positive number as a power of a "base number". 
:::

__Log base 10__

Consider the base number 10. The R function implementing the logarithm base 10 is `log10()`.

To what power do I need to raise the base number 10 in order to obtain the desired number 100? 

```{r}
log10(100)
```

log10(100) = 2 as we need to raise 10 to the power of 2 in order to obtain the desired number 100.

To what power do I need to raise the base number 10 in order to obtain the desired number 1000? 

```{r}
log10(1000)
```

log10(1000) = 3 as we need to raise 10 to the power of 3 in order to obtain the desired number 1000.

Note that you can use this to write any desired number as a power of 10. For example, to which power do we raise 10 in order to obtain 13.5?

```{r}
log10(13.5)
```

So this is telling us that $10^{1.130334} = 13.5$ (with rounding error)

```{r}
10^1.130334
```

An important property is that:

$$10^{\log_{10}(x)} = x$$

For example, $10^{\log_{10}(100)} = 10^{2} = 100$.

```{r}
10^log10(100)
```

__Natural Logarithms (aka Log base e)__  

A special number in mathematics is Euler's (or Napier's) number $e = 2.718282...$. As well as a logarithm in base 10, telling us which power to raise the base 10 to, in order to obtain a desired number, we can also use any other base, such as $e$.

The logarithm with base number $e$ is implemented in the R function `log()`.

To which number do I need to raise $e$ in order to obtain 8?

```{r}
log(8)
```

So this is telling us that $e^{2.079442} = 8$
```{r}
2.718282^2.079442
```

Our property is preserved: 
$$e^{\log(x)} = x$$

```{r}
2.718282^log(8)
```

:::


```{r}
#| echo: false
#| label: fig-plodd
#| out-width: "100%"
#| fig-height: 4
#| fig-cap: "Probability, Odds, and Log-Odds"
library(patchwork)
pt <- tibble(x=seq(-4.5,4.5,length=49),lo=x,o=exp(x),p=o/(1+o))

p1 <- pt %>% ggplot(aes(x=x,y=p)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 0,linetype="dashed") +
  ylab("probability p") +
  ggtitle("probability") +
  annotate("text",0,.75,label="p=.5",size=4, vjust=-0.5, angle=90)

p2 <- pt %>% ggplot(aes(x=x,y=o)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 0,linetype="dashed") +
  ylab("odds p/(1-p)") +
  ggtitle("odds") +
  annotate("text",0,70,label="odds=1",size=4, vjust=-0.5, angle=90)

p3 <- pt %>% ggplot(aes(x=x,y=lo)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  geom_vline(xintercept = 0,linetype="dashed") +
  ylab("log(odds)") +
  ggtitle("log-odds (logits)")

p4 <- p3 + annotate("text",0,2,label="log(odds)=0",size=4, vjust=-0.5, angle=90) +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_segment(x=1,xend=2,y=1,yend=1, lty="dotted",col="blue", lwd=1) + 
  geom_segment(x=2,xend=2,y=1,yend=2, col="blue",lwd=1) +
  annotate("text", x=1.5,y=1,vjust=1, label="1",col="blue")+
  annotate("text", x=2,y=1.5,hjust=-1, label="b",col="blue")

p1+p2+p4 & theme(plot.title = element_text(size=14))
```

:::sticky
__Conversions__  

Between probabilities, odds and log-odds ("logit"):    

\begin{align}
&\text{for probability p of Y for observation i:}\\
\qquad \\
odds_i & = \frac{p_i}{1-p_i} \\
\qquad \\
logit_i &= ln(odds_i) = ln(\frac{p_i}{1-p_i}) \\
\qquad \\
p_i & = \frac{odds_i}{1 + odds_i} = \frac{e^{logit_i}}{(1+e^{logit_i})}

\end{align}


:::



__Counts and log-counts__  
We can apply the same log transformation to count data in order to think about how we might consider an association on a linear scale:  
```{r}
#| echo: false
#| label: fig-clcnt
#| fig-height: 4
#| fig-cap: "counts and log(counts)"
pt <- tibble(x=seq(1,4,length=49),c=exp(x))

p1 <- pt %>% ggplot(aes(x=x,y=c)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 1,linetype="dashed") +
  ylab("count") +
  ggtitle("count")

p2 <- pt %>% ggplot(aes(x=x,y=x)) +
  geom_path(size=2,colour="red") +
  scale_x_continuous() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_vline(xintercept = 1,linetype="dashed") +
  ylab("log(count)") +
  ggtitle("log(count)") +
  geom_segment(x=2,xend=3,y=2,yend=2, lty="dotted",col="blue", lwd=1) + 
  geom_segment(x=3,xend=3,y=2,yend=3, col="blue",lwd=1) +
  annotate("text", x=2.5,y=2,vjust=1, label="1",col="blue")+
  annotate("text", x=3,y=2.5,hjust=-1, label="b",col="blue")

p1 + p2
```


:::


<br> 

This little trick of modelling something via a mapping from the outcome variable to the model (i.e. logit, or log) is what underpins the __generalized linear model__. It allows us to be able to talk about _linear_ associations between predictors and the _log-odds of an event_ (or between predictors and _log-counts_). In @fig-plodd, and @fig-clcnt above, we can see that by talking in terms of log-odds (or log-counts), we can model something that is *linear* and *unbounded*. So when we think about these models, what we are really have in mind is that we are capturing the associations seen in the right hand plots of @fig-plodd and @fig-clcnt.  

As we will see below we can even convert our coefficients into something at least a little more interpretable than "log-odds" (but doing so comes at the expense of losing the 'linearity' of the association).  

:::sticky
__The Generalized Linear Model (GLM)__  

In its general form, the GLM looks pretty similar to the standard LM.  
we can write it as:  

$$
\color{green}{g(}\color{red}{y}\color{green}{)} = \mathbf{\color{blue}{b_0 + b_1 \cdot x_1 + ... + b_p \cdot x_p} }
$$
The difference is that $\color{green}{g()}$ is a function (like log, or logit) that links the expected value of $\color{red}{y}$ to the linear prediction of $\color{blue}{b_0 + b_1 \cdot x_1 + ... + b_p \cdot x_k}$.  

:::

<br>

# Fitting GLM in R

In order to fit these models in R, we need to use the `glm()` function. This is (unsurprisingly) a generalized form of the `lm()` function we have already been using. The only difference is that we need to also tell the function the `family` of the outcome variable, and the "link function" (the mapping we use to get from the thing we want to model to the thing we _actually_ model, e.g. `"log"`, or `"logit"`).  

:::sticky
__Linear Regression__  
Outcome $y$ is continuous, $y \in (-\infty, \infty)$  

$$
\color{red}{y} = \mathbf{\color{blue}{b_0 + b_1 \cdot x_1 + ... + b_p \cdot x_p}} 
$$
```{r}
#| eval: false
#| echo: true
linear_model <- lm(continuous_y ~ 1 + x1 + ... + xp, data = df)
```

_As it happens, `lm()` is just a special case of `glm()`, where we use the "gaussian" family (Gaussian is another term for 'normal'), with the "identity" link function (the identity function is just saying 'model y directly'):_

```{r}
#| eval: false
glm(continuous_y ~ 1 + x1 + ... + xp, data = df,
   family = gaussian(link = "identity"))
```
:::

:::sticky
__Logistic Regression__  
Outcome $y$ is binary, $y \in [0,1]$  
$$
\begin{align}
\color{red}{ln \left( \frac{p}{1-p} \right) } &= \mathbf{\color{blue}{b_0 + b_1 \cdot x_1 + ... + b_p \cdot x_p}} \\
\qquad \\
\text{where } \color{red}{p} &= \text{probability of event }y\\
\end{align}
$$
```{r}
#| eval: false
#| echo: true
logistic_model <- glm(binary_y ~  1 + x1 + ... + xp, data = df,
                      family=binomial(link="logit"))
```

- just putting `family = binomial` and `family = "binomial"` will also work (and they will by default use the "logit" link).  
- the outcome can either be 0s and 1s, or coded as a factor with 2 levels.  


::: {.callout-caution collapse="true"}
#### optional: binomial? binary?  

We've seen the "binomial" distribution before (think "number of heads in 20 coin-flips" in [Lecture 4!](https://uoepsy.github.io/usmr/2324/lectures/lecture04.html#/binomial-distribution){target="_blank"}). 

The binomial distribution can be used to model the number of successes in a sequence of $n$ independent trials. If we have a binary variable, this is a special case of the binomial, where $n = 1$.  

The binomial logistic regression model, more generally, can be written as: 

$$
\begin{align}
\color{red}{ln \left( \frac{p}{1-p} \right) } &= \mathbf{\color{blue}{b_0 + b_1 \cdot x_1 + ... + b_p \cdot x_p}} \\
\qquad \\
\text{where } Y &\sim Binomial(n, \color{red}{p})\\
\end{align}
$$

We can fit a more general binomial logistic regression model with any $n$ by specifying our outcome in terms of two things: the number of successes and the number of failures.  
This can be useful if, for instance, we have an outcome that is something like "the number of correct responses from a set of 20 questions":

```{r}
#| echo: false
tibble(
  particpant = paste0("ppt",1:4),
  nr_correct = rbinom(4,20,.66),
  nr_incorrect = 20-nr_correct,
  x1 = "...",
  x2 = "..."
) |> rbind("...") |> as.data.frame()
```

We can fit a model with this using `cbind()` to combine the two columns that together represent the outcome:  

```{r}
#| eval: false
glm(cbind(nr_correct, nr_incorrect) ~ x1 + ... + xp, data = df,
                      family=binomial(link="logit"))
```

:::

:::

:::sticky
__Poisson Regression__  
Outcome $y$ is positive $y \in  (0, \infty)$
$$
\color{red}{ln (y) } = \mathbf{\color{blue}{b_0 + b_1(x_1) + ... + b_k(x_k)}}
$$
```{r}
#| eval: false
#| echo: true
poisson_model <- glm(count_y ~  1 + x1 + ... xk, data = df,
                     family=poisson(link="log"))
```

- `family = poisson` and `family = "poisson"` will also work

::: 


::: {.callout-caution collapse="true"}
#### optional: model estimation

For a linear regression model, R has been fitting our regression models by "minimising the residual sums of squares" (i.e., we rotate the line to find the point at which $\sum{(y - \hat{y})^2}$ is smallest. This gave us the "best fitting line" (@fig-resslineplot).
```{r}
#| label: fig-resslineplot
#| echo: false
#| fig-height: 3.5
#| fig-cap: "linear regression minimises the sum of squared residuals"
mwdata = read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
mwdata %>% mutate(x=outdoor_time,y=wellbeing) -> mwdata
fit = lm(y~x,mwdata)
broom::augment(fit) |>
ggplot(aes(x=x,y=y))+geom_point(size=3,alpha=.5)+
  geom_abline(intercept=coef(fit)[1],slope=coef(fit)[2], lwd=1,col="blue") +
  geom_segment(aes(x=x,xend=x,y=y,yend=.fitted),col="red",lty="dotted",
               lwd=1)+
  labs(x="x",y="y")
```

For the generalized linear models we can't do this. For instance, in logistic regression model, what we're really wanting is the "best fitting *squiggle*" (@fig-loglineplot), and to get to this we must do something else other than looking at our residuals $y - \hat{y}$.  
```{r}
#| label: fig-loglineplot
#| fig-cap: "Modelling the probability of Y across values of X requires fitting a squiggle"
#| fig-height: 3.5
#| echo: false
set.seed(744)
fish <- read_csv("https://uoepsy.github.io/data/fishing_exped.csv") |>
  slice_sample(n=32)

fish %>%
ggplot(aes(x=hours,y=caught_any)) +
  ylab("P(y)") +
  geom_jitter(size=3,height=.02,alpha=.1) +
  geom_smooth(method="glm",method.args=list(family=binomial),se=F) +
  scale_y_continuous(breaks=seq(0,1,by=.2))+
  scale_x_continuous("x", breaks=NULL)
```

Remember that the model is that we are actually fitting is a "logistic model" - i.e. we are fitting things with respect to "log-odds".  

But for our actual observations, the event has either happened or it hasn't, so our observed values $y_i$ aren't really "probabilities" which we can translate into log-odds. In fact, our observed values of 0 and 1, when translated into log odds, become -Infinity and +Infinity (attempted visual in @fig-mlelogodd). This would mean our residuals would also all be infinity, and so it becomes impossible to work out anything! 

```{r}
#| echo: false
tibble(
  observed = c(0,1),
  odds = observed/(1-observed),
  log_odds = log(odds)
) %>% head(2L) %>% rbind(.,"...")
```

```{r}
#| echo: false
#| label: fig-mlelogodd
#| fig-height: 3.5
#| fig-cap: "In the log-odds world, comparing our predicted values to observed values of -Inf and Inf isn't very useful, because all those distances will be infinite (there is no way of measuring the dotted lines, so we can't really use them)"
f2 <- fish %>%
  mutate(fittedp = fitted(glm(caught_any~hours,., family="binomial")),
         fitted = log(fittedp/(1-fittedp)),
         y = ifelse(caught_any==0,min(fitted)-1,max(fitted)+1)
  )

p1 <- ggplot(f2, aes(x=hours))+
  geom_jitter(aes(y=y,col=caught_any==0),size=3,width=0,height=.04,alpha=.2) + 
  #geom_point(aes(y=fitted,col=caught_any==0),size=3,alpha=.8) +
  geom_path(aes(y=fitted),size=1)+
  geom_segment(aes(x=hours,xend=hours,y=fitted,yend=y,col=caught_any==0),lty="dotted", alpha=.5) +
  scale_y_continuous("Log-odds of Y", breaks=c(min(f2$fitted)-1,.5,max(f2$fitted)+1), labels=c("-Inf","0","Inf"))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_hline(aes(yintercept=.5))+
  guides(col=FALSE)+scale_x_continuous("x", breaks=NULL)
p1
```

Instead, **maximum likelihood estimation** is used to find the set of coefficients which best reproduce the observed data. To do this, the computer considers loads of different lines, trying to converge on the one which _maximises the probability of observing the data that we have_.    

To understand this a bit better, let's take consider one possible line in the probability world. The left-hand plot of @fig-mleprob is the same line as the one shown in @fig-mlelogodd, and we can see it translated back into probabilities in the right-hand plot, so that we have a predicted probability of the outcome for each datapoint. From the right-hand plot, we can then ask "what is the probability of seeing our data, given this line?". To get this, we multiply up all the probabilities that observed 1s are predicted as 1s and that observed 0s predicted as 0s. 

```{r}
#| echo: false
#| label: fig-mleprob
#| out-width: "100%"
#| fig-height: 3.5
#| fig-cap: "Note - the dotted lines are not residuals, but just to show the projections of observations down to the line"
p2 <- ggplot(f2, aes(x=hours))+
  geom_point(aes(y=caught_any,col=caught_any==0),size=3,alpha=.2) + 
  #geom_point(aes(y=fittedp,col=caught_any==0),size=3,alpha=.8) +
  geom_line(aes(y=fittedp),size=1)+
  geom_segment(aes(x=hours,xend=hours,y=fittedp,yend=caught_any,col=caught_any==0),lty="dotted", alpha=.4) +
  scale_y_continuous("Probability of Y", breaks=c(0,.5,1))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_hline(aes(yintercept=.5))+
  guides(col=FALSE)+
  scale_x_continuous("x", breaks=NULL)
p1 + p2
```

As an example @fig-mleprob2 shows the predicted probabilities for a couple of datapoints. The furthest left point is an observed 0, and so the probability of seeing this datapoint (according to the line we've drawn) is 1-0.16, or 0.84. The furthest right is an observed 1, so according to our line, the probability of seeing this datapoint as a 1 is 0.98. This means the probability of seeing those two datapoints, given the line we have, is 0.98 * 0.84. But what we want is the probability of seeing _all_ our data, given this line, so we need to do this for _all_ of our datapoints.^[This means we will end up with a lot of multiplying (computationally expensive), and a tiny tiny number. So instead we typically talk about the log-likelihood, and use summation instead of multiplication. The log-likelihood of the two data points highlighted in @fig-mleprob, given the line is `log(0.98)+log(0.84)`.] Once we have this, we can compare the probability of seeing all our data for this line, and for a different candidate line, and for a whole range of different lines.  

```{r}
#| echo: false
#| label: fig-mleprob2
#| fig-cap: "Note - the dotted lines are not residuals, but just to show the projections of observations down to the line"
ll <- f2 %>% arrange(hours) %>% slice(1,nrow(.)) %>% 
  mutate(lab = paste0("p(y) = ",round(fittedp,2)))

p2 +
  geom_segment(data=ll, aes(x=hours,xend=hours,y=fittedp,yend=caught_any,col=caught_any==0),lty="solid", alpha=1) + 
  geom_segment(
    x=ll$hours[1]+.3,xend=ll$hours[1],
    y=ll$fittedp[1]+.1,yend=ll$fittedp[1]
  ) + 
  geom_label(x=ll$hours[1],
             y=ll$fittedp[1]+.1,
             label=ll$lab[1], hjust=0) +
  geom_segment(
    x=ll$hours[2]-.3,xend=ll$hours[2],
    y=ll$fittedp[2]-.1,yend=ll$fittedp[2]
  ) + 
  geom_label(x=ll$hours[2],
             y=ll$fittedp[2]-.1,
             label=ll$lab[2], hjust=1)

```

The cool thing is that computers do all this for us. *Maximum likelihood estimation (MLE)* is just a method to find out which parameters maximise the probability of seeing our data. MLE can be used for all sorts of models, but in the context of a logistic regression, we use it to find the values of coefficients $b_0$ and $b_1$ in our model $\color{red}{ln\left(\frac{p}{1-p} \right)} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_1 }$ that have the maximum likelihood of reproducing the data that we have observed.  


```{r}
#| include: false

# basic model
glm(caught_any~1,fish,family=binomial)  %>% coef()
# odds to p
exp(0.2006707)/(1+exp(0.2006707))
# likelihood
0.55^sum(fish$caught_any) * 0.45^(sum(fish$caught_any==0))
# log likelihood 
log(0.55^sum(fish$caught_any)) + 
  log(0.45^(sum(fish$caught_any==0)))

glm(caught_any~1,fish,family=binomial)  %>% logLik()


tibble(
  y = fish$caught_any,
  p = fitted(glm(caught_any~hours,fish,family=binomial)),
  l = ifelse(y==1,p,1-p),
  ll = log(l)
) %>% 
  #pull(l) %>% prod()
  pull(ll) %>% sum()
glm(caught_any~hours,fish,family=binomial) %>% logLik()
```

:::

# Marshmallows

```{r}
#| echo: false
set.seed(25)
mallow = tibble(
  age = round(runif(100,36,120)),
  taken = rbinom(100, 1, prob = plogis(scale(age*-2)))
) |> mutate(age=age/12)
# write_csv(mallow, "../../data/mallow.csv")
```

Let's suppose we conduct an experiment in which we replicate the [Stanford Marshmallow Experiment](https://youtu.be/JSRmdzofkio?si=Po_kwp16am60ZQ78){target="_blank"}. 100 children took part, ranging from ages 3 to 10 years old. Each child was shown a marshmallow, and it was explained that they were about to be left alone for 10 minutes. They were told that they were welcome to eat the marshmallow while they were waiting, but if the marshmallow was still there after 10 minutes, they would be rewarded with __two__ marshmallows.  

We're interested in how the ability to wait to obtain something that one wants develops with age (which was measured in months, but we have converted it to years).  

Our outcome variable here is "whether or not the child takes the marshmallow", and we have one continuous predictor (age).  

Here is our data:  
```{r}
mallow <- read_csv("https://uoepsy.github.io/data/mallow.csv")
ggplot(mallow, aes(x=age, y=taken))+
  geom_point(alpha=.5, size=3)+
  labs(x="Age (years)", y = "Takes Marshmallow")
```

Practically speaking, fitting our logistic regression model is very straightforward, and the summary output looks fairly similar. We can also easily compare models in a similar (but not identical) way as we have done with linear models. The differences come in when we interpreting the coefficients from these models, and how we compare models to one another.    

```{r}
mallowmod <- glm(taken ~ age, data = mallow,
                 family=binomial)
```


# Comparing models

As we have seen with our linear regression models, we can examine whether certain variables (or _sets_ of variables collectively) are useful in explaining our outcome variable, by comparing a model with a simpler "restricted" model without those variables.  

For linear regression models, this involved examining the 'reduction in residual sums of squares', which resulted in an $F$ statistic.  

In the generalized linear model world, we don't have straightforward residuals, and we compare models on a different metric, known as __deviance__. With a big enough sample size, reductions in deviance are $\chi^2$ distributed, meaning that we can compare them to an appropriate $\chi^2$ distribution to compute a p-value.  

For instance, we can compare our model to the null model (i.e. just an intercept):  

In R we do this comparison as follows:
```{r}
mallowmod0 <- glm(taken ~ 1, data = mallow, family=binomial)

mallowmod1 <- glm(taken ~ age, data = mallow, family=binomial)

anova(mallowmod0, mallowmod1, test = 'Chisq')
```

```{r}
#| echo: false
res = anova(mallowmod0, mallowmod1, test = 'Chisq')
res[,2] <- round(res[,2],1)
```


The output shows the two fitted models, and reports the _Residual Deviance_ of each model, `r res[1,2]` and `r res[2,2]` respectively. So by adding the predictor `age` to the model, we reduce our deviance from `r res[1,2]` to `r res[2,2]`, i.e. we reduce it by `r abs(diff(res[,2]))` (remember, deviance is like badness-of-fit, so smaller is better).   

Is this reduction big enough to be more than just due to random sampling variation? This is what the chi-squared test tells us!  
The `Pr(>Chi)` column is our $p$-value. It tells us that if in the population the predictor `age` were unrelated to taking the marshmallow, it would be very unlikely to see a reduction in deviance as big as or bigger than the one we have seen.  

:::sticky
__What is deviance?__  

Deviance is a measure of deviation/discrepancy/mismatch between the data and the model. You can think of it as a generalisation of the terms making up the residual sum of squares in simple linear regression, in that it measures the misfit, or _badness of fit_. This means (as it was for the residual sum of squares) that smaller deviance is better!  

More technically, deviance is calculated as -2 times the log-likelihood.  

- Likelihood = $P(data | model)$ (the probability of seeing the sample data, given our model parameters). If we had a _perfect_ model, this would be 1.  
    - if you want a (slightly) more in depth look at "likelihood", see [here](lvp.html){target="_blank"}.
- Log-likelihood = the log of the likelihood. The log transformation means that where likelihood ranges from 0 to 1, log-likelihood ranges from negative infinity to 0 (0 would be the perfect model)
- -2Log-likelihood = turns it from negative to positive, and allows us to test it against a $\chi^2$ distribution.  

:::


<!-- Above was Drop-in-deviance GOF test -->
<!-- # Deviance GOF test -->
<!-- mdl_sat = glm(senility ~ factor(1:nrow(sen)), family = binomial, data = sen) -->
<!-- anova(mdl1, mdl_sat, test = 'Chisq') -->


::: {.callout-caution collapse="true"}
#### optional: Akaike and Bayesian Information Criteria

Deviance measures lack of fit, and it can be reduced to zero by making the model more and more complex, effectively estimating the value at each single data point.  

However, this involves adding more and more predictors, which makes the model more complex (and less interpretable). 

Typically, simpler models are preferred when they still explain the data almost as well. This is why information criteria were devised, exactly to account for both the model misfit but also its complexity.

$$
\text{Information Criterion} = \text{Deviance} + \text{Penalty for model complexity}
$$

Depending on the chosen penalty, you get different criteria. Two common ones are the Akaike and Bayesian Information Criteria, AIC and BIC respectively:
$$
\begin{aligned}
\text{AIC} &= \text{Deviance} + 2 p \\
\text{BIC} &= \text{Deviance} + p \log(n)
\end{aligned}
$$

where $n$ is the sample size and $p$ is the number of coefficients in the model. __Models that produce smaller values of these fitting criteria should be preferred.__ 

AIC and BIC differ in their degrees of penalization for number of regression coefficients, with BIC usually favouring models with fewer terms. 

```{r}
AIC(mallowmod0, mallowmod1)
BIC(mallowmod0, mallowmod1)
```

According to both AIC and BIC the model with `age` as a predictor has a lower score, meaning it's preferable to the other.

:::

# Coefficient Interpretation   

An important result of our model capturing the linear associations between the predictors and the **log-odds of outcome Y**, is that our coefficients are all in these units. 
```{r}
#| eval: false
summary(mallowmod)
```
```{r}
#| echo: false
.pp(summary(mallowmod), l=list(3,5:11))
```

```{r}
#| echo: false
broom::tidy(mallowmod) |>
  transmute(
    coefficient = term,
    estimate = round(estimate,2),
    interpretation = c(
      paste0("the log-odds of taking the marshmallow for a child aged zero is ",round(coef(mallowmod)[1],2)),
      paste0("being 1 year older is associated with a ",
             round(coef(mallowmod)[2],2),
             " decrease in the log-odds of a child taking the marshmallow")
    )
  ) |> gt::gt()
```

If you're anything like me, these are not the most useful interpretations. I have no idea what decreasing `r round(coef(mallowmod)[2],2)` log-odds means in any practical terms.  

What we can do instead is translate these back into odds, which might make things easier. The opposite of the natural **logarithm** is the **exponential** (see [here](https://mathonweb.com/help_ebook/functions.htm#functions_3){target="_blank"} if you're interested). We can turn $log(x)$ back into $x$ by raising $e$ to the power of it: $e^{\log(x)} = x$. In R, this is just the two functions `log()` and `exp()`:  `exp(log(2)) == 2`. 

This means we can turn our *log-odds* back into *odds* by using `exp()` on our coefficients! A crucial thing to note is that _addition_ on the log-odds scale translates to _multiplication_ on the odds scale. So while a coefficient on the log-odds scale is interpreted the same way as we interpret normal linear regression coefficients, when we convert these to odds using `exp()`, the effect is in terms of how much the odds get **multiplied** by.  

```{r}
#| echo: false
tibble(
  what=c(
    "linear regression lm()",
    "logistic regrssion glm(), coefficients in log-odds",
    "logistic regression glm(), exponentiated coefficients"
  ),
  interpretation = c(
    "if you go up 1 on X you add [b] on Y",
    "if you go up 1 on X you add [b] on log-odds(Y)",
    "if you go up 1 on X, your odds are *multiplied by* [exp(b)]"
  )
) |> gt::gt()
```

For our marshmallow study, we can see our exponentiated coefficients here:  

```{r}
exp(coef(mallowmod))
```

The intercept is now the odds of a child aged zero taking the marhsmallow: `r round(exp(coef(mallowmod))[1],2)`. This means that we estimate for every `r round(exp(coef(mallowmod))[1],2)+1` children aged zero, we would expect `r round(exp(coef(mallowmod))[1],2)` of them to take the marshmallow, and 1 of them to not take it.  

Our coefficient for `age` tells us that we estimate that for every year older a child is, the odds of them taking the marhsmallow are _multiplied_ by `r round(exp(coef(mallowmod)[2]),2)`.  

The exponentiated coefficient for `age` here is referred to as an "odds ratio".  

:::sticky
__Odds Ratios__  

When we exponentiate coefficients from a model fitted to the log-odds, the resulting association is referred to as an "odds ratio" (OR).  

There are various ways of describing odds ratios:

- "for a 1 unit increase in $x$ the odds of $y$ change by _a ratio_ of `exp(b)`"   
- "for a 1 unit increase in $x$ the odds of $y$ are multiplied by `exp(b)`"  
- "for a 1 unit increase in $x$ there are `exp(b)` increased/decreased odds of $y$"  

Instead of thinking of a coefficient of 0 as indicating "no association", in odds ratios this when the OR = 1. 

- __OR = 1__ : equal odds ($1 \times odds = odds \text{ don't change}$). 
- __OR < 1__ : decreased odds ($0.5 \times odds = odds \text{ are halved}$)
- __OR > 1__ : increased odds ($2 \times odds = odds \text{ are doubled}$)

::: {.callout-caution collapse="true"}
#### OR are not "exp(b) times more likely"  

Often you will hear people interpreting odds ratios as "$y$ is `exp(b)` times as likely".  
Although it is true that increased odds is an increased likelihood of $y$ occurring, double the odds does not mean you will see _twice_ as many occurrences of $y$ - i.e. it does not translate to doubling the probability.  

Here's a little more step-by-step explanation to explain:  

```{r}
#| echo: false
cc = round(coef(mallowmod),2)
ec = round(exp(coef(mallowmod)),2)

tibble(coefficient=c("(Intercept)","age"),b=round(coef(mallowmod),2),`exp(b)`=round(exp(coef(mallowmod)),2)) %>% gt::gt()
```

1. For children aged 2 years old the log-odds of them taking the marshmallow are `r paste0(cc[1]," + ", cc[2],"*2")` = `r cc %*% c(1,2)`.  
2. Translating this to odds, we exponentiate it, so the odds of them taking the marshmallow are $e^{(`r paste0(cc[1]," + ", cc[2],"*2")`)} = e^{`r cc %*% c(1,2)`} = `r round(exp(cc %*% c(1,2)),2)`$.   
(This is the same^[$e^{a+b} = e^a \times e^b$. For example: $2^2 \times 2^3 = 4 \times 8 = 32 = 2^5 = 2^{2+3}$] as $e^{`r cc[1]`} \times e^{`r paste0(cc[2],"*2")`}$)   
3. These odds of `r round(exp(cc %*% c(1,2)),2)` means that (rounded to nearest whole number) if we take 13 children aged 2 years, we would expect 12 of them to take the marshmallow, and 1 to not take it.  
4. If we consider how the odds change for every extra year of age (i.e. for 3 year old as opposed to 2 year old children):
    - the log-odds of taking the marshmallow decrease by `r round(cc[2],2)`.  
    - the odds of taking the marshmallow are multiplied by `r round(ec[2],2)`.  
    - so for a 3 year old child, the odds are $`r round(exp(cc %*% c(1,2)),2)` \times `r round(ec[2],2)` = `r round(exp(cc %*% c(1,3)),2)`$.  
    (And we can also calculate this as $e^{`r round(cc %*% c(1,2),2)` + `r round(cc[2],2)`}$)  
5. So we have gone from odds of 12.4-to-1 for 2 year olds, and 6.7-to-1 for 3 year olds. The odds have been multiplied by `r round(ec[2],2)`.    
But those odds, when converted to probability, these are 0.93 and 0.87. So $0.54 \times odds$ is not $0.54 \times probability$.  

:::

:::


::: {.callout-note collapse="true"}
#### the intercept (but not slopes) can be converted to a probability

Because our intercept is at a single point (it's not an _association_), we can actually convert this to a probability. Remember that $odds = \frac{p}{1-p}$, which means that $p = \frac{odds}{1 + odds}$. So the probability of taking the marshmallow for a child aged zero is $\frac{`r round(exp(coef(mallowmod))[1],2)`}{1 + `r round(exp(coef(mallowmod))[1],2)`} = `r round(round(exp(coef(mallowmod))[1],2)/(1+round(exp(coef(mallowmod))[1],2)),2)`$.  


Unfortunately, we can't do the same for any slope coefficients. This is because while the intercept is "odds", the slopes are "odds ratios" (i.e. changes in odds), and changes in odds are different at different levels of probability.  

Consider how when we multiply odds by 2, the increase in probability is not constant:  

| Odds     | Probability |
| ----------- | ----------- |
| 0.5   | $\frac{1}{1+0.5} = 0.33$  |
| 1   | $\frac{1}{1+1} = 0.5$  |
| 2   | $\frac{2}{1+2} = 0.66$  |
| 4   | $\frac{4}{1+4} = 0.8$  |
| 8   | $\frac{8}{1+8} = 0.88$  |

:::

# Coefficient tests & intervals

:::statbox
__Coefficient Tests__  

You might notice that the summary output of our logistic regression model has $z$ statistics instead of $t$ statistics. However, the logic of the tests remains just the same as it was for the linear model (and for all the tests we have done thus far) - it is a test against the null hypothesis that the coefficient is zero. Or, less formally, we can think of the corresponding p-value as "in the null universe where there is no relationship between $x$ and the occurrence of event $y$, there is a `p-value` chance of seeing a sample of this size with an association as strong as the one we have got"  

```{r}
#| eval: false
summary(mallowmod)
```
```{r}
#| echo: false
.pp(summary(mallowmod),l=list(6:11))
```

:::int
For every additional year older a child is, the odds of them taking the marshmallow decreased by `r round(exp(coef(mallowmod)[2]),2)` ($z = `r round(coefficients(summary(mallowmod))[2,3],2)`$, $p < .001$).  
:::


::: {.callout-caution collapse="true"}
#### optional: why z not t?  

In _linear_ regression, we have to estimate the variance of the residuals ($\sigma^2$ - we could get out the standard deviation $\sigma$ using `sigma(model)`). This term then went in to the calculation of the standard errors of our coefficients.^[COMPLETELY OPTIONAL COMPLEX FORMULA ALERT THAT YOU DON'T NEED TO KNOW: standard errors of coefficients are calculated as $\sigma^2 (X'X)^{-1}$. The $X$ in this case is our "design matrix" (all our predictor variables, plus our intercept, for each observation). $(X'X)$ is just a way of multiplying a matrix by itself - so it represents the relationships - in terms of sums of squares - between all our predictor variables in the model. So the formula for coefficients standard errors is based on the residual variance and the amount of (co)variance in our predictors.] But we are _estimating_ the residual variance, so instead of using $\sigma$ (the population residual variance) we are really using $s$ (a sample estimate). This means we are going to be less certain with smaller samples (it's the exact same thing as when we first learned about $t$ in [3B #t-distributions](03b_inference2.html#t---distributions){target="_blank"}).  

In logistic regression, the variance is directly connect to the mean - we dont have to estimate it separately. If something is binomially distributed with $p$ probability in $n$ trials, then the expected mean is $np$ and the expected variance is $np(1-p)$. So we don't actually need to estimate the variance. The upshot here is that we can use $z$ directly instead. 

:::

:::



:::statbox
__Confidence Intervals__  
With all our linear models (generalized or not), we can obtain confidence intervals using the `confint()` function. Because confidence intervals are just lower and upper bounds for a range of plausible values for our estimates, we can turn these into the confidence intervals around our odds ratios!  

```{r}
exp(confint(mallowmod))
```

:::int
For every additional year older a child is, the odds of them taking the marshmallow decreased by `r round(exp(coef(mallowmod)[2]),2)` (95% CI [`r paste0(round(exp(confint(mallowmod)[2,]),2),collapse=", ")`]).  
:::

:::

# Visualising 

While interpreting coefficients leaves us speaking in the realm of odds-ratios, we can do more with visualisations! This is because we can simply display the predicted probability across values of some predictor. We don't have to try and rely solely on a numerical description of a relationship - a picture speaks a thousand words (and it's a lot easier!). 

```{r}
library(effects)

effect(term = "age", mod = mallowmod, xlevels = 20) |>
  as.data.frame() |>
  ggplot(aes(x=age,y=fit,ymin=lower,ymax=upper))+
  geom_line()+
  geom_ribbon(alpha=.3)
```


::: {.callout-caution collapse="true"}
#### optional: manual plots

As with previous plots in linear models, to make plots manually, we need to create a little plotting data frame and then add to that the model estimated values and interval bounds.   

```{r}
plotdata <- tibble(
  # 50 values between 3 and 10 years old
  age = seq(3,10,length.out=50) 
)
```

For these models, we can actually predict on various different scales (e.g. do we want to predict log-odds values (`predict(mod, type="link")`) or probabilities (`predict(mod, type="response")`)?  

Unfortunately, `augment` doesn't easily give us intervals for these models, so we'll have to make them ourselves. These have to be done on the log-odds scale first. We can't take the estimated probability and then $\pm$ some number, because then it might go over 1 or below 0.  

`augment()` can get us _some_ of the way there. We can get out fitted values (on whatever scale we want, either "link" - the log-odds, or "response" - the probabilities), and we can get out standard errors (on that same scale)  
```{r}
#| eval: false
broom::augment(mallowmod, newdata = plotdata, 
               type.predict = "link",
               se_fit=TRUE)
```
```{r}
#| echo: false
broom::augment(mallowmod, newdata = plotdata, 
               type.predict = "link",
               se_fit=TRUE) |> 
  mutate(across(1:3, ~round(.,2))) |>
  head(4L) |> rbind("...") |> as.data.frame()
```

Because this gets us the estimate and the standard error, we can then create out interval bounds using $estimate \pm 1.96 \cdot SE$. The 1.96 is back, because we're back using $z$ ("normal") distributions.  

However, we're ultimately wanting to convert these back to probabilities for our plotting. We can do this with something like Martin's `l2p()` function in the lecture.^[other functions like `plogis()` are available]

```{r}
l2p <- function(logits) {
  odds <- exp(logits)
  prob <- odds/(1 + odds)
  return(prob)
}

broom::augment(mallowmod, newdata = plotdata, 
               type.predict = "link",
               se_fit=TRUE) |>
  mutate(
    .lower = l2p(.fitted - 1.96*.se.fit),
    .upper = l2p(.fitted + 1.96*.se.fit),
    .fitted = l2p(.fitted)
  ) |> 
  ggplot(aes(x=age,y=.fitted,ymin=.lower,
             ymax=.upper))+
  geom_line()+
  geom_ribbon(alpha=.2)
```

:::




# Assumptions

Because these models don't have the same expected error distribution (we don't expect residuals to be "normally distributed around the mean, with constant variance"), checking the assumptions of logistic regression is a little different.  

Typically, we look at the "deviance residuals". But we __don't__ examine plots for patterns, we simply examine them for potentially outlying observations. If we use a standardised residual, it makes it easier to explore extreme values as we expect most residuals to be within -2, 2 or -3, 3 (depending on how strict we feel).  

:::statbox
__Deviance Residuals__  

There are three ways we can get out deviance residuals, each scaled differently 

- $i$th residual = measure of deviance contributed from the $i$th observation
- $i$th standardized residual = residual / SD(residual)
- $i$th studentized residual = residual / SD(residual from model fitted without observation $i$)

We get these in R using:  
```{r}
#| eval: false
# deviance residuals
residuals(mallowmod, type = 'deviance')
# studentised residuals
rstudent(mallowmod, type = 'deviance')
# standardised residuals
rstandard(mallowmod, type = 'deviance')
```

:::

We can check whether any residuals are larger than 2 or 3 in absolute value:
```{r}
plot(rstudent(mallowmod, type = 'deviance'), 
     ylab = 'Studentized Deviance Residuals')
```
_*Warning:* Don't inspect this plot for patterns!!!_  

There appears to be 1 residual with a value slightly larger than 2 in absolute value. We will keep these in mind and check later if they are also influential points (using `cooks.distance`)

Sometimes a __binned plot__^[Gelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511790942] can be more informative, but not always! It works by combining together all responses for people having the same covariate $x_i$ value, and taking the average studentized Pearson residual for those.

Before using this function, make sure you have installed the **arm** package!

There don't appear to be any extreme residuals:  

```{r}
arm::binnedplot(fitted(mallowmod), 
                rstudent(mallowmod, type = 'deviance'),
                xlab = 'Prob. of Taking Marshmallow', 
                ylab = 'Studentized Deviance Residuals')
```

# summary.lm and summary.glm

::: {.callout-note collapse="true"}
#### lm summary, annotated
```{r}
#| echo: false
#| label: fig-summarylman
#| fig.cap: "lm summary, annotated. click the image to zoom in, click the image to zoom out"
knitr::include_graphics("images/glm/lmsummary.png")
```
:::
::: {.callout-note collapse="true"}
#### glm summary, annotated
```{r}
#| echo: false
#| label: fig-summaryglman
#| fig.cap: "glm summary, annotated. click the image to zoom in, click the image to zoom out"
knitr::include_graphics("images/glm/glmsummary.png")
```
:::


<!-- `r optbegin("Optional: Pearson Residuals", olabel=FALSE)` -->
<!-- __Pearson Residuals__   -->

<!-- There is another form of residual we can use, called "Pearson residuals". These are similar to the Pearson residuals you might remember from the [chi-squared test (4A)](04a_chisq.html){target="_blank"}, which compared Observed and Expected values, but in this case Expected means Predicted from the model: -->
<!-- $$ -->
<!-- Pres_i = \frac{Observed - Expected}{\sqrt{Expected}} -->
<!-- $$ -->

<!-- In logistic regression, this is  -->
<!-- $$ -->
<!-- Pres_i = \frac{y_i - \hat p_i}{\sqrt{\hat p_i (1 - \hat p_i)}} -->
<!-- $$ -->
<!-- where  -->

<!-- - $y_i$ is the observed response for unit $i$, either 0 (failure) or 1 (success) -->
<!-- - $\hat p_i$ is the model-predicted probability of success -->

<!-- We can get these out from R, and use them to create similar plots to the one we made for deviance residuals.   -->
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- # pearson residuals -->
<!-- residuals(mallowmod, type = 'pearson') -->
<!-- # standardised pearson residuals -->
<!-- rstandard(mallowmod, type = 'pearson') -->
<!-- # studentised pearson residuals -->
<!-- rstudent(mallowmod, type = 'pearson') -->
<!-- ``` -->
<!-- `r optend()` -->
