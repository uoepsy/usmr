---
title: "9A: Interactions"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(xaringanExtra)
xaringanExtra::use_panelset()
set.seed(993)
contcont <- tibble(
  x1 = rnorm(60),
  x2 = rnorm(60,2,1),
  y = 1*x1 + 2.2*x2 + 1*x1*x2 + rnorm(60),
  partner_time = round(7+(scale(x1)[,1]*3),2),
  relationship_qual = round((scale(x2)[,1]*10) + 60),
  wellbeing = 20 + (scale(y)[,1]*5)
) %>% mutate(x1=partner_time,x2=relationship_qual,y=wellbeing)

ptdat <- contcont %>%
  select(partner_time,relationship_qual,wellbeing)
concont <- contcont %>%
  select(x1,x2,y)


contcat <- tibble(
  x1 = rnorm(60),
  x2 = sample(0:1,60,T),
  y = -.8*x1 - 3*x2 - .6*x1*x2 + rnorm(60),
  apoe4 = x2,
  aqpi = round((scale(x1)[,1]*100) + 180),
  mmse = round((scale(y)[,1]*4) + 22),
) %>% mutate(x1=aqpi, x2=apoe4, y=mmse)

airpol <- contcat %>%
  select(aqpi, apoe4, mmse) %>% mutate(apoe4=ifelse(apoe4==0,"neg","pos"))
contcat <- contcat %>%
  select(x1,x2,y)
set.seed(913)
catcat <- tibble(
  x1 = sample(0:1,60, T),
  x2 = sample(0:1,60, T),
  y = round(8 + 2*x1 - .5*x2 + 4*x1*x2 + rnorm(60)),
  anonymity = ifelse(x1==1,"anonymous","identifiable"),
  asgroup = ifelse(x2==1,"group","alone"),
  candybars = y
)
candy <- catcat %>% select(anonymity, asgroup, candybars)
# write_csv(ptdat, "../../data/usmr_partnertime.csv")
# write_csv(candy, "../../data/usmr_candy.csv")
# write_csv(airpol, "../../data/usmr_airpol.csv")
```


# Holding Constant...

So far, when we've been using multiple linear regression models, all our coefficients have been being interpreted as the "change in $y$ for a 1-unit change in $x$ **while holding constant** the other predictors in the model".   

Consider a model with the following structure:    
$$
y = b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 + \epsilon
$$

When we fit this model structure to some data, we get out our estimated values for the coefficients $b_0$, $b_1$, and $b_2$, e.g.,:   

```{r}
mydata <- read_csv("https://uoepsy.github.io/data/usmr_mlr.csv")
mymodel <- lm(y ~ x1 + x2, data = mydata)
```
```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept) -2.39138    3.67735  -0.650  0.51867   
x1           0.17570    0.06435   2.730  0.00888 **
x2          -0.64756    0.19959  -3.244  0.00217 **
```

The coefficient we get out for $x_1$ tells us that the model estimates that $y$ will increase by 0.17 for every increase of 1 unit in $x_1$, _provided that we hold $x_2$ constant._  

So for _any_ value of $x_2$, a one unit increase in $x_1$ is associated with a $b_1$ change in $y$, provided that $x_2$ value stays as it is.  

Let's use our primate brains data for a more realistic example: 

```{r}
#| output: false
braindata <- read_csv("https://uoepsy.github.io/data/usmr_braindata.csv")
braindata <- braindata %>% mutate(
  isMonkey = ifelse(species != "Human", "YES", "NO")
)

monkeymod <- lm(mass_brain ~ age + isMonkey, data = braindata)
summary(monkeymod)
```
```
...
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.668308   0.050478  13.240 1.57e-14 ***
age         -0.004081   0.002337  -1.746   0.0904 .  
isMonkeyYES -0.246933   0.044152  -5.593 3.54e-06 ***
---
```

Our coefficient for `isMonkeyYES` coefficient is the estimated difference in brain mass between a human and a monkey **of the same age**.  
Similarly, our coefficient for `age` is the estimated change in brain mass for every additional year of age, provided we are not comparing a monkey to a human. 

The idea of this can be seen on the plot in @fig-constplot. Imagine 4 different participants in our data. The coefficient for `isMonkeyYES` is the difference between A and C, or the difference between B and D. It doesn't matter where on the x-axis we go, it is the _vertical_ distance between the two lines.  
The `age` coefficient is the comparison between A and B, or between C and D.  

```{r}
#| label: fig-constplot
#| fig-cap: "multiple regression model of brain mass predicted by age and isMonkey. The age coefficient is the slope of the lines, and the isMonkey coefficient is the difference in their heights"
#| echo: false
mm = data.frame(
  who = c("A\nHuman age 10","C\nMonkey age 10","B\nHuman age 11","D\nMonkey age 11"),
  age = c(10,10,11,11),
  agelab = c(6,6,16,16),
  isMonkey=rep(c("NO","YES"),2)
) |> broom::augment(monkeymod, newdata=_)

expand_grid(
  age = min(braindata$age):max(braindata$age),
  isMonkey = c("NO","YES")
) |>
  broom::augment(monkeymod, 
                 newdata = _, 
                 interval="confidence") |>
  ggplot(aes(x=age,y=.fitted))+
  geom_line(aes(col=isMonkey))+
  geom_ribbon(aes(col=isMonkey,fill=isMonkey,
                  ymin=.lower,ymax=.upper), alpha=.2)+
  geom_point(data=mm)+
  geom_segment(data=mm,aes(x=agelab,xend=age,y=.fitted-.1,yend=.fitted))+
  geom_label(data=mm,aes(x=agelab,y=.fitted-.1,label=who), fill="white")
```

::: {.callout-note collapse="true"}
#### *another* example (if you want one)

We're interested in estimating the association between BMI and cognition, after controlling for differences due to age.  

We want to use this model:  
$$
\text{score on test of cognition} = b_0 + b_1 \cdot \text{age} + b_2 \cdot \text{BMI} + \epsilon
$$
which we can fit using `lm()`:  
```{r}
#| eval: false
lm(score ~ 1 + age + BMI, data = ourdata)
```
and we might get some coefficients (estimates for the $b_?$ values) such as those below:  
_(I've just made up some nice round numbers to make it easier to think about)_  

```
Coefficients:
            Estimate    ...   ...
(Intercept)  160.000    ...   ...
age         -1.50000    ...   ...
BMI         -2.50000    ...   ...
```

The coefficient of interest, the one for BMI, is telling us that "a 1 unit increase in BMI is associated with a -2.5 decrease in Scores on the cognitive test, _holding age constant_".  

Consider 3 people: 

```{r}
#| label: fig-pplconstant
#| fig-cap: "Three theoretical people"  
#| echo: false
knitr::include_graphics("images/ints/constant.png")
```


The coefficient for BMI represents the difference in cognitive scores we would expect between Person A and Person B.  

Think about why this is.  
For some person $i$, their model predicted score is:  
$$
\hat{score_i} = b_0 + b_1 \cdot age_i + b_2 \cdot BMI_i
$$
Which from our model estimates is:  
$$
\hat{score_i} = 160 - 1.5 \cdot age_i - 2.5 \cdot BMI_i
$$

- Person A's score = $160 - (1.5*50) - (2.5*22) = 30$
- Person B's score = $160 - (1.5*50) - (2.5*23) = 27.5$
- Person C's score = $160 - (1.5*60) - (2.5*23) = 12.5$

The difference in model estimated Score between Person A and Person B is the coefficient of BMI, because those two people _only differ_ on BMI. Person A and Person C _also differ_ on age. This is how the coefficient of BMI is interpreted as "holding age constant" - it is a comparison between two hypothetical people who differ on BMI but are identical with respect to the other predictors in the model.  
:::

<div class="divider div-transparent div-dot"></div>

# It Depends...

Sometimes, however, we might have reason to think that a certain association should *not* be the same at every value of another variable.  

There are lots of practical cases where we might think that the relationship between two variables _depends on_ the value of a third. 

Below are some examples of this idea, where the explanatory variables (the predictors) are of different types (e.g. continuous, categorical, etc):  

:::panelset

:::panel
#### Example 1

> The amount to which spending time with partner influences wellbeing _depends on_ the quality of the relationship.  

Variables:  

- __Outcome:__ Wellbeing (questionnaire score ranging from 0 to 32)
- __Continuous Predictor:__ Time spent with partner (hours per day)  
- __Continuous Predictor:__ Relationship Quality (rating from 0 to 100)  


:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ genetic status.  

Variables:  

- __Outcome:__ Cognitive Functioning measured via the MMSE. Possible scores range from 0 to 30. Lower scores indicate poorer cognitive functioning
- __Continuous Predictor:__ Air-Quality Pollution Index (AQI). Ranges from 0 to 500. The higher the AQI value, the greater the level of air pollution.
- __Categorical Predictor:__ APOE-4 Genotype status: Negative vs Positive  


:::
:::panel
#### Example 3

> Trick-or-treating children are more greedy when their costume hides their identity. But this is different _depending on_ whether they are alone or in a group.  


Variables:  

- __Outcome__: Number of candy bars taken while trick or treating
- __Categorical Predictor:__  Whether or not the childs' identity is hidden by their costume (anonymous vs identifiable)
- __Categorical Predictor:__ Whether the child is trick or treating as a group or alone (alone vs group)  


:::

:::

<div class="divider div-transparent div-dot"></div>

# Interactions!

In each of the above examples, we can no longer think about "the relationship between [outcome] and [predictor]" without discussing the level of the other predictor.  

:::: {.columns}
::: {.column width="55%"}
Using Example 1 above, let's return to thinking about this in terms of two different observations (i.e. two different people in @fig-pplint).  

How do we think each person's wellbeing will change if they spend an extra hour with their partner?  

Person A (who has a really great relationship), will probably increase in wellbeing if they were to spend more timewith their partner.  

Will it be the same for Person B? Person B doesn't have a great relationship. If they spend an extra hour with their partner, we probably wouldn't expect their wellbeing to increase _to the same extent as Person A_. It might even go down for Person B! 

:::
::: {.column width="45%"}
```{r}
#| label: fig-pplint
#| fig-cap: "Two theoretical people"  
#| echo: false
#| out-height: "400px"
knitr::include_graphics("images/ints/ints.png")
```

:::
::::


To capture this, what we need is an _extra_ bit of information to tell us "how _much_ does the association between 'time spent with partner' and wellbeing change as relationship quality changes?" And this is something we can include in our model, and get an estimate for!   

We can model the idea of "the association between $x_1$ and $y$ changes _depending on_ the level of $x_2$" by including a product (multiplication) term between the two predictors.   

Such a model would take the form:  


$$
y = b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 + b_3 \cdot x_1 \cdot x_2 + \epsilon
$$

What this is doing is saying that our outcome $y$ is predicted by: 

- some amount of $x_1$
- some amount of $x_2$
- and a little addition to each of those amounts depending on the value of of the other variable.   

To provide a visual intuition and build on how we have been thinking of multiple regression upto this point, when we have two predictors that interact, our regression surface is no longer flat, but _twists_. This is because the slope along values of $x_1$ _changes_ as we move up $x_2$:   


::: {.callout-caution collapse="true"}
#### why is it multiplication?  

If we think about the simple additive model (additive = **without** the interaction), then what we're saying is that the effect of $x_1$ on $y$ is simply $b_1$, which is a _constant_ (i.e. just a single number).  
  
$$
y = b_0 + \underbrace{b_1}_{\text{effect}\\ \text{of } x_1} \cdot x_1 + b_2 \cdot x_2 + \epsilon
$$
But what we want to model is that "the effect of $x_1$ _depends_ on $x_2$", so the effect is not just a single constant number, it's more like "some number plus an amount of $x_2$".  
For "an amount of $x_2$" let's use $b_3 \cdot x_2$:  

$$
y = b_0 + \underbrace{(\overbrace{b_1}^{\,\,\text{ some}\\ \text{number}} + \overbrace{b_{3} \cdot x_2}^{\text{some amount}\\ \,\,\,\,\,\,\,\,\,\text{of }x_2})}_{\text{effect of } x_1} \cdot x_1 + b_2 \cdot x_2 + \epsilon
$$

Which really expands out to:  

$$
y = b_0 + b_1 \cdot x_1 + b_3 \cdot x_2 \cdot x_2 + b_2 \cdot x_2 + \epsilon
$$

:::




:::panelset

:::panel
#### Example 1

> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.^[This example comes from our lovely tutor Ian!]  

```{r}
#| echo: false
#| label: fig-i.contcont
#| fig-cap: "Interaction between two continuous predictors, viewed from two angles"

fit<-lm(y~x1*x2, data=contcont)
steps=50
x1 <- with(contcont, seq(min(x1),max(x1),length=steps))
x2 <- with(contcont, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2)
y <- matrix(predict(fit, newdat), steps, steps)

par(mfrow=c(1,2))
p <- persp(x1,x2,y, theta = 25,phi=20, col = NA,
           xlab="Time spent\nwith partner",ylab="Relationship Quality",zlab="Wellbeing")
obs <- with(contcont, trans3d(x1,x2, y, p))
pred <- with(contcont, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

p <- persp(x1,x2,y, theta = 60,phi=20, col = NA,
           xlab="Time spent\nwith partner",ylab="Relationship Quality",zlab="Wellbeing")
obs <- with(contcont, trans3d(x1,x2, y, p))
pred <- with(contcont, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)
par(mfrow=c(1,1))

```

At high values of relationship quality, the amount wellbeing increases with time spent is greater than it is at low values of relationship quality.  
And we can phrase this the other way around: at high amounts of time spent with partner, relationship quality has a bigger effect on wellbeing than it does for low amounts of time spent. 

In the model with this interaction:  
$$
\text{wellbeing} = b_0 + b_1 \cdot \text{time} + b_2 \cdot \text{quality} + b_3 \cdot \text{time} \cdot \text{quality}
$$
The interaction coefficient $b_3$ is the adjustment we make to the slope of wellbeing with "time spent with partner", as we move 1 up in "relationship quality".^[And vice versa! It is also the adjustment we make to the slope of wellbeing with "relationship quality", as we move 1 up in "time spent with partner"]  

:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.^[and this example comes from Otto!]  

```{r}
#| echo: false
#| label: fig-i.contcat
#| fig-cap: "Interaction between a continuous and a binary categorical predictors"
plot(contcat$y~contcat$x1,col=ifelse(contcat$x2,"red","black"), xlab="air quality pollution index (AQPI)",ylab="Cognition (MMSE)")
abline(lm(y~x1,contcat[contcat$x2==0,]))
abline(lm(y~x1,contcat[contcat$x2==1,]),col="red")
legend(300, 29, legend=c("APOE4 positive", "APOE4 negative"),
       col=c("red", "black"), lty=1)
```

This kind of interaction (where one predictor is continuous and the other is categorical), is sometimes the easiest to think about.  

We can see in @fig-i.contcat that cognition decreases as air pollution increases, __but__ this is different depending on genetic status. In the APOE4-positive group, the association is steeper than in the APOE4-negative group. The interaction is evident in that the two lines are non-parallel.   

In the model:  

$$
\text{Cognition} = b_0 + b_1 \cdot\text{pollution} + b_2\cdot\text{APOE4} + b_3\cdot\text{pollution} \cdot \text{APOE4}
$$

The interaction term $b_3$ is the estimated adjustment made to the slope of cogition across air pollution when we move from one group to the other. e.g. the slope for the APOE4-positive group is equal to the slope of the APOE4-negative group _plus the interaction term_.

:::
:::panel
#### Example 3

> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.^[This example was suggested by Tia]  

```{r}
#| echo: false
#| label: fig-i.catcat
#| fig-cap: "Interaction between two categorical predictors"
ggplot(candy, aes(x=anonymity, y=candybars, col=asgroup)) + 
  stat_summary(geom="pointrange", size=1) +
  stat_summary(geom="line",lwd=1,lty="dotted",aes(group=asgroup))
```

For interactions between two categorical variables, we tend to plot the mean of the outcome variable for each combination of levels. We can see the interaction in @fig-i.catcat in the fact that the two dotted lines we have added to join the group-means are not parallel. 

Children who are anonymous tend to take more candybars than those who are identifiable, but this difference is much greater when children are trick of treating in a group than when they are doing so alone!  

In the model
$$
\text{candy} = b_0 + b_1 \cdot\text{isAnon} + b_2\cdot\text{inGroup} + b_3\cdot\text{isAnon} \cdot \text{inGroup}
$$

The interaction term $b_3$ is going to be the estimated adjustment to the difference between anonymous vs identifiable for children in a group vs those alone. Put another way, it is how the difference between the two blue dots in alone vs group for is different from the difference between the two red dots.  
e.g. the difference between anonymous & identifiable children in a group is equal to the difference between anonymous & identifiable children who are alone _plus the interaction term_.

:::

:::

<div class="divider div-transparent div-dot"></div>

# Fitting interactions in R

In R, the interaction term gets denoted by a colon `:` between the two predictor variables. We can fit an interaction term in a regression model quite easily in R:  

```{r}
#| eval: false
# we don't *need* to put the 1, it's up to you :)
lm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)
```
This maps to the model equation we saw earlier: 
$$
y = b_0 + b_1\cdot x_1 + b_2 \cdot x_2 + b_3 \cdot x_1 \cdot x_2 + \epsilon
$$

:::rtip
__Shortcut__  

To express `x1 + x2 + x1:x2`, we can also use just `x1*x2`.  

These two models are equivalent: 

```{r}
#| eval: false
lm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)
lm(y ~ 1 + x1*x2, data = dataset)
```

:::

:::imp

If we fit the interaction `x1:x2`, we almost _always_ want to also fit the separate effects `x1` and `x2`.  

_"Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of $x_1$ depends on the level of $x_2$ but that there is no effect of $x_1$."_ [Ramsey & Schafer](https://www.cengage.uk/c/the-statistical-sleuth-3e-ramsey-schafer/9781133490678/){target="_blank"}

:::

<div class="divider div-transparent div-dot"></div>

# Interpretation

When we include an interaction term in our model, we are saying that two effects of the two predictors on the outcome are _dependent upon_ one another. This means that with an interaction in the model `lm(y ~ x1 + x2 + x1:x2)` we can no longer talk about the "effect of $x_1$ on $y$_'holding $x_2$ constant'_". Instead we have to talk about **marginal effects** - e.g. the effect of $x_1$ _at a **specific** value_ of $x_2$.   

When we fit a model with an interaction in R, we get out coefficients for both predictors, and for the interaction. The coefficients for each individual predictor reflect the effect on the outcome _when the other predictor is zero_. 

```
lm(formula = y ~ x1 * x2, data = df)
...
...
Coefficients:
             Estimate Std. Error t value Pr(>|t|)  
(Intercept)  ...        ....       ...      ...  
x1           ...        ....       ...      ...  
x2           ...        ....       ...      ...  
x1:x2        ...        ....       ...      ...  
---
```


:::sticky
**marginal effects when the other predictor is zero**  

The individual coefficients for each predictor that is involved in an interaction are estimated _when the other predictor in the interaction is zero._  

:::


These coefficients can be interpreted, in turn as:  

| Coefficient      | Interpretation |
| ----------- | ----------- |
| `(Intercept)` | the estimated $y$ when all predictors ($x_1$ and $x_2$) are zero is [*estimate*] |
| `x1`  | **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a [*estimate*] change in $y$ |
| `x2`  | **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a [*estimate*] change in $y$. |
| `x1:x2`  | as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by [*estimate*]<br>_**or**_<br>as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by [*estimate*] |





::: {.callout-note collapse="true"}
#### What if there are other things in the model too?

Note that the interaction `x1:x2` changes how we interpret the individual coefficients for `x1` and `x2`.  

It does __*not*__ change how we interpret coefficients for other predictors that might be in our model. For variables that **aren't** involved in the interaction term, these are still held constant.  

For example, suppose we _also_ had another predictor $c_1$ in our model: 
```
lm(y ~ c1 + x1 + x2 + x1:x2)
```

| Coefficient      | Interpretation |
| ----------- | ----------- |
| `(Intercept)` | the estimated $y$ when all predictors ($c_1$, $x_1$ and $x_2$) are zero is [*estimate*] |
| `c1` | a 1 unit increase in $c_1$ is associated with a [*estimate*] increase in $y$, holding constant all other variables in the model ($x_1$ and $x_2$) |
| `x1`  | holding $c_1$ constant, **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a [*estimate*] change in $y$ |
| `x2`  | holding $c_1$ constant, **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a [*estimate*] change in $y$. |
| `x1:x2`  | holding $c_1$ constant, as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by [*estimate*]<br>_**or**_<br>holding $c_1$ constant, as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by [*estimate*] |


:::



<br><br>

:::panelset

:::panel
#### Example 1

> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  

```{r}
# data
ptdat <- read_csv("https://uoepsy.github.io/data/usmr_partnertime.csv")
# model
eg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = ptdat)
summary(eg1mod)
```

- For someone who spends 0 hours with their partner, and who has a relationship quality score of 0, their estimated wellbeing is `r round(coef(eg1mod)[1],2)`.  
- For someone who has a relationship quality score of 0, every hour spent with their partner is associated with a `r round(coef(eg1mod)[2],2)` change in wellbeing.  
- For someone who spends 0 hours with their partner, an increase of 1 in the relationship quality is associated with a `r round(coef(eg1mod)[3],2)` change in wellbeing.  
- For every 1 increase in relationship quality, an extra hour spent with their partner is associated with an _additional_ `r round(coef(eg1mod)[4],2)` change in wellbeing. 


:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  

```{r}
# data
airpol <- read_csv("https://uoepsy.github.io/data/usmr_airpol.csv")
# model
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)
summary(eg2mod)
```

- For people who are APOE4 negative (the reference level, see [8A#multiple-categories-multiple-regression](08a_mlr.html#multiple-categories-multiple-regression){target="_blank"}), living where the Air Quality Pollution Index is 0 (no pollution), the estimated score on the MMSE (mini mental state exam) is `r round(coef(eg2mod)[1],2)`.  
- For people who are APOE4 negative, a 1 unit increase in air pollution is associated with a `r round(coef(eg2mod)[2],2)` change in MMSE scores.  
- For people living with 0 air pollution, being APOE4 positive is associated with `r round(coef(eg2mod)[3],2)` lower MMSE scores compared to APOE4 negative
- Compared to APOE4 negative, being APOE4 positive is associated with `r round(coef(eg2mod)[4],2)` greater decrease in MMSE scores for every 1 increase in air pollution.


:::
:::panel
#### Example 3

> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.

```{r}
# data
candy <- read_csv("https://uoepsy.github.io/data/usmr_candy.csv")
# model
eg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)
summary(eg3mod)
```

- An anonymous child trick or treating alone is estimated to take `r round(coef(eg3mod)[1])` candybars.
- When trick or treating alone, compared to an anonymous child, an identifiable child is estimated to take `r abs(round(coef(eg3mod)[2]))` fewer candybars.  
- When anonymous, compared to trick or treating alone, children who are part of a group are estimated to take `r round(coef(eg3mod)[3],1)` more candybars.  
- The difference between identifiable and anonymous children's candybar taking is `r abs(round(coef(eg3mod)[4]))` fewer when children are trick or treating in a group than when they are alone.  


```{r}
#| eval: false
#| echo: false
fit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))
sjPlot::plot_model(fit, type="int",show.data=T)+geom_line()

# it's just viewing that cube from one side 
library(scatterplot3d)
plt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,
                                 x.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 y.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 main="y~x1*x2\n(x1 and x2 are categorical)"))
fit = lm(y~x1*x2,catcat)
pp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))
pp$y <- predict(fit, pp)

pp1 <- pp[pp$x2==0,]
pp2 <- pp[pp$x2==1,]
pp3 <- pp[pp$x1==0,]
pp4 <- pp[pp$x1==1,]
plt$points(pp1$x1,pp1$x2,pp1$y,type="l")
plt$points(pp2$x1,pp2$x2,pp2$y,type="l")
plt$points(pp3$x1,pp3$x2,pp3$y,type="l")
plt$points(pp4$x1,pp4$x2,pp4$y,type="l")
```

:::

:::

<div class="divider div-transparent div-dot"></div>

# Visualisation

We have seen lots of 3-dimensional plots above to try and help in building our intuition about how we model an interaction. However, we typically don't really want to visualise models like this (in part because our models often have more than 2 predictors, and so have more than 3 dimensions).  

What we therefore need to do is find ways to represent these relationships in 2-dimensional plots. This is slightly different depending on the type of variables we are dealing with. 

- For a continuous $\times$ categorical interaction, we can plot the association of the continuous predictor with the outcome *for each level* of the categorical variable. 
- For a continuous $\times$ continuous interaction, we can plot the association of one predictor *at some judiciously chosen values* of the other (e.g. at the min, mean and max, or at -1 SD, mean, and +1 SD).  
- For a categorical $\times$ categorical interaction, we can plot the various estimated group means, with optional dotted^[dotted is a good way to indicate that there is no data across that line - it is linking two categories. Martin disagrees and thinks you should never have lines] lines to illustrate the non-parallelism   


::: {.callout-note collapse="true"}
#### optional helper packages

We can make these sort of plots very easily using handy packages that are designed to make our lives easier. The _big_ downsides are that reliance on these packages will a) make it harder to customise plots, b) make it harder to troubleshoot when things go wrong, and c) stop us from having to *think* (and thinking is always good!).  

**Use with caution:**    

- From the **sjPlot** package: `plot_model(model, type = "int")` or `plot_model(model, type = "eff", terms = c("x1","x2"))`  
- From the **interactions** package: `interact_plot(model, pred = "x1", modx = "x2")`, and `cat_plot()` for categorical interactions.     


:::



:::panelset

:::panel
#### Example 1

> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  

Plotting continuous by continuous interactions requires choosing a set of values for one of our predictors, at which we plot the slope of the other.  

Here, we could choose to plot the slope of `wellbeing ~ partner_time` for the mean `relationship_qual`, and for 1 SD above and below that mean.  

```{r}
#| code-fold: true
# data
ptdat <- read_csv("https://uoepsy.github.io/data/usmr_partnertime.csv")
# model
eg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = ptdat)

# plot data
plotdat <- expand_grid(
  partner_time = 0:13, #min to max of sample
  relationship_qual = c(
    mean(ptdat$relationship_qual)-sd(ptdat$relationship_qual),
    mean(ptdat$relationship_qual),
    mean(ptdat$relationship_qual)+sd(ptdat$relationship_qual)
  )
)
# plot
broom::augment(eg1mod, newdata = plotdat, interval="confidence") |>
  ggplot(aes(x=partner_time, y=.fitted, group=relationship_qual))+
  geom_line(aes(col=relationship_qual))+
  geom_ribbon(aes(ymin=.lower,ymax=.upper,fill=relationship_qual),
              alpha=.2)

```

::: {.callout-tip collapse="true"}
#### optional helper packages

The interactions package defaults to showing the y~x1 relationship when x2 is at -1 SD below the mean, at the mean, and +1 SD above the mean.  
```{r}
library(interactions)
interact_plot(eg1mod, pred = "partner_time", modx = "relationship_qual", interval = TRUE)
```

The __sjPlot__ package can do this too, but wil default to showing it at the min and max of the other predictor.  
```{r}
library(sjPlot)
plot_model(eg1mod, type="int")
```
We can change these manually. For instance, to make `plot_model` show the association when relationship quality is zero, when it is 50, and when it is 100, we could use: 
```{r}
#| eval: false
plot_model(eg1mod, type = "eff", terms=c("partner_time","relationship_qual [0, 50, 100]"))
```

:::


:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  

This is the most straightforward type of interaction to visualise, as there are only a set number of values that one of the variables can take, meaning only a finite number of lines we need to draw:  

```{r}
#| code-fold: true
# data
airpol <- read_csv("https://uoepsy.github.io/data/usmr_airpol.csv")
# model
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)

# plot data
plotdat <- expand_grid(
  aqpi = 0:500,
  apoe4 = c("neg","pos")
)
# plot
broom::augment(eg2mod, newdata = plotdat, interval="confidence") |>
  ggplot(aes(x=aqpi, y = .fitted, 
             col = apoe4, fill = apoe4)) + 
  geom_line() +
  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)

```


::: {.callout-tip collapse="true"}
#### optional helper packages

```{r}
p1 <- sjPlot::plot_model(eg2mod, type = "int")
p2 <- interactions::interact_plot(eg2mod, pred = "aqpi", modx = "apoe4", interval = TRUE)
p1 / p2
```
:::


:::
:::panel
#### Example 3

> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.

For these types of interactions (between categorical variables), plotting the estimates from our model is best done as our set of estimated group means.   
```{r}
#| code-fold: true
# data
candy <- read_csv("https://uoepsy.github.io/data/usmr_candy.csv")
# model
eg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)

# plot data
plotdat <- expand_grid(
  anonymity = c("anonymous","identifiable"),
  asgroup = c("alone","group")
)

# plot
broom::augment(eg3mod, newdata=plotdat, interval="confidence") |>
  ggplot(aes(x = anonymity, col = asgroup)) + 
  geom_pointrange(aes(y=.fitted, ymin=.lower, ymax=.upper),
                  position = position_dodge(width=.2))
```


::: {.callout-tip collapse="true"}
#### optional helper packages

```{r}
p1 <- sjPlot::plot_model(eg3mod, type = "int") 
p2 <- interactions::cat_plot(eg3mod, pred = "anonymity", modx = "asgroup", interval = TRUE)

p1 / p2
```


:::



```{r}
#| eval: false
#| echo: false
fit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))
sjPlot::plot_model(fit, type="int",show.data=T)+geom_line()

# it's just viewing that cube from one side 
library(scatterplot3d)
plt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,
                                 x.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 y.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 main="y~x1*x2\n(x1 and x2 are categorical)"))
fit = lm(y~x1*x2,catcat)
pp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))
pp$y <- predict(fit, pp)

pp1 <- pp[pp$x2==0,]
pp2 <- pp[pp$x2==1,]
pp3 <- pp[pp$x1==0,]
pp4 <- pp[pp$x1==1,]
plt$points(pp1$x1,pp1$x2,pp1$y,type="l")
plt$points(pp2$x1,pp2$x2,pp2$y,type="l")
plt$points(pp3$x1,pp3$x2,pp3$y,type="l")
plt$points(pp4$x1,pp4$x2,pp4$y,type="l")
```

:::

:::

<div class="divider div-transparent div-dot"></div>

# Getting more from your model

Reporting on interactions is a bit like telling a story. Listing the interpretation of the coefficients is fine, but often reads a bit awkwardly. A good presentation of the results would provide the reader with the overall pattern of results, pulling out the key parts that are of interest. There are various things we can do to get out coefficients that might be of more use to us in telling our story.  

## Mean Centering

By mean centering a continuous predictor, we change what "0" means. Normally, when we don't have an interaction, this simply changes the intercept value (see [8A #centering-and-scaling-predictors](08a_scaling.html#centering-and-scaling-predictors){target="_blank"}). If we have the interaction `y ~ x1 + x2 + x1:x2`, then mean centering `x1` will make the coefficient for `x2` now represent "the association between $x_2$ and $y$ for someone at the _average_ of $x_1$". 

Using one of our examples from throughout this reading, we might mean-center the air-pollution so that we can consider the difference in MMSE scores between APOE4 positive and negative people _at the average air-pollution level_

```{r}
# data
airpol <- read_csv("https://uoepsy.github.io/data/usmr_airpol.csv")
# model with original variable
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)
# model with mean centered predictor
eg2mod_cent <- lm(mmse ~ scale(aqpi,scale=FALSE) * apoe4, data = airpol)
```

```{r}
# coefficients:
coef(eg2mod)
coef(eg2mod_cent)
```

This is because the coefficient for APOE4 compares the heights of the two lines when the other predictor is zero. So if we change what "zero" represents, we can change what that estimates. In the model plots below, we can see that _the model doesn't change_, it is just extracting different information (it is the distance to move from the blue dot to the red dot):  

```{r}
#| out-width: "100%"
#| echo: false
airpol <- airpol %>% 
  mutate(
    aqpiC = aqpi - mean(aqpi)
  )
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)
eg2mod_cent <- lm(mmse ~ aqpiC * apoe4, data = airpol)
plot_model(eg2mod, type="int") +
  geom_point(x=0,y=coef(eg2mod)[1], size=4, col="blue")+
  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, col="red")+
  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty="dotted", lwd=1,col="black") + labs(title="Raw AQPI") +
  
plot_model(eg2mod_cent, type="int") +
  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, col="blue")+
  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, col="red")+
  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty="dotted", lwd=1,col="black") + labs(title="Mean Centered AQPI") +
  plot_layout(guides="collect")
```

## Relevelling Factors

Another thing that can be useful (especially when working with categorical variables with lots of levels) is to make sure your variables are `factors` in R, and to set a useful reference level. Typically, the reference level is what we think of as "normal", e.g. if we have 3 groups: Placebo, Drug A, and Drug B, then we might compare each drug to the placebo condition, because that's comparable to most people (i.e. who aren't taking the drug).

For example, when we have two categorical variables:  

- anonymity = "anonymous" vs "identifiable"
- asgroup = "alone" vs "group"

Then the default is to take the alphabetical ordering. We can change the ordering using functions that "relevel" a factor. Note, this only works if the variable is _already_ a `factor` in R (see [2A#categorical](02a_measurement.html#categorical){target="_blank"} for a reminder of 'factors').   

```{r}
# data 
candy <- read_csv("https://uoepsy.github.io/data/usmr_candy.csv")

# make both predictors 'factors'
candy <- 
  candy %>% 
  mutate(
    anonymity = factor(anonymity),
    asgroup = factor(asgroup)
  )
```

Once they are factors, we can see the default levels:  

:::: {.columns}
::: {.column width="47.5%"}
```{r}
levels(candy$anonymity)
```
:::
::: {.column width="5%"}
<!-- empty column to create gap -->
:::
::: {.column width="47.5%"}
```{r}
levels(candy$asgroup)
```
:::

::::


This is the original model, with the default levels:  
```{r}
eg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)
coef(eg3mod)
```

Let's relevel anonymity to have "identifiable" as the first level, and then refit the model:  
```{r}
candy <- 
  candy %>% 
  mutate(
    anonymity = fct_relevel(anonymity, "identifiable")
  )

eg3mod_rel <- lm(candybars ~ anonymity * asgroup, data = candy)

coef(eg3mod_rel)
```

Again, the model doesn't change, we are simply extracting different bits from it:  

::::panelset
:::panel
#### reference level = "anonymous"

```{r}
#| echo: false
#| out-width: "100%"
p1 <- plot_model(eg3mod, type = "int") +
  labs(title="with 'anonymous' and 'alone' as reference levels") + 
  
  
  geom_segment(x = 1, xend = 1, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,3)]), 
                col="green3", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 1, xend = 1.95, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,2)]), 
                col="orange3", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 1.05, xend = 2.05, y = sum(coef(eg3mod)[c(1,3)]), yend=sum(coef(eg3mod)[c(1,2,3)]), 
                col="black", lty="dotted", lwd=.5,alpha=.2) +
  
  geom_segment(x = 2.05, xend = 2.05, y = sum(coef(eg3mod)[c(1:3)]), yend=sum(coef(eg3mod)[c(1:4)]), 
                col="purple", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  
  annotate(geom="label", x=1.1,y=coef(eg3mod)[1],label="Intercept")+
  annotate(geom="label", x=1.01,col="green3", hjust=0,y=coef(eg3mod)[1]+2,label="'asgroup' coefficient") +
  annotate(geom="label",col="orange3", x=1.5,y=coef(eg3mod)[1]-1,label="'anonymity' coefficient") +
  annotate(geom="label",col="purple", x=2.04,hjust=1,y=coef(eg3mod)[1],label="interaction coefficient") 

p1 + theme_bw(base_size = 12)

```

:::
:::panel
#### reference level = "identifiable"

```{r}
#| echo: false
#| out-width: "100%"
p2 <- plot_model(eg3mod, type = "int") +
  labs(title="with 'identifiable' and 'alone' as reference levels") + 
  
  geom_segment(x = 1.95, xend = 1.95, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,3)]), 
                col="green3", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 1.95, xend = 1, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,2)]), 
                col="orange3", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 2, xend = 1, y = sum(coef(eg3mod_rel)[c(1,3)]), yend=sum(coef(eg3mod_rel)[c(1,2,3)]), 
                col="black", lty="dotted", lwd=.5, alpha=.2) +
  
  geom_segment(x = 1, xend = 1, y = sum(coef(eg3mod_rel)[c(1:3)]), yend=sum(coef(eg3mod_rel)[c(1:4)]), 
                col="purple", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  
  annotate(geom="label", x=2,hjust=0,y=coef(eg3mod_rel)[1],label="Intercept")+
  annotate(geom="label", x=1.93,col="green3", hjust=1,y=coef(eg3mod_rel)[1]-.25,label="'asgroup' coefficient") +
  annotate(geom="label",col="orange3", x=1.5,y=coef(eg3mod_rel)[1]+1,label="'anonymity' coefficient") +
  annotate(geom="label",col="purple", x=1.01,hjust=0,y=coef(eg3mod_rel)[1]+4,label="interaction coefficient")


p2 + theme_bw(base_size = 12)
```
:::
::::

<!-- :::sticky -->
<!-- __Mapping model estimates to model plots__   -->

<!-- To help with interpretation of the estimated coefficients from a model, a very useful exercise is to try and map them to what we see in our model plots (as seen above).   -->

<!-- As we've talked about already - as models get more complex, there is a very important distinction to make between plotting the data and plotting the model estimates.   -->
<!-- Whereas previously we could plot a simple regression `lm(y~x)` by plotting the _data_ and adding `geom_smooth(method=lm)`, now that we are working with multiple regression in order to visualise our results we should think of it as plotting the _model_ (and we can add the data if desired).   -->

<!-- ::: -->


::: {.callout-caution collapse="true"}
#### Optional: Interactions and Sum Contrasts  

```{r}
#| include: false
# DATA ----
set.seed(8653)
df <- expand_grid(
  grouping = c("1","2"),
  condition = c("A","B"),
  n = 1:30
) %>%
  mutate(
    lp = 2 + -2*(grouping=="2") + 1*(condition=="B") +
      -2*(grouping=="2" & condition=="B"),
    y = lp + rnorm(n())
  ) %>% mutate_if(is.character,factor)

# group means and marginals ----
# tab <- xtabs( ~ grouping + condition, data = df)
# N <- addmargins(tab)
# addmargins(xtabs(y ~ grouping + condition, data = df)) / N

model <- lm(y ~ condition * grouping, df)

# change contrasts (sum contrasts drops the first level, so we'll relevel it to drop group2 and condB)
df$grouping = fct_relevel(df$grouping, "2")
contrasts(df$grouping) <- "contr.sum"
df$condition = fct_relevel(df$condition, "B")
contrasts(df$condition) <- "contr.sum"

model1 <- lm(y ~ condition * grouping, df)




modplot <- as.data.frame(effects::effect("condition * grouping",model)) %>%
  ggplot(.,aes(x=condition, y=fit, ymin=lower,ymax=upper))+
  geom_pointrange(aes(col=grouping,shape=condition), size=1.5,alpha=.8) +
  geom_line(aes(col=grouping,group=grouping), lty="dashed",linewidth=1)+
  # labs(title="The model", subtitle="lm()") + 
  theme_classic()
```


When we have categorical predictors, our choice of contrasts coding changes the bits that we're getting our of our model.  

Suppose we have a 2x2 design (condition A and B, in groups 1 and 2):  

```{r}
#| label: fig-sumplot
#| fig-cap: "Categorical x Categorical Interaction plot"
#| echo: false
modplot
```

When we are using the default contrasts coding (treatment - [see 8B #contrasts](08b_catpred.html#contrasts){target="_blank"}) in R, then our coefficients for the individual predictors represent moving between the dots in @fig-sumplot.  

```
Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)            1.9098     0.1759  10.855  < 2e-16 ***
conditionB             1.1841     0.2488   4.759 5.65e-06 ***
grouping2             -1.6508     0.2488  -6.635 1.09e-09 ***
conditionB:grouping2  -2.1627     0.3519  -6.146 1.15e-08 ***
---
```

- The intercept is the red circle in @fig-sumplot.  
- The coefficient for condition is the difference between the red circle and the red triangle in @fig-sumplot.  
- The coefficient for grouping is the difference between the red circle and the blue circle in @fig-sumplot.  
- The interaction coefficient is the difference from the slope of the red line to the slope of the blue line.  


However, when we change to using sum contrasts, we're switching where zero is in our model. So if we change to sum contrasts (here we've changed __both__ predictors to using sum contrasts), then we end up estimating the effect of each predictor averaged across the other.  

```
Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)           1.13577    0.08796  12.912  < 2e-16 ***
conditionB            0.05141    0.08796   0.584     0.56    
grouping2            -1.36607    0.08796 -15.530  < 2e-16 ***
conditionB:grouping2 -0.54066    0.08796  -6.146 1.15e-08 ***
---
```

- The intercept is the grey X in @fig-smmplot2.  
- The coefficient for condition is the difference between the grey X and the grey triangle in @fig-sumplot2.  
- The coefficient for grouping is the difference between the grey X and the blue line in @fig-sumplot2.  
- The interaction coefficient is the difference from the slope of the grey line to slope of the blue line.  

```{r}
#| echo: false
#| label: fig-sumplot2
#| fig-cap: "Visualisation of sum-contrasts for categorical x categorical interaction plot"
modplot + 
  geom_segment(x = 1, xend = 2, 
               y = (coef(model1)[1]-coef(model1)[2]), 
               yend = sum(coef(model1)[c(1,2)]),
               lty = "dashed", linewidth = 1, alpha = .05, 
               col="black") +
  geom_segment(x = 1.5, xend = 1.5, 
               y = coef(model1)[1], yend = sum(coef(model1)[c(1,3)]),
               lty = "dotted", linewidth = 1, alpha = .05, 
               col="black") +
  geom_segment(x = 1.5, xend = 1.5, 
               y = coef(model1)[1], yend = coef(model1)[1]-coef(model1)[3],
               lty = "dotted", linewidth = 1, alpha = .05, 
               col="black") +
  geom_point(x=1,
             y=(coef(model1)[1]-coef(model1)[2]),
             size = 7, alpha = .05,
             aes(shape="A")) +
  geom_point(x=2,
             y=sum(coef(model1)[1:2]),
             size = 7, alpha = .05,
             aes(shape="B")) +
  geom_point(x=1.5,
             y=coef(model1)[1],
             size = 7, alpha = .2,
             shape=4) 

```


It can get quite confusing when we start switching up the contrasts, but it's all just because we're changing what "zero" means, and what "moving 1" means:  

::::panelset
:::panel
#### Treatment contrasts

```{r}
#| echo: false
dd = tibble(
  #lab=letters[1:4],
  clab=c(1,1,0,0),
  glab=c(1,0,1,0),
  lab=paste0("condition: ",clab,"\ngroup: ",glab),
  condition=c(2,2,1,1),
  ff=c(
    #c(1,0,0,0) %*% coef(model1),
    #c(1,1,0,0) %*% coef(model1),
    #c(1,0,1,0) %*% coef(model1),
    c(1,1,1,1) %*% coef(model1),
    #c(1,-1,0,0) %*% coef(model1),
    #c(1,0,-1,0) %*% coef(model1),
    c(1,1,-1,-1) %*% coef(model1),
    c(1,-1,1,-1) %*% coef(model1),
    c(1,-1,-1,1) %*% coef(model1)
  )
)
modplot+
  geom_label(inherit.aes=F,data=dd,aes(x=condition,
                                       y=ff,label=lab),alpha=.8)+
  labs(subtitle="y~condition*group with treatment contrasts")
```



:::
:::panel
#### Sum contrasts

```{r}
#| echo: false
dd = tibble(
  #lab=letters[1:9],
  clab=c(0,1,0,1,-1,0,1,-1,-1),
  glab=c(0,0,1,1,0,-1,-1,1,-1),
  lab=paste0("condition: ",clab,"\ngroup: ",glab),
  condition=c(1.5,2,1.5,2,1,1.5,2,1,1),
  ff=c(
    c(1,0,0,0) %*% coef(model1),
    c(1,1,0,0) %*% coef(model1),
    c(1,0,1,0) %*% coef(model1),
    c(1,1,1,1) %*% coef(model1),
    c(1,-1,0,0) %*% coef(model1),
    c(1,0,-1,0) %*% coef(model1),
    c(1,1,-1,-1) %*% coef(model1),
    c(1,-1,1,-1) %*% coef(model1),
    c(1,-1,-1,1) %*% coef(model1)
  )
)
modplot+
  geom_segment(x = 1, xend = 2, 
               y = (coef(model1)[1]-coef(model1)[2]), 
               yend = sum(coef(model1)[c(1,2)]),
               lty = "dashed", linewidth = 1, alpha = .05, 
               col="black") +
  geom_segment(x = 1.5, xend = 1.5, 
               y = coef(model1)[1], yend = sum(coef(model1)[c(1,3)]),
               lty = "dotted", linewidth = 1, alpha = .05, 
               col="black") +
  geom_segment(x = 1.5, xend = 1.5, 
               y = coef(model1)[1], yend = coef(model1)[1]-coef(model1)[3],
               lty = "dotted", linewidth = 1, alpha = .05, 
               col="black") +
  geom_point(x=1,
             y=(coef(model1)[1]-coef(model1)[2]),
             size = 7, alpha = .05,
             aes(shape="A")) +
  geom_point(x=2,
             y=sum(coef(model1)[1:2]),
             size = 7, alpha = .05,
             aes(shape="B")) +
  geom_point(x=1.5,
             y=coef(model1)[1],
             size = 7, alpha = .2,
             shape=4) + 
  geom_label(inherit.aes=F,data=dd,aes(x=condition,
                                       y=ff,label=lab),alpha=.8)+
  labs(subtitle="y~condition*group with sum contrasts")
```

:::
::::



:::


::: {.callout-caution collapse="true"}
#### Optional Extra: Getting non-linear  

You might be noticing that when we start talking about our regression surface "twisting" (e.g. in @fig-i.contcont), we're starting to see curves in our model.  

We can model _curves_ in a "linear" model!?  

An interaction does in a sense introduce non-linearity to our thinking, because there we no longer think of a linear effect of $x_1$ (it "depends on" $x_2$). This a little bit of a trick, because ultimately our model is still linear - we are estimating our outcome $y$ as the _linear combination_ of a set of predictors. In the model $y = b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 + b_3 \cdot x_1 \cdot x_2$, the adjustment that we make $b_3$ to each of the coefficients $b_1$ and $b_2$ is a constant.  

We can even exploit this to model more clearly "non-linear" associations, such as the age and height example below.  



```{r}
#| echo: false
#| label: fig-nonlin
#| fig-cap: "Two linear models, one with a quadratic term (right)"  
#| out-width: "100%"
set.seed(13)
df <- tibble(
  age = sample(0:16,100,replace=T),
  height = 70 + (7*age) - (17*scale(age)^2) + rnorm(100,0,10)
)
mod1 <- lm(height ~ age, data = df)
mod2 <- lm(height ~ age + I(age^2), data = df)
df <- df %>% mutate(f1 = fitted(mod1),f2=fitted(mod2))

ggplot(df,aes(x=age,y=height))+
  geom_point() + 
  geom_line(aes(y=f1),lwd=1, col="blue")  +
  labs(title="lm(height ~ age)") + 
  
  ggplot(df,aes(x=age,y=height))+
  geom_point() + 
  geom_line(aes(y=f2),lwd=1,col="blue") +
  labs(title="lm(height ~ age + I(age^2))") &     
  theme_bw(base_size = 12)
```

We will cover this a lot more in the multivariate stats course, so don't worry too much about it right now.  
However, it's useful as a means of seeing how we can extend linear models to fit these sort of relationships. In the model on the right of @fig-nonlin, the model returns three coefficients:
```{r}
#| echo: false
.pp(summary(mod2),l=list(10:13))
```

It is estimating a persons' height as:

- an intercept of `r coef(mod2)[1] %>% round(2)` (the estimated height of a newborn baby of age 0)
- plus `r coef(mod2)[2] %>% round(2)` cm for every year of age they have
- plus `r coef(mod2)[3] %>% round(2)` cm for every year of age they have, squared.  

So for a 4 year old, the estimated height is: 

$$
height = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[2] %>% round(2)` \times 4) + (`r coef(mod2)[3] %>% round(2)` \times 4^2) = `r round(coef(mod2),1) %*% c(1,4,16)`
$$
and for a 10 year old, it is:  

$$
height = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[2] %>% round(2)` \times 10) + (`r coef(mod2)[3] %>% round(2)` \times 10^2) = `r round(coef(mod2),1) %*% c(1,10,100)`
$$
We can see how the quadratic "$\text{age}^2$ term has a larger (negative) effect as age increases (for the 4 year old it makes the estimated height $`r coef(mod2)[3] %>% round(2)` \times 4^2 = `r round(coef(mod2)[3]*4^2,1)`$ lower, and for the 10 year old it makes the estimated height $`r coef(mod2)[3] %>% round(2)` \times 10^2 = `r round(coef(mod2)[3]*10^2,1)`$ lower). This captures the plateauing of heights as children get older.  

:::

<!-- # Optional: further exploration -->

<!-- ## Simple Slopes -->

<!-- we've kind of already done it visually.   -->
<!-- plot the slope of y~x1 for certain values of x2.   -->

<!-- we've done this visually, but it would be nice to get out a "is y~x1 significant when x2 = ?"   -->




<!-- ## Johnson Neyman -->



