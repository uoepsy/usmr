---
title: "9A: Interactions"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(xaringanExtra)
xaringanExtra::use_panelset()
set.seed(993)
contcont <- tibble(
  x1 = rnorm(60),
  x2 = rnorm(60,2,1),
  y = 1*x1 + 2.2*x2 + 1*x1*x2 + rnorm(60),
  partner_time = round(7+(scale(x1)[,1]*3),2),
  relationship_qual = round((scale(x2)[,1]*10) + 60),
  wellbeing = 20 + (scale(y)[,1]*5)
) %>% mutate(x1=partner_time,x2=relationship_qual,y=wellbeing)

partnertime <- contcont %>%
  select(partner_time,relationship_qual,wellbeing)
concont <- contcont %>%
  select(x1,x2,y)


contcat <- tibble(
  x1 = rnorm(60),
  x2 = sample(0:1,60,T),
  y = -.8*x1 - 3*x2 - .6*x1*x2 + rnorm(60),
  apoe4 = x2,
  aqpi = round((scale(x1)[,1]*100) + 180),
  mmse = round((scale(y)[,1]*4) + 22),
) %>% mutate(x1=aqpi, x2=apoe4, y=mmse)

airpol <- contcat %>%
  select(aqpi, apoe4, mmse) %>% mutate(apoe4=ifelse(apoe4==0,"neg","pos"))
contcat <- contcat %>%
  select(x1,x2,y)
set.seed(913)
catcat <- tibble(
  x1 = sample(0:1,60, T),
  x2 = sample(0:1,60, T),
  y = round(8 + 2*x1 - .5*x2 + 4*x1*x2 + rnorm(60)),
  anonymity = ifelse(x1==1,"anonymous","identifiable"),
  asgroup = ifelse(x2==1,"group","alone"),
  candybars = y
)
candy <- catcat %>% select(anonymity, asgroup, candybars)
# write_csv(partnertime, "../../data/usmr_partnertime.csv")
# write_csv(candy, "../../data/usmr_candy.csv")
# write_csv(airpol, "../../data/usmr_airpol.csv")
```


# Holding Constant...

When we learned about multiple linear regression last week, we talked about the idea of the coefficients as "holding constant" the other predictors in the model.   

Consider a model with the following structure:    

$$
y = b_0 + b_1(x_1) + b_2(x_2) + \epsilon
$$

When we _fit_ this model to our data, what we get out is some _estimated_ values for the coefficients $b_0$, $b_1$, and $b_2$:  

```{r}
mydata <- read_csv("https://uoepsy.github.io/data/usmr_mlr.csv")
mymodel <- lm(y ~ x1 + x2, data = mydata)
```
```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept) -2.39138    3.67735  -0.650  0.51867   
x1           0.17570    0.06435   2.730  0.00888 **
x2          -0.64756    0.19959  -3.244  0.00217 **
```

The coefficient we get out for $x_1$ tells us that the model estimates that $y$ will increase by 0.17 for every increase of 1 unit in $x_1$, _provided that we hold $x_2$ constant._  

What this means is that whatever value $x_2$ is, provided it stays the same, a one unit increase in $x_1$ is associated with a $b_1$ change in $y$.  

As it happens, we can use `plot_model` to show us the how $y$ changes with $x_1$ _for some specific values of $x_2$_.  
Below, the three lines show the association between $y$ and $x_1$ when:  
  
- $x_2 = 3$ (red line)
- $x_2 = 12$ (blue line)
- $x_2 = 20$ (green line)

As we can see, the slope of $y$ on $x_1$ is the same.  

```{r}
library(sjPlot)
# plot the effect of x1, at 3 specific values of x2
plot_model(mymodel, type = "eff", terms=c("x1", "x2 [3, 12, 20]"))
```


:::frame
__Example__

We're interested in estimating the association between BMI and cognition, after controlling for differences due to age.  

We want to use this model:  
$$
\text{score on test of cognition} = b_0 + b_1(\text{age}) + b_2(\text{BMI}) + \epsilon
$$
which we can fit using `lm()`:  
```{r}
#| eval: false
lm(score ~ 1 + age + BMI, data = ourdata)
```
and we might get some coefficients (estimates for the $b_?$ values) such as those below:  
_(I've just made up some nice round numbers to make it easier to think about)_  

```
Coefficients:
            Estimate    ...   ...
(Intercept)  160.000    ...   ...
age         -1.50000    ...   ...
BMI         -2.50000    ...   ...
```

The coefficient of interest, the one for BMI, is telling us that "a 1 unit increase in BMI is associated with a -2.5 decrease in Scores on the cognitive test, _holding age constant_".  

Consider 3 people: 

```{r}
#| label: fig-pplconstant
#| fig-cap: "Three theoretical people"  
#| echo: false
knitr::include_graphics("images/ints/constant.png")
```


The coefficient for BMI represents the difference in cognitive scores we would expect between Person A and Person B.  

Think about why this is.  
For some person $i$, their model predicted score is:  
$$
\hat{score_i} = b_0 + b_1(age_i) + b_2(BMI_i)
$$
Which from our model estimates is:  
$$
\hat{score_i} = 160 - 1.5(age_i) - 2.5(BMI_i)
$$

- Person A's score = $160 - (1.5*50) - (2.5*22) = 30$
- Person B's score = $160 - (1.5*50) - (2.5*23) = 27.5$
- Person C's score = $160 - (1.5*60) - (2.5*23) = 12.5$

The difference in model estimated Score between Person A and Person B is the coefficient of BMI, because those two people _only differ_ on BMI. Person A and Person C _also differ_ on age. This is how the coefficient of BMI is interpreted as "holding age constant" - it is a comparison between two hypothetical people who differ on BMI but are identical with respect to the other predictors in the model.  
:::

<div class="divider div-transparent div-dot"></div>

# It Depends...

There are lots of practical cases where we might think that the relationship between two variables _depends on_ the value of a third. Put another way, we might think that the effect of one predictor *depends* on another.   

Below are some examples of this idea, where the explanatory variables (the predictors) are of different types (e.g. continuous, categorical, etc):  

:::panelset

:::panel
#### Example 1

> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  

Variables:  

- __Outcome:__ Wellbeing (questionnaire score ranging from 0 to 32)
- __Continuous Predictor:__ Time spent with partner (hours per week)  
- __Continuous Predictor:__ Relationship Quality (rating from 0 to 100)  


:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  

Variables:  

- __Outcome:__ Cognitive Functioning measured via the MMSE. Possible scores range from 0 to 30. Lower scores indicate poorer cognitive functioning
- __Continuous Predictor:__ Air-Quality Pollution Index (AQI). Ranges from 0 to 500. The higher the AQI value, the greater the level of air pollution.
- __Categorical Predictor:__ APOE-4 Genotype status: Negative vs Positive  


:::
:::panel
#### Example 3

> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.  


Variables:  

- __Outcome__: Number of candy bars taken while trick or treating
- __Categorical Predictor:__  Whether or not the childs' identity is hidden by their costume (anonymous vs identifiable)
- __Categorical Predictor:__ Whether the child is trick or treating as a group or alone (alone vs group)  


:::

:::

---

In each of the above examples, we can no longer think about "the relationship between [outcome] and [predictor]" without discussing the level of the other predictor.  


:::: {.columns}
::: {.column width="55%"}
Using Example 1 above, let's return to thinking about this in terms of two different observations (i.e. two different people in @fig-pplint).  

How do we think each person's wellbeing will change if they spend an extra hour with their partner?  

Person A (who has a really great relationship), will probably increase in wellbeing if they were to spend more timewith their partner.  

Will it be the same for Person B? Person B doesn't have a great relationship. If they spend an extra hour with their partner, we probably wouldn't expect their wellbeing to increase _to the same extent as Person A_. It might even go down for Person B! 

:::
::: {.column width="45%"}
```{r}
#| label: fig-pplint
#| fig-cap: "Two theoretical people"  
#| echo: false
#| out-height: "400px"
knitr::include_graphics("images/ints/ints.png")
```

:::
::::


To capture this, what we need is an extra bit of information to tell us "how _much_ does the association between 'time spent with partner' and wellbeing change as relationship quality changes?" And this is something we can include in our model, and get an estimate for!    

<div class="divider div-transparent div-dot"></div>

# Interactions!

We can model the idea of "the association between $x_1$ and $y$ changes _depending on_ the level of $x_2$" by including a product (multiplication) term between the two predictors.   
Such a model would take the form:

$$
y = b_0 + b_1(x_1) + b_2(x_2) + b_3(x_1 \cdot x_2) + \epsilon
$$

What this is doing is saying that our outcome $y$ is predicted by: 

- some amount of $x_1$
- some amount of $x_2$
- and a little addition to each of those amounts depending on the value of of the other variable.   

To provide a visual intuition and build on how we have been thinking of multiple regression upto this point, when we have two predictors that interact, our regression surface is no longer flat, but _twists_. This is because the slope along values of $x_1$ _changes_ as we move up $x_2$:   

:::panelset

:::panel
#### Example 1

> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.^[This example comes from our lovely tutor Ian!]  

```{r}
#| echo: false
#| label: fig-i.contcont
#| fig-cap: "Interaction between two continuous predictors, viewed from two angles"

fit<-lm(y~x1*x2, data=contcont)
steps=50
x1 <- with(contcont, seq(min(x1),max(x1),length=steps))
x2 <- with(contcont, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2)
y <- matrix(predict(fit, newdat), steps, steps)

par(mfrow=c(1,2))
p <- persp(x1,x2,y, theta = 25,phi=20, col = NA,
           xlab="Time spent\nwith partner",ylab="Relationship Quality",zlab="Wellbeing")
obs <- with(contcont, trans3d(x1,x2, y, p))
pred <- with(contcont, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

p <- persp(x1,x2,y, theta = 60,phi=20, col = NA,
           xlab="Time spent\nwith partner",ylab="Relationship Quality",zlab="Wellbeing")
obs <- with(contcont, trans3d(x1,x2, y, p))
pred <- with(contcont, trans3d(x1, x2, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)
par(mfrow=c(1,1))

```

At high values of relationship quality, the amount wellbeing increases with time spent is greater than it is at low values of relationship quality.  
And we can phrase this the other way around: at high amounts of time spent with partner, relationship quality has a bigger effect on wellbeing than it does for low amounts of time spent. 

In the model with this interaction:  
$$
\text{wellbeing} = b_0 + b_1(\text{time}) + b_2(\text{quality}) + b_3(\text{time} \times \text{quality})
$$
The interaction coefficient $b_3$ is the adjustment we make to the slope of wellbeing with "time spent with partner", as we move 1 up in "relationship quality".^[And vice versa! It is also the adjustment we make to the slope of wellbeing with "relationship quality", as we move 1 up in "time spent with partner"]  

:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.^[and this example comes from Otto!]  

```{r}
#| echo: false
#| label: fig-i.contcat
#| fig-cap: "Interaction between a continuous and a binary categorical predictors"
plot(contcat$y~contcat$x1,col=ifelse(contcat$x2,"red","black"), xlab="air quality pollution index (AQPI)",ylab="Cognition (MMSE)")
abline(lm(y~x1,contcat[contcat$x2==0,]))
abline(lm(y~x1,contcat[contcat$x2==1,]),col="red")
legend(300, 29, legend=c("APOE4 positive", "APOE4 negative"),
       col=c("red", "black"), lty=1)
```

This kind of interaction (where one predictor is continuous and the other is categorical), is sometimes the easiest to think about.  

We can see in @fig-i.contcat that cognition decreases as air pollution increases, __but__ this is different depending on genetic status. In the APOE4-positive group, the association is steeper than in the APOE4-negative group. The interaction is evident in that the two lines are non-parallel.   

In the model:  
$$
\text{Cognition} = b_0 + b_1(\text{air pollution}) + b_2(\text{APOE4+}) + b_3(\text{air pollution} \times \text{APOE4+})
$$

The interaction term $b_3$ is the estimated adjustment made to the slope of cogition across air pollution to move from one group to the other. e.g. the slope for the APOE4-positive group is equal to the slope of the APOE4-negative group _plus the interaction term_.

:::
:::panel
#### Example 3

> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.^[This example was suggested by Tia]  

```{r}
#| echo: false
#| label: fig-i.catcat
#| fig-cap: "Interaction between two categorical predictors"
ggplot(candy, aes(x=anonymity, y=candybars, col=asgroup)) + 
  stat_summary(geom="pointrange", size=1) +
  stat_summary(geom="line",lwd=1,lty="dotted",aes(group=asgroup))
```

For interactions between two categorical variables, we tend to plot the mean of the outcome variable for each combination of levels. We can see the interaction in @fig-i.catcat in the fact that the two dotted lines we have added to join the group-means are not parallel. 

Children who are anonymous tend to take more candybars than those who are identifiable, but this difference is much greater when children are trick of treating in a group than when they are doing so alone!  

In the model
$$
\text{candy bars} = b_0 + b_1(\text{anonymity}) + b_2(\text{group}) + b_3(\text{anonymity} \times \text{group})
$$

The interaction term $b_3$ is going to be the estimated adjustment to the difference between alone vs group for anonymous vs identifiable children. Put another way, it is how the difference between red and blue dots changes between the left and right hand side of @fig-i.catcat.  

:::

:::

<div class="divider div-transparent div-dot"></div>

# Fitting interactions in R

In R, the interaction term gets denoted by a colon `:` between the two predictor variables. We can fit an interaction term in a regression model quite easily in R:  

```{r}
#| eval: false
# we don't *need* to put the 1, it's up to you :)
lm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)
```
This maps to the model equation we saw earlier: 
$$
y = b_0 + b_1(x_1) + b_2(x_2) + b_3(x_1 \cdot x_2) + \epsilon
$$

:::rtip
__Shortcut__  

To express `x1 + x2 + x1:x2`, we can also use just `x1*x2`.  

These two models are equivalent: 

```{r}
#| eval: false
lm(y ~ 1 + x1 + x2 + x1:x2, data = dataset)
lm(y ~ 1 + x1*x2, data = dataset)
```

:::

:::imp

If we fit the interaction `x1:x2`, we almost _always_ want to also fit the separate effects `x1` and `x2`.  

_"Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of $x_1$ depends on the level of $x_2$ but that there is no effect of $x_1$."_ [Ramsey & Schafer](https://www.cengage.uk/c/the-statistical-sleuth-3e-ramsey-schafer/9781133490678/){target="_blank"}

:::

<div class="divider div-transparent div-dot"></div>

# Interpretation

When we include an interaction term in our model, we are saying that two effects on the outcome are _dependent upon_ one another. This means that with an interaction in the model `lm(y ~ x1 + x2 + x1:x2)` we can no longer talk about the "effect of $x_1$ on $y$_'holding $x_2$ constant'_". Instead we have to talk about _conditional effects_ - e.g. the effect of $x_1$ _at a **specific** value_ of $x_2$.    

:::sticky

The individual coefficients for each predictor that is involved in an interaction are estimated _when the other predictor in the interaction is zero._  

:::

 

For example:  
```{r}
#| echo: false
set.seed(933)
tibble(
  c1 = rnorm(100),
  x1 = rnorm(100),
  x2 = rnorm(100),
  y = 70 + 3*c1 + 3*x1 + 2*x2 + (-1.7*x1*x2) + rnorm(100,0,10),
  
) -> df
m <- lm(y~x1+x2+x1:x2,df)
.pp(summary(m), l=list(3,9:14))
```

These coefficients are interpreted, in turn as:  

| Coefficient      | Interpretation |
| ----------- | ----------- |
| `(Intercept)` | the estimated $y$ when all predictors ($x_1$ and $x_2$) are zero is `r round(coef(m)[1],2)` |
| `x1`  | **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a `r round(coef(m)[2],2)` change in $y$ |
| `x2`  | **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a `r round(coef(m)[3],2)` change in $y$. |
| `x1:x2`  | as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by `r round(coef(m)[4],2)`<br>_**or**_<br>as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by `r round(coef(m)[4],2)` |


`r optbegin("What if there are other things in the model too?", olabel=FALSE)`
Note that the interaction `x1:x2` changes how we interpret the individual coefficients for `x1` and `x2`.  

It does __*not*__ change how we interpret coefficients for other predictors that might be in our model.  

For example, suppose we _also_ had another predictor $c_1$ in our model: 
```{r}
#| echo: false
m <- lm(y~c1+x1+x2+x1:x2,df)
.pp(summary(m), l=list(3,9:15))
```

| Coefficient      | Interpretation |
| ----------- | ----------- |
| `(Intercept)` | the estimated $y$ when all predictors ($c_1$, $x_1$ and $x_2$) are zero is `r round(coef(m)[1],2)` |
| `c1` | a 1 unit increase in $c_1$ is associated with a `r round(coef(m)[2],2)` increase in $y$, holding constant all other variables in the model ($x_1$ and $x_2$) |
| `x1`  | holding $c_1$ constant, **when $x_2$ is zero,** a 1 unit increase in $x_1$ is associated with a `r round(coef(m)[3],2)` change in $y$ |
| `x2`  | holding $c_1$ constant, **when $x_1$ is zero,** a 1 unit increase in $x_2$ is associated with a `r round(coef(m)[4],2)` change in $y$. |
| `x1:x2`  | holding $c_1$ constant, as $x_2$ increases by 1, the association between $x_1$ and $y$ changes by `r round(coef(m)[5],2)`<br>_**or**_<br>holding $c_1$ constant, as $x_1$ increases by 1, the association between $x_2$ and $y$ changes by `r round(coef(m)[5],2)` |

`r optend()`

<br><br>

:::panelset

:::panel
#### Example 1

> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  

```{r}
partnertime <- read_csv("https://uoepsy.github.io/data/usmr_partnertime.csv")
eg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = partnertime)
summary(eg1mod)
```

- For someone who spends 0 hours with their partner, and who has a relationship quality score of 0, their estimated wellbeing is `r round(coef(eg1mod)[1],2)`.  
- For someone who has a relationship quality score of 0, every hour spent with their partner is associated with a `r round(coef(eg1mod)[2],2)` change in wellbeing.  
- For someone who spends 0 hours with their partner, an increase of 1 in the relationship quality is associated with a `r round(coef(eg1mod)[3],2)` change in wellbeing.  
- For every 1 increase in relationship quality, an extra hour spent with their partner is associated with an _additional_ `r round(coef(eg1mod)[4],2)` change in wellbeing. 


:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  

```{r}
airpol <- read_csv("https://uoepsy.github.io/data/usmr_airpol.csv")
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)
summary(eg2mod)
```

- For people who are APOE4 negative (the reference level, see [8A#multiple-categories-multiple-regression](08a_mlr.html#multiple-categories-multiple-regression){target="_blank"}), living where the Air Quality Pollution Index is 0 (no pollution), the estimated score on the MMSE (mini mental state exam) is `r round(coef(eg2mod)[1],2)`.  
- For people who are APOE4 negative, a 1 unit increase in air pollution is associated with a `r round(coef(eg2mod)[2],2)` change in MMSE scores.  
- For people living with 0 air pollution, being APOE4 positive is associated with `r round(coef(eg2mod)[3],2)` lower MMSE scores compared to APOE4 negative
- Compared to APOE4 negative, being APOE4 positive is associated with `r round(coef(eg2mod)[4],2)` greater decrease in MMSE scores for every 1 increase in air pollution.


:::
:::panel
#### Example 3

> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.

```{r}
candy <- read_csv("https://uoepsy.github.io/data/usmr_candy.csv")
eg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)
summary(eg3mod)
```

- An anonymous child trick or treating alone is estimated to take `r round(coef(eg3mod)[1])` candybars.
- When trick or treating alone, compared to an anonymous child, an identifiable child is estimated to take `r round(coef(eg3mod)[2])` fewer candybars.  
- When anonymous, compared to an trick or treating alone, children who are part of a group are estimated to take `r round(coef(eg3mod)[3])` more candybars.  
- When children trick or treat as part of a group, the difference between identifiable and anonymous children's candybar taking is `r round(coef(eg3mod)[4])` fewer than the difference between identifiable and anonymous children who trick or treat alone.  


```{r}
#| eval: false
fit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))
sjPlot::plot_model(fit, type="int",show.data=T)+geom_line()

# it's just viewing that cube from one side 
library(scatterplot3d)
plt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,
                                 x.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 y.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 main="y~x1*x2\n(x1 and x2 are categorical)"))
fit = lm(y~x1*x2,catcat)
pp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))
pp$y <- predict(fit, pp)

pp1 <- pp[pp$x2==0,]
pp2 <- pp[pp$x2==1,]
pp3 <- pp[pp$x1==0,]
pp4 <- pp[pp$x1==1,]
plt$points(pp1$x1,pp1$x2,pp1$y,type="l")
plt$points(pp2$x1,pp2$x2,pp2$y,type="l")
plt$points(pp3$x1,pp3$x2,pp3$y,type="l")
plt$points(pp4$x1,pp4$x2,pp4$y,type="l")
```

:::

:::

<div class="divider div-transparent div-dot"></div>

# Visualisation

We have seen lots of 3-dimensional plots above to try and help in building our intuition about how we model an interaction. However, we typically don't really want to visualise models like this (in part because our models often have more than 2 predictors, and so have more than 3 dimensions).  

What we therefore need to do is find ways to represent these relationships in 2-dimensional plots. This is slightly different depending on the type of variables we are dealing with. 

- For a continuous $\times$ categorical interaction, we can plot the association of the continuous predictor with the outcome for each level of the categorical variable. 
- For a continuous $\times$ continuous interaction, we can plot the association of one predictor at some judiciously chosen values of the other (e.g. at the min, mean and max, or at -1 SD, mean, and +1 SD).  
- For a categorical $\times$ categorical interaction, we can plot the various group means, with optional dotted^[dotted is a good way to indicate that there is no data across that line - it is linking two categories] lines to illustrate the non-parallelism   

Fortunately, we can do a lot of this with a variety of useful packages, such as:  

- From the **sjPlot** package: `plot_model(model, type = "int")` or `plot_model(model, type = "eff", terms = c("x1","x2"))  
- From the **interactions** package: `interact_plot(model, pred = "x1", modx = "x2")`, and `cat_plot()`    


:::panelset

:::panel
#### Example 1

> The amount to which spending time with my partner influences my wellbeing _depends on_ the quality of our relationship.  

Plotting continuous by continuous interactions requires choosing a set of values for one of our predictors, at which we plot the slope of the other.  
By default, the `plot_model` function chooses to show `y~x1` when x2 is at its min and max.  
The interactions package defaults to showing the `y~x1` relationship when x2 is at -1 SD below the mean, at the mean, and +1 SD above the mean.  

```{r}
library(sjPlot)
library(interactions)
library(patchwork)
partnertime <- read_csv("https://uoepsy.github.io/data/usmr_partnertime.csv")
eg1mod <- lm(wellbeing ~ partner_time * relationship_qual, data = partnertime)

plot_model(eg1mod, type = "int") / 
interact_plot(eg1mod, pred = "partner_time", modx = "relationship_qual", interval = TRUE)
```

We can change these manually, for instance. For instance, to make `plot_model` show the association when relationship quality is zero, when it is 50, and when it is 100, we could use: 
```{r}
#| eval: false
plot_model(eg1mod, type = "eff", terms=c("partner_time","relationship_qual [0, 50, 100]"))
```

:::
:::panel
#### Example 2

> The influence of air-pollution on cognitive functioning _depends on_ your genetic status.  

This is the most straightforward type of interaction to visualise, as there are only a set number of values that one of the variables can take, meaning only a finite number of lines we need to draw:  

```{r}
airpol <- read_csv("https://uoepsy.github.io/data/usmr_airpol.csv")
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)

plot_model(eg2mod, type = "int") / 
interact_plot(eg2mod, pred = "aqpi", modx = "apoe4", interval = TRUE)
```


:::
:::panel
#### Example 3

> The influence of being anonymous on childrens' greedy behaviours _depends on_ if they are alone or part of a group.

For these types of interactions (between categorical variables), plotting the estimates from our model is best done as our set of group means. This what functions like `plot_model()` and `cat_plot()` will do!  

```{r}
candy <- read_csv("https://uoepsy.github.io/data/usmr_candy.csv")
eg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)

plot_model(eg3mod, type = "int") / 
cat_plot(eg3mod, pred = "anonymity", modx = "asgroup", interval = TRUE)
```


```{r}
#| eval: false
#| echo: false
fit = lm(y~x1*x2,catcat %>% mutate(across(x1:x2,factor)))
sjPlot::plot_model(fit, type="int",show.data=T)+geom_line()

# it's just viewing that cube from one side 
library(scatterplot3d)
plt <- with(catcat,scatterplot3d(x1,x2,y, scale.y=.8,angle=-30,
                                 x.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 y.ticklabs = c(0,NA,NA,NA,NA,1), 
                                 main="y~x1*x2\n(x1 and x2 are categorical)"))
fit = lm(y~x1*x2,catcat)
pp <- expand_grid(x1=seq(0,1,.1), x2=seq(0,1,.1))
pp$y <- predict(fit, pp)

pp1 <- pp[pp$x2==0,]
pp2 <- pp[pp$x2==1,]
pp3 <- pp[pp$x1==0,]
pp4 <- pp[pp$x1==1,]
plt$points(pp1$x1,pp1$x2,pp1$y,type="l")
plt$points(pp2$x1,pp2$x2,pp2$y,type="l")
plt$points(pp3$x1,pp3$x2,pp3$y,type="l")
plt$points(pp4$x1,pp4$x2,pp4$y,type="l")
```

:::

:::

<div class="divider div-transparent div-dot"></div>

# Getting more from your model

Reporting on interactions is a bit like telling a story. Listing the interpretation of the coefficients is fine, but often reads a bit awkwardly. A good presentation of the results would provide the reader with the overall pattern of results, pulling out the key parts that are of interest. There are various things we can do to get out coefficients that might be of more use to us in telling our story.  

## Mean Centering

By mean centering a continuous predictor, we change what "0" means. Normally, when we don't have an interaction, this simply changes the intercept value. If we have the interaction `y ~ x1 + x2 + x1:x2`, then mean centering `x1` will make the coefficient for `x2` now represent "the association between $x_2$ and $y$ for someone at the _average_ of $x_1$". 

Using one of our examples from throughout this reading, we might mean-center the air-pollution so that we can consider the difference in MMSE scores between APOE4 positive and negative people _at the average air-pollution level_

```{r}
airpol <- read_csv("https://uoepsy.github.io/data/usmr_airpol.csv")
# make a new variable that is mean centered air pollution
airpol <- airpol %>% 
  mutate(
    aqpiC = aqpi - mean(aqpi)
  )
# model with original variable
eg2mod <- lm(mmse ~ aqpi * apoe4, data = airpol)
# model with mean centered predictor
eg2mod_cent <- lm(mmse ~ aqpiC * apoe4, data = airpol)
```

```{r}
# coefficients:
coef(eg2mod)
coef(eg2mod_cent)
```

This is because the coefficient for APOE4 compares the heights of the two lines when the other predictor is zero. So if we change what "zero" represents, we can change what that estimates. In the model plots below, we can see that _the model doesn't change_, it is just extracting different information (it is the distance to move from the blue dot to the red dot):  

```{r}
#| out-width: "100%"
#| echo: false
plot_model(eg2mod, type="int") +
  geom_point(x=0,y=coef(eg2mod)[1], size=4, col="blue")+
  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, col="red")+
  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty="dotted", lwd=1,col="black") + labs(title="Raw AQPI") +
  
plot_model(eg2mod_cent, type="int") +
  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, col="blue")+
  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, col="red")+
  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty="dotted", lwd=1,col="black") + labs(title="Mean Centered AQPI") +
  plot_layout(guides="collect")
```

## Relevelling Factors

Another thing that can be useful (especially when working with categorical variables with lots of levels) is to make sure your variables are `factors` in R, and to set a useful reference level. Typically, the reference level is what we think of as "normal", e.g. if we have 3 groups: Placebo, Drug A, and Drug B, then we might compare each drug to the placebo condition, because that's comparable to most people (i.e. who aren't taking the drug).

For example, when we have two categorical variables:  

- anonymity = "anonymous" vs "identifiable"
- asgroup = "alone" vs "group"

Then the default is to take the alphabetical ordering. We can change the ordering using functions that "relevel" a factor. Note, this only works if the variable is _already_ a `factor` in R (see [2A#categorical](02a_measurement.html#categorical){target="_blank"} for a reminder of 'factors').   

```{r}
candy <- read_csv("https://uoepsy.github.io/data/usmr_candy.csv")

# make both predictors 'factors'
candy <- 
  candy %>% 
  mutate(
    anonymity = factor(anonymity),
    asgroup = factor(asgroup)
  )
```

Once they are factors, we can see the default levels:  
```{r}
levels(candy$anonymity)
levels(candy$asgroup)
```

This is the original model, with the default levels:  
```{r}
eg3mod <- lm(candybars ~ anonymity * asgroup, data = candy)
```

Let's relevel anonymity to have "identifiable" as the first level.  
```{r}
candy <- 
  candy %>% 
  mutate(
    anonymity = fct_relevel(anonymity, "identifiable")
  )
levels(candy$anonymity)
```

And refit the model: 
```{r}
eg3mod_rel <- lm(candybars ~ anonymity * asgroup, data = candy)
```

Here are the coefficients:
```{r}
coef(eg3mod)
coef(eg3mod_rel)
```

Again, the model doesn't change, we are simply extracting different bits from it:  
```{r}
#| echo: false
#| out-width: "100%"
p1 <- plot_model(eg3mod, type = "int") +
  labs(title="with 'anonymous' and 'alone' as reference levels") + 
  
  
  geom_segment(x = 1, xend = 1, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,3)]), 
                col="black", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 1, xend = 1.95, y = coef(eg3mod)[1], yend=sum(coef(eg3mod)[c(1,2)]), 
                col="black", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 1.05, xend = 2.05, y = sum(coef(eg3mod)[c(1,3)]), yend=sum(coef(eg3mod)[c(1,2,3)]), 
                col="black", lty="dotted", lwd=1) +
  
  geom_segment(x = 2.05, xend = 2.05, y = sum(coef(eg3mod)[c(1:3)]), yend=sum(coef(eg3mod)[c(1:4)]), 
                col="black", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  
  annotate(geom="label", x=1.1,y=coef(eg3mod)[1],label="Intercept")+
  annotate(geom="label", x=1.01, hjust=0,y=coef(eg3mod)[1]+2,label="'asgroup' coefficient") +
  annotate(geom="label", x=1.5,y=coef(eg3mod)[1]-1,label="'anonymity' coefficient") +
  annotate(geom="label", x=2.04,hjust=1,y=coef(eg3mod)[1],label="interaction coefficient") 



p2 <- plot_model(eg3mod, type = "int") +
  labs(title="with 'identifiable' and 'alone' as reference levels") + 
  
  geom_segment(x = 1.95, xend = 1.95, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,3)]), 
                col="black", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 1.95, xend = 1, y = coef(eg3mod_rel)[1], yend=sum(coef(eg3mod_rel)[c(1,2)]), 
                col="black", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 2, xend = 1, y = sum(coef(eg3mod_rel)[c(1,3)]), yend=sum(coef(eg3mod_rel)[c(1,2,3)]), 
                col="black", lty="dotted", lwd=1) +
  
  geom_segment(x = 1, xend = 1, y = sum(coef(eg3mod_rel)[c(1:3)]), yend=sum(coef(eg3mod_rel)[c(1:4)]), 
                col="black", lwd=1,
                arrow = arrow(length = unit(0.5, "cm"))) +
  
  annotate(geom="label", x=2,hjust=0,y=coef(eg3mod_rel)[1],label="Intercept")+
  annotate(geom="label", x=1.93, hjust=1,y=coef(eg3mod_rel)[1]-.25,label="'asgroup' coefficient") +
  annotate(geom="label", x=1.5,y=coef(eg3mod_rel)[1]+1,label="'anonymity' coefficient") +
  annotate(geom="label", x=1.01,hjust=0,y=coef(eg3mod_rel)[1]+4,label="interaction coefficient")

p1 + theme_bw(base_size = 12)
p2 + theme_bw(base_size = 12)
```

:::sticky
__Mapping model estimates to model plots__  

To help with interpretation of the estimated coefficients from a model, a very useful exercise is to try and map them to what we see in our model plots (as seen above).  

As models get more complex, there is a very important distinction to make between plotting the data, and plotting the model estimates. Whereas previously we could plot a simple regression `lm(y~x)` by plotting the _data_ and adding `geom_smooth(method=lm)`, now that we are working with multiple regression in order to visualise our results we should think of it as plotting the _model_ (and we can add the data if desired).  

:::


<div class="divider div-transparent div-dot"></div>

# Optional Extra: Getting non-linear  

You might be noticing that when we start talking about our regression surface "twisting" (e.g. in @fig-i.contcont), we're starting to see curves in our model.  

We can model _curves_ in a "linear" model!?  

An interaction does in a sense introduce non-linearity to our thinking, because there we no longer think of a linear effect of $x_1$ (it "depends on" $x_2$). This a little bit of a trick, because ultimately our model is still linear - we are estimating our outcome $y$ as the _linear combination_ of a set of predictors. In the model $y = b_0 + b_1(x_1) + b_2(x_2) + b_3(x_1 \cdot x_2)$, the adjustment that we make $b_3$ to each of the coefficients $b_1$ and $b_2$ is a constant.  

We can even exploit this to model more clearly "non-linear" associations, such as the age and height example below.  



```{r}
#| echo: false
#| label: fig-nonlin
#| fig-cap: "Two linear models, one with a quadratic term (right)"  
#| out-width: "100%"
set.seed(13)
df <- tibble(
  age = sample(0:16,100,replace=T),
  height = 70 + (7*age) - (17*scale(age)^2) + rnorm(100,0,10)
)
mod1 <- lm(height ~ age, data = df)
mod2 <- lm(height ~ age + I(age^2), data = df)
df <- df %>% mutate(f1 = fitted(mod1),f2=fitted(mod2))

ggplot(df,aes(x=age,y=height))+
  geom_point() + 
  geom_line(aes(y=f1),lwd=1, col="blue")  +
  labs(title="lm(height ~ age)") + 
  
  ggplot(df,aes(x=age,y=height))+
  geom_point() + 
  geom_line(aes(y=f2),lwd=1,col="blue") +
  labs(title="lm(height ~ age + I(age^2))") &     
  theme_bw(base_size = 12)
```

We will cover this a lot more in the multivariate stats course, so don't worry too much about it right now.  
However, it's useful as a means of seeing how we can extend linear models to fit these sort of relationships. In the model on the right of @fig-nonlin, the model returns three coefficients:
```{r}
#| echo: false
.pp(summary(mod2),l=list(10:13))
```

It is estimating a persons' height as:

- an intercept of `r coef(mod2)[1] %>% round(2)` (the estimated height of a newborn baby of age 0)
- plus `r coef(mod2)[2] %>% round(2)` cm for every year of age they have
- plus `r coef(mod2)[3] %>% round(2)` cm for every year of age they have, squared.  

So for a 4 year old, the estimated height is: 

$$
height = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[1] %>% round(2)` \times 4) + (`r coef(mod2)[1] %>% round(2)` \times 4^2) = `r round(coef(mod2),1) %*% c(1,4,16)`
$$
and for a 10 year old, it is:  

$$
height = `r coef(mod2)[1] %>% round(2)` + (`r coef(mod2)[1] %>% round(2)` \times 10) + (`r coef(mod2)[1] %>% round(2)` \times 10^2) = `r round(coef(mod2),1) %*% c(1,10,100)`
$$
We can see how the quadratic "$\text{age}^2$ term has a larger effect as age increases, capturing the plateauing of heights as children get older.  


<!-- # Optional: further exploration -->

<!-- ## Simple Slopes -->

<!-- we've kind of already done it visually.   -->
<!-- plot the slope of y~x1 for certain values of x2.   -->

<!-- we've done this visually, but it would be nice to get out a "is y~x1 significant when x2 = ?"   -->




<!-- ## Johnson Neyman -->



