---
title: "Exercises: GLM"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(xaringanExtra)
xaringanExtra::use_panelset()
qcounter <- function(){
  if(!exists("qcounter_i")){
    qcounter_i <<- 1
  }else{
    qcounter_i <<- qcounter_i + 1
  }
  qcounter_i
}
```


`r qbegin(qcounter())`
Before doing anything else, watch this video, and try to count _exactly_ how many times the players wearing white pass the basketball.    

<iframe width="336" height="189" src="https://www.youtube.com/embed/vJG698U2Mvo?si=ovazszvjhjobNMUg&amp;start=5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

`r qend()`



# Invisible Gorillas

:::frame
__Data: invisibleg.csv__  

```{r}
#| include: false
#ss=runif(1,1e4,1e6)
ss=393502.4
set.seed(ss)
N=477
invis <- tibble(
  O = rnorm(N),
  C = rnorm(N),
  E = O*.1+rnorm(N),
  A = C*.2 + rnorm(N),
  N = O*-.1 + C*-.2 + rnorm(N),
  gorilla = rbinom(N, 1, prob = plogis(-.5+scale(.21*O + -.1*C + rnorm(N,0,.5))))
)
invis[,1:5] = apply(invis[,1:5],2,scale)
invis$N[sample(1:N,4)] <- (-99)
invis = bind_rows(
  invis, 
  tibble(
    O = rnorm(6),C = rnorm(6),E = rnorm(6),A = rnorm(6),N = rnorm(6),
    gorilla = -99
  )
) |> slice_sample(prop=1) |>
  mutate(
    ppt_id = paste0("ppt_",1:n()),
    ppt_name = randomNames::randomNames(n(),which.names="first")
  ) |> relocate(ppt_id,ppt_name)

invis$ppt_name[sample(1:N,64)] <- NA

#write_csv(invis, "../../data/invisibleg.csv")

# invis2 <- invis |>
#   filter(N!=-99, gorilla!=-99)
# glm(gorilla~.,data=invis2,family=binomial) |> sjPlot::plot_model(type="pred",terms="O")
# glm(gorilla~.,data=invis2,family=binomial) |> sjPlot::tab_model()

```

The data here come from a study of `r nrow(invis)` participants who completed a Big 5 Personality Inventory (providing standardised scores on 5 personality traits of Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), and then completed the selective attention task as seen above, from [Simons & Chabris 1999](https://doi.org/10.1068/p281059){target="_blank"}.  

We're interested in whether and how individual differences in personality are associated with susceptibility to _inattentional blindness_ (i.e. not noticing the gorilla).  

The data are available at [https://uoepsy.github.io/data/invisibleg.csv](https://uoepsy.github.io/data/invisibleg.csv){target="_blank"}. 

```{r}
#| label: tbl-invisdict
#| tbl-cap: "Data dictionary for invisibleg.csv"
#| echo: false
tibble(
  variable = names(invis),
  description = c(
    "Participant ID number",
    "Participant Name (if recorded)",
    "Openness (Z-scored)",
    "Conscientiousness (Z-scored)",
    "Extraversion (Z-scored)",
    "Agreeableness (Z-scored)",
    "Neuroticism (Z-scored)",
    "Whether or not participants noticed the gorilla (0 = did not notice, 1 = did notice)"   
  )
) |> gt::gt()
```



:::

`r qbegin(qcounter())`
Read in the data, have a look around, plot, describe, and generally explore.  
Do any cleaning that you think might be necessary.  


::: {.callout-tip collapse="true"}
#### Hints

There's nothing new to any of the data cleaning that needs done here. We can do everything that needs doing by using something like `ifelse()`.  

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
invis <- read_csv("https://uoepsy.github.io/data/invisibleg.csv")

summary(invis)
```


Right away we can see there's something odd with the Neuroticism variable (`N`). It shouldn't have values of -99.  

As it happens, a well-known statistical software package ([ahem](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/SPSS_logo.svg/1200px-SPSS_logo.svg.png){target="_blank"}) often ends with missing values being stored as "-99", or "-999".  

We can also see that we have some "-99" in the `gorilla` variable.  

So let's replace all those with NA's

```{r}
invis <- invis |>
  mutate(
    N = ifelse(N == -99, NA, N),
    gorilla = ifelse(gorilla == -99, NA, gorilla)
  )

summary(invis)
```

There's not a great deal to describe here, because our variables are already Z-scored (i.e. they all have means of 0 and standard deviations of 1).  

It's still important to look at our data though: 
```{r}
psych::pairs.panels(invis)
```

And we can tabulate the number of participants that do/don't notice the gorilla:  
```{r}
table(invis$gorilla)
```
Only `r round(prop.table(table(invis$gorilla))[2]*100)`% of people noticed the gorilla!  


`r solend()`


`r qbegin(qcounter())`
Here is an "intercept-only" model of the binary outcome 'did they notice the gorilla or not':  

```{r}
#| eval: false
glm(gorilla ~ 1, data = invis, family=binomial) |>
  summary()
```
```
...
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.45640    0.09397  -4.857 1.19e-06 ***
```

1. Convert the intercept estimate from **log-odds** into **odds**  
2. Convert those **odds** into **probability**  
3. What does that probability represent?  
    - *hint:* in `lm(y~1)` the intercept is the same as `mean(y)`


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
#| include: false
m0<-glm(gorilla ~ 1, data = invis, family=binomial)
cc<-coef(m0) |> round(3)
ec<-exp(cc) |> round(3)
```


The intercept estimate of `r cc` is in log-odds.  
  
To convert from log-odds to odds, we exponentiate ($odds = e^{log-odds}$)  

The odds of noticing the gorilla are $e^{`r cc`} = `r ec`$.   
  
To convert this back to probability, we calculate $\frac{odds}{1+odds}$.  

$\frac{`r ec`}{`r ec+1`} = `r round(plogis(cc),3)`$.  

So there is a `r round(plogis(cc),3)` probability of a participant noticing the gorilla.  

Does this number seem familiar?  
Because there's no predictors in the model, our intercept is just the proportion of 1s in our outcome variable!  
```{r}
prop.table(table(invis$gorilla))
```

`r solend()`

`r qbegin(qcounter())`
Does personality (i.e. all our measured personality traits collectively) predict inattentional blindness?  


::: {.callout-tip collapse="true"}
#### Hints

We're wanting to test the influence of a _set_ of predictors here. Sounds like a job for model comparison! (see [10A #comparing-models](10a_glm.html#comparing-models){target="_blank"}).    

BUT WAIT... we might have some missing data...  
_(this depends on whether, during your data cleaning, you a) replaced values of -99 with `NA`, or b) removed those entire rows from the data)._  

Models like `lm()` and `glm()` will exclude any observation (the entire row) if it has a missing value for _any_ variable in the model (outcome _or_ predictor). As we have missing data on the `N` variable, then when we put that in as a predictor, those rows are omitted from the model.  

So we'll need to ensure that _both_ models that we are comparing are fitted to exactly the same set of observations.

:::

`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)`

If we just try a comparison, we get an error:
```{r}
#| eval: false
m0 <- glm(gorilla ~ 1, data = invis, family=binomial)
m1 <- glm(gorilla ~ O+C+E+A+N, data = invis, family=binomial)
anova(m0,m1, test="Chisq")
```
<p style="color:red;font-size:.8em">Error in anova.glmlist(c(list(object), dotargs), dispersion = dispersion,  :<br> 
  models were not all fitted to the same size of dataset</p>
  
So what we need to do is fit both models to the data that has values for all of the variables in our full model.  

If you haven't discovered it already, `na.omit(dataset)` is a quick way to remove all rows that have incomplete data.  

But we need to be careful - if we just use `na.omit()` on the _entire_ dataset, then we could be removing a whole load of data that is actually perfectly useful to us:
```{r}
#| results: 'hold'
# number of rows in entire dataset:
invis |> nrow()
# number of rows with complete data:
na.omit(invis) |> nrow()
# number of rows with complete data in the variables we're using in model:
na.omit(invis[,3:8]) |> nrow()
```

We can see that if we just used `na.omit(invis)` here, we would be removing `r nrow(invis)-nrow(na.omit(invis))` people. As it happens, this is excluding a whole bunch of participants for whom their names are missing. But we're not using their names, so we can actually use these in our model!


```{r}
moddat <- na.omit(invis[,3:8])

m0 <- glm(gorilla ~ 1, data = moddat,
          family=binomial)
m1 <- glm(gorilla ~ O+C+E+A+N, data = moddat, 
          family=binomial)

anova(m0,m1, test="Chisq")
```

```{r}
#| include: false
res=as.data.frame(anova(m0,m1, test="Chisq"))
res$Deviance=round(res$Deviance,2)
res$`Pr(>Chi)`=format.pval(res$`Pr(>Chi)`,eps=.001,digits=3)
```

:::int
The inclusion of the Big 5 Personality Traits (Openness, Conscientiousness, Extraversion, Agreeablenss and Neuroticism) were found to result in a significant improvement in model fit over the null model ($\chi^2(`r res[2,3]`)=`r res[2,4]`, p`r res[2,5]`$), suggesting that personality is useful in predicting inattentional blindness. 
:::
  
`r solend()`


`r qbegin(qcounter())`
How are different aspects of personality associated with inattentional blindness?  

::: {.callout-tip collapse="true"}
#### Hints

The interpretation of logistic regression coefficients is explained in [10A #coefficient-interpretation](10a_glm.html#coefficient-interpretation){target="_blank"}.  

You might want to explain the key finding(s) in terms of odds ratios.  

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The coefficients we get from our model are in log-odds. So values $<0$ represent decreasing probability, and values $>0$ represent increasing probability.  

But that's about as much as we can interpret in this scale.  
When we exponentiate them, we get our odds-ratios. These are "how much the odds are multiplied by", so values $<1$ represent decreased odds (i.e. decreasing probability) and values $>1$ represent increased odds.  

Only Openness (`O`) is significantly associated with the probability of noticing the gorilla.  
```{r}
#| echo: false
ec=round(exp(coef(m1)),2)
broom::tidy(m1) |> 
  transmute(
    coefficient = term, 
    b = case_when(
      p.value<.05 ~ paste0(round(estimate,2),"***"),
      TRUE ~ paste0(round(estimate,2))
    ),
    `exp(b)`= case_when(
      p.value<.05 ~ paste0(round(exp(estimate),2),"***"),
      TRUE ~ paste0(round(exp(estimate),2))
    ),
    interpretation = c(
      paste0("for someone at the mean on all personality traits, the odds of noticing the gorilla are ",ec[1]," to 1"),
      paste0("holding other personality traits constant, being 1 SD higher on openness is associated with ",ec[2]," times the odds of noticing the gorilla"),
      "","","",""
    )
  ) |> gt::gt()
```

`r solend()`

`r qbegin(qcounter())`
Compute confidence intervals for your odds ratios.  

::: {.callout-tip collapse="true"}
#### confidence interval refresher

We haven't been using confidence intervals very much, but we very easily could have been. Functions like `t.test()`, `cor.test()` etc present confidence intervals in their output, and functions like `confint()` can be used on linear regression models to get confidence intervals for our coefficients.  

Confidence intervals (CIs) are often used to make a statement about a null hypothesis _just like_ a p-value (see [3A #inference](03a_inference.html#null-hypothesis-significance-testing-nhst){target="_blank"}. If a 95% CI does not contain zero then we can, with that same level of confidence, reject the null hypothesis that the population value is zero. So a 95% confidence interval maps to $p<.05$, and a 99% CI maps to $p<.01$, and so on.  

However, many people these days prefer confidence intervals to $p$-values as they take the focus (slightly) away from the null hypothesis and toward a range of effect sizes that are compatible with the data.  

The function `confint()` will give you confidence intervals. The function `car::Confint()`^[the colon here means "look in the **car** package and use the `Confint()` function. It saves having to load the package with `library(car)`] will do exactly the same but put them alongside the estimates (which saves you scrolling up and down between different outputs).  
:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
exp(car::Confint(m1))
```
```{r}
#| echo: false
res = exp(car::Confint(m1))
res = apply(res,2,round,2)
```

:::int
`r paste0("holding other personality traits constant, being 1 SD higher on openness is associated with ",ec[2]," (95% CI: ",res[2,2],", ",res[2,3],") times the odds of noticing the gorilla")`
:::

`r solend()`

`r qbegin(qcounter())`
Produce a plot of the predicted probabilities of noticing the gorilla as a function of openness.  


::: {.callout-tip collapse="true"}
#### Hints

There's an example of this at [10A #visualising](10a_glm.html#visualising){target="_blank"}. Using the __effects__ package will be handy.  


:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

the `xlevels` argument here says to give us the fitted values for 20 different values across the predictor  
```{r}
library(effects)

effect(term = "O", mod = m1, xlevels = 20) |>
  as.data.frame() |>
  ggplot(aes(x=O,y=fit,ymin=lower,ymax=upper))+
  geom_line()+
  geom_ribbon(alpha=.3)
```

`r solend()`

`r qbegin(qcounter())`
Try creating an equivalent plot for the other personality traits - before you do, what do you expect them to look like?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

From our coefficients, we should expect the lines to go *up* as E and A increase, and *down* as C and N increase.  
However, for all of these, we should expect a lot of uncertainty (i.e. for all of these, 0 is inside our confidence intervals (i.e. they're non-significant))  
```{r}
car::Confint(m1)
```

e.g., for extraversion:  
```{r}
#| code-fold: true
library(patchwork)
plte <- effect(term = "E", mod = m1, xlevels = 20) |>
  as.data.frame() |>
  ggplot(aes(x=E,y=fit,ymin=lower,ymax=upper))+
  geom_line()+
  geom_ribbon(alpha=.3)

pltc <- effect(term = "C", mod = m1, xlevels = 20) |>
  as.data.frame() |>
  ggplot(aes(x=C,y=fit,ymin=lower,ymax=upper))+
  geom_line()+
  geom_ribbon(alpha=.3)

pltn <- effect(term = "N", mod = m1, xlevels = 20) |>
  as.data.frame() |>
  ggplot(aes(x=N,y=fit,ymin=lower,ymax=upper))+
  geom_line()+
  geom_ribbon(alpha=.3)

plta <- effect(term = "A", mod = m1, xlevels = 20) |>
  as.data.frame() |>
  ggplot(aes(x=A,y=fit,ymin=lower,ymax=upper))+
  geom_line()+
  geom_ribbon(alpha=.3)

(plte + plta) / (pltn + pltc)
```

`r solend()`


# Invisible Marshmallows

:::frame
__Data: mallow2.csv__ 

```{r}
#| include: false
ss=454086.9
set.seed(ss)
tibble(
  name = randomNames::randomNames(300,which.names="first"),
  agemonths = rdunif(300, 39, 115),
  timeofday = rbinom(300,1,plogis(scale(agemonths*-.1))),
  visibility = rep(c("hidden","visible"),e=150),
  lp2 = -3.4*(visibility=="hidden") - 4*(scale(agemonths)) + timeofday*.8 + 3.2*(visibility=="hidden")*(scale(agemonths)),
  taken = rbinom(300,1,plogis(scale(lp2)))
) |> 
  mutate(
    timeofday = ifelse(timeofday==1,"pm","am"),
    taken = ifelse(taken==1,"taken","waited"),
  ) |> select(-lp2) -> mallow2


bind_rows(
  mallow2, 
  tribble(
    ~name,~agemonths,~timeofday,~visibility,~taken,
    "Josiah",12*33,"pm","visible","waited",
    "Martin",12*53,"am","hidden","taken",
    "Dougal",12*3,"noon","visible","taken",
    "Oscar",12*10,"5 pm","hidden","waited"
  )
) |> slice_sample(prop=1) -> mallow2
# write_csv(mallow2, "../../data/mallow2.csv")
```

We already played with some marshmallow-related data in [reading 10A](10a_glm.html#fitting-glm-in-r){target="_blank"}. Here we are extending this study to investigate whether the visibility of the immediate reward moderates age effects on the ability to delay gratification (the ability to forgo an immediate reward for a greater reward at a later point).  

`r nrow(mallow2)` children took part, ranging in ages from 3 to 10 years old. Each child was shown a marshmallow, and it was explained that they were about to be left alone for 10 minutes. They were told that they were welcome to eat the marshmallow while they were waiting, but if the marshmallow was still there after 10 minutes, they would be rewarded with __two__ marshmallows.  

For half of the children who took part, the marshmallow was visible for the entire 10 minutes (or until they ate it!). For the other half, the marshmallow was placed under a plastic cup.  

The experiment took part at various times throughout the working day, and researchers were worried about children being more hungry at certain times of day, so they kept track of whether each child completed the task in the morning or the afternoon, so that they could control for this in their analyses.  

The data are available at [https://uoepsy.github.io/data/mallow2.csv](https://uoepsy.github.io/data/mallow2.csv){target="_blank"}. 

```{r}
#| label: tbl-mallow2dict
#| tbl-cap: "Data dictionary for mallow2.csv"
#| echo: false
tibble(
  variable = names(mallow2),
  description = c(
    "Participant Name",
    "Age in months",
    "Time of day that the experiment took place ('am' = morning, 'pm' = afternoon)",
    "Experimental condition - whether the marshmallow was 'visible' or 'hidden' for the 10 minutes",
    "Whether or not the participant took the marshmallow within the 10 minutes"
  )
) |> gt::gt()
```

:::

`r qbegin(qcounter())`
Read in the data, check, clean, plot, describe, explore.  


::: {.callout-tip collapse="true"}
#### Hints

- It's good practice to set categorical variables as factors (see [](){target="_blank"}).  
- It might be easier to transform age into years rather than months (up to you!)

:::


`r qend()`
`r solbegin(label="Solution Part 1 - Check", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`

```{r}
mallow2 <- read_csv("https://uoepsy.github.io/data/mallow2.csv")
```

We've only got one numeric variable here (age), so I'm going to plot that, and then tabulate the rest: 

Something looks odd here.. a couple of very old children!  
```{r}
hist(mallow2$agemonths)
```
According to the design, our age range should be 3 to 10 years old. So here are all the participants over 10 years old (i.e. over 120 months old):  
```{r}
mallow2[mallow2$agemonths>120, ]
```
Aha... who knows how they got in there! We'll deal with them in a minute.. 

The `visibility` variable looks as we would expect - half in each condition:
```{r}
table(mallow2$visibility)
```

This looks off.. we have one mis-coded as "5 pm", which I'm guessing should just be "pm". And we have one at "noon". There's no way of knowing whether that was morning or afternoon, so I would be inclined to remove it.  
```{r}
table(mallow2$timeofday)
```

And we can see that our outcome variable has got 2 unique values, which is what we expect.  
```{r}
table(mallow2$taken)
```

`r solend()`
`r solbegin(label="Solution Part 2 - Clean", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`

So we need to:

1. replace "5 pm" with "pm" in `timeofday`
2. remove "noon" with `NA` in `timeofday` 
3. remove Martin and Josiah.. they are not the population we're interested in... 
4. make any appropriate variables factors (good practice!)
5. while we're there - I think I'll also make an `age` variable that is in years rather than months

```{r}
mallow2 <- 
  mallow2 |>
    mutate(
      age = agemonths/12,
      timeofday = factor(timeofday, 
                         levels=c("am","pm","5 pm"), # possible levels
                         labels = c("am","pm","pm")), # make the levels these
      visibility = factor(visibility, 
                          levels=c("visible","hidden")),
      taken = factor(taken, 
                     levels=c("waited","taken"))
    ) |>
  filter(agemonths <= 120, !is.na(timeofday))
```

**KEEP TRACK**  
We've removed 3 observations entirely from the data. 
Two of these (Martin & Josiah) were clearly not representative of the population of interest. The other was one that had an unclear `timeofday` value. If we replaced it with `NA`, any analysis using that variable would not include that observation. And since we know we're going to use `timeofday` in our analysis (and since it's just 1 participant), it may be cleaner to just to start with a complete data.  

When writing up, we would want to:

1. Detail any removal of observations and the reasons for doing so.  
2. Describe the final dataset (if the final dataset contains `NA`s, then it's also worth providing descriptions of the patterns of missingness).  

`r solend()`
`r solbegin(label="Solution Part 3 - Describe & Explore", slabel=FALSE,show=TRUE, toggle=params$TOGGLE)`

```{r}
mallow2 |>
  group_by(visibility) |>
  summarise(
    n = n(),
    mean_age = mean(age),
    sd_age = sd(age),
    percent_pm = sum(timeofday=="pm")/n()*100,
    percent_taken = sum(taken=="taken")/n()*100
  ) |>
  gt::gt() # this just a nice way to prettify tables 
```

`r solend()`

`r qbegin(qcounter())`
Fit a model that you can use to address the research aims of the study.  


::: {.callout-tip collapse="true"}
#### Hints

Take a look back at the description of the study.  

- What are we wanting to find out? How can we operationalise this into a model?   
  - *hint:* 'moderation' is another word for 'interaction'.  
- Is there anything that we think it is important to account for? 

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mm1 <- glm(taken ~ timeofday + age * visibility, data = mallow2, family = binomial)
```

`r solend()`


`r qbegin(qcounter())`
What do you conclude?  

::: {.callout-tip collapse="true"}
#### Remember...  

When you have an interaction `Y ~ X1 + X2 + X3 + X2:X3` in your model, the coefficients that involved in the interaction (`X2` and `X3`) represent the associations _when_ the other variable in the interaction is zero.  
The interaction coefficient itself represents the *adjustment* to these associations when we move up 1 in the other variable.  

:::

`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)`
```{r}
#| echo: false
cc = coef(mm1) |> round(2)
ec = coef(mm1) |> exp() |> round(2)
```

Here are the coefficients from the model we have fitted. We've exponentiated them so that they are in odds ratios.  
```{r}
exp(car::Confint(mm1))
```

From our coefficients, it looks like when the marshmallow is visible, the odds of taking the marshmallow are more than halved (`r ec[3]`) for every year older a child is.  

Compared to when the marshmallow is visible, the odds ratio associated with every additional year of age is `r ec[5]` times bigger in the hidden-marshmallow condition.  

This is a bit of a weird one to think about, but it means that instead of having an OR of `r ec[3]` (as we do for the visible condition), for the hidden condition we have an OR that is $`r ec[3]` \times `r ec[5]` = `r round(ec[3]*ec[5],2)`$.  

This is almost always going to be more easily presented as a plot of predicted probabilities:

```{r}
#| echo: false
effect(term = "age*visibility", mod = mm1, xlevels=20) |>
  as.data.frame() |>
  ggplot(aes(x=age, y=fit, col=visibility, fill=visibility)) +
  geom_line() +
  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=.3)
```



`r solend()`

`r qbegin(qcounter())`
Write up the methods and results, providing a plot and regression table.  

A template RMarkown file can be found at [https://uoepsy.github.io/usmr/2324/misc/marshmallows.Rmd](https://uoepsy.github.io/usmr/2324/misc/marshmallows.Rmd){target="_blank"} if you want it. It contains a list of questions try and make sure you answer in your write-up.  

`r qend()`


# Optional Extras

`r qbegin(qcounter())`
Below is the background and design to a study investigating how different types of more active learning strategies improve understanding, in comparison to just studying materials.  

Fit an appropriate model to address the research aims, interpret it, make a plot, etc.  

::: {.callout-note collapse="true"}
#### immersivelearning.csv  

An experiment was run to investigate strategies for learning. Three groups of 30 participants were presented with materials on a novel language to learn.  

All groups were given two hours of preparation time, after which their learning was assessed. The first group (`studystudy`) spent both hours studying the materials. The second group (`studytest`) spent the first hour studying the materials, and the second hour testing themselves on the materials. The third group (`studyimmersion`) spent the first hour studying the materials, and the second hour trying to converse with a native speaker of the language (they were not permitted to attempt to converse in any other language during this time).  

After the two hours were up, participants were the assessed via a series of 30 communication tasks. The number of tasks each participant got correct was recorded.  

Information on two potential covariates was also included - previous language learning experience (novice/experienced), and cognitive aptitude (a 20 item questionnaire leading to a standardised test score).  

The data are available at [https://uoepsy.github.io/data/immersivelearning.csv](https://uoepsy.github.io/data/immersivelearning.csv){target="_blank"}. 

```{r}
#| echo: false
#| label: tbl-immersedict
#| tbl-cap: "Data dictionary: immersivelearning.csv"
set.seed(698646.6)
df = tibble(
  group = rep(c("studystudy","studytest","studyimmersion"),
              e = 30),
  plle = sample(c("novice","experienced"),90,T,prob=c(.8,.2)),
  cog_apt = round(rnorm(90),1),
  lp = -.5 + 
    (group=="studytest")*.2 +
    (group=="studyimmersion")*.5 + rnorm(90,0,.7),
  n_correct = rbinom(90,30,prob=plogis(scale(lp)))
) |> select(-lp) |>
  mutate(PID = paste0("ppt_",1:n())) |>
  relocate(PID)

# write_csv(df, "../../data/immersivelearning.csv")
# m = glm(cbind(n_correct,30-n_correct) ~ plle+cog_apt+group,
#     df |> mutate(
#       group=fct_relevel(factor(group),"studystudy")
#     ), family=binomial)
# summary(m)
# sjPlot::plot_model(m,type="eff",terms=c("group"))

tibble(
  variable = names(df),
  description = c(
    "Participant ID number",
    "Experimental group (studystudy = 2 hours of study, studytest = 1 hour study, 1 hour testing, studyimmersion = 1 hour study, 1 hour conversing)",
    "Previous language learning experience (novice or experienced)",
    "Cognitive Aptitude (Standardised Z Score)",
    "Number of the 30 communication tasks that each participant correctly completed"
  )
) |> gt::gt()
```

:::


::: {.callout-tip collapse="true"}
#### Hints

- This might not be binary (0 or 1), but it's binomial ("how many success in 30 trials").  
- See the optional box under logistic regression in [10A #fitting-glm-in-r](10a_glm.html#fitting-glm-in-r){target="_blank"} for how to fit a binomial model to data like this.   

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Below is the code *we* would use to investigate this. Some of these decisions you might make differently, and that is okay - the important thing is to clearly explain and justify the decision we make.  

```{r}
immers <- read_csv("https://uoepsy.github.io/data/immersivelearning.csv")

immers <- immers |> mutate(
  fct_relevel(factor(group), "studystudy")
)

mst1 = glm(cbind(n_correct,30-n_correct) ~ plle+cog_apt+group,
    data = immers, family=binomial)

summary(mst1)

effect(term = "group", mod = mst1) |>
  as.data.frame() |>
  ggplot(aes(x=group, y=fit,ymin=lower,ymax=upper))+
  geom_pointrange()
```

`r solend()`


`r qbegin(qcounter())`
People and behaviours are a lot more difficult to predict than something like, say, the colour of different wines. 

Build a model that predicts the colour of wine based on all available information. How accurately can it predict wine colours?  

*(Generally speaking, this question doesn't reflect how we do research in psychology. Ideally, we would have a theoretical question that motivates the inclusion (and testing of) specific predictors.)*

::: {.callout-note collapse="true"}
#### usmr_wines.csv

You can download a dataset of 6497 different wines (1599 red, 4898 white) from [https://uoepsy.github.io/data/usmr_wines.csv](https://uoepsy.github.io/data/usmr_wines.csv).  

It contains information on various physiochemical properties such as pH, a measure of level of sulphates, residual sugar, citric acid, volatile acidity and alcohol content, and also quality ratings from a sommelier (wine expert).  All the wines are vinho verde from Portugal, and the data was collected between 2004 and 2007.  

:::

::: {.callout-tip collapse="true"}
#### Hints

- `glm(outcome ~ ., data = mydata)` is a shorthand way of putting _all_ variables in the data in as predictors.  
- See [the lecture slides](https://uoepsy.github.io/usmr/2324/lectures/lecture09.html#/accuracy){target="_blank"} for an example of how we can get a number for "how accurately can my model predict".  

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Below is the code *we* would use to investigate this. Some of these decisions you might make differently, and that is okay - the important thing is to clearly explain and justify the decision we make.  

```{r}
wines <- read_csv("https://uoepsy.github.io/data/usmr_wines.csv")

wines <- 
  wines |> 
  mutate(
    col = factor(col, levels=c("white","red"))
  )

winemod <- glm(col ~ ., data = wines, family = binomial)

# in logit units
guess <- predict(winemod)
# logit 0 is p of .5:
guess <- ifelse(guess > 0, "red", "white")
# how many predicted colours match the observed colours??
hits <- sum(guess == wines$col)
# what percentage?  
hits/length(wines$col)
```



`r solend()`






