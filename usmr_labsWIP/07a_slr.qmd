---
title: "7A: Simple Linear Regression"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
```

# The Linear Model

```{r}
#| include: false
set.seed(023)
df <- tibble(
  x = rnorm(100,3,1),
  y = 0.5+.8*x + rnorm(100,0,1)
)
df$y=df$y+1
model1 <- lm(y ~ x, data = df)
# write_csv(df, "../../data/usmr_slr.csv")
my_data <- df
```

In its simplest form, linear regression is a way to make a model of the relationship between two variables. When both variables are continuous, it is nice and intuitive to envisage this as the 'line of best fit' on a scatterplot. For instance, in @fig-lmintro we see two variables `y` and `x`, and our linear regression model is the blue line.  

```{r}
#| label: fig-lmintro
#| fig-cap: y regressed onto x.
#| echo: false
#| fig-height: 2.5
ggplot(df,aes(x=x,y=y))+
  geom_point(size=3,alpha=.4)+
  geom_smooth(method=lm,se=F)+
  xlim(0,6)+ylim(0,7)+scale_x_continuous(breaks=0:6)
```

We're going to use the data in this plot for the remainder of the reading. If you wish to play around with it yourself, it is available at [https://uoepsy.github.io/data/usmr_slr.csv](https://uoepsy.github.io/data/usmr_slr.csv), and contains a sample of 100 observations on variables `x` and `y`.   

The plot above highlights a linear relationship, where the data points are scattered around an underlying linear pattern with a roughly-constant spread as we move along `x`.  

In [Section 5A](05a_covcor.html){target="_blank"} we have already talked about one way to describe this relationship, by calculating either the covariance or the correlation between `x` and `y`.  
However, as we will see in the coming weeks, the linear model provides us with the scope to extend our analysis to many more situations - it is the building block of many more complex analytical methods.  

The simple linear regression model takes the form:  

$$
\begin{align}
& y = b_0 + b_1 \ x + \epsilon \quad \\
\end{align}
$$
:::column-margin
You will see a variety of different ways of specifying the linear model form in different resources, some use $\beta$, some use $b$. Sometimes you will see $\alpha$ instead of $b_0$.
:::

We typically denote the outcome ('dependent') variable with $y$ and our predictor ('explanatory'/'independent') variables with $x$.  
When we construct a linear model we are trying to re-express our outcome variable $y$ with some linear transformation of our predictor variable $x$.  

You can think of this in broader terms as: 
$$
\begin{align}
& \color{red}{Outcome}\color{} = \color{blue}{Model}\color{black}{} + Error\\
\end{align}
$$
## The Model 

When we fit a simple regression model, the bit we refer to as the 'model' is the line that is defined by two numbers:

- the __intercept__, denoted $b_0$. This is the point at which the line hits the y-axis (i.e. where $x=0$)
- the __slope__, denote $b_1$. This the angle of the line. It is the amount which the line increases for every 1 increase in $x$.  

```{r}
#| label: fig-slr
#| echo: false
#| fig-cap: "Simple linear regression model, with the systematic part of the model in blue"
#| fig-height: 3
betas <- coef(model1)
intercept <- betas[1]
slope <- betas[2]

broom::augment(model1) %>%
ggplot(., aes(x = x, y = y)) +
  geom_point(size=3,alpha=.3)+
  geom_abline(intercept = intercept, slope = slope, 
              color = 'blue', size = 1) + 
  xlim(0,6)+ylim(0,7)+
  geom_vline(xintercept=0,lty="dashed")+
  geom_label(x=0,y=1.8,label="intercept",hjust=0,col="blue")+
  geom_point(aes(x=0,y=intercept),size=4,col="blue")+
  scale_x_continuous(breaks=0:6)+
  geom_segment(col="blue",lty="dashed",x=1,xend=2,y=sum(coef(model1)), yend=sum(coef(model1)))+
  geom_segment(col="blue",lty="dashed",x=2,xend=2,y=sum(coef(model1)), yend=sum(coef(model1))+coef(model1)[2])+
  geom_label(x=2.05,y=2.7,label="slope",hjust=0,col="blue")+
  labs(x = "X (predictor)", 
       y = "Y (outcome)")
```

This line implies some _predicted_ values for our observed $x$ values. For instance, we can see that when $x=3$, the model (the blue line) will predict that $y$ is approximately 4. If we take each of our datapoints, and project them up/down to the line, then we get our fitted values (@fig-slrfit). We often denote these as $\hat y$ (or "y hat"), with the hat indicating that they are the model-estimated values of $y$. 

$$
\begin{align}
\color{red}{Outcome}\color{black} \qquad=\qquad & \color{blue}{Model}\color{black}{} & +\qquad Error\\
\color{red}{y}\color{black} \qquad = \qquad & \color{blue}{\hat y}\color{black} & +\qquad \epsilon \quad \\
\color{red}{y}\color{black} \qquad = \qquad & \color{blue}{b_0 + b_1 \ (x)}\color{black} & +\qquad \epsilon \quad \\
\end{align}
$$

```{r}
#| label: fig-slrfit
#| echo: false
#| fig-cap: "Simple linear regression model, fitted values in blue"
#| fig-height: 3
betas <- coef(model1)
intercept <- betas[1]
slope <- betas[2]

broom::augment(model1) %>%
ggplot(., aes(x = x)) +
  geom_point(aes(y=y),size=3,alpha=.2)+
  
  geom_segment(aes(x=x, xend=x, y=y, yend=.fitted),lty="dotted",alpha=.3)+
  geom_point(aes(y=.fitted),size=3,col="blue")+
  geom_abline(intercept = intercept, slope = slope, color = 'blue', size = .5) + 
  xlim(0,6)+ylim(0,7)+
  geom_vline(xintercept=0,lty="dashed")+
  scale_x_continuous(breaks=0:6)+
  labs(x = "X (predictor)", 
       y = "Y (outcome)")
```



`r optbegin("Optional: regression and cov/cor",olabel=FALSE)`
TODO
`r optend()`


## The Error 

Our model is not perfect. It is a model - i.e. it is a simplification of the world, and so is inherently going to be inaccurate for individuals. This innaccuracy can be seen in @fig-slr and @fig-slrfit. Some points are higher than the model predicts, some lower.  

These deviations  (the red lines in @fig-slr2) from the model are the random error component $\hat \epsilon$, or "residuals".  

```{r}
#| label: fig-slr2
#| echo: false
#| fig-cap: "Simple linear regression model, with the systematic part of the model in blue, and residuals in red"
#| fig-height: 3
betas <- coef(model1)
intercept <- betas[1]
slope <- betas[2]

broom::augment(model1) %>%
ggplot(., aes(x = x, y = y)) +
  geom_point(size=3,alpha=.3)+
  geom_abline(intercept = intercept, slope = slope, 
              color = 'blue', size = 1) + 
  xlim(0,6)+ylim(0,7)+
  geom_vline(xintercept=0,lty="dashed")+
  scale_x_continuous(breaks=0:6)+
  labs(x = "X (predictor)", 
       y = "Y (outcome)")+
  geom_segment(aes(x=x, xend=x, y=y, yend=.fitted), col="red",lty="dotted")
```

In full, we should really write our linear regression model out as: 

$$
\begin{align}
& y = b_0 + b_1 \ x + \epsilon \quad \\
& \text{where} \\
& \epsilon \sim N(0, \sigma) \text{ independently}
\end{align}
$$

The new bit here: "$\epsilon \sim N(0, \sigma) \text{ independently}$" means that the errors around the line have mean zero and constant spread as x varies (we'll read more about what this means later in this course, when we discuss the assumptions underlying regression). You can think of $\sim N(0, \sigma)$ as meaning "normally distributed with a mean of zero and a standard deviation of $\sigma$".  

The standard deviation of the errors, denoted by $\sigma$ is an important quantity that our model estimates. It measures how much individual data points tend to deviate above and below the regression line. A small $\sigma$ indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large $\sigma$ suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.

$\sigma$ is estimated by summing the squared residuals:  
$$
\begin{align}
& \hat \sigma = \sqrt{\frac{SS_{Residual}}{n - 2}} \\
\qquad \\
& \text{where} \\
& SS_{Residual} = \textrm{Sum of Squared Residuals} = \sum_{i=1}^n{(\epsilon_i)^2}
\end{align}
$$

<div class="divider div-transparent div-dot"></div>

# Fitting Linear Models in R

## lm()

In R it is very easy to fit linear models, we just need to use the `lm()` function.  

The syntax of the `lm()` function is:  

```
model_name <- lm(outcome ~ 1 + predictor, data = dataframe)
```

`r optbegin("Why ~1?", olabel=F,toggle=params$TOGGLE)`
The fitted model can be written as
$$
\hat y = \hat b_0 + \hat b_1 x
$$
The predicted values for the outcome are equal to our intercept, $\hat b_0$, plus our slope $\hat b_1$ multiplied by the value on our explanatory variable $x$.  
The intercept is a _constant_. That is, we could write it as multiplied by 1:
$$
\hat y = \color{blue}{\hat b_0}\color{black}{}(\color{green}{1}\color{black}{})\color{blue}{ + \hat b_1 }\color{black}{}(\color{green}{x}\color{black}{})
$$

When we specify the linear model in R, we include after the tilde sign `~` all the things which appear to the right of each of the $\hat b$s (the bits in green in the equartion above). That's why the 1 is included. It is just saying "we want the intercept, $b_0$, to be estimated".   
`r optend()`

## Model Summary

We can then view lots of information by giving our model to the `summary()` function:

```{r}
#| include: false
#| echo: true
my_data <- read_csv("https://uoepsy.github.io/data/usmr_slr.csv")
my_model <- lm(y ~ x, data = my_data)
summary(my_model)
```
```{r}
#| label: fig-lmoutput
#| fig-cap: "Output of lm() for a simple regression in R"
#| echo: false
knitr::include_graphics("images/slr/slr1.png")
```

We can see in @fig-lmoutput our estimated values for the intercept and slope of the line. Our model is:  

$$
\begin{align}
y &= 1.54 + 0.78 (x) + \varepsilon \\
\end{align}
$$
```{r}
#| label: fig-slrest
#| echo: false
#| fig-cap: "Simple linear regression model, estimated intercept and slope included"
#| fig-height: 3
betas <- coef(model1)
intercept <- betas[1]
slope <- betas[2]

broom::augment(model1) %>%
ggplot(., aes(x = x, y = y)) +
  geom_point(size=3,alpha=.3)+
  geom_abline(intercept = intercept, slope = slope, 
              color = 'blue', size = 1) + 
  xlim(0,6)+ylim(0,7)+
  geom_vline(xintercept=0,lty="dashed")+
  geom_label(x=0,y=1.8,label="1.54",hjust=0,col="blue")+
  geom_point(aes(x=0,y=intercept),size=4,col="blue")+
  scale_x_continuous(breaks=0:6)+
  geom_segment(col="blue",lty="dashed",x=1,xend=2,y=sum(coef(model1)), yend=sum(coef(model1)))+
  geom_segment(col="blue",lty="dashed",x=2,xend=2,y=sum(coef(model1)), yend=sum(coef(model1))+coef(model1)[2])+
  geom_label(x=2.05,y=2.7,label="0.78",hjust=0,col="blue")+
  labs(x = "X (predictor)", 
       y = "Y (outcome)")
```

## Model Predictions

Furthermore, we can get out the model predicted values for $y$, the "y hats" ($\hat y$), using functions such as:  

- `predict(my_model)`
- `fitted(my_model)`
- `fitted.values(my_model)`
- `my_model$fitted.values`

A nice package which will come in handy is the __broom__ package. It allows us to use the function `augment()`, which gives us out lots of information, such as the model predicted values, the residuals, and many more:  

```{r}
library(broom)
augment(my_model)
```

We can also compute model-predicted values for other (unobserved) data. For instance, what about for an observation where $x=10$, or $20$?  

```{r}
# make a dataframe with values for the predictor:
some_newdata <- data.frame(x=c(10, 20))
# model predicted values of y, for the values of x inside the 'some_newdata' object:
predict(my_model, newdata = some_newdata)
```

Given that our fitted model takes the form below, we can work this out ourselves as well: 

$$
\begin{align}
y &= 1.54 + 0.78\cdot x \\
y &= 1.54 + 0.78\cdot 10 \\
y &= 1.54 + 7.80\\
y &= 9.34 \\
\end{align}
$$


<div class="divider div-transparent div-dot"></div>

# Inference for regression coefficients  

Now that we have fitted a linear model, and we know how we interpret our coefficient estimates, we would like to be able to make a statement on whether these relationships are likely to hold in the population.  
Our coefficients accurately describe the relationship between $y$ (outcome) and $x$ (predictor) in our sample, but we are yet to perform a statistical test. A test will enable us to discuss how likely it is that we would see this relationship in our sample, if the relationship doesn't hold for the population.  

```{r}
#| label: fig-sillyfig
#| fig.cap: "Estimates without inference"
#| echo: false
knitr::include_graphics("images/slr/conv1a.png")
```

Much like our discussion of sample means and intervals in [Section 2B](02b_sampling.html){target="_blank"}, we have our coefficients: 
```{r}
coef(my_model)
```

and to quantify the amount of uncertainty in each estimated coefficient that is due to sampling variability, we use the standard error (SE)^[Recall that a standard error gives a numerical answer to the question of how variable a statistic will be because of random sampling.] of the coefficient.

The standard errors are found in the column "Std. Error" of the `summary()` of a model:
```{r}
summary(my_model)$coefficients
```

In this example the slope, 0.78, has a standard error of 0.10. One way to envision this is as a distribution. Our best guess (mean) for the slope parameter is 0.78. The standard deviation of this distribution is 0.10, which indicates the precision (uncertainty) of our estimate.

```{r}
#| label: fig-sampbeta
#| echo: false
#| fig.cap: 'Sampling distribution of the slope coefficient. The distribution is approximately bell-shaped with a mean of 0.78 and a standard error of 0.10.'
#| fig.height: 2.5
ggplot(tibble(x = c(-3 * 0.10 + 0.78, 3 * 0.1 + 0.78)), aes(x = x)) +
    stat_function(fun = dnorm, args = list(mean = 0.78, sd = 0.10)) +
  labs(x = "Estimate for slope of x", y = '')
```

We can perform a test against the null hypothesis that the estimate is zero. The reference distribution in this case is a t-distribution with $n-2$ degrees of freedom^[Why $n-2$? The most intuitive answer is that we have already used up 2 pieces of information in estimating the intercept and the slope. Once these things are fixed, $n-2$ of the datapoints could be wherever they like around that line, but the remaining 2 must be placed in such a way that results in that line], where $n$ is the sample size, and our test statistic is:  

$$
t = \frac{\hat b_1 - 0}{SE(\hat b_1)}
$$

This allows us to test the hypothesis that the population slope is zero --- that is, that there is no linear association between income and education level in the population.  

We don't actually have to **do** anything for this, it's all provided for us in the `summary()` of the model! The information is contained in the row corresponding to the variable "education" in the output of `summary()`, which reports the t-statistic under `t value` and the p-value under `Pr(>|t|)`:
```{r}
summary(my_model)$coefficients
```

:::int
A significant association was found between x and y ($t(98) = 7.83,\ p < .001$, two-sided).
:::
:::column-margin
Recall that the p-value `5.92-e12` in the `Pr(>|t|)` column simply means $5.92 \times 10^{-12}$. This is a very small value, hence we will report it as <.001 following the APA guidelines.
:::

```{r}
#| label: fig-sillyfig2
#| fig.cap: "Conversations with statisticians"
#| echo: false
knitr::include_graphics("images/slr/conv2a.png")
```


<div class="divider div-transparent div-dot"></div>


# Model evaluation 

## Partitioning Variance

We might ask ourselves if the model is useful in explaining the variance in our outcome variable $y$. To quantify and assess model utility, we split the total variability of the response into two terms: the variability explained by the model plus the variability left unexplained in the residuals.

$$
\begin{align}
& \qquad \qquad \qquad \qquad \text{total variability in response } =  \\
& \text{variability explained by model } + \text{unexplained variability in residuals}
\end{align}
$$
Each term can be quantified by a sum of squares:

$$
\begin{aligned}
SS_{Total} &= SS_{Model} + SS_{Residual} \\
\sum_{i=1}^n (y_i - \bar y)^2 &= \sum_{i=1}^n (\hat y_i - \bar y)^2 + \sum_{i=1}^n (y_i - \hat y_i)^2 \\
\quad \\
\text{Where:} \\
& y_i = \text{observed value} \\
&\bar{y} = \text{mean} \\
& \hat{y}_i = \text{model predicted value} \\
\end{aligned}
$$

```{r}
#| label: fig-sstssrssm
#| fig-cap: "Total Sums of Squares = Model Sums of Squares + Residual Sums of Squares"
#| echo: false
df <- my_data %>% rename(x1=x)
plt_sst = 
  ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=mean(df$y)), lty="dashed",col="red")+
  geom_text(x=1,y=6,label="bar(y)",parse=T, size=6)+
  geom_curve(x=1.1,xend=2,y=6,yend=mean(df$y),curvature=-.2)+
  labs(title="SS Total")

plt_ssr = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=fitted(lm(y~x1,df))), lty="dashed",col="red")+
  labs(title="SS Residual")

plt_ssm = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,yend=mean(df$y),y=fitted(lm(y~x1,df))), lty="dashed",col="blue")+
  labs(title="SS Model")

plt_sst / (plt_ssm + plt_ssr) & scale_x_continuous("x",breaks=NULL) & scale_y_continuous("y",breaks=NULL)
```

## $R^2$

A useful statistic is the $R^2$, which shows us the proportion of the total variability in the outcome (`y`) that is explained by the linear relationship with the predictor (`x`).

:::sticky
The $R^2$ coefficient is defined as the proportion of the total variability in the outcome variable which is explained by our model:  
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$
:::

We can find the $R^2$ easily in the `summary()` of the model! 

```{r}
summary(my_model)
```

The output of `summary()` displays the R-squared value in the following line:
```
Multiple R-squared:  0.3847
```
:::column-margin
For the moment, ignore "Adjusted R-squared". We will come back to this later on. 
:::

:::int
Approximately 38\% of the total variability in `y` is explained by the linear association with `x`.
:::

`r optbegin("Optional - Manual calculation of R-Squared", olabel=F, toggle=params$TOGGLE)`

```{r}
my_model_fitted <- my_data %>%
  mutate(
    y_hat = predict(my_model),
    resid = y - y_hat
  )
head(my_model_fitted)

my_model_fitted %>%
  summarise(
    SSModel = sum( (y_hat - mean(y))^2 ),
    SSTotal = sum( (y - mean(y))^2 )
  ) %>%
  summarise(
    RSquared = SSModel / SSTotal
  )
```

`r optend()`


## The $F$ Statistic

This will become more relevant in coming weeks, but we can also perform a test to investigate if the model is 'useful' --- that is, a test to see if the explanatory variable is a useful predictor of the outcome.  
We test the following hypotheses:

$$
\begin{aligned}
H_0 &: \text{the model is ineffective, } b_1 = 0 \\
H_1 &: \text{the model is effective, } b_1 \neq 0
\end{aligned}
$$
:::sticky
The relevant test-statistic is the F-statistic:

$$
\begin{split}
F = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}
\end{split}
$$

which compares the amount of variation in the response explained by the model to the amount of variation left unexplained in the residuals.

The sample F-statistic is compared to an F-distribution with $df_{1} = 1$ and $df_{2} = n - 2$ degrees of freedom.^[
$SS_{Total}$ has $n - 1$ degrees of freedom as one degree of freedom is lost in estimating the population mean with the sample mean $\bar{y}$.
$SS_{Residual}$ has $n - 2$ degrees of freedom. There are $n$ residuals, but two degrees of freedom are lost in estimating the intercept and slope of the line used to obtain the $\hat y_i$s.
Hence, by difference, $SS_{Model}$ has $n - 1 - (n - 2) = 1$ degree of freedom.
]

`r optbegin('Optional: Another formula for the F-test.', olabel=FALSE, toggle=params$TOGGLE)`
With some algebra we can also show that:
$$
F = \frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }
$$

Proof:

$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\frac{SS_{Model}}{SS_{Total}}}{\frac{SS_{Residual}}{SS_{Total}} \cdot \frac{1}{(n - 2)}} 
= \frac{R^2 / 1}{(1 - R^2) / (n - 2)}
\end{aligned}
$$
`r optend()`

:::


`r optbegin('Optional: Equivalence of t-test for the slope and model utility F-test in simple regression.', olabel = FALSE,  toggle=params$TOGGLE)`
**In simple linear regression only** (where we have just __one__ predictor), the F-statistic for overall model significance is equal to the square of the t-statistic for $H_0: b_1 = 0$.

You can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:
```{r}
summary(my_model)$fstatistic['value']
summary(my_model)$coefficients['x','t value']
```
$$
t^2 = F \\
7.827194^2 = 61.26497
$$

Here we will show the equivalence of the F-test for model effectiveness and t-test for the slope.

Recall the formula of the sum of squares due to the model. We will rewrite it in an equivalent form below:
$$
\begin{aligned}
SS_{Model} &= \sum_i (\hat y_i - \bar y)^2 \\
&= \sum_i (\hat b_0 + \hat b_1 x_i - \bar y)^2 \\
&= \sum_i (\bar y - \hat b_1 \bar x + \hat b_1 x_i - \bar y)^2 \\
&= \sum_i (\hat b_1 (x_i - \bar x))^2 \\
&= \hat b_1^2 \sum_i (x_i - \bar x)^2
\end{aligned}
$$

The F-statistic is given by:
$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\hat b_1^2 \sum_i (x_i - \bar x)^2}{\hat \sigma^2} 
= \frac{\hat b_1^2 }{\hat \sigma^2 / \sum_i (x_i - \bar x)^2}
\end{aligned}
$$

Now recall the formula of the t-statistic,
$$
t = \frac{\hat b_1}{SE(\hat b_1)} = \frac{\hat b_1}{\hat \sigma / \sqrt{\sum_i (x_i - \bar x)^2}}
$$

It is evident that the latter is obtained as the square root of the former.

`r optend()`



<div class="divider div-transparent div-dot"></div>


# Binary predictors {#binpred}

Let's suppose that instead of having measured $x$ so accurately, we simply had information on whether $x>3$ or not. Our predictor variable would be binary categorical (think back to our discussion of [types of data in Reading 2A](02a_measurement.html){target="_blank"}) - it would have 2 levels:    
```{r}
my_data <- 
  my_data %>% 
  mutate(
    x_cat = ifelse(x < 3, "level1","level2")
  )
```

We may then plot our relationship as a boxplot. If you want to see the individual points, you could always "jitter" them (right-hand plot below)
```{r}
ggplot(my_data, aes(x = x_cat, y = y)) + 
  geom_boxplot() +
ggplot(my_data, aes(x = x_cat, y = y)) + 
  geom_jitter(height=0, width=.05)
```

:::statbox
__Binary predictors in linear regression__

We can include categorical predictors in a linear regression, but the interpretation of the coefficients is very specific. Whereas we talked about coefficients being interpreted as "the change in $y$ associated with a 1-unit increase in $x$", for categorical explanatory variables, coefficients can be considered to examine differences in group means. However, they are actually doing exactly the same thing - the model is simply translating the levels (like "Level1"/"Level2", or "Yes"/"No", or "High"/"Low") in to 0s and 1s!  
So while we may have in our dataframe a categorical predictor like the middle column "x_cat", below, what is inputted into our model is more like the third column, "isLevel2". 
```{r echo=FALSE}
my_data %>% sample_n(size=n()) %>%
  mutate(
    isLevel2 = ifelse(x_cat == "level2", 1, 0)
  ) %>% select(y, x_cat, isLevel2)
```

Our coefficients are just the same as before. The intercept is where our predictor equals zero, and the slope is the change in our outcome variable associated with a 1-unit change in our predictor.  
However, "zero" for this predictor variable now corresponds to a whole level. This is known as the "reference level". Accordingly, the 1-unit change in our predictor (the move from "zero" to "one") corresponds to the difference between the two levels. 


```{r echo=FALSE}
cstat = coef(lm(y~x_cat,my_data))
my_data %>%
  mutate(
    isLevel2 = ifelse(x_cat=="level2", 1, 0)
  ) %>% ggplot(.,aes(x=factor(isLevel2), y=y))+
  #geom_boxplot(fatten=NULL)+
  geom_jitter(height=0,width=.05)+
  geom_smooth(method="lm",aes(x=isLevel2+1), se=F)+
  geom_segment(aes(x="1",xend="1",y=cstat[1],yend=cstat[1]+cstat[2]), lty="dashed",col="blue")+
  geom_segment(aes(x="0",xend="1",y=cstat[1],yend=cstat[1]), lty="dashed",col="blue")+
  annotate("text",x=2.2,y=mean(c(cstat[1], sum(cstat)))-.1,label=expression(paste(beta[1], " (slope)")), col="blue")+
  geom_point(x=1,y=cstat[1], col="blue",size=3)+
  annotate("text",x=1,y=cstat[1],label=expression(paste(beta[0], " (intercept)")), col="blue", hjust=1.1)+
  labs(x="isLevel2")
```
:::  

<div class="divider div-transparent div-dot"></div>

# Basic Tests are Linear Models!  

TODO


<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
