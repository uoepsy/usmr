---
title: "Practical Inference"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
```

In the previous section we saw how we can apply the logic of _Null Hypothesis Significance Testing_ (NHST), allowing us to draw inferences about parameters in the __population__, based on statistics computed on the __sample__ we have collected.  

:::statbox
__NHST__  

We have a sample ($n=10$):
```{r}
mysample <- c(1, -4, 6, 4, -2, 3, 2, -5, 6, 8)
```
And a sample mean:
```{r}
mean(mysample)
```
We want to test this against the null hypothesis that the mean in the population is actually 0. 

Remember, there are lots of samples of size $n=10$ that we _could_ take, and they all have different means. To quantify the spread of these different means we can use the __standard error__, calculated using $SE = \frac{\sigma}{\sqrt{n}}$:  
```{r}
sd(mysample) / sqrt(length(mysample))
```

We can use this information to express how far away from the null hypothesis (mean = 0) our observed sample is, in terms of standard errors: 

$$
Z \ = \ \frac{\text{estimate}-\text{null}}{SE} \ = \ \frac{1.9 - 0}{1.39} \ = \ 1.36
$$
We then ask, __if__ the mean in the population _is_ 0, what is the probability of obtaining a $Z$-statistic from a sample of this size at least as extreme as the one we _have_ observed?  

The resulting probability is our __p-value:__  
```{r}
2*pnorm(1.36, mean = 0, sd = 1, lower.tail = FALSE)
```

```{r}
#| label: fig-pnorm23
#| echo: false
#| fig-cap: "2*pnorm gives the two tails"
#| fig-height: 2.5
new_theme_empty <- theme_bw()
new_theme_empty$line <- element_blank()
new_theme_empty$rect <- element_blank()
new_theme_empty$axis.text.y <- element_blank()
new_theme_empty$plot.title <- element_blank()
new_theme_empty$axis.title.y <- element_blank()

zscore = 1.36
df <- tibble(x=c(-5,5))
g <- df %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1) +
  labs(x = "Z-statistic")
ld <- layer_data(g) %>% filter(x>= zscore)
ld2 <- layer_data(g) %>% filter(x<= -zscore)
g = g + geom_area(data=ld,aes(x=x,y=y),fill="red") +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1)
g + geom_area(data=ld2,aes(x=x,y=y),fill="red") +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1)+
  scale_x_continuous(breaks=-3:3)+
  new_theme_empty
```

As our $p$-value is above our threshold of $\alpha=.05$, we fail to reject the null hypothesis that the mean in the population is zero.  

We can get to the same conclusion by constructing a 95% confidence interval: 
```{r}
xbar = mean(mysample)
se = sd(mysample) / sqrt(length(mysample))
c(xbar - (1.96 * se), xbar + (1.96 * se))
```
As this interval includes zero, then at the 5% level we fail to reject the null hypothesis that the population mean is zero.^[Remember that confidence intervals provide a range of plausible values for the population mean. In this case, zero is a plausible value.]


:::

While in practice NHST follows the logic described above, there is something important that we have been sweeping under the carpet.  

In our estimation of the __standard error__ we have used the formula that includes $\sigma$, which refers to the __population__ standard deviation. However, we never know this value (because we don't have data for the population), so we have been using the __sample__ standard deviation $s$ instead. This is an _approximation_, and might be okay when we have a very large $n$ (meaning $s$ provides accurate estimate of $\sigma$), but in practice is not always feasible.
$$
SE = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}}
$$

# $t$-distributions  

To resolve the issues with this approximation (using $s$ in place of $\sigma$), instead of using the normal distribution, we tend to instead use the $t$-distribution.  
The $t$ distribution is very similar to the normal distribution, but it has slightly _heavier_ tails:  
```{r}
#| label: fig-tnorm
#| fig-cap: "Normal distribution (black) vs t-distribution with 3 degrees of freedom (red)"
#| echo: false
#| fig-height: 3
df <- tibble(x=c(-4,4))
g <- df %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1,aes(col="normal")) +
  stat_function(fun=dt,args=list(df=3),size=1,aes(col="t(3)"),lty="dashed")+
  scale_x_continuous(NULL,breaks=NULL)+
  scale_color_manual("Distribution",values=c("normal"="black","t(3)"="blue"))+
  new_theme_empty
g
```

$t$-distributions are always centered on zero, and the precise shape (how heavy the tails are) depends upon a parameter known as the __degrees of freedom.__  

:::sticky
__Degrees of Freedom - $df$__ 

'Degrees of freedom' is a tricky concept. One of the most intuitive ways to understand it is to think of it as the number of independent bits of information that go into calculating an estimate. Put another way, it is the number of datapoints that are _free to vary_.  

`r optbegin("Degrees of freedom (df)", olabel=FALSE)`

Suppose we have four unknown numbers ($a$, $b$, $c$ and $d$) which *must* have a mean of 5.  

Do the following, *in order:*  

1. Choose a value for $a$.
1. Choose a value for $b$.
1. Choose a value for $c$.
1. Can you choose a value for $d$ while ensuring the mean of the four numbers you have chosen is 5?

You are free to choose anything you like for $a$, $b$ and $c$.  
But once those are fixed, you have no freedom to choose $d$.  

Example:  

+ $a$ = 1  
+ $b$ = 2  
+ $c$ = 3  

We know that $\frac{1+2+3+d}{4} = 5$
So there is only one possible value for $d$:  
$\frac{1+2+3+d}{4} = 5$  
$1+2+3+d = 5*4$  
$1+2+3+d = 20$  
$d = 20-3-2-1$   
$d = 14$  

`r optend()`

:::


When we estimate the mean from a sample, we use up one of our degrees of freedom, and so our test of a single mean will require us to use a $t$-distribution with $n-1$ degrees of freedom. For $t$-distributions, as the $df$ increases the distribution becomes closer and closer to a normal distribution (see @fig-tdf) - the use of these $t$-distributions is exactly what we need to account for using $s$ in our calculation of the standard error.  

```{r}
#| label: fig-tdf
#| fig-cap: "t distributions with various degrees of freedom." 
#| echo: false
#| fig-height: 3
df <- tibble(x=c(-4,4))
g <- df %>% ggplot(aes(x=x)) +
  stat_function(fun=dnorm,args=list(mean=0,sd=1),size=1,aes(col="normal")) +
  stat_function(fun=dt,args=list(df=3),size=.5,aes(col="t (df=3)"))+
  stat_function(fun=dt,args=list(df=5),size=.5,aes(col="t (df=5)"))+
  stat_function(fun=dt,args=list(df=10),size=.5,aes(col="t (df=10)"))+
  stat_function(fun=dt,args=list(df=20),size=.5,aes(col="t (df=20)"))+
  stat_function(fun=dt,args=list(df=30),size=.5,aes(col="t (df=30)"))+
  scale_x_continuous(NULL,breaks=NULL)+
  scale_color_manual("Distribution",values=c("normal"="black",
                                             "t (df=3)" = "blue",
                                             "t (df=5)" = "green",
                                             "t (df=10)" ="orange",
                                             "t (df=20)" ="red",
                                             "t (df=30)" ="pink"
                                             )
                     )+
  new_theme_empty
g
```

In order to utilise the $t$-distribution in hypothesis testing, we need to move to performing $t$-tests! 

The logic remains the same as before, but where we previously were relying on the normal distribution: 

- `pnorm()` for our $p$-values
- `qnorm()` in order to calculate our confidence intervals (`qnorm(0.975)` gives the 1.96 we have been using)

We can use `pt()` and `qt()` to conduct the same process but in reference to the appropriate $t$-distribution.  

:::statbox
__Demonstration: t-test for a single mean__  

Recalling our sample, we'll now perform the appropriate test against the null hypothesis that the mean in the population is zero.  
```{r}
mysample <- c(1, -4, 6, 4, -2, 3, 2, -5, 6, 8)
```

Now that we move to using $t$-distributions, our test-statistic is going to be a $t$-statistic:  

$$
\begin{align}
& t =  \frac{\bar x - \mu_0}{\frac{s}{\sqrt{n}}}\\
\ \\
& \text{where:} \\ 
& \bar x : \text{sample mean} \\
& \mu_0 : \text{hypothesised population mean} \\
& s : \text{sample standard deviation} \\
& n : \text{sample size} \\
\end{align}
$$
Which can be calculated as: 
```{r}
xbar = mean(mysample)
se = sd(mysample) / sqrt(length(mysample))
tstat = (xbar - 0) / se
tstat
```

Because we have $n=10$, and we are estimating a sample mean, we are going to be referring to a $t$-distribution with 9 ($10-1$) degrees of freedom (we lose one by calculating the mean).  

Our p-value can be found with the `pt()` function:  
```{r}
2*pt(tstat, df = 9, lower.tail = FALSE)
```

And our confidence interval can be constructed using: 
$$
\text{95% CI} = \bar{x} \pm t^* \times SE \\
$$
Note that $t^*$ has replaced the 1.96 we saw in previous chapters, because we obtained that using the normal distribution. The code `qnorm(c(0.025, 0.975))` shows us that 95% of __normal__ distribution is beyond 1.96 from the mean. But what we actually want to know is where 95% of the $t$-distribution with $df=9$ lies:  
So instead we can use:
```{r}
qt(c(0.025, 0.975), df = 9)
```
And our confidence interval is:
```{r}
xbar = mean(mysample)
se = sd(mysample) / sqrt(length(mysample))
c(xbar - (2.262 * se), xbar + (2.262 * se))
```
:::

# Making things easier  

All of the above is crucial for understanding how this all works, but in practice we can avoid all of the rigmarole of ever calculating the standard error or using functions like `pt()`, `qt()`. This is where R starts to become far more powerful - there are functions that do all this sort of stuff for us - in just _one single line of code!_ 

Take a look at the output of the function below. I have given it the sample, and specified that we want it to test against the null hypothesis that $\mu=0$.  

```{r}
t.test(mysample, mu = 0)
```

The `t.test()` function here gives us the $t$-statistic, the $df$, the $p$-value, the 95% CI, the mean $\bar x$, and it even tells us the alternative hypothesis (that the true mean is $\neq 0$).  
  
All of these numbers will match those that we calculated above (there may be a small bit of rounding error).  

It's __that__ easy!  

# Checking/Testing Assumptions  

These sort of tests often require us to meet a set of conditions in order for our inferences to be valid. When we perform tests like these that involve estimating a mean, a common requirement is that the deviations from that mean are close to normally distributed.  

We can evaluate how close to normal a distribution is by visualising it and making a judgment call, but this can sometimes be hard:  
```{r}
#| label: fig-normalquestion
#| fig-cap: "Am I normal?"
#| fig-height: 2.5
#| echo: false
data <- tibble(mysample = mysample)
ggplot(data,aes(x=mysample))+geom_histogram(bins=14)
```
A useful visualisation tool is the __QQplot__. The closer to the diagonal line, the closer our data is to being normally distributed: 

```{r}
#| label: fig-normalqq
#| fig-cap: "A QQplot"
#| fig-height: 2.5
#| echo: false
ggplot(data = NULL, aes(sample = mysample)) + 
  geom_qq()+
  geom_qq_line()
```

There are also, if we wish to make use of them, specific hypothesis tests that assess normality, such as the 'Shapiro-Wilks' Test. The null hypothesis for this test is that the data we give it are drawn from a normal distribution. This means that __we want a p-value *greater* than .05__. So in the example below we are okay, and it is not inappropriate for us to conduct the t-test.  

```{r}
shapiro.test(mysample)
```

<div class="divider div-transparent div-dot"></div>

# Basic Tests  

Now that we've gone through all the nitty-gritty bits of how hypothesis testing works, the heavy lifting is done.  
we're going to start to look at some of the different basic hypothesis tests that we can perform.  

For each test below we show an example conducted the quick way (e.g. `t.test()` function), and also the manually computations (for those of you who are interested!). We've already seen the one sample $t$-test in the example above, so you might want to skim over that section.

:::imp
These tests are the simple hypothesis tests that were developed in the 19th and 20th centuries, and provide a good basis of understanding the _null hypothesis significance testing_ framework.  

In the latter half of this course, we move to focus on a modelling based approach for analysing data. We will start to see how many of these simple tests that we are learning now are actually special cases of a more general statistical model. 

:::

<!-- For each test, we'll detail:   -->

<!-- - Purpose -->
<!-- - Hypotheses -->
<!-- - Test statistic -->
<!-- - P-value -->
<!-- - Assumptions -->

<!-- And then we will show an example conducted the quick way (e.g. `t.test()` function), and also the manually computations (for those of you who are interested!).   -->
<!-- We've already seen the one sample $t$-test in the example above, so you might want to skim over that section.   -->


## One sample t-test  

:::statbox
__Purpose__  

The one sample t-test is what we have already seen above. We use it to test whether the mean is different from/greater than/less than some hypothesised value.  

- __Examples:__
  - Is the mean age of USMR students different from 20? 
  - Is the mean IQ different from 100?  
  - Do people read more than 250 words per minute?  
  
__Assumptions:__

- The data are continuous (not discrete)
- The data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)
- The data are normally distributed _OR_ the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed
  
:::

<!-- :::statbox -->
<!-- __Hypotheses__  -->

<!-- Our hypotheses will take one of the combinations below, depending on if we want to perform the test of whether the mean $\mu$ is less than/different from/greater than some hypothesised value $\mu_0$.   -->

<!-- | Null Hypothesis   | Alternative Hypothesis | -->
<!-- | ----------- | ----------- | -->
<!-- | $H_0: \mu = \ \mu_0$  | $H_1: \mu \neq \ \mu_0$ | -->
<!-- | $H_0: \mu \leq \ \mu_0$  | $H_1: \mu > \ \mu_0$ | -->
<!-- | $H_0: \mu \geq \ \mu_0$  | $H_1: \mu < \ \mu_0$ | -->


<!-- ::: -->

<!-- :::statbox -->
<!-- __Test-statistic__  -->

<!-- $$ -->
<!-- \begin{align} -->
<!-- & t =  \frac{\bar x - \mu_0}{\frac{s}{\sqrt{n}}}\\ -->
<!-- \ \\ -->
<!-- & \text{where:} \\  -->
<!-- & \bar x : \text{sample mean} \\ -->
<!-- & \mu_0 : \text{hypothesised population mean} \\ -->
<!-- & s : \text{sample standard deviation} \\ -->
<!-- & n : \text{sample size} \\ -->
<!-- \end{align} -->
<!-- $$ -->

<!-- ::: -->

<!-- :::statbox -->
<!-- __P-value:__  -->

<!-- Our $p$-value will be computed from the $t$-distribution with $n-1$ degrees of freedom.   -->

<!-- If the alternative hypothesis is $H_1: \mu \neq \mu_0$, then we are asking about the probability of a test statistic at least as extreme _in either direction._   -->
<!-- So we would use:   -->
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- # abs() takes the absolute value, ensuring we get the smaller tail -->
<!-- 2 * pt(abs(tstatistic), df = n-1, lower.tail = FALSE) -->
<!-- ``` -->

<!-- If $H_1: \mu > \mu_0$, then this would be  -->
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- pt(tstatistic, df = n-1, lower.tail = FALSE) -->
<!-- ``` -->
<!-- And if $H_1: \mu < \mu_0$:   -->
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- pt(tstatistic, df = n-1, lower.tail = TRUE) -->
<!-- ``` -->

<!-- ::: -->

<!-- :::statbox -->
<!-- __Assumptions:__  -->

<!-- - The data are continuous (not discrete) -->
<!-- - The data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way) -->
<!-- - The data are normally distributed _OR_ the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed -->

<!-- ::: -->

```{r}
#| eval: false
#| echo: false
set.seed(993)
wpmtime<-tibble(
  id = paste0("ppt_",1:50),
  wpm = round(rnorm(50, 259, 30))
)
# write_csv(wpmtime, "../../data/usmr_tread.csv")
```

:::frame

> **Research Question:**  Do people read more than 250 words per minute? 

Fifty participants were recruited and tasked with reading a passage of text that was 2000 words long. Their reading times (in words per minute) was recorded, and these are accessible at [https://uoepsy.github.io/data/usmr_tread.csv](https://uoepsy.github.io/data/usmr_tread.csv).  

```{r}
wpmtime <- read_csv("https://uoepsy.github.io/data/usmr_tread.csv")
head(wpmtime)
```
:::

`r optbegin("The quick and easy way", olabel=F)`

Below are some quick descriptives, and we should also make sure that our distribution is roughly or less normally distributed: 
```{r}
mean(wpmtime$wpm)
sd(wpmtime$wpm)
hist(wpmtime$wpm)
shapiro.test(wpmtime$wpm)
```

Paying careful attention to the research question ("Do people read more than 250 words per minute?"), our null hypothesis here is that reading time is $\leq 250$ words per minute (wpm), and our alternative hypothesis is that it is $>250$ wpm.  

We specify the alternative in the `t.test()` function:  

```{r}
t.test(wpmtime$wpm, mu = 250, alternative = "greater")
```
:::int
A one-sample t-test was conducted in order to determine if the average reading time was significantly ($\alpha=.05$) higher than 250 words per minute (wpm).  
The sample of 50 participants read on average at 258 words per minute (Mean=`r round(mean(wpmtime$wpm))`, SD=`r round(sd(wpmtime$wpm))`). This was significantly above 250 ($t(49)=1.84, p = .036$, one-tailed).
:::

`r optend()`

`r optbegin("Manually", olabel=F)`

Our test-statistic is calculated as 
$$
t =  \frac{\bar x - \mu_0}{\frac{s}{\sqrt{n}}}
$$

There's a lot of brackets here, so go through it piece by piece if you are unsure of how it matches to the formula above  
```{r}
(mean(wpmtime$wpm) - 250 ) / (sd(wpmtime$wpm) / sqrt(nrow(wpmtime)))
```

The test we are performing is against the null hypothesis that $\mu_0 \leq 250$. So we will only reject the null hypothesis if we get a test statistic indicating the mean is $>250$. This means that our p-value will be just the one tail of the $t$-distribution:  

```{r}
pt(1.842338, df = 49, lower.tail = FALSE)
```

`r optend()`

## Two sample t-test  

:::statbox
__Purpose__  
The two sample t-test is used to test whether the mean of one group is different from/greater than/less than the mean of another.  


- __Examples:__
  - Is the mean age of cat people different from the mean age of dog people? 
  - Do people who identify as "morning people" have a higher average rating of sleep quality than those who identify as "evening people"?
  - Is the average reaction time different between people who do and don't drink caffeinated drinks?  

__Assumptions:__  

- The data are continuous (not discrete)
- The data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)
- The data are normally distributed _for each group_, OR the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed
- The variance is equal across groups*. 

*We can relax this assumption by using an adjusted test called the "Welch $t$-test", which calculates the standard error slightly differently, and estimates the degrees of freedom differently too. This is actually the default in R, and we change this easily in R using `t.test(...., var.equal = FALSE/TRUE)`  

:::

<!-- :::statbox -->
<!-- __Hypotheses:__    -->

<!-- Our hypotheses can be stated in terms of 2 population means, $\mu_1$ and $\mu_2$, representing the mean of each group.  -->
<!-- The null hypothesis is typically that the difference between the two means is zero:   -->

<!-- $H_0: \mu_1 - \mu_2 = 0$  -->

<!-- And our alternative hypothesis again can take on the following forms:     -->

<!-- $H_1: \mu_1 - \mu_2 \neq 0$   -->
<!-- $H_1: \mu_1 - \mu_2 > 0$   -->
<!-- $H_1: \mu_1 - \mu_2 < 0$   -->


<!-- ::: -->

```{r}
#| eval: false
#| echo: false
set.seed(243)
tcaff <- tibble(
  rt = c(rnorm(60, 447, 100), rnorm(40, 420, 100)),
  caff = c(rep("yes",60),rep("no",40))
)
tcaff <- sample_n(tcaff, size = n())
#var.test(rt~caff,data=tcaff)
#t.test(rt~caff,data=tcaff)
#write_csv(tcaff, "../../data/usmr_tcaff.csv")
```


:::frame

> **Research Question:**  Is the average reaction time different between people who do and don't drink caffeinated drinks? 

One hundred participants were recruited and completed a simple reaction time task. They were also surveyed on whether they regularly drank caffeine in any form. The data are accessible at [https://uoepsy.github.io/data/usmr_tcaff.csv](https://uoepsy.github.io/data/usmr_tcaff.csv).  

```{r}
tcaff <- read_csv("https://uoepsy.github.io/data/usmr_tcaff.csv")
head(tcaff)
```
:::


`r optbegin("The quick and easy way", olabel=F)`

First some quick descriptives, and a plot: 
```{r}
tcaff %>% 
  group_by(caff) %>%
  summarise(
    m = mean(rt),
    s = sd(rt)
  )
```
```{r}
#| fig-height: 3
ggplot(tcaff, aes(x = rt)) +
  geom_histogram() + 
  facet_wrap(~caff)
```
The data look fairly close to normally distributed for each group here. One thing to note is that the variances look like they may be different between the two groups. The caffeine drinkers' reaction time's have a standard deviation of 109ms, and the non-caffeine drinkers have an sd of only 89ms. 

However, we'll perform the Welch t-test here, which doesn't require us to assume equal variances. 

We can give R the two sets of data in two ways. 
Either by extracting the relevant entries:  
```{r}
#| eval: false
t.test(x = tcaff$rt[tcaff$caff=="no"], 
       y = tcaff$rt[tcaff$caff=="yes"])
```
Or using the **formula** notation, with the `~` ("tilde") symbol. In R, you can interpret `y ~ x` as "y is modeled as a function of x". By splitting the numeric values (`rt` variable) by the categories of the `caff` variable, we can conduct a $t$-test using:  
```{r}
t.test(rt ~ caff, data = tcaff)
```

:::int

An Welch two sample t-test was used to assess whether the mean reaction time of people who regularly drink caffeine ($n = 60$) was different to that of people who do not ($n=40$). There was a significant difference in average reaction time between the caffeine (Mean=`r round(mean(tcaff$rt[tcaff$caff=="yes"]))`; SD=`r round(sd(tcaff$rt[tcaff$caff=="yes"]))`) and
non-caffeine (Mean=`r round(mean(tcaff$rt[tcaff$caff=="no"]))`; SD=`r round(sd(tcaff$rt[tcaff$caff=="no"]))`) groups ($t(94)=-2.85, p = .005$, two-tailed). Therefore, we reject the null hypothesis that there is no difference in reaction times between caffeine drinkers and non-caffeine drinkers.   

```{r}
#| fig-height: 3
#| code-fold: true
ggplot(tcaff, aes(x = caff, y = rt)) +
  geom_boxplot()+
  labs(x="drinks caffeine",y="reaction time (ms)")
```
:::

`r optend()`

`r optbegin("Manually", olabel=F)`

Our test statistic here is:  
$$
\begin{align}
& t =  \frac{\bar x_1 - \bar x_2}{SE}\\
\ \\
& \text{where:} \\
& \bar x_1 : \text{sample mean group 1} \\
& \bar x_2 : \text{sample mean group 2} \\
& SE : \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}} \\
& s_1 : \text{sample standard deviation of group 1} \\
& s_2 : \text{sample standard deviation of group 2} \\
& n_1 : \text{sample size group 1} \\
& n_2 : \text{sample size group 2} \\
\end{align}
$$
We can calculate each part:
```{r}
tcaff %>%
  group_by(caff) %>%
  summarise(
    xbar = mean(rt),
    s = sd(rt),
    s2 = var(rt),
    n = n()
  )
```
plugging these bits in gives us: 
$$
\begin{align}
SE & = \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}} \\
& = \sqrt{\frac{7906}{40} + \frac{11892}{60}} \\
& = \sqrt{395.85} \\
& = 19.9
\end{align}
$$
and
$$
\begin{align}
& t =  \frac{\bar x_1 - \bar x_2}{SE} \\
\qquad \\
& =  \frac{408 - 465}{19.9} \\
\qquad \\
& = -2.86 \\
\end{align}
$$
Our $p$-value is determined against a $t$-distribution with a specific number of degrees of freedom. We are estimating two means here, the standard two-sample t-test uses $df = n-2$. However, the Welch t-test, which we performed quickly with `t.test()`, where we didn't assume equal variances, makes the calculation of the degrees of freedom much more complicated. 

Using the same degrees of freedom as was used in the quick use of `t.test()` above, we get out our same p-value (or thereabouts - we have some rounding error):  
```{r}
2*pt(abs(-2.86), df = 93.971, lower.tail = FALSE)
```

`r optend()`



## Paired sample t-test  

:::statbox
__Purpose__  

The paired sample t-test is used to test whether the mean difference between two sets of _paired_ observations is different from 0. 

- __Examples:__
  - Is the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?  
  - Are scores on test 1 different on average from scores on test 2 (all participants complete both tests).
  
__Assumptions:__

- The data are continuous (not discrete)
- The data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)
- The _differences_ are normally distributed _OR_ the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed
  
:::

:::frame

> **Research Question:** Is the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?  

Addenbrookeâ€™s Cognitive Examination-III (ACE-III) is a brief cognitive test that assesses five cognitive domains: attention, memory, verbal fluency, language and visuospatial abilities. The total score is 100 with higher scores indicating better cognitive functioning. A research project is examining changes in cognitive functioning with age, and administers the ACE-III to a set of participants at age 60, then again at age 70. The data is accessible at [https://uoepsy.github.io/data/usmr_tcaff.csv](https://uoepsy.github.io/data/usmr_tcaff.csv).  

```{r}
acedata <- read_csv("https://uoepsy.github.io/data/acedata.csv")
head(acedata)
```
:::


`r optbegin("The paired t test is the one sample t test in disguise", olabel=F)`
We can either perform this with the data exactly as it is: 

```{r}
t.test(x = acedata$ace_60, y = acedata$ace_70, 
       paired = TRUE)
```

Or we can compute the differences, and perform a one sample test on the mean of those differences being different from 0.  
It's just the same result:  
```{r}
acedata <- acedata %>%
  mutate(diff_score = ace_60 - ace_70)

t.test(acedata$diff_score, mu = 0)
```

`r optend()`
