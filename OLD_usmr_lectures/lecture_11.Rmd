---
title: "<b>Week 11: Some Kind of End to the Course</b>"
subtitle: "Univariate Statistics and Methodology using R"
author: ""
institute: "Department of Psychology<br/>The University of Edinburgh"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - xaringan-themer.css
      - mc_libs/tweaks.css
    nature:
      beforeInit: "mc_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
library(knitr)
library(tidyverse)
library(ggplot2)
library(patchwork)
source('R/pres_theme.R')
knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
source('R/myfuncs.R')

library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.8rem"
)
```

class: inverse, center, middle
# Part 1
## Week 7 - 10 Recap

---
# Describing a pattern with a line

```{r include=FALSE}
set.seed(913)
df <- tibble(
  x1 = round(pmax(0,rnorm(50,4,2)),1),
  y = 8 + .6*x1 + rnorm(50,0,4)
)

```


.pull-left[
![](lecture_11_files/figure-html/lm1-1.svg)
]
.pull-right[
```{r eval=FALSE}
ggplot(df, aes(x = x1, y = y)) + 
  geom_point()
```
```{r lm1,fig.asp=.8,fig.show='hide', echo = FALSE}
ggplot(df, aes(x = x1, y = y)) + 
  geom_point(size=3) +
  geom_smooth(method=lm, se = FALSE, fullrange = TRUE) +
  scale_x_continuous(limits=c(0,9),breaks=0:9)
```
]
???
- started by looking for a pattern
- used a line to capture this

---
# Defining a line

.pull-left[
```{r echo=FALSE, fig.asp=.8}
m = lm(y~x1,df)
ggplot(df, aes(x = x1, y = y)) + 
  geom_point(size=3) +
  geom_smooth(method=lm, se = FALSE, fullrange = TRUE) +
  scale_x_continuous(limits=c(0,9),breaks=0:9) +
  geom_segment(x=0,xend=0,y=0,yend=coef(m)[1],col="blue") +
  geom_segment(x=0,xend=1,y=coef(m)[1],yend=coef(m)[1],col="blue")+
  geom_segment(x=1,xend=1,y=coef(m)[1],yend=sum(coef(m)),col="blue")+
  geom_segment(x=1,xend=2,y=sum(coef(m)),yend=sum(coef(m)),col="blue")+
  geom_segment(x=2,xend=2,y=sum(coef(m)),yend=sum(coef(m))+coef(m)[2],col="blue")
  
```
]
.pull-right[

$$\Large y_i = b_0 + b_1(x_{1i}) + \varepsilon_i$$

A line can be defined by two values:  

  - A starting point (Intercept)
  - A slope ($y$ across $x1$)

Fitting a linear model to some data provides coefficient estimates $\hat{b}_0$ and $\hat{b}_1$ that minimise $\sigma_\varepsilon$.  

]

???
- need to think about how we define the line
- two values. intercept and slope
- the values are found such that they minimise the vertical distances from each point to the line
  - [SIDE note] this can be done algebraically - unlike we do for GLM

---
# Testing the coefficients (1)

.pull-left[
```{r echo=FALSE, fig.asp=.8}
m = lm(y~x1,df)
sims = as.data.frame(MASS::mvrnorm(2e3, mu=c(0,0),Sigma=vcov(m)))
names(sims) = c("i","s")

p <- ggplot(df, aes(x = x1, y = y)) + 
  geom_point(size=3) +
  geom_abline(data=sims, aes(intercept=i,slope=s),alpha=.05) +
  geom_smooth(method=lm, se = FALSE, fullrange = TRUE) +
  scale_x_continuous(limits=c(0,9),breaks=0:9)+
  ylim(-5,17)
p
```
]
.pull-right[
$$\Large \hat{y} = \color{red}{\hat{b}_0} + \hat{b}_1(x_{1})$$

In the "null universe" where $b_0 = 0$, when sampling this many people, what is the probability that we will find an intercept at least as extreme as the one we _have_ found?  

```{r echo=FALSE,highlight.output=3}
.pp(summary(m),l=list(9:12))
```
]

???
- when we have our coefs, we can test them
- and for each one, we're testing whether it is unlikely to have seen this value, if we are living in the "null universe" where in the population it is zero
- [DESCRIBE PLOT]



---
# Testing the coefficients (2)

.pull-left[
```{r echo=FALSE, fig.asp=.8}
p
```
]
.pull-right[
$$\Large \hat{y} = \hat{b}_0 + \color{red}{\hat{b}_1}(x_{1})$$

In the "null universe" where $b_1 = 0$, when sampling this many people, what is the probability that we will find a relationship at least as extreme as the one we _have_ found?  

```{r echo=FALSE,highlight.output=4}
.pp(summary(m),l=list(9:12))
```
]
???
- same applies to all coefs. tested against whether they're zero
- "have to assert that universe is governed by chance, because we don't have special access to the rules, so it's the only one we can model mathematically"  


---
# Coefficient Sampling Variability

.pull-left[
```{r echo=FALSE, fig.asp=.8}
m = lm(y~x1,df)

ggplot(df, aes(x = x1, y = y)) + 
  geom_point(size=3) +
  geom_smooth(method=lm, se = TRUE, fullrange = TRUE) +
  scale_x_continuous(limits=c(0,9),breaks=0:9)
```
]
.pull-right[
$$\Large \hat{y} = \hat{b}_0 + \hat{b}_1(x_{1})$$

Plausible range of values for $b_0$ and $b_1$: 
```{r eval=FALSE}
confint(model)
```
```{r echo=FALSE}
car::Confint(m)
```
]

???
- can also use this idea of sampling variabilty 
  - "what i might get if i sampled a different n people and fitted this model to them"
- to get some confidence intervals 

---
# Testing the model

```{r echo=FALSE,fig.asp=.5}
broom::augment(m) %>%
ggplot(., aes(x = x1, y = y)) + 
  geom_point(size=2) +
  #geom_smooth(method=lm, se = FALSE, fullrange = TRUE) +
  geom_abline(intercept=mean(df$y),slope=0,col="blue",lwd=1)+
  geom_segment(aes(x=x1,xend=x1,y=mean(df$y),yend=y),lty="dotted",lwd=1,col="red") +
  labs(title="SS Total") +

broom::augment(m) %>%
ggplot(., aes(x = x1, y = y)) + 
  geom_point(size=2) +
  geom_smooth(method=lm, se = FALSE, fullrange = TRUE) +
  geom_abline(intercept=mean(df$y),slope=0,col="blue",lwd=1)+
  geom_segment(aes(x=x1,xend=x1,y=mean(df$y),yend=.fitted),lty="dotted",lwd=1,col="red")+labs(title="SS Model") +
  
broom::augment(m) %>%
ggplot(., aes(x = x1, y = y)) + 
  geom_point(size=2) +
  geom_smooth(method=lm, se = FALSE, fullrange = TRUE) +
  geom_segment(aes(x=x1,xend=x1,y=.fitted,yend=y),lty="dotted",lwd=1,col="red")+
  labs(title="SS Residual")
```

.pull-left[
$$
\small 
\begin{align}
& F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
& df_{model} = \text{nr predictors} \\
& df_{residual} = \text{sample size} - \text{nr predictors} - 1 \\
\end{align}
$$
]
.pull-right[
```{r highlight.output=c(3), echo=FALSE}
.pp(summary(m),l=list(c(1,17,18)))
```
]

???
- another way we can test things is by testing the model overall.
- done by looking at the sums of squares.
  - total = all variance in y
  - model = the part of total that is captured by model
  - residual = the part of total that is _not_ captured by model



---
# More predictors

```{r echo=FALSE}
set.seed(913)
df <- tibble(
  x1 = round(pmax(0,rnorm(50,4,2)),1),
  x2 = round(pmax(0,rnorm(50,4,2)),1),
  y = 8 + .6*x1 + .5*x2 + rnorm(50,0,4)
)
```

.pull-left[
```{r echo=FALSE,fig.asp=.6}
ggplot(df,aes(x=x1,y=y)) +
  geom_point(size=3) + 
  geom_smooth(method=lm,se=F)
```
]
.pull-right[ 
```{r echo=FALSE,fig.asp=.6}
ggplot(df,aes(x=x2,y=y)) +
  geom_point(size=3) + 
  geom_smooth(method=lm,se=F)
```
]

???
- what if we think other things are relevant?  
- y is associaed with x1 and it's associated with x2

---
# More predictors (2)

<br><br>

$$\Large y_i = b_0 + b_1(x_{1i}) + b_2(x_{2i}) + \varepsilon_i$$

- A starting point (Intercept)
- A slope (across $x_1$)
- _Another_ slope (across $x_2$)

Coefficient estimates $\hat{b}_0$, $\hat{b}_1$, $\hat{b}_2$ minimise $\sigma_\varepsilon$.  

???
- we can just add this in. 
- model now has
  - starting point
  - slope across x1
  - _another_ slope (across x2)
- so it's not just a line anymore

---
# More predictors (3)

```{r echo=FALSE, fig.asp=.9}
modl2 <- lm(y~x1+x2,df)

x1_pred <- df |> with(seq(min(x1),max(x1),length.out=100))
x2_pred <- df |> with(seq(min(x2),max(x2),length.out=100))

ac <- expand.grid(x1=x1_pred,x2=x2_pred) 

ypred <- matrix(predict(modl2,type="response",
                             newdata=ac),nrow=100)

library(plot3D)

persp3D(x=x1_pred,y=x2_pred,z=ypred,phi=25,
        type="surface",xlab="x1",ylab="x2",zlab="y",
        zlim=c(min(df$y),max(df$y)),colkey=FALSE)
points3D(x=df$x1,y=df$x2,z=df$y,col="black",add=TRUE)
```

???
- it's a surface
- [PLOT] b0, b1, b2

---
# _Even_ more predictors...  

<br><br>

$$\Large y_i = b_0 + b_1(x_{1i}) + b_2(x_{2i}) + \, ... \, + b_k(x_{ki}) + \varepsilon_i$$
- A starting point (Intercept)
- A slope (across $x_1$)
- A slope (across $x_2$)
- ...
- ...
- A slope (across $x_k$)

???
- we can add in many many more
- the logic stays the same, it is a slope across values of each predictor
- but we can't visualise it because we can only see in 3d
- in all these models, we have just a single value for the slope
- slopes are the same no matter where you are
- slope value represents change moving up 1 in x1, provided we don't also change in x2. - [PREV SURF SLIDE]

---
# associations that depend on other things

```{r echo=FALSE, fig.asp=.6}
set.seed(913)
df <- tibble(
  x1 = round(pmax(0,rnorm(50,4,2)),1),
  x2 = round(pmax(0,rnorm(50,4,2)),1),
  y = 8 + .6*x1 + .5*x2 + .8*x1*x2 + rnorm(50,0,4),
  yb = rbinom(50,1,plogis(scale(y)))
)

df %>% 
  mutate(x2c=cut_interval(x2,3,labels=c("0<x2<3","3<x2<6","6<x2<9"))) %>%
  ggplot(., aes(x=x1,y=y)) +
  geom_point(size=3) + 
  geom_smooth(method=lm,se=F)+
  facet_wrap(~x2c) -> p1
  
df %>% 
  mutate(x1c=cut_interval(x1,3,labels=c("0<x1<3","3<x1<6","6<x1<9"))) %>%
  ggplot(., aes(x=x2,y=y)) +
  geom_point(size=3) + 
  geom_smooth(method=lm,se=F)+
  facet_wrap(~x1c) -> p2
```

.pull-left[
```{r echo=FALSE}
p1
```
]
.pull-right[
```{r echo=FALSE}
p2
```
]

???
- we might not want that
- [SLIDE]

---
# interactions

<br><br>

$$\Large y_i = b_0 + b_1(x_{1i}) + b_2(x_{2i}) + b_3(x_{1i}\cdot x_{2i}) + \varepsilon_i$$
- starting point (Intercept)
- A slope (across $x_1$)
- A slope (across $x_2$)
- ...
- How slope across $x_1$ changes across $x_2$

???
- we can include in model, an interaction term
- how much the slope across x1 changes as we move up x2


---
# interactions

.pull-left[
```{r echo=FALSE, fig.asp=.9}
modl2 <- lm(y~x1*x2,df)

x1_pred <- df |> with(seq(min(x1),max(x1),length.out=100))
x2_pred <- df |> with(seq(min(x2),max(x2),length.out=100))

ac <- expand.grid(x1=x1_pred,x2=x2_pred) 

ypred <- matrix(predict(modl2,type="response",
                             newdata=ac),nrow=100)

library(plot3D)

persp3D(x=x1_pred,y=x2_pred,z=ypred,theta=65,phi=15,
        type="surface",xlab="x1",ylab="x2",zlab="y",
        xlim=c(0,9), ylim=c(0,8),
        zlim=c(min(df$y),max(df$y)+2),colkey=FALSE)
points3D(x=df$x1,y=df$x2,z=df$y,col="black",add=TRUE)
```
]
.pull-right[

]
???
- and we're getting a twisting surface
- the amount it twists is contant


---
# interactions (2)

.pull-left[

```{r echo=FALSE, fig.asp=.9}
modl2 <- lm(y~x1*x2,df)

x1_pred <- df |> with(seq(min(x1),max(x1),length.out=100))
x2_pred <- df |> with(seq(min(x2),max(x2),by=1))

library(plot3D)
lines3D(x=x1_pred,y=rep(1,100),theta=65,phi=15,
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=1)),
        xlab="x1",ylab="x2",zlab="y",
        xlim=c(0,9), ylim=c(0,8),
        zlim=c(min(df$y),max(df$y)+2),colkey=FALSE)
lines3D(x=x1_pred,y=rep(2,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=2)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(3,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=3)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(4,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=4)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(5,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=5)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(6,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=6)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(7,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=7)),
        colkey=FALSE,add=TRUE)
```
]
.pull-right[

```{r echo=FALSE, fig.asp=.9}
modl2 <- lm(y~x1*x2,df)

x1_pred <- df |> with(seq(min(x1),max(x1),by=1))
x2_pred <- df |> with(seq(min(x2),max(x2),length.out=100))

library(plot3D)
lines3D(x=rep(1,100),y=x2_pred,theta=65,phi=15,
        z=predict(modl2, newdata=tibble(x1=1,x2=x2_pred)),
        xlab="x1",ylab="x2",zlab="y",
        xlim=c(0,9), ylim=c(0,8),
        zlim=c(min(df$y),max(df$y)+2),colkey=FALSE)
lines3D(x=rep(2,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=2,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(3,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=3,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(4,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=4,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(5,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=5,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(6,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=6,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(7,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=7,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
```
]
???
- [SLIDE]
- every one x2 we go up, the slope of x1 changes by some amount b3
- every one x1 we go up, the slope of x2 changes by some amount b3

---
# other outcomes

.pull-left[

$$ln(\frac{p}{1-p}) = b_0 + b_1(x_{1i}) + b_2(x_{2i})$$

```{r echo=FALSE}
set.seed(913)
df <- tibble(
  x1 = round(pmax(0,rnorm(50,4,2)),1),
  x2 = round(pmax(0,rnorm(50,4,2)),1),
  y = 8 + .6*x1 + .5*x2 + rnorm(50,0,4),
  yb = rbinom(50,1,plogis(scale(y)))
)

modl2 <- glm(yb~x1+x2,df,family=binomial)

x1_pred <- df |> with(seq(min(x1),max(x1),length.out=100))
x2_pred <- df |> with(seq(min(x2),max(x2),length.out=100))

ac <- expand.grid(x1=x1_pred,x2=x2_pred) 

ylo <- matrix(predict(modl2,type="link",
                             newdata=ac),nrow=100)

ypred <- matrix(predict(modl2,type="response",
                             newdata=ac),nrow=100)
#ypred <- exp(ylo)

df$ylo = ifelse(df$yb==1,max(ylo)+.2,min(ylo)-1)

library(plot3D)
persp3D(x=x1_pred,y=x2_pred,z=ylo,theta=35,phi=15,
        type="surface",xlab="x1",ylab="x2",zlab="log(p/(1-p))",colkey=FALSE)
points3D(x=df$x1,y=df$x2,z=df$ylo,col="grey",add=TRUE)
```
]
.pull-right[

$$
ln(\frac{p}{1-p}) \, \Rightarrow \, \frac{p}{1-p} \, \Rightarrow \, p
$$

```{r echo=FALSE}
persp3D(x=x1_pred,y=x2_pred,z=ypred,theta=35,phi=15,
        type="surface",xlab="x1",ylab="x2",zlab="probability",
        zlim=c(0,1),colkey=FALSE)
points3D(x=df$x1,y=df$x2,z=df$yb,col="black",add=TRUE)
```
]
???
- finally we looked last week at how we can use linear models to model things that aren't linear
- little trickery by modelling logodds rather than probability, allows us to fit models in one world
- that map on to our observed world
- SIDENOTE - we talked about how the fitting of these models can't be done with the minimising residuals anymore. 
  - it's done by searching possible models in LOGODDs world that maximise probability of seeing data in OBSERVED world. 

---
# Checking Assumptions:  Linear Models

.pull-left[
### required

- linearity of relationship

- for the _residuals_:
  + normality
  + homogeneity of variance
  + independence
]
.pull-right[
### desirable
- uncorrelated predictors

- no "bad" (overly influential) observations
]

???
- a note on assumptions here
- to generalise from our linear models and make conclusions about the world 
  - (e.g. conclusions like "we have reason to reject the idea that we are living in the null universe")
- 


---
# Checking Assumptions:  Logit Models

.pull-left[
### required

- linearity of relationship .red[between IVs and log-odds]

- for the _residuals_:
  + .red[~~normality~~]
  + .red[~~homogeneity of variance~~]
  + independence
]
.pull-right[
### desirable
- uncorrelated predictors

- no "bad" (overly influential) observations

- .red[large samples (due to maximum likelihood fitting)]
]

???
- for logistic models, we don't have all of these anymore. 
- but we do require larger samples, because of how we're fitting the models 


---
class: inverse, center, middle, animated, flipInY
# End of Part 1

---
class: inverse, center, middle
# Part 2
## Common Tests as linear models

```{r}
usmr <- read_csv("https://uoepsy.github.io/data/usmr2022.csv")
```

???
- like to just draw some quick comparisons between our linear models,
- and the tests we were working with in the first block


---
# lm vs correlation  

```{r echo=FALSE}
usmr$catdog <- factor(usmr$catdog)
```


.pull-left[
#### regression, continuous predictor
```{r}
summary(lm(sleeprating ~ height, data = usmr))
```
]
.pull-right[
#### Correlation
```{r}
cor.test(usmr$height, usmr$sleeprating)
```
]
???
- correlation was a way of measuring the strength of the relationshpi beteen two continuous variables
- we actually started talking about lm by building on this. correlation is going to map to the line we fit

---
# lm vs t.test

.pull-left[
#### regression, intercept
```{r}
summary(lm(height ~ 1, data = usmr))
```
]
.pull-right[
#### one sample t.test
```{r}
t.test(usmr$height, mu=0)
```
]

???
- the one sample t test was how we ask "is the mean of y different from some number?" 
- in the lm world. predict y from one number - this will be the mean. 
  - test if that number is different from 0

---
# lm vs t.test (2)

.pull-left[
#### regression, binary predictor
```{r}
summary(lm(height ~ catdog, data = usmr))
```
]
.pull-right[
#### two sample t.test
```{r}
t.test(height ~ catdog, data = usmr,
       var.equal = TRUE)
```
]

???
- two sample t test was how we ask "is the mean of y for this group different from the mean of y for that group?"
- we can phrase this as y predicted by the grouping
- the intercept will give us one group mean (reference level)
- slope gives us mean difference

- note var.equal = TRUE
- this is because the t.test has some alternatives that are a little more robust (Welch), which may make them more appropriate in some situations  



---
# lm vs Traditional ANOVA 

> If you should say to a mathematical statistician that you have discovered that linear multiple regression and the analysis of variance (and covariance) are identical systems, he would mutter something like "Of course&mdash;general linear model," and you might have trouble maintaining his attention.  If you should say this to a typical psychologist, you would be met with incredulity, or worse.  Yet it is true, and in its truth lie possibilities for more relevant and therefore more powerful research data.
.tr[
Cohen (1968)
]

???
- i'd like to talk briefly about the more traditional anova
- throughout the course, we've seen the `anova()` function to examine the variance explained by a predictor, or set of predictors
  - also phrased this as a model comparison, with and without predictors
- some of you may not have taken stats courses prior to this, so you might not have heard of "ANOVA" in the caps-lock. that's completely fine. 
- traditional anova is this same thing, only viewed from a different angle. 

---
# History

.pull-left[
.br3.pa2.bg-gray.white[
### .white[Multiple Regression]

- introduced c. 1900 in biological and behavioural sciences

- aligned to "natural variation" in observations

- tells us that means $(\bar{y})$ are related to groups $(g_1,g_2,\ldots,g_n)$
]]
.pull-right[
.br3.pa2.bg-gray.white[
### .white[ANOVA]

- introduced c. 1920 in agricultural research

- aligned to experimentation and manipulation

- tells us that groups $(g_1,g_2,\ldots,g_n)$ have different means $(\bar{y})$
]]

.pt2[
- both produce $F$-ratios, discussed in different language, but identical
]

???
- they come from different fields, but look at the same thing.
- the linear model is looking at how y changes across groups 
- the traditional ANOVA is looking at how groups have different means of y 

- same thing

- in lm, we fit lines
- in traditional anova, data is grouped, means are calculated, and residual and model SS are computed as "within" and "between" SS 



---
# lm vs Traditional ANOVA 

.pull-left[
#### regression, binary predictor
```{r eval=FALSE}
summary(lm(height ~ eyecolour, data = usmr))
```
```{r echo=FALSE}
.pp(summary(lm(height ~ eyecolour, data = usmr)),
    l=list(c(1,3,9:23)))
```
]
.pull-right[
#### anova
```{r}
summary(aov(height ~ eyecolour, data = usmr))
```
]
???
- if you want, you can do this in R
- note the F is the same as the lm


---
# Why Teach LM/Regression?

- LM has less restrictive assumptions

  + especially true for unbalanced designs/missing data
  
- LM is far better at dealing with covariates

  + can arbitrarily mix continuous and discrete predictors
  
- LM is the gateway to other powerful tools

  + mixed models and factor analysis (â†’ MSMR)

  + structural equation models

???
- so why do we go down the lm route?
- well, lm is much more capable for unbalanced designs
- we can have all sorts of covariates - continuous and discrete
- and it extends to all sorts of fun things like mixed models, factor analysis, and SEM

---
class: inverse, center, middle, animated, flipInY
# End

---
background-image: url(lecture_11_files/img/playmo_goodbye.jpg)
background-size: contain

# Goodbye!
