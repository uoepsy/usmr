[
  {
    "objectID": "01_ex.html",
    "href": "01_ex.html",
    "title": "Week 1 Exercises: Intro R",
    "section": "",
    "text": "Pet Data\n\nWe’re going to play with some data on a sample of licensed pets from the city of Seattle, USA. It can be downloaded (or read directly into R) from https://uoepsy.github.io/data/pets_seattle.csv. It contains information on the license ID, year of issue, as well as the species, breeds and weights of each pet. You can find a data dictionary in @tab-petdict\n\n\n\nSeattle Pets: Data dictionary \n\n\nVariable\nDescription\n\n\n\n\nlicense_year\nYear in which license was issued\n\n\nlicense_number\nUnique license ID number\n\n\nanimals_name\nFull name of pet\n\n\nspecies\nSpecies of pet\n\n\nprimary_breed\nPrimary breed of pet\n\n\nsecondary_breed\nSecondary breed of pet\n\n\nweight_kg\nWeight in kilograms\n\n\n\n\n\n\n\nQuestion 1\n\n\nWrite a line of code that reads in the data to your R session. Then examine the dimensions of the dataset, and take a look at the first few lines lines.\n\nHints: You’ll need the read.csv() function. Remember to assign it a name to store it in your environment. Reading 1B contains an example of reading in data from a URL. You’ll then want to play with functions like dim() and head().\n\n\n\n\n\nQuestion 2\n\n\nWhat are the names of the 47th and the 200th animals in the dataset?\n\nHints: You’ll probably want to make use of the square brackets data[rows, columns].\n\n\n\n\n\nQuestion 3\n\n\nSubset the data to only the data for the dogs, and store this object as another named object in your environment. Subset the data to only the data for the cats, and store this object as another named object in your environment.\n\nHints: You’ll want to think about how we access data via asking for those entries that meet a specific condition (see this section of Reading 1B)\n\n\n\n\n\nQuestion 4\n\n\nFind the name and weight of the heaviest cat, and of the lightest dog.\n\nHints: You could do this using the original data you read in from question 1, or use the subsets you created in question 3. You’ll again want to supply a condition within square brackets. That condition may well have something to do with being equal to the min() or the max() of some variable.\n\n\n\n\n\nQuestion 5\n\n\nDoes the data contain only dogs and cats?\n\nHints: Given what you did in question 3, you might be able to answer this by just looking at your environment.\n\n\n\n\n\nQuestion 6\n\n\nExtract the entries of the dataset for which the species is neither “Dog” nor “Cat”? What are the names and species of these animals?\n\nHints: This is a slightly complex one. The section on more complex conditions in Reading 1B might help you here.\n\n\n\n\n\nQuestion 7\n\n\nCreate a new variable in the data, which contains the weights of all the animals, but rounded to the nearest kg.\n\nHints: Try looking up the help documentation for the function round(). Try playing with it in the console, e.g. round(c(3.5, 4.257, 1.1111)). You may find it helpful to look back at Reading 1B/‘Adding/Changing a variable’.\n\n\n\n\n\nQuestion 8\n\n\nTry giving the dataset to the function summary(). You’ll get out some information on each of the variables. It is likely that you’ll get more useful information for the variables containing information on the animal’s weights than for those containing their names, breeds etc because these variables are vectors of “characters”. We’ll start to look more about different types of data next week.\n\n\n\n\n\n\n\n\nSimulating Dice\n\nQuestion 9\n\n\nCopy the code from the lecture which creates a custom function called dice() (copied below). Be sure to run the code (highlight it all with your cursor, and hit “run” in the top right, or press Ctrl/Cmd+Enter).\n\ndice <- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n\n\n\n\n\nWhat did that code do?\nIn a sense, this code does nothing: It won’t give you any output when you run it. What it is actually doing, though, is defining a function called dice(). If you look at your environment panel (top right), you’ll see dice appear when you run the code.\nTo produce some output, we have to call the function dice() (by writing it into code: dice(4), for example). dice() wants to be supplied with some information (in the argument num). If no information is supplied, num will take a default value of 1. (So writing dice() is equivalent to writing dice(1)).\nWhat does dice() do with num? It calls another function, sample(), with 3 arguments. We didn’t write sample(): it’s a function that’s “supplied with” R. To find out more about what sample() does:\n\nclick inside the brackets just after sample() in your R script;\npress TAB (⇥), then F1\nyou should see some help appear in the bottom right-hand panel of RStudio.\n\nYou will find that “sample() takes a sample … from the elements of x …” If you compare the code in RStudio to the code under “Usage” you’ll see that where the help has x, we have 1:6. So what does 1:6 mean? One way to find out is to open the console in RStudio (bottom left) and just type stuff in. What happens when you type 1:6? What about 2:17? (What about 6:1?)\nThe console is the place to “try stuff out” (don’t worry, you can’t break it). Watch the video below and then try it out yourself:\nTODO redo with script not rmd\n\n\n\n\n\nWhat you will discover is that 1:6 creates a vector (list of similar things, in this case numbers) of the numbers 1-6. The next bit of the sample() function is size. In the dice() function, the num passes down to the size of the sample(): Looking through the help, size is the number of items to choose. So sample(1:6, 1) would choose one number from the numbers 1-6 at random; sample(1:6, 3) would choose 3, and so on. The last argument, replace=TRUE, tells sample() what to do with a number once it’s been picked: Does it go ‘back into the bag’ to be picked again (TRUE) or not? (FALSE)?\nAround the outside is sum() which simply sums the numbers on however many (num) dice you “rolled”.\nPutting it all together, our dice() function “throws a die num times” by sample()ing from the numbers 1-6 num times, replaceing each number when it’s been picked, and sums the numbers of all the dice.\n\n\nQuestion 10\n\n\nLook up the function replicate(). We can use it to do something in R lots of times! For instance, replicate(20, 1+1) will evaluate 1+1 twenty times. Use replicate() to simulate 100 rolls of a single dice, and store the results in an object in your environment. Give it an easily identifiable name.\n\nHints: A single dice means num = 1\n\n\n\n\n\nQuestion 11\n\n\nCreate a barplot showing the frequency with which each number was landed on in the 100 rolls.\n\nHints: the functions table() and barplot() were used to do this in the lecture.\n\n\n\n\n\n\n\n\nQuestion 12\n\n\nDo the same for 1,000 rolls, and then for 10,000. What do you notice?\n\n\n\n\n\n\n\nQuestion 12\n\n\nCopy the code below into your script and run it. It creates a new function called wdice() which simulates the rolling of num dice which are slightly weighted. Roll a single weighted die 100 times and plot the frequency distribution. Do the same for 1,000 and 10,000 rolls of a single die. Does a pattern emerge? At how many rolls?\n\nwdice <- function(num = 1){\n    sum(sample(1:6, num, replace=TRUE, prob = c(0.15,0.15,0.15,0.15,0.15,0.25)))\n}\n\n\n\n\n\n\n\n\nQuestion 14\n\n\nRemember, wdice() and dice() are really just relying on different functions, like sample(). Try playing around with sample() in the console again - what does the prob = c(....) bit do?\n\n\n\n\nQuestion 15\n\n\nLet’s try to modify the wdice() function. Edit the code for wdice() so that 50% of the time it lands on number 6.\n\nHints: To test out your modified function, you will need to re-run the code which defines the function. When we use wdice() we use the function which is in our environment. If we want to edit the function, we need to overwrite (or “replace”/“reassign”) the object in our environment. We need to be careful to remember that the probability of different outcomes should sum to 1 (i.e., it’s not possible to “50% of the time land on 6” as well as “70% of the time land on 5”!).\n\n\n\n\n\n\n\n\nQuestion 16\n\n\nCan you observe the weighting in your new die (the one which 50% of the time lands on number 6) in only 100 rolls?\n\n\n\n\nQuestion 17\n\n\nConceptually, what can we learn from this toy example?"
  },
  {
    "objectID": "01a_R.html",
    "href": "01a_R.html",
    "title": "1A: A first look at R & RStudio",
    "section": "",
    "text": "R is a calculator\nWhen we first open RStudio, we should see something which looks more or less like the image in Figure 1, where there are several little windows. We are going to explore what each of these little windows offer by just diving in and starting to do things.\n\n\n\n\n\nFigure 1: RStudio, freshly opened\n\n\n\n\nStarting in the left-hand window, you’ll notice the blue sign >.\nThis is where we R code gets executed.\nType 2+2, and hit Enter ↵.\nYou should discover that R is a calculator - R responds by telling us the answer (4).\nLet’s work through some basic operations (adding, subtracting, etc). For instance, can you work out what R will give you for each of these operations?\n\n\nArithmetic operations\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n2 + 5\n\n\n\n10 - 4\n\n\n\n2 * 5\n\n\n\n10 - (2 * 5)\n\n\n\n(10 - 2) * 5\n\n\n\n10 / 2\n\n\n\n3^2\n(the ^ symbol is “to the power of”)\n\n\n\n\n\nShow me the output\n\n\n\nCode\nOutput\n\n\n\n\n2 + 5\n7\n\n\n10 - 4\n6\n\n\n2 * 5\n10\n\n\n10 - (2 * 5)\n0\n\n\n(10 - 2) * 5\n40\n\n\n10 / 2\n5\n\n\n3^2\n9(the ^ symbol is “to the power of”)\n\n\n\n\n\n\nR can get stuck\nWhenever you see the blue sign >, it means R is ready and waiting for you to provide a command.\nIf you type 10 + and press Enter, you’ll see that instead of > you are left with +. This means that R is waiting for more.\nEither give it more (finish the command), or cancel the command by pressing the Escape key on your keyboard.\n\nAs well as performing arithmetic calculations, we can ask R things for which the answer is TRUE or FALSE, such as “Is 3 less than 5?”. If we type 3 < 5 and press Enter, then R should tell us that the statement we gave it is TRUE.\nThese computations don’t return numbers, but instead return logical values. There are few operators that we need to learn about here:\n\nEquality/Inequality: We use the symbols == to mean “is equal to”, and the symbols != for “is not equal to”.\n\nLess Than/Greater Than: To determine whether a value is less/greater than another value, we have our typical symbols < and >. We also have <= and >= when we want to include “less/greater than or equal to”.\n\nWe can combine these with & for “and”, | for “or”, and ! for “not”, to ask R all sorts of things.\nTry and work out what R should give you for each of these (or try it out yourself!):\n\n\nLogical operations\n\n\n\nCode\nOutput\n\n\n\n\n3 > 5\n\n\n\n3 <= 5\n\n\n\n3 >= 3\n\n\n\n3 == 5\n\n\n\n(2 * 5) == 10\n\n\n\n(2 * 5) != 11\n\n\n\n(2 == 2) & (3 == 4)\n\n\n\n(2 == 2) | (3 == 4)\n\n\n\n(2 == 2) & !(3 == 4)\nTRUE\n\n\n\n\n\nShow me the output\n\n\n\nCode\nOutput\n\n\n\n\n3 > 5\nFALSE\n\n\n3 <= 5\nTRUE\n\n\n3 >= 3\nTRUE\n\n\n3 == 5\nFALSE\n\n\n(2 * 5) == 10\nTRUE\n\n\n(2 * 5) != 11\nTRUE\n\n\n(2 == 2) & (3 == 4)\nFALSE\n\n\n(2 == 2) | (3 == 4)\nTRUE\n\n\n(2 == 2) & !(3 == 4)\nTRUE\n\n\n\n\n\n\nFALSE and TRUE as 0 and 1\nIt will become useful to think of these logical values (TRUE and FALSE) as also having intrinsic numeric values of 0 and 1.\nThis is how R will treat them if you ask it to do something that requires the values to be numeric. For example, the code TRUE + 3 will return 4, and FALSE + 3 will return 3.\n\n\n\n\n\n\nR has a memory\nWe can also store things in R’s memory, and to do that we just need to give them a name. Type x <- 5 and press Enter.\nWhat has happened? We don’t get any answer like we did with calculations such as 2 + 4. What we’ve done is stored in R’s memory something named x which has the value 5. We can now refer to the name and it will give us the value!\n\nAssigning names to things in R\nThe <- symbol, pronounced arrow, is what we use to assign a value to a named object:\n\nname <- value\n\n\nIf we now type x and press Enter, it gives us whatever we assigned to the name “x”. So it gives us the number 5.\nWhat is going to happen when we type x * 3? It will give us 15!\nIf you are working along with us, you might have also noticed that something else happened when we executed the code x <- 5. The thing we named x with a value of 5 suddenly appeared in the top-right window. This is known as the environment (Figure 2), and it shows everything that we store in R.\n\n\n\n\n\nFigure 2: Assigning names to objects stores them in R’s environment.\n\n\n\n\nNote, there are a few rules about names in R:\n\nNo spaces - spaces inside a name are not allowed (the spaces around the <- don’t matter):\nlucky_number <- 5 ✔ lucky number <- 5 ❌\nNames must start with a letter:\nlucky_number <- 5 ✔ 1lucky_number <- 5 ❌\nCase sensitive:\nlucky_number is different from Lucky_Number\nhere is a set of words you can’t use as names, including: if, else, for, in, TRUE, FALSE, NULL, NA, NaN, function (Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these).\n\n\n\n\n\n\nThe Console and The Environment\nWhat we’ve done so far has made use of a couple of the different panes that we see in RStudio. When we’ve been executing R code (e.g. typing 2+5 or x<-5 and pressing Enter), we’ve been doing it in the console. The console is where all R code gets executed. However, as we’ll see below, it isn’t where all R code gets written.\nWe’ve also been learning about how we can store things in R’s memory (the environment) by assigning a name to them using the <- operator. The top-right pane of RStudio shows us the environment, where we can see everything that we have stored in R. Note that this also means we can keep track of what objects we have saved that are available for our use. If we never stored an object named “peppapig”, then R will give us an error when we do something like:\n\n2*peppapig\n\nError in eval(expr, envir, enclos): object 'peppapig' not found\n\n\nNow we have an idea of what the console and the environment are for. If you want a silly analogy, the console is like R’s mouth, where we feed it things, and the environment is just its memory, where it remembers what things are what. We can see these in Figure 3. Note however, that the console has been moved down to the bottom-left, as we are introducing a new pane above it. This is where we move to next.\n\n\n\n\n\nFigure 3: RStudio panes: Code is executed in the console, and objects are stored in the environment.\n\n\n\n\n\n\n\n\n\nR Scripts and the Editor\nWhat if we want to edit our code? Whatever we write in the console just disappears upwards. What if we want to change things we did earlier on?\nWhile the console is where code gets executed, it doesn’t have to be where code gets written.. We can write and edit our code in a separate place before we then send it to the console to be executed!!\nThe standard place to write and edit things is in an R scipt. We can open one by doing File > New File > R script, and a new file will open in the top-left pane of RStudio. The console will be shoved down to the bottom-left.\nIn the R script, we can write code. For instance, we might write:\n\nx <- 210\ny <- 15\nx / y\n\nNotice that nothing happens when we write each line. It’s not like writing in the console where R tells us the answers. This is because this code is not yet being executed. We haven’t actually fed it to R.\nThere are a couple of useful ways we can send the code to R.\n\nPosition your text-cursor (blinking vertical line) on the line of code we wish to run and press Ctrl+Enter (Windows) or Cmd+Enter (MacOS)\n\nPosition your text-cursor (blinking vertical line) on the line of code we wish to run and press the “Run” button in the top right of the script.\n\nWhen we do this, the line of code will be sent down to the console, where it will be executed, and R will do it’s thing.\nFor example, if we had sent the line x <- 210 down to the console, R would then store the number 210 with the name x in our environment (as in Figure 4). Additionally, it will move the text-cursor to the next line, so we can just press Ctrl+Enter again to run the next line (and so on.).\n\n\n\n\n\nFigure 4: Code written in the script can be sent down to the console, where it is executed. In this example, the execution of the code stores an object in the environment.\n\n\n\n\nBy writing our code in a file such as an R script before sending it to the console we can edit, save, and share our code. This makes it so much more useful than just using the console (which is more like writing on scratch paper, where we can’t keep anything).\nFor instance, let’s say we made a mistake earlier, and instead of “x” being 210, it should have been 211. Well, we can just edit the script, and re-run it.\n\nRegularly save your scripts.\nTo save an R script that is open, we just\n\nFile > Save (or Ctrl+S)\nLocate to the folder where we want to save the file.\n\ngive it an appropriate name, and click save.\n\nNOTE: When you save R script files, they terminate with a .R extension.\n\n\nLooking ahead to Rmarkdown\n\nIn addition to R scripts, there is another type of document we can create, known as “Rmarkdown”.\nRmarkdown documents combine the analytical power of R and the utility of a text-processor. We can have one document which contains all of our analysis as well as our written text, and can be compiled into a nicely formatted report. This saves us doing analysis in R and copying results across to Microsoft Word. It ensures our report accurately reflects our analysis. Everything that you’re reading now has all been written in Rmarkdown!\n\n\n\nFigure 5: An example RMarkdown document\n\n\nWe’re going to learn more about Rmarkdown documents and how to write them later on, but the broad idea is that we can writing normal text interspersed with “code-chunks” (i.e., chunks of code!). RMarkdown documents looks much like an R script, only the code is contained within the grey-boxes, and text is written in between (see Figure 5). RMarkdown documents can then be compiled to create a lovely .pdf, .html, or .docx file.\n\n\n\n\n\nFigure 6: RMarkdown Workflow\n\n\n\n\n\n\n\n\n\n\nThe Four RStudio Panes\nWe’ve now seen almost all the different panes in RStudio:\n\n\nThe console is where R code gets executed\nThe environment is R’s memory, you can assign something a name and store it here, and then refer to it by name in your code.\nThe editor is where you can write and edit R code in R scripts and Rmarkdown documents. You can then send this to the console for it to be executed.\n\n\n\n\n\n\n\n\nFigure 7: The Four Panes of RStudio\n\n\n\n\n\nWe are yet to use the bottom-right window, but this is an easy one to explain. It is where we can see any plots that we create, where we can browse our files, and where we can ask R for some help documentation. We’ll make more use of this later on, but for now try typing plot(x = 4, y = 2) into the console and seeing what happens.\n\n\nProjects and file organisation\nWe’re not going to speak too much about this here but one key thing to remember is that R is working from a specific place in your computer. You can find out where by typing getwd() into the console.\nAn easy way to keep things organised is to set up an “R project”. This basically associates a specific folder on your computer with your working in R, and it means it will automatically look for things in that folder.\nWe recommend that you start a project for this course (call it something like “usmr”). This will the be project that you open whenever you work on this course (RStudio will usually re-open the previous project you were working on when you closed it).\nWith that project open, we suggest that you start a new script for each week, in which you complete your exercises, and which you then remember to save!\nIf you haven’t already, we suggest you start an R project by using (in the top menu of RStudio), File > New Project and following the instructions. It will create a folder on your computer somewhere of your choosing, and you will now notice that if you click in the “Files” tab in the bottom right pane of RStudio, you can see the project folder!\n\n\n\n\n\n\nGood Habits\nAlong with regular saving of work and organising your files, it will be very useful in the long-run if we get used to always “starting fresh” when we open R.\nWe need to start thinking of the code that we write in an R script as a set of consecutive instructions that we can give to R in order to achieve our goal. It’s just a blank slate on which we write (in language R understands) “do this. now do this. now do this..” and so on.\nThis means that the script contains all the information needed.\nSo we can now:\n\nEmpty our environment\nRestart R\nRun all the code in our script (highlight multiple lines of code to run them all at once)\n\nand we’re back to where we are! This is great for when we make mistakes (we’re going to make many many mistakes!), because we can just clear everything, start at the top of our script, and work downwards to figure out what has gone wrong.\n\nTidying up\n\nTo empty our environment, we can click on the little broomstick icon .\nTo restart the R Session (not always necessary, but good practice) in the top menu, we choose Session > Restart R (or press Ctrl+Shift+F10).\n\n\nThe other very useful thing that we can do in a script is to write comments for ourselves or for others. By starting a line with a #, R will know that that entire line is not code, and so it won’t try to do anything with it. For instance, if we write these lines in our script, and send them both down to the console, nothing happens for the first line:\n\n\nComments\n\n# The line below will add 5 to 2. \n2+5\n\n[1] 7\n\n\n\n\nIf we forget the #\n\nThe line below will add 5 to 2. \n2+5\n\n\nError: unexpected symbol in “The line”\n\n\n\n\n\n\n\n\nUseful Settings\nBelow are a couple of our recommended settings for you to change as you begin your journey in R. After you’ve changed them, take a 5 minute break before moving on to the next reading.\n\n1. Clean environments\nAs you use R more, you will store lots of things with different names. Throughout this course alone, you’ll probably name hundreds of different things. This could quickly get messy within our project.\nWe can make it so that we have a clean environment each time you open RStudio. This will be really handy.\n\nIn the top menu, click Tools > Global Options…\nThen, untick the box for “Restore .RData into workspace at startup”, and change “Save workspace to .RData on exit” to Never:\n\n\n\n\n2. Wrapping code\nIn the editor, you might end up with a line of code which is really long, but you can make RStudio ‘wrap’ the line, so that you can see it all, without having to scroll:\n\nx <- 1+2+3+6+3+45+8467+356+8565+34+34+657+6756+456+456+54+3+78+3+3476+8+4+67+456+567+3+34575+45+2+6+9+5+6\n\n\nIn the top menu, click Tools > Global Options…\nIn the left menu of the box, click “Code”\nTick the box for “Soft-wrap R source files”"
  },
  {
    "objectID": "01b_data.html",
    "href": "01b_data.html",
    "title": "1B: More R",
    "section": "",
    "text": "The best way to learn R is to use it.\nTry following along with this reading by typing the code into your R script and running them. You will hopefully get the same output as is presented on this page below each bit of code.\nIf you get errors and warnings, don’t panic - read them!"
  },
  {
    "objectID": "01b_data.html#accessing-subsets-of-data",
    "href": "01b_data.html#accessing-subsets-of-data",
    "title": "1B: More R",
    "section": "Accessing subsets of data",
    "text": "Accessing subsets of data\nWhat if we want to extract certain subsections of our dataset, such as specific observational units or variables? This is where we learn about two important bits of R code used to access parts of data - the dollar sign $, and the square brackets [].\n\nThe dollar sign $\nThe dollar sign allows us to extract a specific variable from a dataframe. For instance, we can pull out the variable named “eye_color” in the data, by using $eye_color after the name that we gave our dataframe.\nRemember that each variable in a dataframe is a vector (a set of values). Once extracted, we will have a vector and not a dataframe.\n\nstarwars2$eye_color\n\n [1] \"blue\"          \"yellow\"        \"red\"           \"yellow\"       \n [5] \"brown\"         \"blue\"          \"blue\"          \"red\"          \n [9] \"brown\"         \"blue-gray\"     \"blue\"          \"blue\"         \n[13] \"blue\"          \"brown\"         \"black\"         \"orange\"       \n[17] \"hazel\"         \"blue\"          \"yellow\"        \"brown\"        \n[21] \"red\"           \"brown\"         \"blue\"          \"orange\"       \n[25] \"blue\"          \"brown\"         \"black\"         \"red\"          \n[29] \"blue\"          \"orange\"        \"orange\"        \"orange\"       \n[33] \"yellow\"        \"orange\"        NA              \"brown\"        \n[37] \"yellow\"        \"pink\"          \"hazel\"         \"yellow\"       \n[41] \"black\"         \"orange\"        \"brown\"         \"yellow\"       \n[45] \"black\"         \"brown\"         \"blue\"          \"orange\"       \n[49] \"yellow\"        \"black\"         \"blue\"          \"brown\"        \n[53] \"brown\"         \"blue\"          \"yellow\"        \"blue\"         \n[57] \"blue\"          \"brown\"         \"brown\"         \"brown\"        \n[61] \"brown\"         \"yellow\"        \"yellow\"        \"black\"        \n[65] \"black\"         \"blue\"          \"unknown\"       \"unknown\"      \n[69] \"gold\"          \"black\"         \"green, yellow\" \"blue\"         \n[73] \"brown\"         \"black\"         NA             \n\n\n\n\nThe square brackets []\nSquare brackets are used to do what is known as indexing (finding specific entries in your data).\nWe can retrieve bits of data by identifying the \\(i^{th}\\) entry(s) inside the square brackets, for instance:\n\n# assign the numbers 10, 20 ... 100 to the name \"somevalues\"\nsomevalues <- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n\n# pull out the 3rd entry\nsomevalues[3]\n\n[1] 30\n\n\nIn the above example, we have a vector (a single sequence of values), and so we can retrieve entries with the syntax:\n\nvector[entry]\n\n In a dataframe we have an extra dimension - we have rows and columns. Using square brackets with a dataframe needs us to specify both:\n\n\ndataframe[rows, columns]\n\n\nLet’s look at some examples:\n\n\nExamples of Indexing\n\nSpecifying row number and column number:\n\n\n# first row, fourth column\nstarwars2[1, 4]\n# tenth row, first column\nstarwars2[10, 1]\n\n\nIf we leave either rows or columns blank, then we will get out all of them:\n\n\n# tenth row, all columns\nstarwars2[10, ]\n# all rows, 2nd column\nstarwars2[ , 2]\n\n\nThere are is another way to identify column - we can use the name in quotation marks:\n\n\n# first row, \"species\" column\nstarwars2[1, \"species\"]\n\n\nWe can also ask for multiple rows, or multiple columns, or both! To do that, we use c():\n\n\n# the 1st AND the 6th rows, and the 1st AND 3rd columns\nstarwars2[c(1,6), c(1,3)] \n\n\nAnd we can specify a sequence using the colon, from:to:\n\n\n# FROM the 1st TO the 6th row, all columns\nstarwars2[1:6, ] \n\nWhy? Because the colon operator, `from:to`, creates a vector from the value     `from` to the value `to` in steps of 1.\n\n1:6\n\n[1] 1 2 3 4 5 6\n\n\n\nWe can even use the two accessors in combination:\n\n\n# extract the variable called \"name\" and show the 20th entry\nstarwars2$name[20]  \n\nThis represents the 20th name in the data.  \nNote: When we do this, we don't have the comma inside the square brackets. When we use the `$` to pull out a variable, such as `starwars2$name`, we no longer have a dataframe - `starwars2$name` doesn't have rows and columns, it just has a series of values - _it's a vector!_. So when you are using `[]` with a __vector__ (1 dimension) rather than a __dataframe__ (2 dimensions), you don't specify `[rows, columns]`, but simply `[entry]`. \n\n\nShow me the output\n\nSpecifying row number and column number:\n\n\n# first row, fourth column\nstarwars2[1, 4]\n\n[1] \"blue\"\n\n# tenth row, first column\nstarwars2[10, 1]\n\n[1] \"Obi-Wan Kenobi\"\n\n\n\nIf we leave either rows or columns blank, then we will get out all of them:\n\n\n# tenth row, all columns\nstarwars2[10, ]\n\n             name height    hair_color eye_color homeworld species\n10 Obi-Wan Kenobi    182 auburn, white blue-gray   Stewjon   Human\n\n# all rows, 2nd column\nstarwars2[ , 2]\n\n [1] 172 167  96 202 150 178 165  97 183 182 188 180 228 180 173 175 170 180 170\n[20] 183 190 177 175 180 150  88 160 191 170 196 224 206 137 112 170 163 175 180\n[39] 178  94 122 163 188 198 196 171 184 188 264 188 196 185 157 183 183 170 166\n[58] 165 193 191 183 168 198 229 213 167  79 193 191 178 216 234 188 206 180\n\n\n\nThere are is another way to identify column - we can use the name in quotation marks:\n\n\n# first row, \"species\" column\nstarwars2[1, \"species\"]\n\n[1] \"Human\"\n\n\n\nWe can also ask for multiple rows, or multiple columns, or both! To do that, we use c():\n\n\n# the 1st AND the 6th rows, and the 1st AND 3rd columns\nstarwars2[c(1,6), c(1,3)] \n\n            name  hair_color\n1 Luke Skywalker       blond\n6      Owen Lars brown, grey\n\n\n\nAnd we can specify a sequence using the colon, from:to:\n\n\n# FROM the 1st TO the 6th row, all columns\nstarwars2[1:6, ] \n\n            name height  hair_color eye_color homeworld species\n1 Luke Skywalker    172       blond      blue  Tatooine   Human\n2          C-3PO    167        <NA>    yellow  Tatooine   Human\n3          R2-D2     96        <NA>       red     Naboo   Droid\n4    Darth Vader    202        none    yellow  Tatooine   Human\n5    Leia Organa    150       brown     brown  Alderaan   Human\n6      Owen Lars    178 brown, grey      blue  Tatooine   Human\n\n\n\nWe can even use the two accessors in combination:\n\n\n# extract the variable called \"name\" and show the 20th entry\nstarwars2$name[20]  \n\n[1] \"Boba Fett\"\n\n\n\n\n\nThe dollar sign $\nUsed to extract a variable from a dataframe:\n\ndataframe$variable\n\nThe square brackets []\nUsed to extract parts of an R object by identifying rows and/or columns, or more generally, “entries”. Left blank will return all.\n\nvector[entries]\ndataframe[rows, columns]"
  },
  {
    "objectID": "01b_data.html#accessing-by-a-condition",
    "href": "01b_data.html#accessing-by-a-condition",
    "title": "1B: More R",
    "section": "Accessing by a condition",
    "text": "Accessing by a condition\nWe can also do something really useful, which is to access all the entries in the data for which a specific condition is true.\nLet’s take a simple example to start:\n\nsomevalues <- c(10, 10, 0, 20, 15, 40, 10, 40, 50, 35)\n\nTo only select values which are greater than 20, we can use:\n\nsomevalues[somevalues > 20]\n\n[1] 40 40 50 35\n\n\n\nUnpacking: somevalues[somevalues > 20]\n First, let’s look at what somevalues > 20 does. It returns TRUE for the entries of somevalues which are greater than 20, and FALSE for the entries of somevalues that are not (that is, which are less than, or equal to, 20.\nThis statement somevalues > 20 is called the condition.\n\nsomevalues > 20\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\nWe can give a name to this sequence of TRUEs and FALSEs\n\ncondition <- somevalues > 20\ncondition\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\nNow consider putting the sequence of TRUEs and FALSEs inside the square brackets in somevalues[]. This returns only the entries of somevalues for which the condition is TRUE.\n\nsomevalues[condition]\n\n[1] 40 40 50 35\n\n\nSo what we can do is use a condition inside the square brackets to return all the values for which that condition is TRUE.\nNote that you don’t have to always give a name to the condition. This works too:\n\nsomevalues[somevalues > 20]\n\n[1] 40 40 50 35\n\n\n\n We can extend this same logic to a dataframe. Let’s suppose we want to access all the entries in our Star Wars data who have the value “Droid” in the species variable. To work out how to do this, we first need a line of code which defines our condition - one which returns TRUE for each entry of the species variable which is “Droid”, and FALSE for those that are not “Droid”.\nWe can use the dollar sign to pull out the species variable:\n\nstarwars2$species\n\n [1] \"Human\"        \"Human\"        \"Droid\"        \"Human\"        \"Human\"       \n [6] \"Human\"        \"Human\"        \"Droid\"        \"Human\"        \"Human\"       \n[11] \"Human\"        \"Human\"        \"Wookiee\"      \"Human\"        \"Rodian\"      \n[16] \"Hutt\"         \"Human\"        \"Human\"        \"Human\"        \"Human\"       \n[21] \"Trandoshan\"   \"Human\"        \"Human\"        \"Mon Calamari\" \"Human\"       \n[26] \"Ewok\"         \"Sullustan\"    \"Neimodian\"    \"Human\"        \"Gungan\"      \n[31] \"Gungan\"       \"Gungan\"       \"Toydarian\"    \"Dug\"          \"unknown\"     \n[36] \"Human\"        \"Zabrak\"       \"Twi'lek\"      \"Twi'lek\"      \"Vulptereen\"  \n[41] \"Xexto\"        \"Toong\"        \"Human\"        \"Cerean\"       \"Nautolan\"    \n[46] \"Zabrak\"       \"Tholothian\"   \"Iktotchi\"     \"Quermian\"     \"Kel Dor\"     \n[51] \"Chagrian\"     \"Human\"        \"Human\"        \"Human\"        \"Geonosian\"   \n[56] \"Mirialan\"     \"Mirialan\"     \"Human\"        \"Human\"        \"Human\"       \n[61] \"Human\"        \"Clawdite\"     \"Besalisk\"     \"Kaminoan\"     \"Kaminoan\"    \n[66] \"Human\"        \"Aleena\"       \"Skakoan\"      \"Muun\"         \"Togruta\"     \n[71] \"Kaleesh\"      \"Wookiee\"      \"Human\"        \"Pau'an\"       \"unknown\"     \n\n\nAnd we can ask R whether each value is equal to “Droid” (Remember: in R, we ask whether something is equal to something else by using a double-equals, ==). A single equal sign would be wrong, as it denotes assignment.\n\nstarwars2$species == \"Droid\"\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE\n\n\nFinally, we can use this condition inside our square brackets to access the entries of the data for which this condition is TRUE:\n\n# I would read the code below as: \n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$species==\"Droid\" is TRUE, and give me all the columns.\"\n\nstarwars2[starwars2$species == \"Droid\", ]\n\n   name height hair_color eye_color homeworld species\n3 R2-D2     96       <NA>       red     Naboo   Droid\n8 R5-D4     97       <NA>       red  Tatooine   Droid"
  },
  {
    "objectID": "01b_data.html#more-complex-conditions",
    "href": "01b_data.html#more-complex-conditions",
    "title": "1B: More R",
    "section": "More complex conditions",
    "text": "More complex conditions\nThinking back to Reading 1A when we first introduced R, we talked briefly about “logical operators”. Specifically, the operators &, |, and ! (for “and”, “or”,” and “not”), will come in handy now.\nFor instance, we can now extract all those in the dataset which are humans and taller than 190cm:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$species==\"Human\" AND starwars2$height > 190 are TRUE, \n# and give me all the columns.\"\nstarwars2[starwars2$species == \"Human\" & starwars2$height > 190, ]\n\n                  name height hair_color eye_color homeworld species\n4          Darth Vader    202       none    yellow  Tatooine   Human\n59               Dooku    193      white     brown   Serenno   Human\n60 Bail Prestor Organa    191      black     brown  Alderaan   Human\n\n\nOr we can extract all those in the dataset which are either droids or ewoks:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$species==\"Droid\" OR starwars2$species==\"Ewok\" is TRUE, \n# and give me all the columns.\"\nstarwars2[starwars2$species == \"Droid\" | starwars2$species == \"Ewok\", ]\n\n                    name height hair_color eye_color homeworld species\n3                  R2-D2     96       <NA>       red     Naboo   Droid\n8                  R5-D4     97       <NA>       red  Tatooine   Droid\n26 Wicket Systri Warrick     88      brown     brown     Endor    Ewok"
  },
  {
    "objectID": "01b_data.html#editing-specific-entries",
    "href": "01b_data.html#editing-specific-entries",
    "title": "1B: More R",
    "section": "Editing specific entries",
    "text": "Editing specific entries\nNow that we’ve seen a few ways of accessing sections of data, we can learn how to edit them! One of the most common reasons you will need to modify entries in your data is in data cleaning. This is the process of identifying incorrect/incomplete/irrelevant data, and replacing/modifying/deleting them.\nAbove, we looked at the subsection of the data where the species variable had the entry “Droid”. Some of you may have noticed earlier that we had some data on C3PO. Is he not also a droid?\n\n\n\n(Looks pretty Droid-y to me! disclaimer: I know nothing about Star Wars 🙂 )\nJust as we saw above how to access specific entries, e.g.:\n\n# 2nd row, all columns\nstarwars2[2, ]\n\n   name height hair_color eye_color homeworld species\n2 C-3PO    167       <NA>    yellow  Tatooine   Human\n\n# 2nd row, 6th column (the \"species\" column)\nstarwars2[2,6]\n\n[1] \"Human\"\n\n\nWe can change these by assigning them a new value (remember the <- symbol). In doing so, we replace / overwrite / reassign the entry in the 2nd row and 6th column of the data (starwars2[2,6]) with the value “Droid”.\n\n# C3PO is a droid, not a human\nstarwars2[2,6] <- \"Droid\"\n# Look at the 2nd row now -\n# the entry in the \"species\" column has changed:\nstarwars2[2, ]\n\n   name height hair_color eye_color homeworld species\n2 C-3PO    167       <NA>    yellow  Tatooine   Droid"
  },
  {
    "objectID": "01b_data.html#editing-entries-via-a-condition",
    "href": "01b_data.html#editing-entries-via-a-condition",
    "title": "1B: More R",
    "section": "Editing entries via a condition",
    "text": "Editing entries via a condition\nWe saw above how to access parts of data by means of a condition, with code such as:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$homeworld==\"Naboo\" is TRUE, and give me all the columns.\"\nstarwars2[starwars2$homeworld==\"Naboo\", ]\n\n            name height hair_color eye_color homeworld species\n3          R2-D2     96       <NA>       red     Naboo   Droid\n19     Palpatine    170       grey    yellow     Naboo   Human\n30 Jar Jar Binks    196       none    orange     Naboo  Gungan\n31  Roos Tarpals    224       none    orange     Naboo  Gungan\n32    Rugor Nass    206       none    orange     Naboo  Gungan\n52  Gregar Typho    185      black     brown     Naboo   Human\n53         Cordé    157      brown     brown     Naboo   Human\n58         Dormé    165      brown     brown     Naboo   Human\n\n\nWhat if we wanted to modify it so that every character from “Naboo” was actually of species “Nabooian”?\nWe can do that in a number of ways, all of which do the same thing - namely, they access parts of the data and assign them the new value “Nabooian”.\nStudy the lines of code below and their interpretations:\n\n# In the starwars2 data, give the rows for which condition \n# starwars2$homeworld==\"Naboo\" is TRUE, and select only the \"species\" column. \n# Assign to these selected entries the value \"Nabooian\".\nstarwars2[starwars2$homeworld==\"Naboo\", \"species\"] <- \"Nabooian\"\n\n# In the starwars2 data, give the rows for which condition \n# starwars2$homeworld==\"Naboo\" is TRUE, and select only the 6th column. \n# Assign to these selected entries the value \"Nabooian\".\nstarwars2[starwars2$homeworld==\"Naboo\", 6] <- \"Nabooian\"\n\n# Extract the species variable from the starwars2 data (it's a vector).\n# Pick the entries for which the condition starwars2$homeworld==\"Naboo\" is TRUE.\n# Assign to these selected entries the value \"Nabooian\".\nstarwars2$species[starwars2$homeworld==\"Naboo\"] <- \"Nabooian\""
  },
  {
    "objectID": "01b_data.html#addingchanging-a-variable",
    "href": "01b_data.html#addingchanging-a-variable",
    "title": "1B: More R",
    "section": "Adding/Changing a variable",
    "text": "Adding/Changing a variable\nAnother thing we might want to do is change a whole variable (a whole column) in some way.\nThe logic is exactly the same, for instance, we can take the variable “height” from the dataframe “starwars2”, dividing it by 100 via starwars2$height / 100, and then assign the result to the same variable name in the data, i.e. we overwrite the column:\n\nstarwars2$height <- starwars2$height / 100\n\nWe could instead have added a new column named “height_m” with those values if we did not want to overwrite “height”:\n\nstarwars2$height_m <- starwars2$height / 100\n\nThis would have left the “height” variable as-is, and created a new one called “height2” which was the values in “height” divided by 100."
  },
  {
    "objectID": "01b_data.html#removing-rows-or-columns",
    "href": "01b_data.html#removing-rows-or-columns",
    "title": "1B: More R",
    "section": "Removing rows or columns",
    "text": "Removing rows or columns\nLastly, we might want to change the data by removing a row or a column. Again, the logic remains the same, in that we use <- to assign the edited data to a name (either a new name, thus creating a new object, or an existing name, thereby overwriting that object).\nFor instance, notice that the 35th and 75th rows of our data probably aren’t a valid observation - I’m reasonably sure that Marge and Homer Simpson never appeared in Star Wars:\n\nstarwars2[c(35,75), ]\n\n            name height hair_color eye_color   homeworld species\n35 Marge Simpson    1.7       Blue      <NA> Springfield unknown\n75 Homer Simpson    1.8       <NA>      <NA> Springfield unknown\n\n\nWe can remove a certain row(s) by using a minus sign - inside the square brackets\n\n# everything minus the 75th row\nstarwars2[-75, ]\n# everything minus the (35th and 75th rows)\nstarwars2[-c(35, 75), ]\n\nAnd we can simply re-use the name “starwars2” to overwrite the data and make this change take effect (rather than just print out the result, which the code above did):\n\nstarwars2 <- starwars2[-c(35, 75), ]\n\n(now, in the environment pane of Rstudio, the object named “starwars2” will say 73 observations, rather than 75, which it had before - we’ve removed the 2 rows)\n The same logic applies for columns:\n\n# Create a new object called \"anonymous_starwars2\" and assign it \n# to the values which are the \"starwars2\" dataframe minus the \n# 1st column (the \"name\" column):\nanonymous_starwars2 <- starwars2[, -1]\n# dimensions of our initial data\ndim(starwars2)\n\n[1] 73  6\n\n# the data we just assigned has one fewer columns\ndim(anonymous_starwars2)\n\n[1] 73  5"
  },
  {
    "objectID": "02_ex.html",
    "href": "02_ex.html",
    "title": "Week 2 Exercises: More R; Estimates & Intervals",
    "section": "",
    "text": "Data manipulation & visualisation\n\nQuestion 1\n\n\nCreate a new variable in the dataset which indicates whether people are taller than 6 foot (182cm).\n\nYou might use mutate(). Remember to make the changes apply to the objects in your environment, rather than just printing it out.\ndata <- data %>% mutate(...)\n\n\n\n\n\nQuestion 2\n\n\nWhat percentage of respondents to the survey are greater than 6 foot tall?\n\nTry table(), and then think about how we can convert the counts to percentages (what does sum() of the table give you?).\nSee also the discussion of categorical data in Reading 2A.\n\n\n\n\n\nQuestion 3\n\n\nhow many of USMR students in 2022/23 are born in the same month as you?\n\nThis will involve filtering your data to current USMR students first.\nIn tidyverse you can make a table using ... %>% select(variable1, variable2) %>% table()\nYou can also try ... %>% count(variable1, variable2)\n\n\n\n\n\nQuestion 4\n\n\nCalculate the mean and standard deviation of heights of all respondents to the survey.\nCan you (also) do this using the tidyverse syntax?\n\nWe can do it with mean(data$height), but it will be useful to practice tidyverse style. You’ll want to summarise() the data.\nWe’re likely to have missing data in here, so na.rm=TRUE will be handy (see Reading 2A on numeric variables)\n\n\n\n\n\nQuestion 5\n\n\nPlot the distribution of heights of all respondents. Try to make it ‘publication ready’.\n\nNo hist() here, we’re going to want ggplot, which was introduced in Reading 2A.\n\n\n\n\n\nQuestion 6\n\n\nFor respondents from each of the different courses, calculate the mean and standard deviation of heights.\n\nThis is just like an earlier question - we want to summarise our data. Only this time we need to group_by something else first.\n\n\n\n\n\nQuestion 7\n\n\nPlot the distributions of heights for each course.\nBased on your answer to the previous question, can you picture what the distributions are going to look like before you plot them?\n\nYou’ll probably want to add an extra aesthetic mapping from the course variable to some feature of your plot (e.g. colour).\nOr try looking up the documentation for ?facet_wrap.\n\n\n\n\n\nQuestion 8\n\n\nWhat proportion of respondents to the survey are taller than you?\n\nRemember that we can sum() a condition as a quick way of counting.\nsum(data$variable == \"thing\") adds up all the TRUE responses.\nWe can actually use this logic inside tidyverse functions like summarise and count too.\n\n\n\n\n\n\n\n\n\nEstimates & Intervals\n\nQuestion 9\n\n\nSuppose we are interested in the average life-satisfaction rating of MSc students at Edinburgh University.\nWe only asked this question to students in USMR in 2022/23, so to make things easier, let’s create a subset of the dataset which includes only those students.\n\nThis will be some filtering, and assigning (usmrdata <-) to a new name.\n\n\n\n\n\nQuestion 10\n\n\nFor the USMR students in 2022/23, calculate the following:\n\nmean life satisfaction rating\nstandard deviation of life satisfaction ratings\nnumber of respondents who completed life satisfaction rating\n\n\nYou can do this with things like mean(data$variable), or you can do it all in tidyverse (see the example of summarise in the intro to tidyverse in Reading 2A). If you do this all inside summarise() then you might find the function n() useful.\n\n\n\n\n\nQuestion 11\n\n\nUsing your answers to the previous question, construct a 95% confidence interval for the mean life satisfaction rating.\nWhy might it be a bad idea to use this as an estimate of the average life-satisfaction rating of all MSc students at Edinburgh University?\n\nThe previous question gives you all the pieces that you need. You’ll just need to put them together in the way seen in Reading 2B/Confidence Intervals.\nThink about who makes up the sample (e.g. USMR students). Are they representative of the population we are trying to generalise to?"
  },
  {
    "objectID": "02a_measurement.html",
    "href": "02a_measurement.html",
    "title": "2A: Measurement & Distributions",
    "section": "",
    "text": "The best way to learn R is to use it.\nTry following along with this reading by typing the code into your R script and running them. You will hopefully get the same output as is presented on this page below each bit of code.\nIf you get errors and warnings, don’t panic - read them!"
  },
  {
    "objectID": "02a_measurement.html#categorical",
    "href": "02a_measurement.html#categorical",
    "title": "2A: Measurement & Distributions",
    "section": "Categorical",
    "text": "Categorical\n\nCategorical variables tell us what group or category each individual belongs to. Each distinct group or category is called a level of the variable.\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nNominal (Unordered categorical)\nA categorical variable with no intrinsic ordering among the levels.\nSpecies: Dog, Cat, Parrot, Horse, …\n\n\nOrdinal (Ordered categorical)\nA categorical variable which levels possess some kind of order\nLevel: Low, Medium, High\n\n\nBinary categorical\nA special case of categorical variable with only 2 possible levels\nisDog: Yes or No.\n\n\n\nIf we want to summarise a categorical variable into a single number, then the simplest approach is to use the mode:\n\nMode: The most frequent value (the value that occurs the greatest number of times).\n\nWhen we have ordinal variables, there is another option, and that is to use the median:\n\nMedian: For ordinal variables only, this is the value for which 50% of observations are lower and 50% are higher. It is the mid-point of the values when they are rank-ordered.\n\nWhen we use the median as our measure of “central tendency” (i.e. the middle of the distribution) and we want to discuss how spread out the spread are around it, then we will want to use quartiles. The Inter-Quartile Range (IQR) is obtained by rank-ordering all the data, and finding the points at which 25% (one quarter) and 75% (three quarters) of the data falls below (this makes the median the “2nd quartile”).\n\nIn our dataset on passwords, we have various categorical variables, such as the type of password (categories like “animal”, “fluffy” etc).\nThere are various ways we might want to summarise categorical variables like this. We have already seen the code to do this in our example of the dice simulation - we can simply counting the frequencies in each level:\n\ntable(pwords$type)\n\n\n             animal          cool-macho              fluffy                food \n                 29                  79                  44                  11 \n               name           nerdy-pop    password-related     rebellious-rude \n                183                  30                  15                  11 \nsimple-alphanumeric               sport \n                 61                  37 \n\n\nThis shows us that the mode (most common) is “name” related passwords.\nWe could also convert these to proportions, by dividing each of these by the total number of observations. For instance, here are the percentages of passwords of each type:\n\ntable(pwords$type) / sum(table(pwords$type)) * 100\n\n\n             animal          cool-macho              fluffy                food \n                5.8                15.8                 8.8                 2.2 \n               name           nerdy-pop    password-related     rebellious-rude \n               36.6                 6.0                 3.0                 2.2 \nsimple-alphanumeric               sport \n               12.2                 7.4 \n\n\n\nOften, if the entries in a variable are characters (letters), then many functions in R (like table()) will treat it the same as if it is a categorical variable. However, this is not always the case, so it is good to tell R specifically that each variable is a categorical variable. There is a special way that we tell R that a variable is categorical - we set it to be a “factor”. Note what happens when we make the “type” and “strength_cat” variables to be a factor:\n\npwords$type <- factor(pwords$type)\npwords$strength_cat <- factor(pwords$strength_cat)\nsummary(pwords)\n\n      rank         password                          type        cracked      \n Min.   :  1.0   Length:500         name               :183   Min.   : 1.290  \n 1st Qu.:125.8   Class :character   cool-macho         : 79   1st Qu.: 3.430  \n Median :250.5   Mode  :character   simple-alphanumeric: 61   Median : 3.720  \n Mean   :250.5                      fluffy             : 44   Mean   : 5.603  \n 3rd Qu.:375.2                      sport              : 37   3rd Qu.: 3.720  \n Max.   :500.0                      nerdy-pop          : 30   Max.   :92.270  \n                                    (Other)            : 66                   \n    strength      strength_cat\n Min.   : 1.000   medium:402  \n 1st Qu.: 6.000   strong: 25  \n Median : 7.000   weak  : 73  \n Mean   : 6.768               \n 3rd Qu.: 8.000               \n Max.   :10.000               \n                              \n\n\nR now recognises that there a set number of possible response options, or “levels”, for these variables. We can see what they are using:\n\nlevels(pwords$strength_cat)\n\n[1] \"medium\" \"strong\" \"weak\"  \n\n\nThe “strength_cat” variable specifically has an ordering to the levels, so we might be better off also telling R about this ordering. We do this like so:\n\npwords$strength_cat <- factor(pwords$strength_cat, ordered = TRUE, levels = c(\"weak\",\"medium\",\"strong\"))\n\n\nSometimes, we might have a variable that we know is categorical, but we might want to treat it as a set of numbers instead. A very common example in psychological research is Likert data (questions measured on scales such as “Strongly Disagree”>>“Disagree”>>…>>“Strongly Agree”).\nIt is often useful to have these responses as numbers (e.g. 1 = “Strongly Disagree” to 5 = “Strongly Agree”), as this allows us to use certain functions in R more easily. For instance, the median() and IQR() functions require the data to be numbers.\nThis will not work:\n\nmedian(pwords$strength_cat)\n\nError in median.default(pwords$strength_cat): need numeric data\n\n\nWhen we ask R to convert a factor to a numeric variable, it will give turn the first category into 1, the second category to 2, and so on. As R knows that our strength_cat variable is the ordered categories “weak”>>“medium”>>“strong”, then as.numeric(pwords$strength_cat) will turn these to 1s, 2s, and 3s.\n\nmedian(as.numeric(pwords$strength_cat))\n\n[1] 2"
  },
  {
    "objectID": "02a_measurement.html#numeric",
    "href": "02a_measurement.html#numeric",
    "title": "2A: Measurement & Distributions",
    "section": "Numeric",
    "text": "Numeric\n\nNumeric (or quantitative) variables consist of numbers, and represent a measurable quantity. Operations like adding and averaging make sense only for numeric variables.\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nContinuous\nVariables which can take any real number within the specified range of measurement\nHeight: 172, 165.2, 183, …\n\n\nDiscrete\nVariables which can only take integer number values. For instance, a counts can only take positive integer values (0, 1, 2, 3, etc.)\nNumber_of_siblings: 0, 1, 2, 3, 4, …\n\n\n\nOne of the most frequently used measures of central tendency for numeric data is the mean. The mean is calculated by summing all of the observations together and then dividing by the total number of obervations (\\(n\\)).\n\nMean: \\(\\bar{x}\\)\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the mean is:\n\\[\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\]\n\n Help reading mathematical formulae.\n\n\nThis might be the first mathematical formula you have seen in a while, so let’s unpack it.\nThe \\(\\sum\\) symbol is used to denote a series of additions - a “summation”.\nWhen we include the bits around it: \\(\\sum\\limits_{i = 1}^{n}x_i\\) we are indicating that we add together all the terms \\(x_i\\) for values of \\(i\\) between \\(1\\) and \\(n\\):\n\\[\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n\\]\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the \\(1^{st}\\) to the \\(n^{th}\\) (where \\(n\\) is the total number of values), and we divide that by \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we are using the mean as our as our measure of central tendency, we can think of the spread of the data in terms of the deviations (distances from each value to the mean).\nRecall that the mean is denoted by \\(\\bar{x}\\). If we use \\(x_i\\) to denote the \\(i^{th}\\) value of \\(x\\), then we can denote deviation for \\(x_i\\) as \\(x_i - \\bar{x}\\).\nThe deviations can be visualised by the red lines in Figure 2.\n\n\n\n\n\nFigure 2: Deviations from the mean\n\n\n\n\n\nThe sum of the deviations from the mean, \\(x_i - \\bar x\\), is always zero\n\\[\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n\\]\nThe mean is like a center of gravity - the sum of the positive deviations (where \\(x_i > \\bar{x}\\)) is equal to the sum of the negative deviations (where \\(x_i < \\bar{x}\\)).\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider squared deviations.\nSquaring the deviations makes them all positive. Observations far away from the mean in either direction will have large, positive squared deviations. The average squared deviation is known as the variance, and denoted by \\(s^2\\)\n\nVariance: \\(s^2\\)\nThe variance is calculated as the average of the squared deviations from the mean.\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the variance is:\n\\[s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\\]\n\n Optional: Why n minus 1?\n\n\nThe top part of the equation \\(\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\\) can be expressed in \\(n-1\\) terms, so we divide by \\(n-1\\) to get the average.\n Example: If we only have two observations \\(x_1\\) and \\(x_2\\), then we can write out the formula for variance in full quite easily. The top part of the equation would be:\n\\[\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\]\nThe mean for only two observations can be expressed as \\(\\bar{x} = \\frac{x_1 + x_2}{2}\\), so we can substitute this in to the formula above.\n\\[\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 \\qquad = \\qquad \\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2\n\\]\nWhich simplifies down to one value:\n\\[\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2 \\qquad = \\qquad  \\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n\\]\n So although we have \\(n=2\\) datapoints (\\(x_1\\) and \\(x_2\\)), the top part of the equation for the variance has only 1 (\\(n-1\\)) units of information. In order to take the average of these bits of information, we divide by \\(n-1\\).\n\n\n\n\nOne difficulty in interpreting variance as a measure of spread is that it is in units of squared deviations. It reflects the typical squared distance from a value to the mean.\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the standard deviation.\n\nStandard Deviation: \\(s\\)\nThe standard deviation, denoted by \\(s\\), is a rough estimate of the typical distance from a value to the mean.\nIt is the square root of the variance (the typical squared distance from a value to the mean).\n\\[\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n\\]\n\n\nIn the passwords dataset, we only have one continuous variable, and that is the “cracked” variable, which if we recall is the “Time to crack by online guessing”. You might be questioning whether the “strength” variable, which ranges from 1 to 10 is numeric? This depends on whether we think that statements like “a password of strength 10 is twice as strong as a password of strength 5”.\nFor now, we’ll just look at the “cracked” variable.\nTo calculate things like means and standard deviations in R is really easy, because there are functions that do them all for us.\nFor instance, we can do the calculation by summing the cracked variable, and dividing by the number of observations (in our case we have 500 passwords):\n\n# get the values in the \"cracked\" variable from the \"pwords\" dataframe, and\n# sum them all together. Then divide this by 500\nsum(pwords$cracked)/500\n\n[1] 5.60266\n\n\nOr, more easily, we can use the mean() function:\n\nmean(pwords$cracked)\n\n[1] 5.60266\n\n\nWe can get R to calculate the variance and standard deviation with the var() and sd() functions:\n\nvar(pwords$cracked)\n\n[1] 71.16618\n\nsd(pwords$cracked)\n\n[1] 8.436005\n\n\nand just to prove to ourselves:\n\nsd(pwords$cracked)^2 == var(pwords$cracked)\n\n[1] TRUE\n\n\n\nIf a column of our dataset contains only numbers, R will typically just interpret it as a numeric variable. However, we should still be careful; remember what happens if we have just one erroneous entry in there - they can all change to be characters (surrounded by quotation marks):\n\nc(1,3,6,\"peppapig\",3)\n\n[1] \"1\"        \"3\"        \"6\"        \"peppapig\" \"3\"       \n\n\nWe can force a variable to be numeric by using as.numeric(), which will also coerce any non-numbers to be NA (not applicable):\n\nas.numeric(c(1,3,6,\"peppapig\",3))\n\n[1]  1  3  6 NA  3\n\n\nIf there is an NA in the variable, many functions like mean(), var() and sd() will not compute:\n\nx <- c(1, 3, 6, NA, 3)\nmean(x)\n\n[1] NA\n\n\nHowever, we can ask these functions to remove the NAs prior to the computation:\n\nmean(x, na.rm = TRUE)\n\n[1] 3.25"
  },
  {
    "objectID": "02a_measurement.html#boxplots",
    "href": "02a_measurement.html#boxplots",
    "title": "2A: Measurement & Distributions",
    "section": "Boxplots",
    "text": "Boxplots\nBoxplots provide a useful way of visualising the interquartile range (IQR). You can see what each part of the boxplot represents in Figure Figure 4.\n\n\n\n\n\nFigure 4: Anatomy of a boxplot\n\n\n\n\nWe can create a boxplot of our age variable using the following code:\n\n# Notice, we put strength on the x axis, making the box plot vertical. \n# If we had set aes(y = strength) instead, then it would simply be rotated 90 degrees \nggplot(data = pwords, aes(x = strength)) +\n  geom_boxplot()"
  },
  {
    "objectID": "02a_measurement.html#histograms",
    "href": "02a_measurement.html#histograms",
    "title": "2A: Measurement & Distributions",
    "section": "Histograms",
    "text": "Histograms\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these map to how visualisations of numeric variables look.\nWe can visualise numeric data using a histogram, which shows the frequency of values which fall within bins of an equal width.\nTo do this, we’re going to use some new data, on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests. The data are available at https://uoepsy.github.io/data/wechsler.csv\n\nwechsler <- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n\n\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\nWe can specifiy the width of the bins:\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\n\nLet’s take a look at the means and standard deviations of participants’ scores on the other tests (the test1 and test2 variables).\nNote how nicely we can do this with our newfound tidyverse skills!\n\nwechsler %>% \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n\n# A tibble: 1 × 4\n  mean_test1 sd_test1 mean_test2 sd_test2\n       <dbl>    <dbl>      <dbl>    <dbl>\n1       49.3     7.15       51.2     14.4\n\n\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1."
  },
  {
    "objectID": "02a_measurement.html#density",
    "href": "02a_measurement.html#density",
    "title": "2A: Measurement & Distributions",
    "section": "Density",
    "text": "Density\nIn addition to grouping numeric data into bins in order to produce a histogram, we can also visualise a density curve.\nBecause there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, …), we could group the data into infinitely many bins. This is essentially what we are doing with a density curve.\nYou can think of “density” as a bit similar to the notion of “relative frequency” (or “proportion”), in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\nArea under the curve\nThink about the barplots we have been looking at in the exercises where we simulate dice rolling :\n\n# our function to simulate the roll of a die/some dice\ndice <- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n# simulate 1000 rolls of a single die\nroll1000 <- replicate(1000, dice(1))\n# tabulate and plot:\ntable(roll1000) %>%\n  barplot(.,ylab=\"count\")\n\n\n\n\n\n\n\n\nTo think about questions like “what proportion of 1000 rolls does the die land on 6?”, we are simply interested in the count of 6s divided by the count of all rolls:\n\ntab1000 <- table(roll1000)\ntab1000\n\nroll1000\n  1   2   3   4   5   6 \n162 169 167 189 152 161 \n\ntab1000[6] / sum(tab1000)\n\n    6 \n0.161 \n\n\nSo Another way of thinking of this is that we are just dividing the count in each category by the total number. Or, Put another way, imagine we divide the area of each bar by the total area. The area now sums to 1, and our question is asking about the ratio of the red area to the total area (grey + red):\n\n\n\n\n\n\n\n\n\nNothing really changes with a density curve! If we want to ask what proportion of our distribution of IQ scores is >110, then we are asking about the area under the curve that is to the right of 110:\n\n\n\n\n\n\n\n\n\nIt looks like about a third, maybe a little less. Let’s calculate this proportion directly:\n\nsum(wechsler$iq>110) / length(wechsler$iq)\n\n[1] 0.2\n\n\nIt might seem a little odd to think about area under the curve when we are asking about “what proportion of the data is …?”. If we have the data, then we can just calculate the answer (like we did above). However, a lot of statistics is really concerned with the probability of events. When we discuss probability, we move from talking about a specific set of observed data to thinking about a theoretical/mathematical model that defines the way in which data is generated. This where it becomes more useful to think about distributions in a more abstract sense.\nFor instance, with a fair six-sided die, we have a probability distribution in which each side is given the probability \\(\\frac{1}{6}\\):\n\\[\n\\begin{gather*}\nP(x) = \\begin{cases}\n  \\frac{1}{6} & \\text{if $x \\in \\{1,2,3,4,5,6\\}$}\\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "02b_sampling.html",
    "href": "02b_sampling.html",
    "title": "2B: Curves & Sampling",
    "section": "",
    "text": "Probability distributions\nWe’ve seen some ways of describing and visualising the distributions of variables that we might observe when we collect data. Such a collection of observations on a single variable is often termed a “sample distribution”.\nAnother type of distribution that will prove to be very useful is a “probability distribution”.\n\nA probability distribution is the (mathematical) description of the probabilities of occurrences of observing the different possible outcomes.\n\nNote an important jump we are making is that we are moving from talking about distributions that we observe, to something more conceptual. Typically, this is because we want to talk more generally about the underlying process which generates the data.\nFor example, the function that governs the behaviour of rolling a single die is uniform in that each possible response has an equal probability (\\(\\frac{1}{6}\\)) of being observed (below left). When we collect data by actually rolling a die 100 times, we will observe a sample distribution (below right).\n\n\n\n\n\n\n\n\n\n\nUniformity\nWhen an equal probability is assigned to each possible response, we have what is known as the uniform distribution.\nFor a fair 6-sided die, the probability of the die landing on each side is 1/6, and the probabilities of all the possible responses sum to 1 (because it has to land on one of the sides).\n\n\n\n\n\n\n\n\n\nThe dice-rolling example is one involving a categorical distribution - i.e. data which has a discrete set of response options. We don’t have to use a 6-sided die - if it follows a uniform probability distribution, and there are \\(n\\) possible responses, then the probability of each response ocurring is \\(1/n\\).\nHowever, the uniform probability distribution can be relevant for a continuous numeric variable as well (e.g. something which as well as taking the values 4 and 5 can also take 4.1, 4.11, 4.1111111111, 4.764968473 etc.. - they can take any real value). We can preserve the idea that probability sums to 1 for this sort of variable by having the probability as \\(\\frac{1}{b-a}\\), where \\(a\\) and \\(b\\) are the lower and upper bounds of the response domain. Why? Because this makes the area of the distribution equal to 1 (area of a rectangle = width \\(\\times\\) height. \\((b-a) \\times \\frac{1}{(b-a)} = \\frac{b-a}{b-a} = 1)\\). This means we can compute areas of parts of the distribution in order to calculate probabilities!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal distributions\nWe have actually already been talking about various aspects of one of the most important distributions in statistics, those that we call “normal distributions”. Sometimes you will hear it referred to as a “bell curve” due to its resemblance to the shape of a bell. We started to see distributions (Figure 1) which were roughly bell-shaped, and saw how the shape of such distributions depends on parameters such as the mean and variance/standard deviation1.\n\n\nCode\nwechsler <- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\nggplot(data = wechsler, aes(x = test1)) + \n  geom_histogram()+\n  xlim(0,100) +\nggplot(data = wechsler, aes(x = test2)) + \n  geom_histogram()+\n  xlim(0,100)\n\n\n\n\n\nFigure 1: Scores on test 2 have a larger standard deviation, as seen in the distribution of test 2 being wider.\n\n\n\n\n\nDefining moments\nThe “moments” of a distribution are the metrics that relate to the shape of that distribution. We’ve already seen the primary two moments that define the shapes of these distributions: the mean and the variance. The mean moves the distribution right or left, and the variance makes the distribution wider or narrower.\nThere are two more, “skewness” and “kurtosis” which tend to be of less focus of investigation (the questions we ask tend to be mainly concerned with means and variances). Skewness is a measure of asymmetry in a distribution. Distributions can be positively skewed or negatively skewed, and this influences our measures of central tendency and of spread to different degrees. The kurtosis is a measure of how “pointy” vs “rounded” the shape of a distribution is.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can motivate the relevance of normal distributions in various ways. For instance, as was discussed in the lecture, when we take a measurement of something such as the length of a stick, then we always have a bit of imprecision - our measurements will vary a bit. Assuming that our measurement tool is unbiased, we would expect the measurements of the stick to be centered around the true length of the stick.\n\n\n\n\n\nFigure 2: Snapshots from lecture slides on measurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are certain properties of normal distributions which we can exploit, in order to determine how plausible an observed value is relative to a distribution.\n\nWhen a distribution is normal (symmetric and bell-shaped):\n\n68% of values will lie within 1 standard deviation of the mean.\n95% of values will lie within 1.96 standard deviations of the mean.\n99.7% of values will lie within 3 standard deviations of the mean.\n\n\n\n\n\n\n\n\n\n\n\nLet’s return to the IQ data we saw in the Reading 2A. We have our observed sample distribution of IQ scores:\n\n\nCode\nwechsler <- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\nWe know how to address questions such as “what proportion of our sample has an IQ >120?”. We can use the data we have to calculate the proportion:\n\n# number of IQ scores less than 70 divided by number of scores\nsum(wechsler$iq < 120) / length(wechsler$iq)\n\n[1] 0.9\n\n\nWe are using IQ as an example here because IQ scales are developed and updated in attempts to standardise the tests so that the scores have an average of 100 and a standard deviation of 15.2 If we think of these two parameters (mean of 100, standard deviation of 15) as defining the expected distribution of IQ scores, then we can ask the question:\n“what is the probability of observing someone with an IQ >120?”\nWhat we’re asking here is for the amount of the normal distribution with mean = 100, sd = 15 that falls beyond 120 (Figure 3). Note that the distribution we’re talking about here is an abstract one that represents all the possible IQ scores that we might observe if we were to randomly sample the population.\n\n\n\n\n\nFigure 3: what is the probability of observing someone with an IQ >120?\n\n\n\n\nYears ago we would have to work out how many standard deviations this is above the mean (\\(\\frac{120 - 100}{15} = 1\\frac{1}{3}\\)) and then look up in a big table to work out the probability of observing something which is 1.33 standard deviations above the mean.\nConveniently, there are some functions which can do this for us. The pnorm() function will give us the area to the left or right (depending on whether we put lower.tail = TRUE/FALSE) of a given number:\n\npnorm(120, mean = 100, sd = 15, lower.tail = FALSE)\n\n[1] 0.09121122\n\n\n\n\n\n\n\nFigure 4: pnorm() can be used to give us the area to the left/right of some value\n\n\n\n\nNote that if we translate our “IQ >120” to being in terms of standard deviations - \\(\\frac{120 - 100}{15} = 1\\frac{1}{3}\\) - then this is the same computation when we compare against a normal distribution with mean of 0 and standard deviation of 1, which are the defaults for the pnorm() function:\n\npnorm((120-100)/15, lower.tail = FALSE)\n\n[1] 0.09121122\n\n\n\n\n\n\n\nFigure 5: pnorm() with the ‘standard normal distribution’: the normal distribution with mean = 0 and sd = 1\n\n\n\n\nThe probability of observing IQ <120 is easily obtained by changing the lower.tail argument:\n\npnorm(120, mean = 100, sd = 15, lower.tail = TRUE)\n\n[1] 0.9087888\n\n\n\n\n\n\n\nFigure 6: pnorm() can be used to get the area either side by changing the lower.tail argument\n\n\n\n\nAnd we know that the total area under the curve is 1, so we can also use this to get the area to the right again:\n\n1 - pnorm(120, mean = 100, sd = 15, lower.tail = TRUE)\n\n[1] 0.09121122\n\n\nThe opposite functionality of pnorm() comes in the qnorm() function, which takes a specified area (e.g. 5%) and gives us the value at which that area falls above (or below, depending on the lower.tail).\nSo to find out at what score is the top 5% percent of IQ scores, we would need to ask the point at which there is 5% to the right, or 95% to the left:\n\n# either: \nqnorm(0.05, mean = 100, sd = 15, lower.tail = FALSE)\nqnorm(0.95, mean = 100, sd = 15, lower.tail = TRUE)\n\n\n\n[1] 124.6728\n\n\n\n\n\n\n\nFigure 7: qnorm() takes an area and gives us the value on the x-axis at which point the are is to the left/right of. it is the opposite of pnorm()\n\n\n\n\n\n\npnorm() takes a value on the x-axis and returns the area to the left (lower.tail = TRUE) or to the right (lower.tail = FALSE).\nqnorm() takes an area (from 0 to 1) and returns the value on the x-axis at which point that area is to the left (lower.tail = TRUE) or to the right (lower.tail = FALSE).\n\n\n\n\nThere are a couple of related functions which it is also worth knowing about:\n\ndnorm() takes a value on the x-axis and returns the height of the curve (the density).\n\nrnorm() returns n randomly generated values.\n\n\n\n\n\n\n\nSampling & Sampling Distributions\nOften, what we’re really interested does not concern a specific individual but the wider population in general - we are typically interested in things on average, and want to make generalisations such as “drug X will increase life expectancy by on average 2 years” or “higher levels of conscientiousness is typically associated with more happiness”. These sort of statements are made in reference to a population, not about individuals.3\nIn practice, it is rarely feasible to directly measure the entire population to calculate the average, so when we do research we tend to collect data from a subset, or sample. By using a random sample to represent a population of interest, we introduce uncertainty (due to sampling variability) in how accurate our sample is as to provide an estimate of something in the population.\nFor us to better understand the idea of “sampling variability”, it’s going to be necessary for us to clearly distinguish between populations and samples.\n\nSamples & Populations\nA sample statistic is a computed value based on our sample data, the we use to estimate a population parameter (the value of which is unknown).\nWe use different symbols to denote each of these:\n\n\n\nSample Statistic\nPopulation Parameter\n\n\n\n\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\n\\(s\\)\n\\(\\sigma\\)\n\n\n\\(s^2\\)\n\\(\\sigma^2\\)\n\n\n\n\nWhen we discussed IQ above, we had the idea of an underlying population distribution of everybody’s IQ scores. When we randomly choose one person (a sample of \\(n=1\\)), we might get someone who has an IQ a bit lower, or a bit higher, than the average.\nThe same applies when we take are taking the mean of a sample. Suppose we are interested in the average “life satisfaction rating” (Figure 8) for the entire adult population. If we take a sample of say, \\(n=30\\) people, we might just happen to have a few more people in our sample who are highly satisfied, or a few more people who are more dissatisfied. Consequently, the mean of our sample will be a little bit higher/lower than the population average.\n\n\n\n\n\nFigure 8: Life Satisfaction rating, slider scale 0-100\n\n\n\n\nWhen we use the mean rating from our sample as an estimate of the mean rating in the population, it would be good to be able to quantify how much certainty we have in that estimate - i.e. how much do we expect the mean life satisfaction rating from samples of \\(n=30\\) to vary just due to our random sampling?\n\nLet’s look at this with a little simulation.\n\nSimulated Sampling\nLet’s pretend that the average life satisfaction rating (measured on the slider in Figure 8) of the entire global adult population is exactly 65, and that the standard deviation of ratings is exactly 12. However, let’s also pretend that we do not know this, and that we are interested in trying to estimate the average rating. All we have is the measurements of 30 people who we randomly sampled. We want to use the mean rating of our sample as an estimate of the mean rating of the population.\nIn R, we can simulate the act of randomly sampling 30 people’s ratings from the population with \\(\\mu = 65\\) and \\(\\sigma = 12\\) using rnorm():\n\nour_sample <- rnorm(n = 30, mean = 65, sd = 12)\nmean(our_sample)\n\n[1] 64.28839\n\n\nNote that the mean of our sample (64.29) is not quite the same as the mean of the population (65 exactly). As we know, samples vary. If we do the same thing again, R will take a different sample of 30, and so the mean of this new sample will also be different:\n\nmean(rnorm(n = 30, mean = 65, sd = 12))\n\n[1] 68.22701\n\n\nEach time we get a new sample, we get a different mean:\n\nmean(rnorm(n = 30, mean = 65, sd = 12))\nmean(rnorm(n = 30, mean = 65, sd = 12))\nmean(rnorm(n = 30, mean = 65, sd = 12))\n\n\n\n[1] 64.20016\n\n\n[1] 65.32239\n\n\n[1] 63.86801\n\n\nWhat we’re wanting to do here is think about all possible samples of 30 people we could have taken, and all the possible resulting mean ratings. Let’s consider 1000 different samples of 30 people that we could have taken, and for each one we calculate the mean rating. Where would all these different means fall? Some would be above our population parameter (i.e. we just might happened to have sampled some slightly more satisfied people) and some would be below.\nWe can use R to enact this repeated sampling: the replicate() function allows us to repeatedly execute a bit of code, which means we can take lots of samples and calculate their means. These means we can then visualise using hist():\n\nmanysamplemeans <- replicate(1000, mean(rnorm(n = 30, mean = 65, sd = 12)))\nhist(manysamplemeans)\n\n\n\n\n\n\n\n\nWhat does the simulation show us?\nWhat we’re doing here is showing the process of taking many samples of the same size from a population and calculating a statistic (the mean) for each sample. The distribution of these sample statistics shows how the statistic will vary from sample to sample due to chance. Provided that our sampling is truly random, the sample statistics will be centered around the population parameter (the unknown value that we’re trying to estimate).\nIn the above example, for samples of \\(n=30\\) drawn from a population with mean \\(\\mu=65\\) and standard deviation \\(\\sigma=12\\), the sample means are centered around 65, and we’re quite likely to get sample means between 60 and 70, but less likely to see sample means \\(<60\\) and \\(>70\\). Importantly, we can quantify this. The distribution of means from samples of size \\(n=30\\) has a standard deviation of:\n\nsd(manysamplemeans)\n\n[1] 2.253651\n\n\nThis metric, the standard deviation of the sampling distribution of a statistic, is known as the standard error.\nWhat happens with different sample sizes?\nNote what happens to the distribution when we consider the means from 1000 different samples of size \\(n=200\\), rather than \\(n=30\\). Many more of the of the values are in a much narrower bracket (pay careful attention to the x-axis) than when we took lots of samples of \\(n=30\\).\n\nmanysamplemeans200 <- replicate(1000, mean(rnorm(n = 200, mean = 65, sd = 12)))\nhist(manysamplemeans200)\n\n\n\n\n\n\n\n\nWe can see that an estimate from a sample of 200 is more precise (it has a greater chance of being closer to the true population value) than an estimate from a sample of 20.\n\n\nSampling Distribution and Standard Error\n\nThe theoretical distribution of how sample statistics will vary on repeated sampling is known as the sampling distribution.\n\nThe standard deviation of the sampling distribution is known as the standard error.\n\nNote that the bigger our sample size, the smaller our standard error - i.e., the more precise our sample means are going to be as estimates of the population mean:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Error in practice\nIn practice, we cannot actually take lots and lots of samples in order to construct a sampling distribution, and nor do we know the population parameters which are required to simulate samples like we did above (we do not know the population mean \\(\\mu\\) or standard deviation \\(\\sigma\\))\nInstead, we start with just one observed sample, e.g. here are the life satisfaction ratings of the 30 people that I surveyed:\n\nobserved_sample <- c(53.8, 59, 51.1, 66.7, 86.1, 71, 65.3, 72.6, 56.6, 56.8, 50.1, 57.3, 60, 74, 73.4, 68.3, 53.5, 85.5, 68.8, 67.6, 67.4, 47.9, 46.3, 96.1, 52.8, 78.9, 74.8, 50.9, 78.2, 63.4)\n\nWhat we can do is either:\n\nA: Estimate the standard error using a formula:\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}  \\\\\n\\quad \\\\\n\\begin{align}\n& \\text{Where} \\\\\n& \\sigma = \\text{standard deviation} \\\\\n& n = \\text{sample size} \\\\\n\\end{align}\n\\] Note that \\(\\sigma\\) is the standard deviation of the population, which is unknown to us. However, we can use the standard deviation of our sample (\\(\\hat \\sigma\\) or \\(s\\)) as our estimate of this:\n\n# SE = standard deviation / square root of n\nsd(observed_sample)/sqrt(length(observed_sample))\n\n[1] 2.275565\n\n\nor B: Simulate lots of sampling via bootstrapping.\nThis uses resampling with replacement4 from our original sample as a means of imitating repeated sampling. Note the replace = TRUE:\n\n# bootstrap means of resamples with replacement of the same size (30) as observed sample\nbootstrap_means <- replicate(1000, mean(sample(observed_sample, size = 30, replace = TRUE)))\n# SE = sd of bootstrap resample means \nsd(bootstrap_means)\n\n[1] 2.251539\n\n\n\n\n\n\n\n\nCentral Limit Theorem (CLT)\nProvided we have a sufficiently large sample, the notion of the standard error holds even when the underlying population distribution is not normally distributed.\nLet’s imagine we are interested in estimating these three things:\n\nThe average life satisfaction rating of people in Scotland\nThe proportion of people over 6 foot in Scotland\nThe average income of people in Scotland\n\nIf we could, we would collect data from everyone in Scotland, and might find distributions like these:\n\n\n\n\n\nFigure 9: Population distributions on three variables (numbers like 4e+05 are a compact way of writing 400000)\n\n\n\n\nIf we repeatedly collect samples of say, 50 people, and calculated the mean life satisfaction, the mean income, and the proportion >6 foot tall for each sample of 50, then the distribution of those sample statistics will be normal:\n\n\n\n\n\nFigure 10: Sampling distributions for three statistics computed on samples of n=50\n\n\n\n\n\nThe central limit theorem (CLT) states that when we take sufficiently large random samples from a population, the distribution of the sample means will be approximately normally distributed. This holds regardless of whether the population is normal (or skewed).\nYou can find little applets online which help to illustrate this, such as those from StatKey and WISE.\n\n\n\n\n\n\nConfidence Intervals\nOne thing that is often useful to do when using a sample estimate is to construct a range of plausible values, providing a view of our uncertainty, rather than just a point estimate (a single value).\nIn our simulation example (where we repeatedly take samples of the same size), we can simply ask for the points at which 2.5% of the sample means are below, and 2.5% are above. The quantile() function essentially orders our 1000 sample means and gives us the 25th and the 975th:\n\nmanysamplemeans <- replicate(1000, mean(rnorm(n = 30, mean = 65, sd = 12)))\nquantile(manysamplemeans, c(0.025, 0.975))\n\n    2.5%    97.5% \n60.82298 69.51470 \n\n\nHowever, in real life remember we don’t have lots of sample means. We just have the one:\n\nobserved_sample <- c(53.8, 59, 51.1, 66.7, 86.1, 71, 65.3, 72.6, 56.6, 56.8, 50.1, 57.3, 60, 74, 73.4, 68.3, 53.5, 85.5, 68.8, 67.6, 67.4, 47.9, 46.3, 96.1, 52.8, 78.9, 74.8, 50.9, 78.2, 63.4)\n\nWe now know, however, that we can approximate the standard error using the formula \\(SE = \\frac{sigma}{\\sqrt{n}}\\):\n\nsd(observed_sample)/sqrt(length(observed_sample))\n\n[1] 2.275565\n\n\nRemember that the standard error is the standard deviation of the theoretical distribution of all the possible sample statistics we might have computed (from all the samples of \\(n\\) that we might have taken). And recall also that from central limit theorem we can rely on assuming the sampling distribution to be normally distributed.\nCombine these with the rules of thumb for normal distributions that we saw above, where 68%/95%/99.7% of values will lie within 1/1.96/3 standard deviation of the mean.\nWe can use these to create intervals which X% of the time will contain the true population value. For 95%, we simply use 1.96 \\(\\times\\) standard error either side of our sample mean. This is called a confidence interval (CI).\n\\[\n\\begin{align}\n& \\text{95% CI} = \\bar{x} \\pm 1.96 \\times SE \\\\\n\\qquad \\\\\n& \\text{Where: } \\\\\n& \\bar{x} = \\text{sample mean} \\\\\n& SE = \\text{standard error}\n\\end{align}\n\\]\nOur confidence interval5 is therefore:\n\nxbar = mean(observed_sample)\nse = sd(observed_sample)/sqrt(length(observed_sample))\n\nxbar - (1.96*se)\n\n[1] 60.67989\n\nxbar + (1.96*se)\n\n[1] 69.60011\n\n\n\nConfidence Interval\nA confidence interval is a range of plausible values for an estimate.\nWhen we say that we have “95% confidence”, we mean that if we were to do the whole process (take a sample, compute a mean, compute a standard error, construct a confidence interval) over and over again, then 95 of every 100 confidence intervals we might construct will contain the true population parameter.\nThink of it like a game of ring toss: our intervals are the rings, and the value we are trying to estimate is the stake. We are [X]% confindent that the ring will land on the stake.\n\n\n\n\n\n\nFootnotes\n\n\nremember that standard deviation is \\(\\sqrt{\\text{variance}}\\)↩︎\nOften in neuropsychological testing, a set of “normative values” are provided in order to define “what is expected” (sometimes in reference to a specific population).↩︎\nand the statements may not hold for all individuals - for certain people, some drugs just won’t work! but what is important for a healthcare system deciding on whether or not to purchase supplies of a drug is the average treatment effect.↩︎\nImagine a bag full of coloured marbles. If we sample with replacement, then we take a marble out, record its colour, and put it back. Then we take a marble out, record its colour, and put it back. And so on. This means we might get the same marble more than once.↩︎\nusing the formula \\(\\frac{\\sigma}{\\sqrt{n}}\\) for standard error↩︎"
  },
  {
    "objectID": "03a_inference.html",
    "href": "03a_inference.html",
    "title": "Foundations of Inference",
    "section": "",
    "text": "The best way to learn R is to use it.\nTry following along with this reading by typing the code into your R script and running them. You will hopefully get the same output as is presented on this page below each bit of code.\nIf you get errors and warnings, don’t panic - read them!\nWe use statistics primarily to estimate parameters in a population. Whether we are polling people to make predictions about the proportion of people who will vote for a certain party in the next election, or conducting a medical trial and assessing the change in blood pressure for patients given drug X vs those given a placebo in order tp decide whether to put the drug into circulation in health service.\nWe have seen this already last week: We observed a sample of peoples’ life satisfaction ratings (scale 0-100), and we wanted to use these to make some statement about the wider population.\nA sample estimates is not going to be spot-on. By taking only a subset of people, we introduced sampling variability - we have uncertainty in the accuracy of our estimate. We saw previously how to make a confidence interval as a means of capturing this uncertainty, providing a range of plausible values.\nLet’s look at this with a different example.\nFirst, we read in the data and take a look at it. How many rows, how many columns, and so on.\nWhat we are interested in is the differences between the matching and mismatching times. For someone who took 10 seconds for the matching set, and 30 seconds for the mismatching set, we want to record their score as a difference of 20 seconds.\nWe can do that easily, storing the new values in a new variable:\nAnd to quickly prove to ourselves that is has worked:\nAnd let’s get some summary statistics and a visualisation of that distribution too.\nWhat we’re really interested in talking about is the average ‘mismatching - matching’ score for everybody, not just our sample of 131. But we only have those 131 people, so we’ll have to make do and use their data to provide us with an estimate.\nRemember that there are many many different samples of \\(n=131\\) that we might have taken. And if we had taken a different sample, then our mean ‘mismatching - matching’ score (the mean(stroopdata$diff) value) would be different.\nPreviously we learned about how the distribution of all possible sample means we might take is known as a sampling distribution. We also learned that these tend to be normally distributed (regardless of the underlying population distribution) and so we can use the standard deviation of the sampling distribution (known as the standard error) to quantify the variation due to sampling.\nTo actually get at the standard error, one of our options is to simulate the act of ‘taking many samples of size \\(n\\)’ by taking lots of samples with replacement from our original sample.\nAs our analyses become more advanced, then this will become more complex. In this example, because we are just interested in estimating a single mean value, we can use replicate to do calculate the means from 1000 resamples from our original sample. We can then simply calculate the standard deviation of all these means:\nAlternatively (and more conventionally), we use a formula of \\(\\frac{\\sigma}{\\sqrt{n}}\\), which we can calculate in R.\n\\(\\sigma\\) is the standard deviation of the population (which we are going to approximate by using \\(s\\), the standard deviation of our sample), and \\(n\\) is the size of our sample.\nFinally, we can use our standard error to construct a range of plausible values for our estimated ‘mismatching - matching’ score. The range is centered around our point estimate (the mean score in our sample), and we widen it to include X% of the possible means we might also see from a sample of the same size. This is achieved by multiplying the standard error by a value to corresponds our confidence level.\nFor a 95% interval2:"
  },
  {
    "objectID": "03a_inference.html#p-values",
    "href": "03a_inference.html#p-values",
    "title": "Foundations of Inference",
    "section": "p-values",
    "text": "p-values\nThe p-value is a formal way of testing a sample statistic against a null hypothesis.\nTo introduce the p-value, instead of thinking first about what we have observed in our sample, we need to think about what we would expect to observe if our null hypothesis is true.\nWith our Stroop Task example, our null hypothesis is that there is no difference between matching and mismatching conditions (\\(H_0: \\mu = 0\\)).\nUnder \\(H_0\\), the average ‘mismatching-matching’ score in the population is zero, and we would expect most of the samples we might take to have a mean ‘mismatching-matching’ score of close to this (not exactly 0, but centered around 0).\nWe saw earlier that we could express the sampling distribution of means taken from samples of size \\(n=131\\) using the standard error. Under \\(H_0\\) we would expect the samples of \\(n=131\\) we might take to have means that follow something like the distribution in Figure 2.\n\n\nCode\nstroopdata %>% \n  summarise(\n    s = sd(diff),\n    n = n(),\n    SE = s/sqrt(n)\n)\n\n\n# A tibble: 1 × 3\n      s     n    SE\n  <dbl> <int> <dbl>\n1  5.02   131 0.438\n\n\n\n\n\n\n\nFigure 2: Sampling distribution for mean of sample size 131, assuming population mean = 0. Observed sample mean shown in red\n\n\n\n\nWe can think of this as the sampling distribution of \\(\\bar{x}\\), but centered on our null hypothesis (in this case, \\(\\mu = 0\\)). We call this the ‘null distribution’.\nThe p-value tells us how likely it is to see values at least as extreme as our observed sample statistic, if the null is true.\nWe have seen how we can calculate this already: the pnorm() function gives us the area of a distribution to the one side of a given value:\n\npnorm(??, mean = 0, sd = 0.44, lower.tail = FALSE)\n\n\n\n\n\n\nFigure 3: the p-value is the area of the null distribution, as found with pnorm()\n\n\n\n\nFor our Stroop Task example, we observed a sample mean of\n\n\nCode\nmean(stroopdata$diff)\n\n\n[1] 2.402977\n\n\nIf the null hypothesis were true, and there was no ‘mismatching-matching’ difference, then the probability that we would see a sample (\\(n=131\\)) with a mean at least that large is:\n\n# (we calculated that SE = 0.44 above)\npnorm(2.40, mean = 0, sd = 0.44, lower.tail = FALSE)\n\n[1] 2.454914e-08\n\n\nwhich is R’s way of printing 0.00000002454914.\nThere is one last thing, and that the direction of our hypotheses. Recall from earlier that we stated \\(H_0: \\mu = 0\\) and \\(H_1: \\mu \\neq 0\\). This means that we are interested in the probability of getting results this far away from 0 in either direction.\nWe are interested in both tails:\n\n2 * pnorm(2.40, mean = 0, sd = 0.44, lower.tail = FALSE)\n\n[1] 4.909828e-08\n\n\n\n\n\n\n\nFigure 4: 2*pnorm gives the two tails\n\n\n\n\n\np-value\nThe p-value is the probability4 that we observe a test statistic at least as extreme as the one we observed, assuming the null hypothesis \\(H_0\\) to be true.\n\nNow that we have our p-value of 0.00000004909828, we need to use it to make a decision about our hypotheses.\nTypically, we pre-specify the probability level at which we will consider results to be so unlikely to have arisen from the null distribution that we will take them as evidence to reject the null hypothesis. This pre-specified level is commonly referred to as \\(\\alpha\\) (“alpha”). Setting \\(\\alpha = 0.05\\) means that we will reject \\(H_0\\) when we get a result which is extreme enough to only occur 0.05 (5%) of the time or less if the \\(H_0\\) is true.\nIn our case, 0.00000004909828 \\(< 0.05\\), so we reject the null hypothesis that there is no difference in the mismatching/matching conditions of the Stroop Task.\n\nThe language of NHST\nThere’s a lot of convention to how we talk about NHST, but the typical process is as follows:\n\nClearly specify the null and alternative hypotheses.\n\nSpecify \\(\\alpha\\)\nCalculate statistic\nCompute p-value\n\nIf \\(p<\\alpha\\), then reject the null hypothesis.\nIf \\(p\\geq\\alpha\\), then fail to reject* the null hypothesis.\n\n\n*Note, we don’t “accept” the null, we just “fail to reject” it. Think of it like a criminal court - the null hypothesis is “innocent until proven guilty”"
  },
  {
    "objectID": "03b_inference2.html",
    "href": "03b_inference2.html",
    "title": "Practical Inference",
    "section": "",
    "text": "logic of everything we have learned so far is going to apply, but there’s something we have been sweeping under the carpet.\nwe were using this formula for our standard error:\nSE eq, where sigma\nwe don’t ever know sigma, so we use \\(s\\) instead. this is okay when we have a large \\(n\\) and \\(s\\) provides accurate estimate of \\(\\sigma\\)\nintroduce t\nfat tails = correction\nt sticky\ndf sticky\nwhen we learned about CIs and p-values, we used qnorm to construct confidence intervals (we got 1.96 by usign qnorm to ask qnorm(.025, .975)) we used pnorm to calculate p-values\nwhat would be more appropriate is to, instead of normal dist, use the reference t dist with corresponding df. qt and pt\nrather than using the SE to define the reference dist, we standardise the “how far from null?” bit in terms of SE t = xbar / se\nwe can then use pt for p-values and qt for CIs\n\nstroopdata <- read_csv(\"../../data/stroop.csv\")\nstroopdata <- \n  stroopdata %>% \n  mutate(\n    diff = mismatching - matching\n  )\n\nse = sd(stroopdata$diff)/sqrt(nrow(stroopdata))\n\ntstat = mean(stroopdata$diff) / se\ntstat\n\n[1] 5.483367\n\n2*pt(tstat, df = 130, lower.tail=FALSE)\n\n[1] 2.092441e-07\n\nmean(stroopdata$diff) + (qt(c(.025, .975), df = 130) * se)\n\n[1] 1.535991 3.269963\n\nt.test(stroopdata$diff, mu=0)\n\n\n    One Sample t-test\n\ndata:  stroopdata$diff\nt = 5.4834, df = 130, p-value = 2.092e-07\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1.535991 3.269963\nsample estimates:\nmean of x \n 2.402977 \n\n\n\n“t.test”\nall this seems like a lot of effort, and you probably won’t actually use much of the code from the last two chapters. We can easily conduct many tests without having to manually calculate p-values or manually construct confidence intervals.\nInstead, functions in R do it all for us.\nt.test\ndifferent types of t test we have done a one-sample mean test. we test whether the mean of a set of values is different from hypothesised value xbar - mu_0 e.g.\nthe paired t.test is actually the same thing in disguise, and actually, this is what we did all along in the stroop task example. to test whether paired variables mean difference is other than zero. rephrased, test whether mean difference from var1 and var2 is neq 0.\nstroopdata diff stroopdata paired\nindependent samples"
  },
  {
    "objectID": "04_ex.html",
    "href": "04_ex.html",
    "title": "Week 4 Exercises: Chi-Square Tests",
    "section": "",
    "text": "\\(\\chi^2\\) Test of Independence\n\nData: TipJokes\n\nResearch Question: Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?\n\nA study published in the Journal of Applied Social Psychology1 investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. The waiter randomly assigned coffee-ordering customers to one of three groups. When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all.\nThe data are available at https://uoepsy.github.io/data/TipJoke.csv.\nThe dataset contains the variables:\n\nCard: None, Joke, Ad.\nTip: 1 = The customer left a tip, 0 = The customer did not leave tip.\n\n\n\nQuestion 9\n\n\nProduce a plot and a table to display the relationship between whether or not the customer left a tip, and what (if any) card they received alongside the bill.\n\n\n\n\n Solution \n\n\n\ntipjoke <- read_csv('https://uoepsy.github.io/data/TipJoke.csv')\n\ntable(tipjoke$Card, tipjoke$Tip)\n\n      \n        0  1\n  Ad   60 14\n  Joke 42 30\n  None 49 16\n\nplot(table(tipjoke$Card, tipjoke$Tip))\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nWhat would you expect the cell counts to look like if there were no relationship between what the waiter left and whether or not the customer tipped?\n\n\n\n\n Solution \n\n\nIn total, 60 customers tipped (14+30+16), and 151 did not. So overall, 0.28 (\\(\\frac{60}{(60+151)}\\)) of customers tip.\n74 customers got an Ad card, 72 customers got a Joke, and 65 got None. If this were independent of whether or not they left a tip, we would expect equal proportions of tippers in each group.\nSo we would expect 0.28 of each group to leave a tip.\nYou can think about observed vs expected by looking at the two-way table along with the marginal row and column totals given:\n\n\n\n\n \n  \n      \n    0 \n    1 \n     \n  \n \n\n  \n    Ad \n     \n     \n    74 \n  \n  \n    Joke \n     \n     \n    72 \n  \n  \n    None \n     \n     \n    65 \n  \n  \n     \n    151 \n    60 \n    211 \n  \n\n\n\n\n\nFor a given cell of the table we can calculate the expected count as \\(\\text{row total} \\times \\frac{\\text{column total}}{\\text{samplesize}}\\):\nExpected:\n\n\n\n\n \n  \n      \n    0 \n    1 \n     \n  \n \n\n  \n    Ad \n    52.96 \n    21.04 \n    74 \n  \n  \n    Joke \n    51.53 \n    20.47 \n    72 \n  \n  \n    None \n    46.52 \n    18.48 \n    65 \n  \n  \n     \n    151.00 \n    60.00 \n    211 \n  \n\n\n\n\n\n(If you’re wondering how we do this in R, we saw in the lectures briefly a complicated bit of code using %o% which could do this for us):\n\nt <- tipjoke %>%\n  select(Card, Tip) %>% table()\n\ne <- rowSums(t) %o% colSums(t) / sum(t)\ne\n\n            0        1\nAd   52.95735 21.04265\nJoke 51.52607 20.47393\nNone 46.51659 18.48341\n\n\n\n\n\n\nQuestion 11\n\n\nJust like we gave the chisq.test() function a table of observed frequencies when we conducted a goodness of fit test in earlier exercises, we can give it a two-way table of observed frequencies to conduct a test of independence.\nTry it now.\n\n\n\n\n Solution \n\n\n\nchisq.test(table(tipjoke$Card, tipjoke$Tip))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(tipjoke$Card, tipjoke$Tip)\nX-squared = 9.9533, df = 2, p-value = 0.006897\n\n\n\n\n\n\nNote that for the test we just performed (chisq.test(table(tipjoke$Card, tipjoke$Tip))), the degrees of freedom is given as 2.\nThis is because the degrees of freedom for a \\(\\chi^2\\) test of independence is calculated as:\n\\[\n\\begin{align}\n& df = (r - 1)(c - 1) \\\\\n& \\text{Where:} \\\\\n& r = \\text{number of rows} \\\\\n& c = \\text{number of columns} \\\\\n\\end{align}\n\\]\nWhy is this?\nWell, remember that the degrees of freedom is the number of values that are free to vary as we estimate parameters. In a \\(3 \\times 2\\) table like the one we have for Cards \\(\\times\\) Tips, the degrees of freedom is the number of cells in the table that can vary before we can simply calculate the values of the other cells (where we’re constrained by the need to sum to our row/column totals).\n\n\n\n\n\n\nSome RMarkdown\n\nQuestion 12\n\n\nCan you create an RMarkdown document which:\n\nReads in the https://uoepsy.github.io/data/TipJoke.csv data.\nConducts and reports a \\(\\chi^2\\) test of independence examining whether telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer.\nSuccessfully compiles (“knits”) into an .html file.\n\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nGueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. Journal of Applied Social Psychology, 32(9), 1955-1963.↩︎"
  },
  {
    "objectID": "04b_revistnhst.html",
    "href": "04b_revistnhst.html",
    "title": "Reading 4A: Revisiting NHST",
    "section": "",
    "text": "In the last couple of weeks you have performed a number of different types of statistical hypothesis test, it is worth revisiting the general concept in order to consolidate what you’ve been doing.\n\nStep 1. We have been starting by considering what a given statistic is likely to be if a given hypothesis (the null) were true.\n\nFor the \\(t\\)-tests, if the null hypothesis is true (there is no difference between group means/between our observed mean and some value), then our \\(t\\)-statistics (if we could do our study loads of times) will mainly fall around 0, and follow a \\(t\\)-distribution. The precise \\(t\\)-distribution depends on the degrees of freedom, which in turn depends on how much data we have.\n\nFor the \\(\\chi^2\\) tests, if the null hypothesis is true and there is no difference between the observed and expected frequencies, then our \\(\\chi^2\\)-statistics will follow the \\(\\chi^2\\) distribution (i.e., with 2 categories, most of them will be between 0 and 2, with fewer falling >2, see the yellow line in Figure 1).\n\nStep 2. We calculate our statistic from our observed data.\nStep 3. We ask what the probability is of getting a statistic at least as extreme as we get from Step 2, assuming the null hypothesis we stated in Step 1.\n\n\n\n\n\n\n\n\nFigure 1: Chi-Square Distributions\n\n\n\n\n\nIf you’re finding the programming easy, but the statistical concepts difficult\nAnother way which might help to think about this is that if we can make a computer do something over and over again, we can do stats! You may already be familiar with this idea from exercises with the function replicate()!\n\n\nmake the computer generate random data, based on some null hypothesis. Do it lots of times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhat proportion of the simulations produce results similar to the observed data (i.e., as extreme or more extreme)? This is \\(p\\). The only difference between this and “statistics” is that we calculate \\(p\\) using math, rather than having to generate random data.\n\n\n\n\n\n\nStatistical vs Practical Significance\nLet’s suppose that an agricultural company is testing out a new fertiliser they have developed to improve tomato growth. They know that, on average, for every 5cm taller a tomato plant is, it tends to provide 1 more tomato. Taller plants = more tomatoes.\nThey plant 1000 seeds (taken from the same tomato plant) in the same compost and place them in positions with the same amount of sunlight. 500 of the plants receive 100ml of water daily, and the other 500 receive a 100ml of the fertiliser mixed with water. After 100 days, they measure the height of all the tomato plants (in cm).\n\n\n\n\n\n\n\n\n\nYou can find the data at https://uoepsy.github.io/data/tomatogrowth.csv.\nWe want to conduct the appropriate test to determine whether the fertiliser provides a statistically significant improvement to tomato plant growth.\nOur outcome variable is growth, which is continuous, and our predictor variable is the grouping (whether they received fertiliser or not). So we’re looking at whether there is a difference in mean growth between the two groups. A t-test will do here.\nOur alternative hypothesis is that the difference in means \\((treatment - control)\\) is greater than 0 (i.e., it improves growth). The t.test() function will use alphabetical ordering of the group variable, so if we say alternative=\"less\" then it is the direction we want \\((control - treatment < 0)\\):1:\n\ntomato <- read_csv(\"https://uoepsy.github.io/data/tomatogrowth.csv\")\nt.test(tomato$height ~ tomato$group, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  tomato$height by tomato$group\nt = -2.0085, df = 997.97, p-value = 0.02243\nalternative hypothesis: true difference in means between group control and group treatment is less than 0\n95 percent confidence interval:\n       -Inf -0.2296311\nsample estimates:\n  mean in group control mean in group treatment \n               115.1955                116.4692 \n\n\n\n\n\nHooray, it is significant! So should we use this fertiliser on all our tomatoes? We need to carefully consider the agricultural company’s situation: given that the fertiliser is comparitively pricey for them to manufacture, is it worth putting into production?\nWhile the fertiliser does improve plant growth to a statistically significant (at \\(\\alpha=0.05\\)) degree, the improvement is minimal. The difference in means is only 1.2737cm. Will this result in many more tomatoes? Probably not.\nFurthermore, if we take a look at the confidence interval provided by the t.test() function, we can see that a plausible value for the true difference in means is 0.23cm, which is tiny!\n\n\n\n\n\nFurther Thoughts\nThe above example is just a silly demonstration that whether or not our p-value is below some set criteria (e.g., .05, .01, .001) is only a small part of the picture. There are many things which are good to remember about p-values:\n\nWith a big enough sample size, even a tiny tiny effect is detectable at <.05. For example, you might be interested in testing if the difference in population means across two groups is 0 (\\(\\mu_1 - \\mu_2 = 0\\). Your calculated sample difference could be \\(\\bar{x}_1 - \\bar{x}_2 = 0.00002\\) but with a very small p-value of 0.00000001. This would tell you that there is strong evidence that the observed difference in means (0.00002) is significantly different from 0. However, the practical difference, that is - the magnitude of the distance between 0.00002 and 0 - is negligible and of pretty much no interest to practitioners. This is the idea we saw in the tomato-plant example.\nThe criteria (\\(\\alpha\\)) which we set (at .05, .01, etc.), is arbitrary.\nTwo things need to be kept in mind: there is the true status of the world (which is unknown to us) and the collected data (which are available and reveal the truth only in part).\nAn observed p-value smaller than the chosen alpha does not imply the true presence of an effect. The observed difference might be due to sampling variability.\n\n\n\n\n\n\nFigure 2: Two possible samples (blue dots) drawn from two populations with same mean. On the left, the selected sample shows a big difference. On the right, the sample shows no difference. Samples such as that on the left are very unlikely to happen (e.g., 5% of the time). It is for these unlikely samples that we would reject the null hypothesis incorrectly 5% of the time.\n\n\n\n\n\nEven if a null hypothesis about the population is actually true, then 5% (if \\(\\alpha\\) = 0.05) of the test-statistics computed on different samples from that population would result in a p-value <.05. If you were to obtain 100 random samples from that population, five out of the 100 p-values are likely to be <.05 even if the null hypothesis about the population was actually true.\nIf you have a single dataset, and you perform several tests of hypotheses on those data, each test comes with a probability of incorrectly rejecting the null (making a type I error) of 5%. Hence, considering the entire family of tests computed, your overall type I error probability will be larger than 5%. In simple words, this means that if you perform enough tests on the same data, you’re almost sure to reject one of the null hypotheses by mistake. This concept is known as multiple comparisons.\n\n\n\n\n\n\nFurther Reading (Optional)\nThere are many different competing approaches to doing statistical analyses.\nIn this course we are learning about what is known as the frequentist framework. Roughly speaking, this is where probabilities are defined as “long-run frequencies” (i.e., the probability of \\(x\\) happening over many many trials2). Even within the frequentist approach, there are different views as to how to how this definition of probability is best utilised.\nThe following links provide some introductory readings to some of the different schools of thought:\n\nPerezgonzalez, J. D. (2015). Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing. Frontiers in Psychology, 6, 223.\nCalin-Jageman, R. J., & Cumming, G. (2019). The new statistics for better science: ask how much, how uncertain, and what else is known. The American Statistician, 73(sup1), 271-280.\nThe correctly-used p value needs an effect size and CI - don’t worry too much about the background of this blog, but it offers some useful visualisations to show how important it is to remember about the uncertainty in our estimates.\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nWe could instead make the group variable a factor and specify the order of the levels↩︎\nFor those of you who are interested in what alternative definitions there are, do a google search for “frequentist vs bayesian”. Be prepared that this will open a big can of worms!↩︎"
  },
  {
    "objectID": "05_ex.html",
    "href": "05_ex.html",
    "title": "Week 5 Exercises: Cov, Cor, Models",
    "section": "",
    "text": "Q1: Go to http://guessthecorrelation.com/ and play the “guess the correlation” game for a little while to get an idea of what different strengths and directions of \\(r\\) can look like.\n\n\n\n\nCorrelation\n\nData: Sleep levels and daytime functioning\nA researcher is interested in the relationship between hours slept per night and self-rated effects of sleep on daytime functioning. She recruited 50 healthy adults, and collected data on the Total Sleep Time (TST) over the course of a seven day period via sleep-tracking devices.\nAt the end of the seven day period, participants completed a Daytime Functioning (DTF) questionnaire. This involved participants rating their agreement with ten statements (see @tab-sleepitems). Agreement was measured on a scale from 1-5. An overall score of daytime functioning can be calculated by:\n\nreversing the scores for items 4,5 and 6 (because those items reflect agreement with positive statements, whereas the other ones are agreement with negative statement);\nsumming the scores on each item; and\nsubtracting the sum score from 50 (the max possible score). This will make higher scores reflect better perceived daytime functioning.\n\nThe data is available at https://uoepsy.github.io/data/sleepdtf.csv.\n\n\n# A tibble: 10 × 2\n   Item    Statement                                        \n   <chr>   <chr>                                            \n 1 Item_1  I often felt an inability to concentrate         \n 2 Item_2  I frequently forgot things                       \n 3 Item_3  I found thinking clearly required a lot of effort\n 4 Item_4  I often felt happy                               \n 5 Item_5  I had lots of energy                             \n 6 Item_6  I worked efficiently                             \n 7 Item_7  I often felt irritable                           \n 8 Item_8  I often felt stressed                            \n 9 Item_9  I often felt sleepy                              \n10 Item_10 I often felt fatigued                            \n\n\n\n\nQuestion 2\n\n\nRead in the data, and calculate the overall daytime functioning score, following the criteria outlined above. Make this a new column in your dataset.\n\nTo reverse items 4, 5 and 6, we we need to make all the scores of 1 become 5, scores of 2 become 4, and so on… What number satisfies all of these equations: ? - 5 = 1, ? - 4 = 2, ? - 3 = 3?\nTo quickly sum accross rows, you might find the rowSums() function useful (you don’t have to use it though)\n\n\n\n\n\n Solution \n\n\n\nsleepdtf <- read_csv(\"https://uoepsy.github.io/data/sleepdtf.csv\")\nsummary(sleepdtf)\n\n      TST             item_1         item_2         item_3         item_4    \n Min.   : 4.900   Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00  \n 1st Qu.: 7.225   1st Qu.:1.00   1st Qu.:2.00   1st Qu.:1.25   1st Qu.:1.00  \n Median : 7.900   Median :1.00   Median :2.00   Median :2.00   Median :1.00  \n Mean   : 8.004   Mean   :1.58   Mean   :2.46   Mean   :2.38   Mean   :1.26  \n 3rd Qu.: 9.025   3rd Qu.:2.00   3rd Qu.:3.00   3rd Qu.:3.00   3rd Qu.:1.00  \n Max.   :11.200   Max.   :3.00   Max.   :5.00   Max.   :5.00   Max.   :3.00  \n     item_5         item_6         item_7         item_8        item_9    \n Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.0   Min.   :1.00  \n 1st Qu.:2.00   1st Qu.:2.00   1st Qu.:1.00   1st Qu.:2.0   1st Qu.:2.00  \n Median :2.00   Median :3.00   Median :2.00   Median :2.5   Median :3.00  \n Mean   :2.36   Mean   :2.78   Mean   :2.04   Mean   :2.5   Mean   :2.96  \n 3rd Qu.:3.00   3rd Qu.:4.00   3rd Qu.:3.00   3rd Qu.:3.0   3rd Qu.:4.00  \n Max.   :4.00   Max.   :5.00   Max.   :4.00   Max.   :4.0   Max.   :5.00  \n    item_10    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :3.00  \n Mean   :2.54  \n 3rd Qu.:3.00  \n Max.   :5.00  \n\n# To reverse the items, we can simply do 6 minus the score.   \nsleepdtf <- \n  sleepdtf %>% mutate(\n    item_4=6-item_4,\n    item_5=6-item_5,\n    item_6=6-item_6\n  ) \n\n# Now using rowSums(), and subtracting it from 50 (the max score)\nsleepdtf$dtf = 50-rowSums(sleepdtf[, 2:11])\n\n\n\n\n\nQuestion 3\n\n\nCalculate the correlation between the total sleep time (TST) and the overall daytime functioning score calculated in the previous question.\nConduct a test to establish the probability of observing a correlation this strong in a sample of this size assuming the true correlation to be 0.\n Write a sentence or two summarising the results.\n\n\n\n\n Solution \n\n\n\ncor.test(sleepdtf$TST, sleepdtf$dtf)\n\n\n    Pearson's product-moment correlation\n\ndata:  sleepdtf$TST and sleepdtf$dtf\nt = 6.244, df = 48, p-value = 1.062e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4807039 0.7989417\nsample estimates:\n      cor \n0.6694741 \n\n\n\nThere was a strong positive correlation between total sleep time and self-reported daytime functioning score (\\(r\\) = 0.67, \\(t(48)\\) = 6.24, \\(p < .001\\)) in the current sample. As total sleep time increased, levels of self-reported daytime functioning increased.\n\n\n\n\n\nQuestion 4 (open-ended)\n\n\nThink about this relationship in terms of causation.\n Claim: Less sleep causes poorer daytime functioning.\n Why might it be inappropriate to make the claim above based on these data alone? Think about what sort of study could provide stronger evidence for such a claim.\n\nThings to think about:\n\ncomparison groups.\n\nrandom allocation.\n\nmeasures of daytime functioning.\n\nmeasures of sleep time.\n\nother (unmeasured) explanatory variables.\n\n\n\n\n\n\n\n\n\n\nFunctions and Models\n\nQuestion 5\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the data from the Scottish National Gallery using ggplot().\nWe know that there is a mathematical model for the relationship between the side-length and perimeter of squares: \\(perimeter = 4 \\times \\ side\\).\nTry adding the following line to your plot:\n\n  stat_function(fun = ~.x * 4)\n\n\n\n\n\n Solution \n\n\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot(data = sng, aes(x = side, y = perimeter)) +\n  geom_point(colour = 'black', alpha = 0.5, size = 3) +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')+\n  stat_function(fun = ~.x * 4)\n\n\n\n\nFigure 1: The exact relationship between side and perimeter of squares.\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\nQuestion 6\n\n\nUse our mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery, but we can work out the perimeter of an hypothetical square painting with 1.5m sides, using our model.\nYou can obtain this prediction either using the plot from the previous question, or calculating it algebraically.\n\n\n\n\n Solution \n\n\nVisual approach\n\n\n\n\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing… Let’s try the next approach.\n Algebraic approach\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 \\times \\ x = 4 \\times \\ (1.5) = 6\n\\]\n\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m.\n\nNOTE: Don’t forget to always include the measurement units when reporting/writing-up results!\n\n\n\n\nData: HandHeight\nThis dataset, from Jessican M Utts and Robert F Heckard. 2015. Mind on Statistics (Cengage Learning)., records the height and handspan reported by a random sample of 167 students as part of a class survey.\nThe variables are:\n\nheight, measured in inches\nhandspan, measured in centimetres\n\nThe data are available at https://uoepsy.github.io/data/handheight.csv\n\n\nQuestion 7\n\n\nConsider the relationship between height (in inches) and handspan (in cm).\nRead the handheight data into R, and investigate how handspan varies as a function of height for the students in the sample.\nDo you notice any outliers or points that do not fit with the pattern in the rest of the data?\nComment on any main differences you notice between this relationship and the relationship between sides and perimeter of squares.\n\n\n\n\n Solution \n\n\nThe handheight data set contains two variables, height and handspan, which are both numeric and continuous. We display the relationship between two numeric variables with a scatterplot.\nWe can also add marginal boxplots for each variable using the package ggExtra. Before using the package, make sure you have it installed via install.packages('ggExtra').\n\nhandheight <- read_csv(file = 'https://uoepsy.github.io/data/handheight.csv')\n\nlibrary(ggExtra)\n\nplt <- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nggMarginal(plt, type = 'boxplot')\n\n\n\n\nFigure 2: The statistical relationship between height and handspan.\n\n\n\n\nOutliers are extreme observations that do not seem to fit with the rest of the data. This could either be:\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in @fig:handheight-scatterplot do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive (that is, increasing) linear relationship between height and handspan.\nRecall Figure 1, displaying the relationship between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure @ref(fig:handheight-scatterplot), instead, displays the relationship between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\nQuestion 8\n\n\nUsing the following command, superimpose on top of your scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\ngeom_smooth(method = lm, se = FALSE)\nComment on any differences you notice with the line summarising the linear relationship between side and perimeter.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\n\nThe best-fit line.\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one. That is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan, instead, is only able to predict the average handspan for a given value of height. This is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\nThe mathematical model \\(Perimeter = 4 \\times \\ Side\\) represents the exact relationship between side-length and perimeter of squares.\nIn contrast, the relationship between height and handspan shows deviations from an “average pattern”. Hence, we need to create a model that allows for deviations from the linear relationship. This is called a statistical model.\nA statistical model includes both a deterministic function and a random error term:\n\\[\nHandspan = \\beta_0 + \\beta_1 \\ Height + \\epsilon\n\\]\nor, in short,\n\\[\ny = \\underbrace{\\beta_0 + \\beta_1 \\ x}_{f(x)} + \\underbrace{\\epsilon}_{\\text{random error}}\n\\]\nThe deterministic function need not be linear if the scatterplot displays signs of nonlinearity. In the equation above, the terms \\(\\beta_0\\) and \\(\\beta_1\\) are numbers specifying where the line going through the data meets the y-axis and its slope (rate of increase/decrease).\n\n\nQuestion 9\n\n\n\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\ Height\n\\]\nWhat is your best guess for the handspan of a student who is 73in tall?\nAnd for students who are 5in?\n\n\n\n\n Solution \n\n\nThe predicted average handspan for students who are 73in tall is \\(-3 + 0.35 * 73 = 22.55\\)cm.\nThe predicted average handspan for students who are 5in tall is \\(-3 + 0.35 * 5 = -1.25\\)cm. But wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\n\nFigure 3: Source: Randall Munroe, xkcd.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nYes, the error term is gone. This is because the line of best-fit gives you the prediction of the average handspan for a given height, and not the individual handspan of a person, which will almost surely be different from the prediction of the line.↩︎"
  },
  {
    "objectID": "05a_covcor.html",
    "href": "05a_covcor.html",
    "title": "Covariance, Correlation, and Modelling",
    "section": "",
    "text": "Covariance & Correlation\nOur data for this walkthrough is from a (hypothetical) study on memory. Twenty participants studied passages of text (c500 words long), and were tested a week later. The testing phase presented participants with 100 statements about the text. They had to answer whether each statement was true or false, as well as rate their confidence in each answer (on a sliding scale from 0 to 100). The dataset contains, for each participant, the percentage of items correctly answered, and the average confidence rating. Participants’ ages were also recorded.\nLet’s take a look at the relationships between the percentage of items answered correctly (recall_accuracy) and participants’ average self-rating of confidence in their answers (recall_confidence):\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nrecalldata <- read_csv(\"https://uoepsy.github.io/data/recalldata.csv\")\n\nggplot(recalldata, aes(x=recall_confidence, recall_accuracy))+\n  geom_point() + \nggplot(recalldata, aes(x=age, recall_accuracy))+\n  geom_point()\n\n\n\n\n\n\n\n\nThese two relationships look quite different.\n\nFor participants who tended to be more confident in their answers, the percentage of items they correctly answered tends to be higher.\n\nThe older participants were, the lower the percentage of items they correctly answered tended to be.\n\nWhich relationship should we be more confident in and why?\nIdeally, we would have some means of quantifying the strength and direction of these sorts of relationship. This is where we come to the two summary statistics which we can use to talk about the association between two numeric variables: Covariance and Correlation.\n\n\nCovariance\n\n Covariance is the measure of how two variables vary together. It is the change in one variable associated with the change in another variable.\nFor samples, covariance is calculated using the following formula:\n\\[\\mathrm{cov}(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})\\]\nwhere:\n\n\\(x\\) and \\(y\\) are two variables; e.g., age and recall_accuracy;\n\\(i\\) denotes the observational unit, such that \\(x_i\\) is value that the \\(x\\) variable takes on the \\(i\\)th observational unit, and similarly for \\(y_i\\);\n\\(n\\) is the sample size.\n\nIn R\nWe can calculate covariance in R using the cov() function.\ncov() can take two variables cov(x = , y = ).\n\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 118.0768\n\n\n\n\n Optional: Manually calculating covariance\n\n\n\nCreate 2 new columns in the memory recall data, one of which is the mean recall accuracy, and one which is the mean recall confidence.\n\n\nrecalldata <-\n  recalldata %>% mutate(\n    maccuracy = mean(recall_accuracy),\n    mconfidence = mean(recall_confidence)\n  )\n\n\nNow create three new columns which are:\n\nrecall accuracy minus the mean recall accuracy - this is the \\((x_i - \\bar{x})\\) part.\n\nconfidence minus the mean confidence - and this is the \\((y_i - \\bar{y})\\) part.\n\nthe product of i. and ii. - this is calculating \\((x_i - \\bar{x})\\)\\((y_i - \\bar{y})\\).\n\n\n\nrecalldata <- \n  recalldata %>% \n    mutate(\n      acc_minus_mean_acc = recall_accuracy - maccuracy,\n      conf_minus_mean_conf = recall_confidence - mconfidence,\n      prod_acc_conf = acc_minus_mean_acc * conf_minus_mean_conf\n    )\n\nrecalldata\n\n# A tibble: 20 × 9\n   ppt    recall_accuracy recall_confidence   age maccuracy mconfidence\n   <chr>            <dbl>             <dbl> <dbl>     <dbl>       <dbl>\n 1 ppt_1               72              66.6    72      69.2        55.4\n 2 ppt_2               66              47.1    35      69.2        55.4\n 3 ppt_3               47              43.8    48      69.2        55.4\n 4 ppt_4               84              58.9    52      69.2        55.4\n 5 ppt_5               84              75.1    46      69.2        55.4\n 6 ppt_6               58              53.5    41      69.2        55.4\n 7 ppt_7               52              48.5    86      69.2        55.4\n 8 ppt_8               76              67.1    58      69.2        55.4\n 9 ppt_9               41              40.4    59      69.2        55.4\n10 ppt_10              67              46.8    22      69.2        55.4\n11 ppt_11              60              50.6    62      69.2        55.4\n12 ppt_12              67              28.7    40      69.2        55.4\n13 ppt_13              76              69.0    47      69.2        55.4\n14 ppt_14              93              67.9    51      69.2        55.4\n15 ppt_15              71              54.5    34      69.2        55.4\n16 ppt_16              71              64.6    37      69.2        55.4\n17 ppt_17              99              66.3    37      69.2        55.4\n18 ppt_18              66              49.0    51      69.2        55.4\n19 ppt_19              77              58.5    41      69.2        55.4\n20 ppt_20              58              51.4    57      69.2        55.4\n# … with 3 more variables: acc_minus_mean_acc <dbl>,\n#   conf_minus_mean_conf <dbl>, prod_acc_conf <dbl>\n\n\n\nFinally, sum the products, and divide by \\(n-1\\)\n\n\nrecalldata %>%\n  summarise(\n    prod_sum = sum(prod_acc_conf),\n    n = n()\n  )\n\n# A tibble: 1 × 2\n  prod_sum     n\n     <dbl> <int>\n1    2243.    20\n\n2243.46 / (20-1)\n\n[1] 118.0768\n\n\nWhich is the same result as using cov():\n\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 118.0768\n\n\n\n\n\n\n Optional: Covariance explained visually\n\n\n\n\n\nConsider the following scatterplot:\n\n\n\n\n\n\n\n\n\n Now let’s superimpose a vertical dashed line at the mean of \\(x\\) (\\(\\bar{x}\\)) and a horizontal dashed line at the mean of \\(y\\) (\\(\\bar{y}\\)):\n\n\n\n\n\n\n\n\n\n Now let’s pick one of the points, call it \\(x_i\\), and show \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\).\n Notice that this makes a rectangle.\n As \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\) are both positive values, their product - \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) - is positive.\n\n\n\n\n\n\n\n\n\n In fact, for all these points in red, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is positive (remember that a negative multiplied by a negative gives a positive):\n\n\n\n\n\n\n\n\n\n And for these points in blue, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is negative:\n\n\n\n\n\n\n\n\n\n Now take another look at the formula for covariance:\n\\[\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}\\]\nIt is the sum of all these products divided by \\(n-1\\). It is the average of the products!\n\n\n\n\n\nCorrelation - \\(r\\)\n\n You can think of correlation as a standardized covariance. It has a scale from negative one to one, on which the distance from zero indicates the strength of the relationship.\nJust like covariance, positive/negative values reflect the nature of the relationship.\nThe correlation coefficient is a standardised number which quantifies the strength and direction of the linear relationship between two variables. In a population it is denoted by \\(\\rho\\), and in a sample it is denoted by \\(r\\).\nWe can calculate \\(r\\) using the following formula:\n\\[\nr_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\n\\]\nWe can actually rearrange this formula to show that the correlation is simply the covariance, but with the values \\((x_i - \\bar{x})\\) divided by the standard deviation (\\(s_x\\)), and the values \\((y_i - \\bar{y})\\) divided by \\(s_y\\):\n\\[\nr_{(x,y)}=\\frac{1}{n-1} \\sum_{i=1}^n \\left( \\frac{x_{i}-\\bar{x}}{s_x} \\right) \\left( \\frac{y_{i}-\\bar{y}}{s_y} \\right)\n\\]\n The correlation is the simply the covariance of standardised variables (variables expressed as the distance in standard deviations from the mean).\nProperties of correlation coefficients\n\n\\(-1 \\leq r \\leq 1\\)\nThe sign indicates the direction of association\n\npositive association (\\(r > 0\\)) means that values of one variable tend to be higher when values of the other variable are higher\nnegative association (\\(r < 0\\)) means that values of one variable tend to be lower when values of the other variable are higher\nno linear association (\\(r \\approx 0\\)) means that higher/lower values of one variable do not tend to occur with higher/lower values of the other variable\n\nThe closer \\(r\\) is to \\(\\pm 1\\), the stronger the linear association\n\\(r\\) has no units and does not depend on the units of measurement\nThe correlation between \\(x\\) and \\(y\\) is the same as the correlation between \\(y\\) and \\(x\\)\n\nIn R\nJust like R has a cov() function for calculating covariance, there is a cor() function for calculating correlation:\n\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 0.6993654\n\n\n\n\n Optional: Manually calculating correlation\n\n\nWe calculated above that \\(\\text{cov}(\\text{recall-accuracy}, \\text{recall-confidence})\\) = 118.077.\nTo calculate the correlation, we can simply divide this by the standard deviations of the two variables \\(s_{\\text{recall-accuracy}} \\times s_{\\text{recall-confidence}}\\)\n\nrecalldata %>% summarise(\n  s_ra = sd(recall_accuracy),\n  s_rc = sd(recall_confidence)\n)\n\n# A tibble: 1 × 2\n   s_ra  s_rc\n  <dbl> <dbl>\n1  14.5  11.6\n\n118.08 / (14.527 * 11.622)\n\n[1] 0.6993902\n\n\nWhich is the same result as using cor():\n\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 0.6993654\n\n\n\n\n\n\n\n\n\n\nCorrelation Test\nNow that we’ve seen the formulae for covariance and correlation, as well as how to quickly calculate them in R using cov() and cor(), we can use a statistical test to establish the probability of finding an association this strong by chance alone.\n\nHypotheses\nThe hypotheses of the correlation test are, as always, statements about the population parameter (in this case the correlation between the two variables in the population - i.e., \\(\\rho\\)).\nNull Hypothesis:\n\n\\(H_0: \\rho = 0\\). There is not a linear relationship between \\(x\\) and \\(y\\) in the population.\n\nAlternative Hypothesis:\n\n\\(H_1: \\rho > 0\\) There is a positive linear relationship between \\(x\\) and \\(y\\).\n\\(H_1: \\rho < 0\\) There is a negative linear relationship between \\(x\\) and \\(y\\).\n\\(H_1: \\rho \\neq 0\\) There is a linear relationship between \\(x\\) and \\(y\\).\n\n\n\nTest Statistic\nThe test statistic for this test here is another \\(t\\) statistic, the formula for which depends on both the observed correlation (\\(r\\)) and the sample size (\\(n\\)):\n\\[t = r \\sqrt{\\frac{n-2}{1-r^2}}\\]\n\n\np-value\nWe calculate the p-value for our \\(t\\)-statistic as the long-run probability of a \\(t\\)-statistic with \\(n-2\\) degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed \\(t\\)-statistic.\n\n\nIn R\nWe can test the significance of the correlation coefficient really easily with the function cor.test():\n\ncor.test(recalldata$recall_accuracy, recalldata$recall_confidence)\n\n\n    Pearson's product-moment correlation\n\ndata:  recalldata$recall_accuracy and recalldata$recall_confidence\nt = 4.1512, df = 18, p-value = 0.0005998\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3719603 0.8720125\nsample estimates:\n      cor \n0.6993654 \n\n\n\n\n Optional: Manually conducting the correlation test\n\n\nOr, if we want to calculate our test statistic manually:\n\n#calculate r\nr = cor(recalldata$recall_accuracy, recalldata$recall_confidence)\n\n#get n\nn = nrow(recalldata)\n\n#calculate t    \ntstat = r * sqrt((n - 2) / (1 - r^2))\n\n#calculate p-value for t, with df = n-2 \n2*(1-pt(tstat, df=n-2))\n\n[1] 0.0005998222\n\n\n\nAssumptions\nFor a test of Pearson’s correlation coefficient \\(r\\), we need to make sure a few conditions are met:\n\nBoth variables are quantitative\nBoth variables should be drawn from normally distributed populations.\nThe relationship between the two variables should be linear.\n\n\n\n\n\n\n\n\n\n\nCautions!\nCorrelation is an invaluable tool for quantifying relationships between variables, but must be used with care.\nBelow are a few things to be aware of when we talk about correlation.\n\nCorrelation can be heavily affected by outliers. Always plot your data!\nThe two plots below only differ with respect to the inclusion of one observation. However, the correlation coefficient for the two sets of observations is markedly different.\n\n\n\n\n\n\n\n\n\n\n\nr = 0 means no linear association. The variables could still be otherwise associated. Always plot your data!\nThe correlation coefficient in Figure 1 below is negligible, suggesting no linear association. The word “linear” here is crucial - the data are very clearly related.\n\n\n\n\n\nFigure 1: Unrelated data?\n\n\n\n\nSimilarly, take look at all the sets of data in Figure 2 below. The summary statistics (means and standard deviations of each variable, and the correlation) are almost identical, but the visualisations suggest that the data are very different from one another.\n\n\n\n\n\nFigure 2: Datasaurus! From Matejka, J., & Fitzmaurice, G. (2017, May): Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing.\n\n\n\n\n\n\nCorrelation does not imply causation!\n\n\n\n\n\nFigure 3: https://twitter.com/quantitudepod/status/1309135514839248896\n\n\n\n\nYou will have likely heard the phrase “correlation does not imply causation”. There is even a whole wikipedia entry devoted to the topic.\nJust because you observe an association between x and y, we should not deduce that x causes y\nAn often cited paper which appears to fall foul of this error took a correlation between a country’s chocolate consumption and its number of nobel prize winners (see Figure 4) to suggest a causal relationship between the two (“chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates”):\n\n\n\n\n\nFigure 4: Chocolate consumption causes more Nobel Laureates?"
  },
  {
    "objectID": "07_ex.html",
    "href": "07_ex.html",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "",
    "text": "less guided. research q\nhints: read data plot and describe variables, and the relationship fit model interpret the coefficients map summary sections to plot"
  },
  {
    "objectID": "07_ex.html#different-data-formats",
    "href": "07_ex.html#different-data-formats",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Different Data Formats",
    "text": "Different Data Formats\nData can come in lots of different formats, meaning that we need lots of different ways to read data into R. Below is some information on some of the more common functions for reading and writing different types of data.\nText based files\n\n\n\n\n\n\n\n\n\nfiletype\ndescription\nreading\nwriting\n\n\n\n\n.csv\ncomma separated values\ntidyverse - read_csv()read.csv()read.table(..., sep = \",\")\ntidyverse - write_csv()write.csv()write.table(..., sep=\",\")\n\n\n.tsv\ntab separated values\ntidyverse - read_tsv()read.table(..., sep = \"\\t\")\ntidyverse - write_tsv()write.table(..., sep = \"\\t\")\n\n\n.txt\nanything-separated values!\nread.table(..., sep = ...)\nwrite.table(..., sep = ...)\n\n\n\nR files\n\n\n\n\n\n\n\n\n\nfiletype\ndescription\nreading\nwriting\n\n\n\n\n.RDS\n1 file = a single R object\nreadRDS()\nsaveRDS()\n\n\n.RData\n1 file = a collection of R objects\nload()\nsave()save.image() - to save all objects in the environment)\n\n\n\nExcel files\nThe package readxl provides a variety of functions for reading in different types of Microsoft Excel spreadsheet, such as read_excel(), read_xls(), read_xlsx().\nOther software\nThe package haven provides functions for files which have been saved from other statistical software, for instance with read_spss()/read_sav() and read_sas() for files from SPSS and SAS.\nGoogle sheets\nThe googlesheets4 package can read in data directly from a spreadsheet stored on google drive. You simply find the id of the sheet (it’s the big long string of numbers & letters in the url of your google sheet), and pass it to read_sheet().\nIt will prompt you to authenticate your account via your browser, but it’s really easy!\n\nQuestion B1\n\n\nRead in the two data-sets. Take care to look at the file extension (e.g., .csv, .tsv, .xlsx) as indicators of what function to try.\nMake sure you assign them identifiable names.\nOnce you’ve loaded the data-set, take a look at them using functions like summary(), str(), dim()/nrow(), or viewing them by clicking on them in the environment.\nHints:\n\nSome functions like read_excel() don’t allow you to download directly from a url, like we have been doing with .csv files.\n\nSolution 1:\n\nDownload the data to your computer\nupload to the rstudio server if you are using it\nDirect the function to read it from the place you stored it.\n\nSolution 2:\n\nMake R download the data directly to somewhere in your working directory (see download.file()).\n\n\nDo both the data-sets have column names? By default R will assume the first row is the name of the column. Look in the help documentation to see how to stop this from happening.\n\n\n\n\n\n Solution \n\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndownload.file('https://uoepsy.github.io/data/blink_eyegaze.xlsx', 'data/blink_eyegaze.xlsx', mode=\"wb\")\neyedata <- read_excel(path = 'data/blink_eyegaze.xlsx')\n\nsetupdata <- read_csv(\"https://uoepsy.github.io/data/blink_setup.csv\", col_names = FALSE)"
  },
  {
    "objectID": "07_ex.html#renaming-columns",
    "href": "07_ex.html#renaming-columns",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Renaming Columns",
    "text": "Renaming Columns\nYou can access the column names from a data-set using names() or colnames().\n\nnames(data)\ncolnames(data)\n\nAnd we can easily rename these using indexing:\n\n#name the third column \"peppapig\"\nnames(data)[3]<-\"peppapig\"\n\nOr in tidyverse, using rename():\n\ndata %>%\n  rename(newname = currentname)\n\n\nQuestion B2\n\n\nProblem\nThe blink_setup.csv file doesn’t have any column names!\nWe know that there are 20 trials for each participant, and we can see that the 2nd column has information about which subject it is.\nColumns 3:22 are trials 1 to 20.\n\nhead(setupdata)\n\n\n\n   X1        X2                       X3 ...\n1   1 subject_1 /files/vids/blinks_1.mp4 ...\n2   2 subject_2 /files/vids/blinks_2.mp4 ...\n3   3 subject_3 /files/vids/blinks_4.mp4 ...\n4   4 subject_4 /files/vids/blinks_4.mp4 ...\n5   5 subject_5 /files/vids/blinks_1.mp4 ...\n6   6 subject_6 /files/vids/blinks_2.mp4 ...\n7 ...       ...                      ... ...\n8 ...       ...                      ... ...\n\n\nTask\n\nRemove the first column\nRename columns 2 to 22 with sensible names.\n\nHints:\n\nnames(setupdata) # what are the names\nnames(setupdata)[2] # what is the 2nd name\nnames(setupdata) <- c(\"...\", \"...\", \"...\",..) # set the names\n\n\nc(\"kermit\", paste(\"peppapig\", 1:3, sep=\"_\"))\n\n[1] \"kermit\"     \"peppapig_1\" \"peppapig_2\" \"peppapig_3\"\n\n\n\n\n\n\n Solution \n\n\nremove the first column\n\nsetupdata <- setupdata[,-1]\n\nSet the names\n\nnames(setupdata) <- c(\"sub\",paste(\"trial\", 1:20, sep = \"_\"))\n\nCheck:\n\nhead(setupdata)\n\n\n\n        sub                  trial_1 ...\n1 subject_1 /files/vids/blinks_1.mp4 ...\n2 subject_2 /files/vids/blinks_2.mp4 ...\n3 subject_3 /files/vids/blinks_4.mp4 ...\n4 subject_4 /files/vids/blinks_4.mp4 ...\n5 subject_5 /files/vids/blinks_1.mp4 ...\n6 subject_6 /files/vids/blinks_2.mp4 ...\n7       ...                      ... ...\n8       ...                      ... ..."
  },
  {
    "objectID": "07_ex.html#reshaping-data",
    "href": "07_ex.html#reshaping-data",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Reshaping data",
    "text": "Reshaping data\nPivot!\nOne of the more confusing things to get to grips with is the idea of reshaping a dataframe.\nFor different reasons, you might sometimes want to have data in wide, or in long format.\n\n\n\n\n\nSource: https://fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/\n\n\n\n\nWhen the data is wide, we can make it long using pivot_longer(). When we make data longer, we’re essentially making lots of columns into 2 longer columns. Above, in the animation, the wide variable x, y and z go into a new longer column called name that specifies which (x/y/z) it came from, and the values get put into the val column.\nThe animation takes a shortcut in the code it displays above, but you could also use pivot_longer(c(x,y,z), names_to = \"name\", values_to = \"val\"). To reverse this, and put it back to being wide, we tell R which columns to take the names and values from: pivot_wider(names_from = name, values_from = val).\n\nQuestion B3\n\n\nProblem\nThe blink_setup.csv file has the data in a different shape to the blink_eyegaze.xlsx file.\n\nblink_setup.csv : one row per participant\n\nblink_eyegaze.xlsx : one row per trial\n\nTask\nReshape the data to make it so that there is one row per trial.\nHint\n\nin the tidyverse functions, you can specify all columns between column x and column z by using the colon, x:z.\n\n\n\n\n\n Solution \n\n\n(Note that this will depend on what you called your columns in the previous question - we just called them “trial_1”, … , “trial_20”).\n\nsetuplong <- \n  setupdata %>%\n  pivot_longer(trial_1:trial_20, names_to = \"trial_number\", values_to = \"video\")\n\nsetuplong\n\n# A tibble: 460 × 3\n   sub       trial_number video                    \n   <chr>     <chr>        <chr>                    \n 1 subject_1 trial_1      /files/vids/blinks_1.mp4 \n 2 subject_1 trial_2      /files/vids/blinsk_8.mp4 \n 3 subject_1 trial_3      /files/vids/blinks_1.mp4 \n 4 subject_1 trial_4      /files/vids/blinks_5.mp4 \n 5 subject_1 trial_5      /files/vids/blinks_4.mp4 \n 6 subject_1 trial_6      /files/vids/blinks_10.mp4\n 7 subject_1 trial_7      /files/vids/blinks_1.mp4 \n 8 subject_1 trial_8      /files/vids/blinks_5.mp4 \n 9 subject_1 trial_9      /files/vids/blinks_6.mp4 \n10 subject_1 trial_10     /files/vids/blinks_4.mp4 \n# … with 450 more rows"
  },
  {
    "objectID": "07_ex.html#dealing-with-character-strings",
    "href": "07_ex.html#dealing-with-character-strings",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Dealing with character strings",
    "text": "Dealing with character strings\nThere are loads of functions we can use to do various things with character strings in R.\nHere are a few examples:\n\n gsub()\n\n\nsubstitute a string of characters for another string:\n\ngsub(\"don't like\",\"love\", \"i really really really don't like statistics!\")\n\n[1] \"i really really really love statistics!\"\n\n\n\n\n\n\n separate()\n\n\nseparate a column into multiple columns by splitting at a set of characters\n\nmupsimp <- read_csv(\"https://uoepsy.github.io/data/muppet_simp.csv\")\nmupsimp\n\n# A tibble: 18 × 1\n   show_name                     \n   <chr>                         \n 1 simpsons_Marge Simpson        \n 2 muppets_Scooter               \n 3 muppets_Rowlf the Dog         \n 4 muppets_Fozzie Bear           \n 5 simpsons_Abraham Simpson      \n 6 muppets_Walter                \n 7 muppets_Pepe the King Prawn   \n 8 muppets_Gonzo                 \n 9 simpsons_Santa's Little Helper\n10 simpsons_Snowball II/V        \n11 simpsons_Maggie Simpson       \n12 simpsons_Lisa Simpson         \n13 simpsons_Bart Simpson         \n14 muppets_Animal                \n15 simpsons_Homer Simpson        \n16 muppets_Miss Piggy            \n17 muppets_Rizzo the Rat         \n18 muppets_Kermit the Frog       \n\nmupsimp %>% \n  separate(show_name, into = c(\"show\",\"name\"), sep = \"_\")\n\n# A tibble: 18 × 2\n   show     name                 \n   <chr>    <chr>                \n 1 simpsons Marge Simpson        \n 2 muppets  Scooter              \n 3 muppets  Rowlf the Dog        \n 4 muppets  Fozzie Bear          \n 5 simpsons Abraham Simpson      \n 6 muppets  Walter               \n 7 muppets  Pepe the King Prawn  \n 8 muppets  Gonzo                \n 9 simpsons Santa's Little Helper\n10 simpsons Snowball II/V        \n11 simpsons Maggie Simpson       \n12 simpsons Lisa Simpson         \n13 simpsons Bart Simpson         \n14 muppets  Animal               \n15 simpsons Homer Simpson        \n16 muppets  Miss Piggy           \n17 muppets  Rizzo the Rat        \n18 muppets  Kermit the Frog      \n\n\n\n\n\n\n substr()\n\n\nExtract or replace substrings in a character vector.\n\n# get the first 3 letters\nsubstr(mupsimp$show_name, 1, 3)\n\n [1] \"sim\" \"mup\" \"mup\" \"mup\" \"sim\" \"mup\" \"mup\" \"mup\" \"sim\" \"sim\" \"sim\" \"sim\"\n[13] \"sim\" \"mup\" \"sim\" \"mup\" \"mup\" \"mup\"\n\n\nCan be combined with functions like nchar() (to find the number of characters in each string). Additionally, can be used in tidyverse easily:\n\nmupsimp %>%\n  mutate(\n    first3 = substr(show_name, 1, 3),\n    last3 = substr(show_name, nchar(show_name)-2, nchar(show_name))\n  )\n\n# A tibble: 18 × 3\n   show_name                      first3 last3\n   <chr>                          <chr>  <chr>\n 1 simpsons_Marge Simpson         sim    son  \n 2 muppets_Scooter                mup    ter  \n 3 muppets_Rowlf the Dog          mup    Dog  \n 4 muppets_Fozzie Bear            mup    ear  \n 5 simpsons_Abraham Simpson       sim    son  \n 6 muppets_Walter                 mup    ter  \n 7 muppets_Pepe the King Prawn    mup    awn  \n 8 muppets_Gonzo                  mup    nzo  \n 9 simpsons_Santa's Little Helper sim    per  \n10 simpsons_Snowball II/V         sim    I/V  \n11 simpsons_Maggie Simpson        sim    son  \n12 simpsons_Lisa Simpson          sim    son  \n13 simpsons_Bart Simpson          sim    son  \n14 muppets_Animal                 mup    mal  \n15 simpsons_Homer Simpson         sim    son  \n16 muppets_Miss Piggy             mup    ggy  \n17 muppets_Rizzo the Rat          mup    Rat  \n18 muppets_Kermit the Frog        mup    rog  \n\n\n\n\n\n\n paste()\n\n\npaste() can quickly combine two character vectors:\n\npaste(\"hello\",\"everyone\",sep=\" \")\n\n[1] \"hello everyone\"\n\n\nYou can also use it to collapse a vector into a single string:\n\npaste(mupsimp$show_name, collapse=\" \")\n\n[1] \"simpsons_Marge Simpson muppets_Scooter muppets_Rowlf the Dog muppets_Fozzie Bear simpsons_Abraham Simpson muppets_Walter muppets_Pepe the King Prawn muppets_Gonzo simpsons_Santa's Little Helper simpsons_Snowball II/V simpsons_Maggie Simpson simpsons_Lisa Simpson simpsons_Bart Simpson muppets_Animal simpsons_Homer Simpson muppets_Miss Piggy muppets_Rizzo the Rat muppets_Kermit the Frog\"\n\n\nand paste0() is a quick shortcut for using sep=\"\":\n\npaste0(\"hello\",\"everyone\")\n\n[1] \"helloeveryone\"\n\n\n\n\n\n\nQuestion B4\n\n\nProblem\nIf you look at what data was captured by the software to indicate which video was used in each trial, there is a lot of unnecessary data there. The number of the filename indicates how many blinks are in the video. This is the only bit of information we want.\n\nhead(setuplong$video)\n\n[1] \"/files/vids/blinks_1.mp4\"  \"/files/vids/blinsk_8.mp4\" \n[3] \"/files/vids/blinks_1.mp4\"  \"/files/vids/blinks_5.mp4\" \n[5] \"/files/vids/blinks_4.mp4\"  \"/files/vids/blinks_10.mp4\"\n\n\nTask\n\nIn your (now reshaped to long) blink_setup.csv data, make a new, or edit an existing column, which is a numeric variable containing the number of blinks presented in the video in each trial\n\nHints:\n\nthere are lots of different ways you could do this.\n\nyou can substitute out multiple different strings by separating them with the | symbol:\n\ngsub(\"dog|cat\", \"horse\", \"I have a dog and a cat and the dogs name is Graham\")\n\n[1] \"I have a horse and a horse and the horses name is Graham\"\n\n\n\n\n\n\n\n Solution \n\n\n\nsetuplong <- setuplong %>%\n  mutate(\n    nr_blinks = as.numeric(gsub(\"/files/vids/|blinks_|blinsk_|.mp4\",\"\",video))\n  )\n\nsetuplong\n\n# A tibble: 460 × 4\n   sub       trial_number video                     nr_blinks\n   <chr>     <chr>        <chr>                         <dbl>\n 1 subject_1 trial_1      /files/vids/blinks_1.mp4          1\n 2 subject_1 trial_2      /files/vids/blinsk_8.mp4          8\n 3 subject_1 trial_3      /files/vids/blinks_1.mp4          1\n 4 subject_1 trial_4      /files/vids/blinks_5.mp4          5\n 5 subject_1 trial_5      /files/vids/blinks_4.mp4          4\n 6 subject_1 trial_6      /files/vids/blinks_10.mp4        10\n 7 subject_1 trial_7      /files/vids/blinks_1.mp4          1\n 8 subject_1 trial_8      /files/vids/blinks_5.mp4          5\n 9 subject_1 trial_9      /files/vids/blinks_6.mp4          6\n10 subject_1 trial_10     /files/vids/blinks_4.mp4          4\n# … with 450 more rows"
  },
  {
    "objectID": "07_ex.html#joiningmerging",
    "href": "07_ex.html#joiningmerging",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Joining/merging",
    "text": "Joining/merging\nNow comes a fun bit.\nRecall that the research question is interested in the relationship between the number of times the speaker was seen to blink, and the time the participants spent looking at the distractor object (indicating perceived dishonesty).\nYou may have noticed that these variables are currently in different data-sets! The blink_setup.csv contains information about the numbers of blinks in the videos, and the blink_eyegaze.xlsx contains the data on the fixations.\nSolution: we need to join them together!\nNote that because both data-sets contain information on participant number and trial number, which uniquely identifies each observation, we can join them together matching on these variables!\nThere are lots of different ways to join data-sets, depending on whether we want to keep rows from one data-set or the other, or keep only those in both data-sets etc.\n\n\n\n\n\nCheck out the help documentation for them all using ?full_join.\n\n\n\n\n\nQuestion B5\n\n\nProblem\nVariables are in different data-sets.\nTask\n\nJoin the two data-sets (the reshaped-to-long blink_setup.csv data, and the blink_eyegaze.xlsx data) together, and store the joined data in a new object (you can use your own name, but the solutions will use the name blinks_full).\n\nHints\nWe want to match the observations based on two columns which are present in each data-set, indicating which participant, and which trial.\n\nRemember that R doesn’t have your intelligence - it doesn’t know that in one data-set the variable is called e.g., trial_no and in the other it is called trial_number.\n\nAnother thing which R doesn’t know is that “subject_1” in setup data is the same participant as “1” in the eye gaze data. It needs to match the same symbols, and what is more, it needs the variables to be the same type (character, numeric, factor etc).\n\nyou might want to make use of the skills you learned for manipulating character strings.\n\n\n\n\n\n\n Solution \n\n\nIn this solution, let’s build up a sequence step by step. Work through the steps, adding lines of code each time. Between each step, run the code to quickly see what the output looks like at each step.\n\nFirst, let’s see how we can remove the “subject_” from “subject_1” etc..\n::: {.cell layout-align=“center”}\nsetuplong %>%\n  mutate(\n    sub = gsub(\"subject_\",\"\",sub)\n  )\n:::\nBut we also want it to be numeric, to match the sub variable in the eyegaze data, so let’s edit it to:\n::: {.cell layout-align=“center”}\nsetuplong %>%\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub))\n  )\n:::\nWe’ll also need to do the same for the trial_number variable, so let’s add that line too:\n::: {.cell layout-align=“center”}\nsetuplong %>%\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  )\n:::\nAnd then, we’ll note that we need to have the same name for variables indicating trial number in both data-sets, so lets rename it:\n::: {.cell layout-align=“center”}\nsetuplong %>%\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  ) %>%\n  rename(trial_no = trial_number)\n:::\nAnd now… add the join!\n::: {.cell layout-align=“center”}\nsetuplong %>%\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  ) %>%\n  rename(trial_no = trial_number) %>%\n  full_join(x = ., y = eyedata)\n:::\n\nNOTE the solution has x = ., y = eyedata to make it clear that we are ‘piping in’ (using %>%) the thing coming out of the previous lines of code, and putting it where the . is. .... %>% full_join(eyedata) would do the same.\nWe use full_join here because we want to keep all the data, but left_join would do the same. right_join would be slightly different, because there are 3 observations in the setup data (when reshaped to long, n = 460) which aren’t in the eye gaze data (n = 457). You can see which ones they are by using anti_join.\n6. Finally - we need to give the whole output a name to store it in our environment!\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nblinks_full <- \n  setuplong %>%\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  ) %>%\n  rename(trial_no = trial_number) %>%\n  full_join(x = ., y = eyedata)\n```\n:::"
  },
  {
    "objectID": "07_ex.html#impossible-values",
    "href": "07_ex.html#impossible-values",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Impossible Values",
    "text": "Impossible Values\nIt’s important to check that there are no values in the data which are impossible, given what you know about how the data was measured. This is where exploratory plots and descriptive statistics come in handy.\n\nhist(as.numeric(blinks_full$distractor_fix))\n\n\n\n\n\n\n\n\nIn some trials, participants spent less that 0ms fixating on the distractor object!?!?!?\nWe have a couple of options as to how to deal with them.\n\nDelete the entire row\nChange the specific entry/s in that variable to be NA (Not Applicable) - this has the benefit of keeping the rows should we consider those row to have a valid observation in other variables (for instance the rt - reaction time?)\n\nSome of the tools we learned in the Reading 1B will come in handy here.\n\nQuestion B6\n\n\nProblem\nSome impossible values in the distractor_fix variable.\nTask\n- Assign the entries of the distractor_fix variable which are < 0 to be NA.\n- Are there any other impossible values (or combinations of values) in the data?\nHints:\n\nWhile you’re there, why not convert any variables to the right type (numeric, factor, etc).\n\nWe might not have come across this before, but there is a really useful function called ifelse().\nPlay around with the below code to learn:\n\n\ntibble(x = 1:10) %>%\n  mutate(\n    new_variable = ifelse(x>5,1,0),\n    another_new_variable = ifelse(x>5,\"peppapig\",\"kermit\"),\n    morevariables = ifelse(another_new_variable == \"kermit\",\"kermit the frog\", another_new_variable)\n  )\n\n\n\n\n\n Solution \n\n\nBelow we’ve taken similar steps for both the distractor_fix and rt variables. Neither can be <0 or >5000.\nHowever, we know that the distractor_fix variable has no entries >5000 (because of the histogram above).\n\nblinks_full <- \n  blinks_full %>%\n  mutate(\n    distractor_fix = as.numeric(distractor_fix),\n    distractor_fix = ifelse(distractor_fix<0, NA, distractor_fix),\n    rt = ifelse(as.numeric(rt)>5000 | as.numeric(rt)<0, NA, as.numeric(rt))\n  )\n\nNote how two steps (making it numeric, and replacing values with NAs) are combined for the rt variable. Note also how we have specified that we replace with NAs entries which meet either on condition (>5000) or (using |) another (<0)."
  },
  {
    "objectID": "07_ex.html#missing-data-in-r",
    "href": "07_ex.html#missing-data-in-r",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Missing Data in R",
    "text": "Missing Data in R\nMissing data can be a big problem for statistics. For those of you thinking of taking Multivariate Statistics & Methodology in R next semester, you can look forward to discussions around this sort of issue.\nHere, however, we are simply going to discuss the practicalities of how to make R code work when some of your values are NAs.\nConsider:\n\nvec <- c(1,2,3,4,NA)\nmean(vec)\n\n[1] NA\n\n\nThink about why this is:\n\\[\n\\text{mean(vec)} = \\frac{1+2+3+4+\\text{NA}}{5} = \\frac{\\text{??}}{5} = \\text{??}\n\\]\nThere are numerous different ways that functions in R cope with missing values, but if you’re ever in doubt, try na.rm = TRUE. This will basically tell R to “remove the NAs before doing the calculation”.\n\nmean(vec, na.rm=T)\n\n[1] 2.5\n\n\nOther functions include na.omit(), which remove any row with has an NA anywhere in it:\n\ncomplete_data <- na.omit(data)"
  },
  {
    "objectID": "07_ex.html#outliers",
    "href": "07_ex.html#outliers",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Outliers",
    "text": "Outliers\nOutliers are the extreme - but plausible - values in variables. There is no one way to identify what is extreme enough to consider and outlier, nor is there one way to handle them.\nSome outliers could be considered important observations which we would not want to exclude. However, being an outlier can (but not always) result in an observation exerting too great an influence on our analysis.\n\nSome common approaches to identifying outliers:\n\nobservations which are \\(> 3\\) (sometimes \\(> 2.5\\)) standard deviations away from the mean.\nobservations greater than \\(1.5 \\times IQR\\) below the first quartile \\(Q_1\\) or above the third quartile \\(Q_3\\).\n\nSome common approaches to handling outliers:\n\nExclude now - for instance, set as NA\n“Winsorize” - set to a specified percentile. For example, all observations below the 5th percentile set to the 5th percentile, and all observations above the 95th percentile set to the 95th percentile\nExclude from analysis later, based on measures of influence (we’ll learn about this in future topics)\n\n\n\nQuestion B7\n\n\nMake a bloxplot of the distractor_fix variable. Does it look like there might be any outliers?\n\n\n\n\n Solution \n\n\nThe last line of this is there just because I personally don’t like the default look of geom_boxplot where it is really wide, so this line changes the limits of the x-axis (and also removes the ticks).\n\nggplot(data = blinks_full, aes(y = distractor_fix)) +\n  geom_boxplot()+\n  scale_x_continuous(limits = c(-2,2), breaks = NULL)\n\n\n\n\n\n\n\n\nIt looks like there are possibly some outliers at the upper end of the distribution. One of them looks really quite anomalous!"
  },
  {
    "objectID": "07_ex.html#custom-functions",
    "href": "07_ex.html#custom-functions",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Custom Functions",
    "text": "Custom Functions\n\nQuestion B8\n\n\nWriting your own function\nWe already saw some custom functions in the first week, where we made some called dice() and wdice().\nCan you write a function which, given a vector, returns TRUE if it is an outlier and FALSE if it is not, based on the criterion of being \\(>3\\) sd away from the mean.\n\noutliers <- function(obs){\n ...\n ...\n ...\n}\n\n\n\n\n\n Solution \n\n\n\n Working out the internal code\n\n\nLet’s do the calculation on a little vector, keeping it all outside of a function first:\n\n# a random vector (length = 20)\nvec <- rnorm(n = 20, mean = 0, sd = 1)\n# pick two random entries and make them outliers (one in each direction)\nvec[3] <- 150\nvec[16] <- -150\nvec\n\n [1]   -1.06329659    0.11629788  150.00000000    1.00622505    0.68891294\n [6]    0.26668521    0.25102781   -1.53056917   -0.15229888   -0.83655556\n[11]    1.22174302   -1.04229156   -0.13978572   -1.42372025    0.03735867\n[16] -150.00000000   -0.65065380   -0.19412164    0.60978775   -1.13479916\n\n# deviations from each point to mean\nvec - mean(vec)\n\n [1] -8.647939e-01  3.148006e-01  1.501985e+02  1.204728e+00  8.874156e-01\n [6]  4.651879e-01  4.495305e-01 -1.332066e+00  4.620382e-02 -6.380529e-01\n[11]  1.420246e+00 -8.437889e-01  5.871698e-02 -1.225218e+00  2.358614e-01\n[16] -1.498015e+02 -4.521511e-01  4.381064e-03  8.082904e-01 -9.362965e-01\n\n# and three times the standard deviation\n3 * sd(vec)\n\n[1] 146.0184\n\n# but this won't work because some are below, rather than above the mean. \n(vec - mean(vec)) > (3 * sd(vec))\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# instead we want the ABSOLUTE value \nabs(vec - mean(vec)) > (3 * sd(vec))\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n Writing it as a function\n\n\nOkay, now that we’ve worked out the code, we want to make this a function. The template function in the question had an input called obs:\n\noutliers <- function(obs){\n\n}\n\nSo we would want to add our code to the function, but change it to use obs (which is whatever we give the function)\n\noutliers <- function(obs){\n  abs(obs - mean(obs)) > (3 * sd(obs))\n}\n\n\n\n\n\n Testing the function\n\n\nwe can test it on the vec object we created earlier.\n\noutliers(obs = vec)\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\nWe can use it to access and edit those entries:\n\nvec[outliers(vec)]\n\n[1]  150 -150\n\nvec[outliers(vec)] <- NA\n\n\n\n\n\n Extra - adding more arguments\n\n\nWe could edit the function so that we can also vary how many standard deviations away we are wanting to identify!\n\noutliers <- function(obs, x = 3){\n  abs(obs - mean(obs)) > (x * sd(obs))\n}\n\nthe x = 3 means that the function will default to looking 3 standard deviations away, but if we wanted to use outliers(obs = vec, x = 2) we could identify those which are 2 away!\n\n\n\n\n\n\n\nQuestion B9\n\n\nLook through the solutions to question B8 above, and make sure that you are comfortable with how writing a function works.\nCan you edit the outliers() function you wrote to make it work with vectors which include NAs?\n\n\n\n\n Solution \n\n\n\noutliers <- function(obs, x = 3){\n  abs(obs - mean(obs, na.rm=TRUE)) > (x * sd(obs, na.rm=TRUE))\n}\n\n\n\n\n\nQuestion B10\n\n\nProblem\nPossible outliers in the distractor_fix variable.\nTask\n\nReplace any values of the distractor_fix variable which are \\(>3\\) standard deviations from the mean with NA.\n\nMake a new boxplot of the variable\n\n\n If you skipped questions A8 and A9\n\n\nIf you skipped questions B8 and B9, then copy and run this code into your document. It will give you a function which takes a vector and returns TRUEs and FALSEs based on whether each entry is greater than 3 standard deviations from the mean.\n\noutliers <- function(obs, x = 3){\n  abs(obs - mean(obs, na.rm=TRUE)) > (x * sd(obs, na.rm=TRUE))\n}\n\n\n\n\n\n\n\n\n Solution \n\n\n\nblinks_full$distractor_fix[outliers(blinks_full$distractor_fix)]<- NA\n\nggplot(data = blinks_full, aes(y = distractor_fix)) +\n  geom_boxplot()+\n  scale_x_continuous(limits = c(-2,2), breaks = NULL)"
  },
  {
    "objectID": "07_ex.html#build-a-model",
    "href": "07_ex.html#build-a-model",
    "title": "Exercises: Simple Regression & More Data Wrangling",
    "section": "Build a model!",
    "text": "Build a model!\nWe’re now finally getting to the analysis. As we said earlier, this can sometimes be very straightforward in comparison to the amount of effort involved in cleaning data.\nRecall that we’re interested in whether the perception of whether or not a speaker is lying about the location of some hidden treasure (as measured by the pattern of eye fixations towards the object not referred to by the speaker) is influenced by the number of times the speaker is seen to blink while producing the utterance.\n\nQuestion B11\n\n\nPlot the relationship between the two variables you think will be best used to answer this question.\n\n\n\n\n Solution \n\n\n\nggplot(blinks_full, aes(x=nr_blinks, y = distractor_fix))+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion B12\n\n\nFit the linear model specified below to the data using the lm() function and store the output in the environment as an object named blinks_mdl.\n\\[\n\\begin{align}\n& \\text{Fixation time to distractor} = b_0 + b_1 \\ \\text{Number of blinks} + \\epsilon \\\\\n\\quad \\\\\n& \\text{where} \\quad \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n\\]\n\n\n\n\n Solution \n\n\n\nblinks_mdl <- lm(distractor_fix ~ 1 + nr_blinks, data=blinks_full)\n\nsummary(blinks_mdl)\n\n\nCall:\nlm(formula = distractor_fix ~ 1 + nr_blinks, data = blinks_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-944.47 -253.55    5.58  217.74 1033.81 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  517.296     36.076   14.34   <2e-16 ***\nnr_blinks     64.128      5.808   11.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 355.1 on 447 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.2143,    Adjusted R-squared:  0.2126 \nF-statistic: 121.9 on 1 and 447 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nQuestion B13\n\n\nThe \\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\) bit of our model is an assumption we have to make. It concerns the errors (the deviations from observations to our line). Our model assumes that these are normally distributed and centered on 0.\nWe can plot the distribution of residuals to check how well our assumption holds:\n\nhist(residuals(blinks_mdl))\n\n\n\n\n\n\n\n\nHowever, we also make the statement that the errors are independent - i.e. they are not related to one another.\nFor us, this is not the case, and so we should not be using this simple linear regression here.\nIn what way are we violating the assumption of independence?\n\n\n\n\n Solution \n\n\nThe dataset to which we are fitting our model does not contain independent observations. We have multiple observations from each participant. i.e. Subject 1 makes up 20 rows, and subject 2 makes up 20 rows.\nAs such, this means that our linear regression model is not appropriate. We will learn about how to deal with this sort of study design next semester, in the Multivariate Statistics & Methodoligy using R (MSMR) course."
  },
  {
    "objectID": "07a_slr.html",
    "href": "07a_slr.html",
    "title": "07A: Simple Linear Regression",
    "section": "",
    "text": "This walkthrough covers the basics of simple linear regression conducted in R.\nOur data for this walkthrough is from an hypothetical study into income disparity for employees in a local authority. We’re interested in investigating the link between the level of education and an employee’s income. Those with more formal education seem to be better paid.\nWe begin by loading the required libraries (probably just tidyverse for now), and reading in the riverview data to our R session.\nWe’re going to name it riverview in our environment."
  },
  {
    "objectID": "07a_slr.html#partitioning-variance-rsquared",
    "href": "07a_slr.html#partitioning-variance-rsquared",
    "title": "07A: Simple Linear Regression",
    "section": "Partitioning Variance: Rsquared",
    "text": "Partitioning Variance: Rsquared\nWe might ask ourselves if the model is useful. To quantify and assess model utility, we split the total variability of the response into two terms: the variability explained by the model plus the variability left unexplained in the residuals.\n\\[\n\\begin{align}\n& \\qquad \\qquad \\qquad \\qquad \\text{total variability in response } =  \\\\\n& \\text{variability explained by model } + \\text{unexplained variability in residuals}\n\\end{align}\n\\]\nEach term is quantified by a sum of squares:\n\\[\n\\begin{aligned}\nSS_{Total} &= SS_{Model} + SS_{Residual} \\\\\n\\sum_{i=1}^n (y_i - \\bar y)^2 &= \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 + \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\\\\n\\quad \\\\\n\\text{Where:} \\\\\n& y_i = \\text{observed value} \\\\\n&\\bar{y} = \\text{mean} \\\\\n& \\hat{y}_i = \\text{model predicted value} \\\\\n\\end{aligned}\n\\]\n\nThe \\(R^2\\) coefficient is defined as the proportion of the total variability in the outcome variable which is explained by our model:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\n\nIn our model, the \\(R^2\\) shows us the proportion of the total variability in incomes explained by the linear relationship with education level. We can find this easily in the summary() of the model!\n\nsummary(model1)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nThe output of summary() displays the R-squared value in the following line:\nMultiple R-squared:  0.6317\n\n\nFor the moment, ignore “Adjusted R-squared”. We will come back to this later on.\n\nApproximately 63% of the total variability in employee incomes is explained by the linear association with education level.\n\n\n Optional - Manual calculation of R-Squared\n\n\n\nriverview_fitted <- riverview %>%\n  mutate(\n    income_hat = predict(model1),\n    resid = income - income_hat\n  )\nhead(riverview_fitted)\n\n# A tibble: 6 × 8\n  education income seniority gender  male party       income_hat  resid\n      <dbl>  <dbl>     <dbl> <chr>  <dbl> <chr>            <dbl>  <dbl>\n1         8   37.4         7 male       1 Democrat          32.5   4.92\n2         8   26.4         9 female     0 Independent       32.5  -6.10\n3        10   47.0        14 male       1 Democrat          37.8   9.20\n4        10   34.2        16 female     0 Independent       37.8  -3.65\n5        10   25.5         1 female     0 Republican        37.8 -12.4 \n6        12   46.5        11 female     0 Democrat          43.1   3.35\n\nriverview_fitted %>%\n  summarise(\n    SSModel = sum( (income_hat - mean(income))^2 ),\n    SSTotal = sum( (income - mean(income))^2 )\n  ) %>%\n  summarise(\n    RSquared = SSModel / SSTotal\n  )\n\n# A tibble: 1 × 1\n  RSquared\n     <dbl>\n1    0.632"
  },
  {
    "objectID": "07a_slr.html#testing-model-utility-f-statistic",
    "href": "07a_slr.html#testing-model-utility-f-statistic",
    "title": "07A: Simple Linear Regression",
    "section": "Testing Model Utility: \\(F\\) Statistic",
    "text": "Testing Model Utility: \\(F\\) Statistic\nWe can also perform a test to investigate if the model is ‘useful’ — that is, a test to see if the explanatory variable is a useful predictor of the outcome.\nWe test the following hypotheses:\n\\[\n\\begin{aligned}\nH_0 &: \\text{the model is ineffective, } b_1 = 0 \\\\\nH_1 &: \\text{the model is effective, } b_1 \\neq 0\n\\end{aligned}\n\\]\n\nThe relevant test-statistic is the F-statistic:\n\\[\n\\begin{split}\nF = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}\n\\end{split}\n\\]\nwhich compares the amount of variation in the response explained by the model to the amount of variation left unexplained in the residuals.\nThe sample F-statistic is compared to an F-distribution with \\(df_{1} = 1\\) and \\(df_{2} = n - 2\\) degrees of freedom.1\n\n Optional: Another formula for the F-test.\n\n\nWith some algebra we can also show that:\n\\[\nF = \\frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \\frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }\n\\]\nProof:\n\\[\n\\begin{aligned}\nF = \\frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)}\n= \\frac{\\frac{SS_{Model}}{SS_{Total}}}{\\frac{SS_{Residual}}{SS_{Total}} \\cdot \\frac{1}{(n - 2)}}\n= \\frac{R^2 / 1}{(1 - R^2) / (n - 2)}\n\\end{aligned}\n\\]\n\n\n\n\nAnd yet again, we can look at the output of summary() of our model to find this information. From the summary(model1), the relevant row is just below the \\(R^2\\), where it states:\nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\nThe overall test of model utility was significant \\(F(1, 30) = 51.45, p < .001\\), indicating evidence against the null hypothesis that the model is ineffective (that education is not a useful predictor of income).\n\n\n Optional: Equivalence of t-test for the slope and model utility F-test in simple regression.\n\n\nIn simple linear regression only (where we have just one predictor), the F-statistic for overall model significance is equal to the square of the t-statistic for \\(H_0: b_1 = 0\\).\nYou can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:\n\nsummary(model1)$fstatistic['value']\n\n   value \n51.45152 \n\nsummary(model1)$coefficients['education','t value']\n\n[1] 7.172972\n\n\n\\[\nt^2 = F \\\\\n7.173^2 = 51.452\n\\]\nHere we will show the equivalence of the F-test for model effectiveness and t-test for the slope.\nRecall the formula of the sum of squares due to the model. We will rewrite it in an equivalent form below:\n\\[\n\\begin{aligned}\nSS_{Model} &= \\sum_i (\\hat y_i - \\bar y)^2 \\\\\n&= \\sum_i (\\hat b_0 + \\hat b_1 x_i - \\bar y)^2 \\\\\n&= \\sum_i (\\bar y - \\hat b_1 \\bar x + \\hat b_1 x_i - \\bar y)^2 \\\\\n&= \\sum_i (\\hat b_1 (x_i - \\bar x))^2 \\\\\n&= \\hat b_1^2 \\sum_i (x_i - \\bar x)^2\n\\end{aligned}\n\\]\nThe F-statistic is given by:\n\\[\n\\begin{aligned}\nF = \\frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)}\n= \\frac{\\hat b_1^2 \\sum_i (x_i - \\bar x)^2}{\\hat \\sigma^2}\n= \\frac{\\hat b_1^2 }{\\hat \\sigma^2 / \\sum_i (x_i - \\bar x)^2}\n\\end{aligned}\n\\]\nNow recall the formula of the t-statistic,\n\\[\nt = \\frac{\\hat b_1}{SE(\\hat b_1)} = \\frac{\\hat b_1}{\\hat \\sigma / \\sqrt{\\sum_i (x_i - \\bar x)^2}}\n\\]\nIt is evident that the latter is obtained as the square root of the former."
  },
  {
    "objectID": "08a_mlr.html",
    "href": "08a_mlr.html",
    "title": "08A: Multiple Linear Regression",
    "section": "",
    "text": "In this reading, we move from the simple linear regression model (one outcome variable, one explanatory variable) to the multiple regression model (one outcome variable, multiple explanatory variables).\nEverything we learned about simple linear regression in Reading 7A can be extended (with minor modification) to the multiple regression model. The key conceptual difference is that for simple linear regression we think of the distribution of errors at some fixed value of the explanatory variable, and for multiple linear regression, we think about the distribution of errors at fixed set of values for all our explanatory variables."
  },
  {
    "objectID": "08a_mlr.html#research-question",
    "href": "08a_mlr.html#research-question",
    "title": "08A: Multiple Linear Regression",
    "section": "Research Question",
    "text": "Research Question\nThe data for this walkthrough is from an hypothetical study in which some reseachers are interested in the relationship between psychological wellbeing and time spent outdoors. They know that other aspects of peoples’ lifestyles such as how much social interaction they have can influence their mental well-being.\n\nResearch Question\nIs there a relationship between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions?.\n\n\nData: Wellbeing\nResearchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\nThe researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).\nThe dataset is available at https://uoepsy.github.io/data/wellbeing.csv and contains five attributes:\n\nwellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\noutdoor_time: Self report estimated number of hours per week spent outdoors\n\nsocial_int: Self report estimated number of social interactions per week (both online and in-person)\nroutine: Binary Yes/No response to the question “Do you follow a daily routine throughout the week?”\nlocation: Location of primary residence (City, Suburb, Rural)"
  },
  {
    "objectID": "08a_mlr.html#model-specification",
    "href": "08a_mlr.html#model-specification",
    "title": "08A: Multiple Linear Regression",
    "section": "Model Specification",
    "text": "Model Specification\nTo address the research question we are going to fit the following model:\n\\[\nWellbeing = b_0 \\ + \\ b_1 \\cdot Social Interactions \\ + \\ b_2 \\cdot Outdoor Time \\ + \\ \\epsilon\n\\]"
  },
  {
    "objectID": "08a_mlr.html#exploring-the-data",
    "href": "08a_mlr.html#exploring-the-data",
    "title": "08A: Multiple Linear Regression",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nFirst we need to import the wellbeing data into R. We’ll give them the name mwdata.\n\nlibrary(tidyverse)\n# Read in data\nmwdata = read_csv(\"https://uoepsy.github.io/data/wellbeing.csv\")\n\nNow, as before, we explore and describe the relevant variables and relationships.\nWe will want to:\n\nProduce plots of the marginal distributions (the distributions of each variable in the analysis without reference to the other variables) of the wellbeing, outdoor_time, and social_int variables.\nProduce plots of the marginal relationships between the outcome variable (wellbeing) and each of the explanatory variables.\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the relationships.\n\n\nCorrelation matrix\nA table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the relationship between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\nIn R: We can create a correlation matrix easily by giving the cor() function a dataframe. If we only want to give it a certain set of columns, we can combine this with select(), or giving the column numbers inside [].\n\n\nlibrary(patchwork) #used to arrange plots\nwellbeing_plot <- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/250) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\ndensity\")\n\noutdoortime_plot <- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\ndensity\")\n\nsocial_plot <- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/150) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\ndensity\")\n\n# the \"patchwork\" library allows us to arrange multiple plots\nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\n\nFigure 2: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS is unimodal with a mean of approximately 43. There is variation in scores (SD = 11.7).\n\nThe marginal distribution of weekly hours spend outdoors is unimodal with a mean of approximately 14.8 hours. There is variation in outdoor time (SD = 6.9 hours).\n\nThe marginal distribution of numbers of social interactions per week is unimodal with a mean of approximately 16. There is variation in in numbers of social interactions per week (SD = 4.4).\n\n\n\nwellbeing_outdoor <- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social <- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Number of social interactions per week\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_outdoor | wellbeing_social\n\n\n\n\nFigure 3: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\n\nWe can either use:\n\n# correlation matrix of the first 3 columns\ncor(mwdata[,1:3])\n\nor:\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %>% \n  select(wellbeing, outdoor_time, social_int) %>%\n  cor()\n\n             wellbeing outdoor_time social_int\nwellbeing    1.0000000    0.5815613  0.7939003\noutdoor_time 0.5815613    1.0000000  0.3394469\nsocial_int   0.7939003    0.3394469  1.0000000\n\n\n\nThere is a moderate, positive, linear relationship between weekly outdoor time and WEMWBS scores for the participants in the sample. Participants’ wellbeing scores tend to increase, on average, with the number of hours spent outdoors each week.\nThere is a moderate, positive, linear relationship between the weekly number of social interactions and WEMWBS scores for the participants in the sample. Participants’ wellbeing scores tend to increase, on average, with the weekly number of social interactions. There is also a weak positive correlation between weekly outdoor time and the weekly number of social interactions.\n\n Note that there is a weak correlation between our two explanatory variables (outdoor_time and social_int). We will return to how this might affect our model when later on we look at the assumptions of multiple regression."
  },
  {
    "objectID": "08a_mlr.html#fitting-the-model",
    "href": "08a_mlr.html#fitting-the-model",
    "title": "08A: Multiple Linear Regression",
    "section": "Fitting the Model",
    "text": "Fitting the Model\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n[model name] <- lm([response variable] ~ 1 + [explanatory variable 1] + [explanatory variable 2] + ... , data = [dataframe])\n\\[\nWellbeing = b_0 \\ + \\ b_1 \\cdot Social Interactions \\ + \\ b_2 \\cdot Outdoor Time \\ + \\ \\epsilon\n\\]\n\nwbmodel <- lm(wellbeing ~ 1 + social_int + outdoor_time, data = mwdata)"
  },
  {
    "objectID": "08a_mlr.html#interpreting-coefficients",
    "href": "08a_mlr.html#interpreting-coefficients",
    "title": "08A: Multiple Linear Regression",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\nThe parameters of a multiple regression model are:\n\n\\(b_0\\) (The intercept);\n\\(b_1\\) (The slope across values of \\(x_1\\));\n…\n\n…\n\\(b_k\\) (The slope across values of \\(x_k\\));\n\\(\\sigma\\) (The standard deviation of the errors).\n\n You’ll hear a lot of different ways that people explain multiple regression coefficients.\nFor the model \\(y = b_0 + b_1 x_1 + b_2 x_2 + \\epsilon\\), the estimate \\(\\hat b_1\\) will often be reported as:\nthe increase in \\(y\\) for a one unit increase in \\(x_1\\) when…\n\nholding the effect of \\(x_2\\) constant.\ncontrolling for differences in \\(x_2\\).\npartialling out the effects of \\(x_2\\).\nholding \\(x_2\\) equal.\naccounting for effects of \\(x_2\\).\n\n\n\n\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)  5.3703775  4.3205141 1.242995 2.238259e-01\nsocial_int   1.8034489  0.2690982 6.701825 2.369845e-07\noutdoor_time 0.5923673  0.1689445 3.506284 1.499467e-03\n\n\nThe coefficient 0.59 of weekly outdoor time for predicting wellbeing score says that among those with the same number of social interactions per week, those who have one additional hour of outdoor time tend to, on average, score 0.59 higher on the WEMWBS wellbeing scale. The multiple regression coefficient measures that average conditional relationship.\n\nOne by one, the parameter estimates are:\n\ncoef(wbmodel)\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\\(\\hat \\b_0\\) = 5.37, the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\\(\\hat \\b_2\\) = 1.8, the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant (i.e., when the remaining explanatory variables are held at the same value or are fixed).\n\n\\(\\hat \\b_1\\) = 0.59, the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant."
  },
  {
    "objectID": "08a_mlr.html#sigma",
    "href": "08a_mlr.html#sigma",
    "title": "08A: Multiple Linear Regression",
    "section": "\\(\\sigma\\)",
    "text": "\\(\\sigma\\)\nJust as we had with simple linear regression, we have errors around the a line, here we have error around a 3-dimensional surface. It’s harder to visualise (but see Figure 1), but we can still get an idea of how far away observations are from our fitted model (the surface).\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 6.15. We would expect 95% of wellbeing scores to be within about 12.3 (\\(2 \\hat \\sigma\\)) from the model fit.\n\nsigma(wbmodel)\n\n[1] 6.148276"
  },
  {
    "objectID": "08a_mlr.html#inference",
    "href": "08a_mlr.html#inference",
    "title": "08A: Multiple Linear Regression",
    "section": "Inference",
    "text": "Inference\nMuch like for simple linear regression, we have the tests of the coefficients being zero, which are provided in the summary:\n\nsummary(wbmodel)$coefficients\n\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)  5.3703775  4.3205141 1.242995 2.238259e-01\nsocial_int   1.8034489  0.2690982 6.701825 2.369845e-07\noutdoor_time 0.5923673  0.1689445 3.506284 1.499467e-03\n\n\nWe can also obtain confidence intervals for our estimates (we saw confidence intervals back in Reading 2B. These provide a means of quantifying the uncertainty (or precision) of our estimates.\nThe function confint() can do this for us.\n\nconfint(wbmodel, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)  -3.4660660 14.2068209\nsocial_int    1.2530813  2.3538164\noutdoor_time  0.2468371  0.9378975\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week is between -3.47 and 14.21.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week is associated with a difference in wellbeing scores between 1.25 and 2.35, on average.\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time is associated with a difference in wellbeing scores between 0.25 and 0.94, on average."
  },
  {
    "objectID": "08a_mlr.html#more-model-evaluation",
    "href": "08a_mlr.html#more-model-evaluation",
    "title": "08A: Multiple Linear Regression",
    "section": "More Model Evaluation",
    "text": "More Model Evaluation\n\nAdjusted \\(R^2\\)\nWe know from our work on simple linear regression that the R-squared can be obtained as:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\nHowever, when we add more and more predictors into a multiple regression model, \\(SS_{Residual}\\) cannot increase, and may decrease by pure chance alone, even if the predictors are unrelated to the outcome variable. Because \\(SS_{Total}\\) is constant, the calculation \\(1-\\frac{SS_{Residual}}{SS_{Total}}\\) will increase by chance alone.\nAn alternative, the Adjusted-\\(R^2\\), does not necessarily increase with the addition of more explanatory variables, by including a penalty according to the number of explanatory variables in the model. It is not by itself meaningful, but can be useful in determining what predictors to include in a model.\n\\[\nAdjusted{-}R^2=1-\\frac{(1-R^2)(n-1)}{n-k-1} \\\\\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\nIn R, you can view the mutiple and adjusted \\(R^2\\) at the bottom of the output of summary(<modelname>):\n\n\n\n\n\nFigure 4: Multiple regression output in R, summary.lm(). R-squared highlighted\n\n\n\n\n\n\nF-ratio\nAs in simple linear regression, the F-ratio is used to test the null hypothesis that all regression slopes are zero (it is just that now that we have multiple predictors, “all” is more than 1).\n\\[\n\\begin{aligned}\nH_0: & \\text{the model is ineffective, } \\\\\n& b_1, ..., b_k = 0 \\\\\nH_1: &\\text{the model is effective, } \\\\\n& \\text{any of }b_1, ..., b_k \\neq 0\n\\end{aligned}\n\\]\nIt is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per parameter) versus how much of the variation is unexplained (per remaining degrees of freedom).\n\\[\n\\begin{align}\n& F_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n& \\quad \\\\\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{error} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\nIn R, at the bottom of the output of summary(<modelname>), you can view the F ratio, along with an hypothesis test against the alternative hypothesis that the at least one of the coefficients \\(\\neq 0\\) (under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1):\n\n\n\n\n\nFigure 5: Multiple regression output in R, summary.lm(). F statistic highlighted\n\n\n\n\n\n\n\n\n\nWeekly social interactions and outdoor time explained 72.2% of the variance in well-being scores (adjusted \\(R^2\\) =0.722, \\(F\\)(2,29)=41.3, p<.001)"
  },
  {
    "objectID": "08b_assumpt.html",
    "href": "08b_assumpt.html",
    "title": "08A: Multiple Linear Regression",
    "section": "",
    "text": "So far, we have been fitting and interpreting our regression models. In each case, we first specified the model, then visually explored the marginal distributions and relationships of variables which would be used in the analysis. Then, once we fitted the model, we began to examine the fit by studying what the various parameter estimates represented, and the spread of the residuals. We saw these in the output of summary() of a model - they were shown in the parts of the output inside the red boxes in Figure @ref(fig:mlroutput)).\n\n\n\n\n\nMultiple regression output in R, summary.lm(). Residuals and Coefficients highlighted\n\n\n\n\nWe also discussed drawing inferences using our model estimates, as well as using a model to make predictions. However, we should really not have done this prior to being satisfied that our model meets a certain set of assumptions. All of the estimates, intervals and hypothesis tests (see Figure @ref(fig:mlroutputhyp)) resulting from a regression analysis assume a certain set of conditions have been met. Meeting these conditions is what allows us to generalise our findings beyond our sample (i.e., to the population).\n\n\n\n\n\nMultiple regression output in R, summary.lm(). Hypothesis tests highlighted\n\n\n\n\n\n\n\n\nAssumptions: The broad idea\nAll our work here is in aim of making models of the world.\n\nModels are models. They are simplifications. They are therefore wrong.\n\nOur residuals ( \\(y - \\hat{y}\\) ) reflect everything that we don’t account for in our model.\n\nIn an ideal world, our model accounts for all the systematic relationships. The leftovers (our residuals) are just random noise.\n\nIf our model is mis-specified, or we don’t measure some systematic relationship, then our residuals will reflect this.\nWe check by examining how much “like randomness” the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d (“independent and identically distributed”). These ideas tend to get referred to as our “assumptions”.\n\nWhile we will never know whether our residuals contain only randomness (we can never observe everything), our ability to generalise from the model we fit on sample data to the wider population relies on these assumptions.\n\n\nAssumptions in a nutshell\nIn using linear regression, we have assumptions about our model in that we assume that modelling the outcome variable as a linear combination of the explanatory variables is an appropriate thing to do.\nWe also make certain assumptions about what we have left out of our model - the errors component.\n  \nSpecifically, we assume that our errors have “zero mean and constant variance”.\n\nmean of the residuals = zero across the predicted values on the linear predictor.\n\nspread of residuals is normally distributed and constant across the predicted values on the linear predictor.\n\n\n What does it look like?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n What does it not look like?\n\n\nThings look a bit wrong (like there is something systematic that we haven’t accounted for), when our residuals do not have mean zero:\n::: {.cell layout-align=“center”} ::: {.cell-output-display} \n:::\n:::\nOr do not have constant variance: ::: {.cell layout-align=“center”} ::: {.cell-output-display} \n:::\n:::\n\n\n\n\n\nAssumptions in R\nWe can get a lot of plots for this kind of thing by using plot(model)\nHere’s what it looks like for a nice neat model:\n::: {.cell layout-align=“center”}\n\n\nplot(my_model)\n\n\n\n\n\n\n\n\n\n\n\n\nSome people don’t like the higher-level/broad picture approach to thinking about assumptions of our analysis, and prefer a step-by-step list of things to make sure they tick off. For those of you who would like this, you can find our page on “assumptions & diagnostics: the recipe book way”.\n\n:::\nRecall our last model (from Question C5), in which we fitted a model assessing how we might explain wellbeing by the combination of social interactions, routine, and outdoor time.\nThe code for this model is given below.\nLet’s take a look at the diagnostic plots given by passing the model to the plot() function.\n\nmwdata = read_csv(file = \"https://uoepsy.github.io/data/wellbeing.csv\")\nwbmodel2 <- lm(wellbeing ~ social_int + routine + outdoor_time, data=mwdata)\nplot(wbmodel2)\n\n\n\n\n\n\n\n\n\n\nThe model doesn’t look too bad.\n\n\nThe top left plot (residuals vs fitted) shows a reasonably straight red line, which indicates that the mean of the residuals is close to zero across the fitted values.\n\nThe top right plot (QQplot of residuals) shows that the residuals are fairly close to the dotted line, indicating they follow close to a normal distribution (QQplots plot the values against the associated percentiles of the normal distribution. So if we had ten values, it would order them lowest to highest, then plot them on the y against the 10th, 20th, 30th.. and so on percentiles of the standard normal distribution (mean 0, SD 1)).\n\nThe bottom left plot (scale location plot) shows the square-root of the absolute value of the standardised residuals. This allows us to examine the extent to which the variance of the residuals changes accross the fitted values. A straight red line indicates reasonably constant variance. It’s a bit wiggly here!\n\nThe bottom right plot (residuals vs leverage plot) shows the extent to which datapoints that have higher residuals (are far away from our regression line) have the potential to unduly influence our line. We’ll look at this idea of influence later on.\n\n\n\nQuestion Optional: D1\n\n\nWe can perform tests to examine how (un)likely we would be to see some residuals like those we have got, if they were sampled from a normally distribution.\nThe Shapiro-Wilk test is a test against the alternative hypothesis that the residuals were not sampled from a normally distributed population. We can perform this test quickly in R using shapiro.test(residuals(modelname)).\nConduct this test now on the model we just plotted above. What do you conclude?\n\n\n\n\n Solution \n\n\n\nshapiro.test(residuals(wbmodel2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(wbmodel2)\nW = 0.96791, p-value = 0.4439\n\n\n\nA Shapiro-Wilk test failed to reject the null hypothesis that the residuals were drawn from a normally distributed population (\\(W = 0.97\\), \\(p = .44\\))\n\n\n\n\n\nQuestion Optional: D2\n\n\nThe ncvTest(model) function (from the car package) performs a test against the alternative hypothesis that the error variance changes with the level of the fitted value (also known as the “Breusch-Pagan test”). \\(p >.05\\) indicates that we do not have evidence that the assumption has been violated.\nTry conducting this test now, on the same model as the previous question.\n\n\n\n\n Solution \n\n\n\nlibrary(car)\nncvTest(wbmodel2)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.1018826, Df = 1, p = 0.74958\n\n\n\nVisual inspection of suggested little sign of non-constant variance, with the Breusch-Pagan test failing to reject the null that error variance does not change across the fitted values (\\(\\chi^2(1)=0.102\\), \\(p = .75\\))\n\n\n\n\n\nMulticollinearity\nFor the linear model with multiple explanatory variables, we need to also think about multicollinearity - this is when two (or more) of the predictors in our regression model are moderately or highly correlated.\nRecall our interpretation of multiple regression coefficients as\n\n\n“the effect of \\(x_1\\) on \\(y\\) when holding the values of \\(x_2\\), \\(x_3\\), … \\(x_k\\) constant”\n\nThis interpretation falls down if predictors are highly correlated because if, e.g., predictors \\(x_1\\) and \\(x_2\\) are highly correlated, then changing the value of \\(x_1\\) necessarily entails a change the value of \\(x_2\\) meaning that it no longer makes sense to talk about holding \\(x_2\\) constant.\n We can assess multicollinearity using the variance inflation factor (VIF), which for a given predictor \\(x_j\\) is calculated as:\n\\[\nVIF_j = \\frac{1}{1-R_j^2} \\\\\n\\]\nWhere \\(R_j^2\\) is the coefficient of determination (the R-squared) resulting from a regression of \\(x_j\\) on to all the other predictors in the model (\\(x_j = x_1 + ... x_k + \\epsilon\\)).\nThe more highly correlated \\(x_j\\) is with other predictors, the bigger \\(R_j^2\\) becomes, and thus the bigger \\(VIF_j\\) becomes.\n The square root of VIF indicates how much the SE of the coefficient has been inflated due to multicollinearity. For example, if the VIF of a predictor variable were 4.6 (\\(\\sqrt{4.6} = 2.1\\)), then the standard error of the coefficient of that predictor is 2.1 times larger than if the predictor had zero correlation with the other predictor variables. Suggested cut-offs for VIF are varied. Some suggest 10, others 5. Define what you will consider an acceptable value prior to calculating it.\n\nIn R, the vif() function from the car package will provide VIF values for each predictor in your model.\n\n\n\nQuestion Optional: D3\n\n\nCalculate the variance inflation factor (VIF) for the predictors in the model.\nWrite a sentence summarising whether or not you consider multicollinearity to be a problem here.\n\n\n\n\n Solution \n\n\n\nvif(wbmodel2)\n\n  social_int      routine outdoor_time \n    1.130708     1.014136     1.142259 \n\n\n\nVIF values <5 indicate that multicollinearity is not adversely affecting model estimates.\n\n\n\n\n\n\n\n\nIn linear regression, individual cases in our data can influence our model more than others. There are a variety of measures we can use to evaluate the amount of misfit and influence that single observations have on our model and our model estimates.\n\nTHERE ARE NO HARD RULES FOR WHAT COUNTS AS “INFLUENTIAL” AND HOW WE SHOULD DEAL WITH THESE CASES\nThere are many ways to make a cake. recipes can be useful, but you really need to think about what ingredients you actually have (what data you have).\nYou don’t have to exclude influential observations. Try to avoid blindly following cut-offs, and try to think carefully about outliers and influential points and whether you want to exclude them, and whether there might be some other model specification that captures this in some estimable way. Do these observations change the conclusions you make (you can try running models with and without certain cases).\n\nThere are various measures of outlyngness and influence. Here are a few. You do not need to remember all of these!\nRegression outliers:\nA large residual \\(\\hat \\epsilon_i\\) - i.e., a big discrepancy between their predicted y-value and their observed y-value.\n\nStandardised residuals: For residual \\(\\hat \\epsilon_i\\), divide by the estimate of the standard deviation of the residuals. In R, the rstandard() function will give you these\nStudentised residuals: For residual \\(\\hat \\epsilon_i\\), divide by the estimate of the standard deviation of the residuals excluding case \\(i\\). In R, the rstudent() function will give you these. Values \\(>|2|\\) (greater in magnitude than two) are considered potential outliers.\n\nHigh leverage cases:\nThese are cases which have considerable potential to influence the regression model (e.g., cases with an unusual combination of predictor values).\n\nHat values: are used to assess leverage. In R, The hatvalues() function will retrieve these.\nHat values of more than \\(2 \\bar{h}\\) (2 times the average hat value) are often worth looking at. \\(\\bar{h}\\) is calculated as \\(\\frac{k + 1}{n}\\), where \\(k\\) is the number of predictors, and \\(n\\) is the sample size.\n\nHigh influence cases:\nWhen a case has high leverage and is an outlier, it will have a large influence on the regression model.\n\nCook’s Distance: combines leverage (hatvalues) with outlying-ness to capture influence. In R, the cooks.distance() function will provide these.\nThere are many suggested Cook’s Distance cut-offs.\nDFFit: the change in the predicted value at the \\(i^{th}\\) observation with and without the \\(i^{th}\\) observation is included in the regression.\n\nDFbeta: the change in a specific coefficient with and without the \\(i^{th}\\) observation is included in the regression.\n\nDFbetas: the change in a specific coefficient divided by the standard error, with and without the \\(i^{th}\\) observation is included in the regression.\n\nCOVRATIO: measures the effect of an observation on the covariance matrix of the parameter estimates. In simpler terms, it captures an observation’s influence on standard errors. Values which are \\(>1+\\frac{3(k+1)}{n}\\) or \\(<1-\\frac{3(k+1)}{n}\\) are sometimes considered as having strong influence.\n\n\nYou can get a whole bucket-load of these measures with the influence.measures() function.\n\ninfluence.measures(my_model) will give you out a dataframe of the various measures.\n\nsummary(influence.measures(my_model)) will provide a nice summary of what R deems to be the influential points.\n\n\n\n\nQuestion Optional: D4\n\n\nPlot the Cook’s Distance values, does it look like there may be any highly influential points?\n(You can use plot(model, which = 4) and plot(model, which = 5)).\n\n\n\n\n Solution \n\n\nThese don’t look too bad to me - there aren’t a small number of cases which have exceptionally high values (relative to the others) for Cook’s D.\n\nplot(wbmodel2, which = 4)\n\n\n\n\n\n\n\nplot(wbmodel2, which = 5)"
  },
  {
    "objectID": "09a_moreregression.html",
    "href": "09a_moreregression.html",
    "title": "9A: More Linear Regression",
    "section": "",
    "text": "Often it is useful to make distinctions in notation between the effects that we are interested in (the population parameter) and our best guess (the estimate). With regression coefficients this is the notation we tend to use in this course (there are many other conventions for notation):\n\n\n\nPopulation parameter\nFitted estimate\n\n\n\n\n\\(b\\)\n\\(\\hat b\\)\n\n\n\n\n\nRemember the scale of your variables!\nRecall one of our models from last week, explaining wellbeing by the combination of social interactions and outdoor time:\n\\[\n\\textrm{Wellbeing} = b_0 \\ + \\ b_1 (\\textrm{Outdoor Time}) \\ + \\ b_2 (\\textrm{Social Interactions}) \\ +\\ \\epsilon\n\\]\nWe fitted this as so:\n\nmwdata <- read_csv(file = \"https://uoepsy.github.io/data/wellbeing.csv\")\nwbmodel2 <- lm(wellbeing ~ outdoor_time + social_int, data=mwdata)\n\n\nWellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\nSocial Interactions: Self report estimated number of social interactions per week (both online and in-person)\nOutdoor Time: Self report estimated number of hours per week spent outdoors\n\nThis influences our interpretation of the coefficients (the \\(\\hat b\\)’s):\n\n\\(\\hat b_0\\) = the estimated mean score on the WEMWBS for someone who has zero social interactions per week and zero hours of outdoor time per week.\n\n\\(\\hat b_1\\) = for every increase in 1 hour of outdoor time per week, scores on the WEMWBS are estimated to change by \\(\\hat b_1\\) points.\n\\(\\hat b_2\\) = for every increase in 1 social interaction per week, scores on the WEMWBS are estimated to change by \\(\\hat b_2\\) points.\n\n\n\nQuestion A1\n\n\nThe code below creates a standardised the outdoor_time variable, transforming the values so that the mean of the new variable is 0, and the standard deviation is 1. We can either use the scale() function to do this, or do it manually:\n\nmwdata <- mwdata %>%\n  mutate(\n    outdoor_time_scaled = (outdoor_time - mean(outdoor_time))/sd(outdoor_time)\n  )\n\nNote that the shape of the distribution stays exactly the same, but the units on the x-axis are different.\n\n\n\n\n\n\n\n\n\nHow does the interpretation of \\(\\hat b_0\\) and \\(\\hat b_1\\) change for the model:\n\nlm(wellbeing ~ social_int + outdoor_time_scaled, data=mwdata)\n\n\n\n\n\n Solution \n\n\n\n\\(\\hat b_0\\) = the estimated mean score on the WEMWBS for someone who has zero social interactions per week and has the mean number of hours of outdoor time per week.\n\n\\(\\hat b_1\\) = for every increase in 1 standard deviation of hours of outdoor time per week, scores on the WEMWBS are estimated to change by \\(\\hat b_1\\) points.\n\n\\(\\hat b_2\\) = for every increase in 1 social interaction per week, scores on the WEMWBS are estimated to change by \\(\\hat b_2\\) points.\n\nNote that the significance of \\(\\hat b_1\\) (the effect of outdoor time) is exactly the same, but the estimate is different:\n\nwbmodel2 <- lm(wellbeing ~ social_int + outdoor_time, data=mwdata)\nwbmodel2a <- lm(wellbeing ~ social_int + outdoor_time_scaled, data=mwdata)\nsummary(wbmodel2)$coefficients\n\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)  5.3703775  4.3205141 1.242995 2.238259e-01\nsocial_int   1.8034489  0.2690982 6.701825 2.369845e-07\noutdoor_time 0.5923673  0.1689445 3.506284 1.499467e-03\n\nsummary(wbmodel2a)$coefficients\n\n                     Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)         14.144818  4.4406338 3.185315 3.445184e-03\nsocial_int           1.803449  0.2690982 6.701825 2.369845e-07\noutdoor_time_scaled  4.116262  1.1739670 3.506284 1.499467e-03\n\n\nThis is because it now represents the change in wellbeing associated with a 1 standard deviation change in outdoor time, rather than a 1 hour increase.\n\ncoef(wbmodel2)['outdoor_time'] * sd(mwdata$outdoor_time)\n\noutdoor_time \n    4.116262 \n\n\n\n\n\n\nQuestion A2\n\n\nWe can also standardise our outcome variable, the wellbeing variable. Let’s use the scale() function this time:\n\nmwdata <- mwdata %>%\n  mutate(\n    wellbeing_scaled = scale(wellbeing)\n  )\n\nHow will our coefficients change for the following model?\n\nlm(wellbeing_scaled ~ social_int + outdoor_time_scaled, data = mwdata)\n\n\n\n\n\n Solution \n\n\n\n\\(\\hat b_0\\) = the estimated number of standard deviations from the mean on the WEMWBS for someone who has zero social interactions per week and has the mean number of hours of outdoor time per week.\n\n\\(\\hat b_1\\) = for every increase in 1 standard deviation of hours of outdoor time per week, scores on the WEMWBS are estimated to change by \\(\\hat b_1\\) standard deviations.\n\n\\(\\hat b_2\\) = for every increase in 1 social interaction per week, scores on the WEMWBS are estimated to change by \\(\\hat b_2\\) standard deviations.\n\nThe coefficients now represent the change in standard deviations of wellbeing scores.\n\ncoef(wbmodel2)['social_int'] / sd(mwdata$wellbeing)\n\nsocial_int \n 0.1545345 \n\n\n\n\n\n\nWe can actually standardise all the variables in our model by using the function standardCoefs() from the lsr package.\nIn some notations, the distinction between these “standardised coefficients” and the original coefficients in the raw units, gets denoted by \\(\\hat b\\) changing to \\(\\hat \\beta\\) for the standardised coefficients1:\n\nNotation\n\n\n\nPopulation parameter\nFitted estimate\nStandardised Estimate\n\n\n\n\n\\(b\\)\n\\(\\hat b\\)\n\\(\\hat \\beta\\)\n\n\n\n\n\nlibrary(lsr)\nstandardCoefs(wbmodel2)\n\n                     b      beta\nsocial_int   1.8034489 0.6741720\noutdoor_time 0.5923673 0.3527157\n\n\nThe interpretation of \\(\\hat \\beta\\)s (the “beta”) column, is now:\n\n\\(\\hat \\beta_1\\): The estimated change in standard deviations of wellbeing scores associated with a change of 1 standard deviation of number of hours of outdoor time per week.\n\n\\(\\hat \\beta_2\\): The estimated change in standard deviations of wellbeing scores associated with a change of 1 standard deviation of number of social interactions per week.\n\nA benefit of this is that the standardised coefficients are much more comparable. Comparing \\(\\hat b_1\\) and \\(\\hat b_2\\) is asking whether 1 hour of outdoor time has a bigger effect than 1 social interaction. This is a bit like ‘comparing apples and oranges’.\nIf instead we compare \\(\\hat \\beta_1\\) and \\(\\hat \\beta_2\\), then the comparisons we make are in terms of standard deviations of each predictor. Thus we might consider increasing the number of social interactions each week to have a greater effect on wellbeing than increasing the hours of outdoor time (because \\(0.67 > 0.35\\))."
  },
  {
    "objectID": "09a_moreregression.html#categorical-predictors-with-k-levels",
    "href": "09a_moreregression.html#categorical-predictors-with-k-levels",
    "title": "9A: More Linear Regression",
    "section": "Categorical Predictors with \\(k\\) levels",
    "text": "Categorical Predictors with \\(k\\) levels\n\nWe have seen how a binary categorical variable gets inputted into our model as a variable of 0s and 1s (these typically get called “dummy variables”).\n     > Dummy variables are numeric variables that represent categorical data.\n When we have a categorical explanatory variable with more than 2 levels, our model gets a bit bigger - it needs not just one, but a number of dummy variables. For a categorical variable with \\(k\\) levels, we can express it in \\(k-1\\) dummy variables.\nFor example, the “species” column below has three levels, and can be expressed by the two variables “species_dog” and “species_parrot”:\n\n\n  species species_dog species_parrot\n1     cat           0              0\n2     cat           0              0\n3     dog           1              0\n4  parrot           0              1\n5     dog           1              0\n6     cat           0              0\n7     ...         ...            ...\n\n\n\nThe “cat” level is expressed whenever both the “species_dog” and “species_parrot” variables are 0.\nThe “dog” level is expressed whenever the “species_dog” variable is 1 and the “species_parrot” variable is 0.\nThe “parrot” level is expressed whenever the “species_dog” variable is 0 and the “species_parrot” variable is 1.\n\nR will do all of this re-expression for us. If we include in our model a categorical explanatory variable with 4 different levels, the model will estimate 3 parameters - one for each dummy variable. We can interpret the parameter estimates (the coefficients we obtain using coefficients(),coef() or summary()) as the estimated increase in the outcome variable associated with an increase of one in each dummy variable (holding all other variables equal).\n\n\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n60.28\n1.209\n49.86\n5.273e-39\n\n\nspeciesdog\n-11.47\n1.71\n-6.708\n3.806e-08\n\n\nspeciesparrot\n-4.916\n1.71\n-2.875\n0.006319\n\n\n\n\n\nNote that in the above example, an increase in 1 of “species_dog” is the difference between a “cat” and a “dog”. An increase in one of “species_parrot” is the difference between a “cat” and a “parrot”. We think of the “cat” category in this example as the reference level - it is the category against which other categories are compared against.\n\n\n\n\n\n\n\n\n\nIf you think about it, a regression model with a categorical predictor with \\(k\\) levels is really just a regression model with \\(k-1\\) dummy variables as predictors.\n\n\n Optional: Contrasts!\n\n\nAs it happens, we don’t always have to use 0s and 1s to represent categorical variables.\nBy changing what numbers we use, we change what the various estimates represent. For instance, we might use -1 and 1. Which makes 0 the mid-point, or the overall mean, and this is where our intercept will be. The coefficient will then be the difference from the group to the overall mean. In additional, we can also change what “a change of 1” represents.\n\n\n\n\n\n\n\n\n\nThis can get very confusing very quickly as we move to variables that have more than 2 levels, but the short story is that we can use these “contrasts” to test the specific differences that we might be interested in (depending on what our hypothesis is).\nIn R, we can see quickly how our variables will be encoded in our model by using contrasts().\ntip: it requires a variable to be a factor.\n\nmwdata$location <- factor(mwdata$location)\ncontrasts(mwdata$location)\n\n       Rural Suburb\nCity       0      0\nRural      1      0\nSuburb     0      1\n\n\nIn the output of contrasts(), the columns are the dummy variables which go into our model. So we know that if we use location in a model, the reference level will be “City” (where both variables are zero), and we will get a coefficient for “Rural” which will be the difference “Rural - City”, and a coefficient for “Suburb” which will be the difference “Suburb - City”.\n\nWe can see that here:\n\ncoef(lm(wellbeing ~ location, data = mwdata))\n\n   (Intercept)  locationRural locationSuburb \n     41.000000       7.571429       1.100000 \n\n\nAnd calculating the group means of wellbeing helps to show that it’s doing what we expect:\n\nmwdata %>% group_by(location) %>%\n  summarise(\n    meanwb = mean(wellbeing)\n  ) %>%\n  mutate(diff_from_city = meanwb - meanwb[1])\n\n# A tibble: 3 × 3\n  location meanwb diff_from_city\n  <fct>     <dbl>          <dbl>\n1 City       41             0   \n2 Rural      48.6           7.57\n3 Suburb     42.1           1.10\n\n\n\nWe’re not going to delve too much into contrasts in this course, other than to say that we can use them change the way that information in categorical variables get inputted in our model, thereby changing the group differences that we are estimating. If you would like to learn more, then do please email and ask us. There are also lots of great resources around online, such as this one."
  },
  {
    "objectID": "09a_moreregression.html#numeric-categorical",
    "href": "09a_moreregression.html#numeric-categorical",
    "title": "9A: More Linear Regression",
    "section": "~ Numeric * Categorical",
    "text": "~ Numeric * Categorical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: cogapoe.csv\nNinety adult participants were recruited for a study investigating how cognitive functioning varies by age, and whether this is different depending on whether people carry an APOE-4 gene.\n\ncogapoe <- read_csv(\"../../data/cogapoe4.csv\")\ntibble(\n  variables = names(cogapoe),\n  description = c(\"Participant ID\",\"Age (in Years)\",\"Years of Education\",\"Birthweight (in Kg)\",\"APOE-4 Gene expansion ('none', 'apoe4a', 'apoe4b', apoe4c')\",\"Score on Addenbrooke's Cognitive Examination\")\n) %>% knitr::kable() %>% kableExtra::kable_styling(.,full_width=T)\n\n\n\n \n  \n    variables \n    description \n  \n \n\n  \n    pid \n    Participant ID \n  \n  \n    age \n    Age (in Years) \n  \n  \n    educ \n    Years of Education \n  \n  \n    birthweight_kg \n    Birthweight (in Kg) \n  \n  \n    apoe4 \n    APOE-4 Gene expansion ('none', 'apoe4a', 'apoe4b', apoe4c') \n  \n  \n    acer \n    Score on Addenbrooke's Cognitive Examination \n  \n\n\n\n\n\nDownload Link\nThe data are available at https://uoepsy.github.io/data/cogapoe4.csv.\n\n\nQuestion C1\n\n\n\nResearch Question: Does the relationship between age and cognitive function differ between those with and without the APOE-4 genotype?\n\nRead in the data and explore the variables which you think you will use to answer this research question (create some plots, some descriptive stats etc.)\nSome tips:\n\nThe pairs.panels() function from the psych package is quite a nice way to plot a scatterplot matrix of a dataset.\n\nThe describe() function is also quite nice (from the psych package too).\n\n\n\n\n\n Solution \n\n\n\ncogapoe <- read_csv(\"https://uoepsy.github.io/data/cogapoe4.csv\")\nsummary(cogapoe)\n\n     pid                 age              educ       birthweight_kg  \n Length:90          Min.   : 45.00   Min.   :15.00   Min.   : 0.500  \n Class :character   1st Qu.: 57.25   1st Qu.:17.00   1st Qu.: 5.225  \n Mode  :character   Median : 67.00   Median :19.00   Median : 6.550  \n                    Mean   : 70.53   Mean   :19.31   Mean   : 6.546  \n                    3rd Qu.: 86.75   3rd Qu.:22.00   3rd Qu.: 8.150  \n                    Max.   :100.00   Max.   :24.00   Max.   :11.500  \n    apoe4                acer       \n Length:90          Min.   : 70.42  \n Class :character   1st Qu.: 78.52  \n Mode  :character   Median : 85.02  \n                    Mean   : 86.53  \n                    3rd Qu.: 95.36  \n                    Max.   :100.00  \n\n\nJudging by the research question, we’re going to be interested in participants’ ages, whether they carry the APOE4 gene, and their cognitive functioning.\n\nlibrary(psych)\ncogapoe %>% \n  select(age, apoe4, acer) %>%\n  pairs.panels()\n\n\n\n\n\n\n\ncogapoe %>% \n  select(age, apoe4, acer) %>%\n  describe()\n\n       vars  n  mean    sd median trimmed   mad   min max range  skew kurtosis\nage       1 90 70.53 16.28  67.00   70.28 19.27 45.00 100 55.00  0.21    -1.30\napoe4*    2 90  2.76  1.13   3.00    2.82  1.48  1.00   4  3.00 -0.25    -1.40\nacer      3 90 86.53  9.39  85.02   86.63 13.11 70.42 100 29.58 -0.01    -1.34\n         se\nage    1.72\napoe4* 0.12\nacer   0.99\n\n\n\n\n\n\nQuestion C2\n\n\nCheck the apoe4 variable. It currently has four levels (“none”/“apoe4a”/“apoe4b”/“apoe4c”), but the research question is actually interested in two (“none” vs “apoe4”). We’ll need to fix this. One way to do this would be to use ifelse() to define a variable which takes one value (e.g., “NO”) if the observation meets from some condition, or another value (e.g., “YES”) if it does not. Type ?ifelse in the console if you want to see the help function. You can use it to add a new variable either inside mutate(), or using data$new_variable_name <- ifelse(test, x, y) syntax. We saw it briefly in the previous set of exercises.\n\n\n\n\n Solution \n\n\nCreate a new variable for Rural/Not Rural\n\ncogapoe <- \n  cogapoe %>% \n  mutate(\n    isAPOE4 = ifelse(apoe4 == \"none\", \"No\", \"Yes\")\n  )\n\n\n\n\n\nQuestion C3\n\n\nProduce a visualisation of the relationship between age and cognitive functioning, with separate facets for people with and without the APOE4 gene.\nHint: remember facet_wrap()?\n\n\n\n\n Solution \n\n\n\nggplot(data = cogapoe, aes(x = age, y = acer)) + \n  geom_point() + \n  facet_wrap(~isAPOE4)\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions in linear models\nSpecifying an interaction in a regression involves simply including the product of the two relevant predictor variables in your set of predictors. So if we are wanting to examine how the effect of \\(x\\) on \\(y\\) depends on \\(z\\), we would want to estimate a parameter \\(b\\) such that our outcome is predicted by \\(b(x \\times z)\\). However, we also need to include \\(x\\) and \\(z\\) themselves: \\(y = b_0 + b_1(x) + b_2(z) + b_3(x \\times z)\\).\n\n“Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of \\(X_1\\) depends on the level of \\(X_2\\) but that there is no effect of \\(X_1\\).”\n@Ramsey2012\n\n\n\nQuestion C4\n\n\nTo address the research question, we are going to fit the following model:\n\\[\n\\text{ACE-R} = b_0 + b_1(\\text{Age}) + b_2(\\text{isAPOE4}) + b_3 (\\text{Age} \\times \\text{isAPOE4}) + \\epsilon \\\\\n\\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nFit the model using lm().\n\nTip:\nWhen fitting a regression model in R with two explanatory variables A and B, and their interaction, these two are equivalent:\n\ny ~ A + B + A:B\ny ~ A*B\n\n\n\n\n\n\n Solution \n\n\n\napoe4mod <- lm(acer ~ 1 + age * isAPOE4, data = cogapoe)\n\n\n\n\n\nInterpreting coefficients for A and B in the presence of an interaction A:B\nWhen you include an interaction between \\(x_1\\) and \\(x_2\\) in a regression model, you are estimating the extent to which the effect of \\(x_1\\) on \\(y\\) is different across the values of \\(x_2\\).\nWhat this means is that the effect of \\(x_1\\) on \\(y\\) depends on/is conditional upon the value of \\(x_2\\).\n(and vice versa, the effect of \\(x_2\\) on \\(y\\) is different across the values of \\(x_1\\)).\nThis means that we can no longer talk about the “effect of \\(x_1\\) holding \\(x_2\\) constant”. Instead we can talk about a conditional effect of \\(x_1\\) on \\(y\\) at a specific value of \\(x_2\\).\n\nWhen we fit the model \\(y = b_0 + b_1(x_1)+ b_2(x_2) + b_3(x_1 \\times x_2) + \\epsilon\\) using lm():\n\nthe parameter estimate \\(\\hat b_1\\) is the conditional effect of \\(x_1\\) on \\(y\\) where \\(x_2 = 0\\)\n\nthe parameter estimate \\(\\hat b_2\\) is the conditional effect of \\(x_2\\) on \\(y\\) where \\(x_1 = 0\\)\n\n\n\n\n side note: Regardless of whether or not there is an interaction term in our model, all parameter estimates in multiple regression are “conditional” in the sense that they are dependent upon the inclusion of other variables in the model. For instance, in \\(y = b_0 + b_1(x_1) + b_2(x_2) + \\epsilon\\) the coefficient \\(\\hat b_1\\) is conditional upon holding \\(x_2\\) constant. The difference with an interaction is that they are conditional upon \\(x_2\\) being at some specific value.\n\n\n\n\nInterpreting the interaction term A:B\nThe coefficient for an interaction term can be thought of as providing an adjustment to the slope.\nIn our model: \\(\\text{ACE-R} = b_0 + b_1(\\text{Age}) + b_2(\\text{isAPOE4}) + b_3 (\\text{Age} \\times \\text{isAPOE4}) + \\epsilon\\), we have a numeric*categorical interaction.\nThe estimate \\(\\hat b_3\\) is the adjustment to the slope \\(\\hat b_1\\) to be made for the individuals in the \\(\\text{isAPOE4}=1\\) group.\n\n\n“The best method of communicating findings about the presence of significant interaction may be to present a table of graph of the estimated means at various combinations of the interacting variables.”\n@Ramsey2012\n\n\nQuestion C5\n\n\nLook at the parameter estimates from your model, and write a description of what each one corresponds to on the plot shown in Figure @ref(fig:plot-annotate-int) (it may help to sketch out the plot yourself and annotate it).\n\n\n\n\n\nMultiple regression model: ACER ~ Age * isAPOE4The dotted lines show the extension back to where the x-axis is zero\n\n\n\n\n\n Hints\n\n\nHere are some options to choose from:\n\nThe point at which the blue line cuts the y-axis (where age = 0)\nThe point at which the red line cuts the y-axis (where age = 0)\nThe average vertical distance between the red and blue lines.\nThe vertical distance from the blue to the red line at the y-axis (where age = 0)\nThe vertical distance from the red to the blue line at the y-axis (where age = 0)\nThe vertical distance from the blue to the red line at the center of the plot\nThe vertical distance from the red to the blue line at the center of the plot\nThe slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line\nThe slope of the red line\nThe adjustment to the slope when you move from the blue to the red line\nThe adjustment to the slope when you move from the red to the blue line\n\n\n\n\n\n\n\n\n Solution \n\n\nWe can obtain our parameter estimates using various functions such as summary(apoe4mod),coef(apoe4mod), coefficients(apoe4mod) etc.\n\ncoefficients(apoe4mod)\n\n   (Intercept)            age     isAPOE4Yes age:isAPOE4Yes \n  104.09765349    -0.09796756    -4.37223106    -0.17966769 \n\n\n\n\\(\\hat b_0\\) = (Intercept) = 104.1: The point at which the red line cuts the y-axis (where age = 0).\n\n\\(\\hat b_1\\) = age = -0.1: The slope (vertical change on the y-axis associated with a 1 unit change on the x-axis) of the red line.\n\\(\\hat b_2\\) = isAPOE4Yes = -4.37: The vertical distance from the red to the blue line at the y-axis (where age = 0).\n\n\\(\\hat b_3\\) = age:isAPOE4Yes = -0.18: How the slope of the line changes when you move from the red to the blue line.\n\n\n\n\n\nQuestion C6\n\n\nMake sure the sjPlot package is loaded and try using the function plot_model().\nThe default behaviour of plot_model() is to plot the parameter estimates and their confidence intervals. This is where type = \"est\". Try to create a plot like Figure @ref(fig:plot-annotate-int), which shows the two lines (Hint: what is this set block of exercises all about? type = ???.)\n\n\n\n\n Solution \n\n\n\nlibrary(sjPlot)\nplot_model(apoe4mod, type=\"int\")"
  },
  {
    "objectID": "09a_moreregression.html#numeric-numeric",
    "href": "09a_moreregression.html#numeric-numeric",
    "title": "9A: More Linear Regression",
    "section": "~ Numeric * Numeric",
    "text": "~ Numeric * Numeric\nWe will now look at a multiple regression model with an interaction between two numeric explanatory variables. For these exercises we’ll move onto another different dataset.\n\nData: scs_study.csv\nData from 656 participants containing information on scores on each trait of a Big 5 personality measure, their perception of their own social rank, and their scores on a measure of depression.\nThe data in scs_study.csv contain seven attributes collected from a random sample of \\(n=656\\) participants:\n\nzo: Openness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\nzc: Conscientiousness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\nze: Extraversion (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\nza: Agreeableness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\nzn: Neuroticism (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\nscs: Social Comparison Scale - An 11-item scale that measures an individual’s perception of their social rank, attractiveness and belonging relative to others. The scale is scored as a sum of the 11 items (each measured on a 5-point scale), with higher scores indicating more favourable perceptions of social rank.\ndass: Depression Anxiety and Stress Scale - The DASS-21 includes 21 items, each measured on a 4-point scale. The score is derived from the sum of all 21 items, with higher scores indicating higher a severity of symptoms.\n\nDownload link\nThe data is available at https://uoepsy.github.io/data/scs_study.csv\n\n\nRefresher: Z-scores\nWhen we standardise a variable, we re-express each value as the distance from the mean in units of standard deviations. These transformed values are called z-scores.\nTo transform a given value \\(x_i\\) into a z-score \\(z_i\\), we simply calculate the distance from \\(x_i\\) to the mean, \\(\\bar{x}\\), and divide this by the standard deviation, \\(s\\):\n\\[\nz_i = \\frac{x_i - \\bar{x}}{s}\n\\]\nA Z-score of a value is the number of standard deviations below/above the mean that the value falls.\n\n\nQuestion C7\n\n\n\nResearch question\nPrevious research has identified an association between an individual’s perception of their social rank and symptoms of depression, anxiety and stress. We are interested in the individual differences in this relationship.\nSpecifically: Does the effect of social comparison on symptoms of depression, anxiety and stress vary depending on level of neuroticism?\n\nRead in the Social Comparison Study data and explore the relevant distributions and relationships between the variables of interest to the research question.\n\n\n\n\n Solution \n\n\n\nscs_study <- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\nsummary(scs_study)\n\n       zo                 zc                 ze                 za          \n Min.   :-2.81928   Min.   :-3.21819   Min.   :-3.00576   Min.   :-2.94429  \n 1st Qu.:-0.63089   1st Qu.:-0.66866   1st Qu.:-0.68895   1st Qu.:-0.69394  \n Median : 0.08053   Median : 0.00257   Median :-0.04014   Median :-0.01854  \n Mean   : 0.09202   Mean   : 0.01951   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.80823   3rd Qu.: 0.71215   3rd Qu.: 0.67085   3rd Qu.: 0.72762  \n Max.   : 3.55034   Max.   : 3.08015   Max.   : 2.80010   Max.   : 2.97010  \n       zn               scs             dass      \n Min.   :-1.4486   Min.   :27.00   Min.   :23.00  \n 1st Qu.:-0.7994   1st Qu.:33.00   1st Qu.:41.00  \n Median :-0.2059   Median :35.00   Median :44.00  \n Mean   : 0.0000   Mean   :35.77   Mean   :44.72  \n 3rd Qu.: 0.5903   3rd Qu.:38.00   3rd Qu.:49.00  \n Max.   : 3.3491   Max.   :54.00   Max.   :68.00  \n\n\n\nggplot(data = scs_study, aes(x=dass)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of DASS-21 Scores\", \n       x = \"Depression Anxiety and Stress Scale\", y = \"Probability density\")\n\n\n\n\n\n\n\n\n\nThe marginal distribution of scores on the Depression, Anxiety and Stress Scale (DASS-21) is unimodal with a mean of approximately 45 and a standard deviation of 7.\n\n\nggplot(data = scs_study, aes(x=scs)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Social Comparison Scale (SCS) scores\", \n       x = \"Social Comparison Scale Score\", y = \"Probability density\")\n\n\n\n\n\n\n\n\n\nThe marginal distribution of score on the Social Comparison Scale (SCS) is unimodal with a mean of approximately 36 and a standard deviation of 4. There look to be a number of outliers at the upper end of the scale.\n\n\nggplot(data = scs_study, aes(x=zn)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Neuroticism (Z-Scored)\", \n       x = \"Neuroticism (Z-Scored)\", y = \"Probability density\")\n\n\n\n\n\n\n\n\n\nThe marginal distribution of Neuroticism (Z-scored) is positively skewed, with the 25% of scores falling below -0.8, 75% of scores falling below 0.59.\n\n\nlibrary(patchwork) # for arranging plots side by side\nlibrary(knitr) # for making tables look nice\n\np1 <- ggplot(data = scs_study, aes(x=scs, y=dass)) + \n  geom_point()+\n  labs(x = \"SCS\", y = \"DASS-21\")\n\np2 <- ggplot(data = scs_study, aes(x=zn, y=dass)) + \n  geom_point()+\n  labs(x = \"Neuroticism\", y = \"DASS-21\")\n\np1 | p2\n\n\n\n\n\n\n\n# the kable() function from the knitr package can make table outputs print nicely into html.\nscs_study %>%\n  select(dass, scs, zn) %>%\n  cor %>% \n  kable\n\n\n\n \n  \n      \n    dass \n    scs \n    zn \n  \n \n\n  \n    dass \n    1.0000000 \n    -0.2280126 \n    0.2001885 \n  \n  \n    scs \n    -0.2280126 \n    1.0000000 \n    0.1146687 \n  \n  \n    zn \n    0.2001885 \n    0.1146687 \n    1.0000000 \n  \n\n\n\n\n\n\nThere is a weak, negative, linear relationship between scores on the Social Comparison Scale and scores on the Depression Anxiety and Stress Scale for the participants in the sample. Severity of symptoms measured on the DASS-21 tend to decrease, on average, the more favourably participants view their social rank.\nThere is a weak, positive, linear relationship between the levels of Neuroticism and scores on the DASS-21. Participants who are more neurotic tend to, on average, display a higher severity of symptoms of depression, anxiety and stress.\n\n\n\n\n\nQuestion C8\n\n\nSpecify the model you plan to fit in order to answer the research question (e.g., \\(\\text{??} = b_0 + b_1 (\\text{??}) + .... + \\epsilon\\))\n\n\n\n\n Solution \n\n\n\\[\n\\text{DASS-21 Score} = b_0 + b_1(\\text{SCS Score}) + b_2(\\text{Neuroticism}) + b_3(\\text{SCS score} \\times \\text{Neuroticism}) + \\epsilon\n\\]\n\n\n\n\nQuestion C9\n\n\nWe named the data scs_study in our environment, but you will likely have named yours something different. Edit the code below accordingly to run it on your data.\nThe code takes the dataset, and uses the cut() function to add a new variable called “zn_group”, which is the “zn” variable split into 4 groups.\n\n\nRemember: we have re-assign this output as the name of the dataset (the scs_study <- bit at the beginning) to make these changes occur in our environment (the top-right window of Rstudio). If we didn’t have the first line, then it would simply print the output.\n\n\nscs_study <-\n  scs_study %>%\n  mutate(\n    zn_group = cut(zn, 4)\n  )\n\nWe can see how it has split the “zn” variable by plotting the two against one another:\n(Note that the levels of the new variable are named according to the cut-points).\n\nggplot(data = scs_study, aes(x = zn_group, y = zn)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion C10\n\n\nPlot the relationship between scores on the SCS and scores on the DASS-21, for each group of the variable (zn_group) that we just created.\nHow does the pattern change? Does it suggest an interaction?\nTip: Rather than creating four separate plots, you might want to map some feature of the plot to the variable we created in the data, or make use of facet_wrap()/facet_grid().\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x = scs, y = dass, col = zn_group)) + \n  geom_point() + \n  facet_wrap(~zn_group, scales=\"free_x\") +\n  theme(legend.position = \"none\") # remove the legend\n\n\n\n\n\n\n\n\nThe relationship between SCS scores and DASS-21 scores appears to be different between these groups. For those with a relatively high neuroticism score, the relationship seems stronger, while for those with a low neuroticism score there is almost no discernable relationship. This suggests an interaction - the relationship of DASS-21 ~ SCS differs across the values of neuroticism!\n\n\n\n\nCutting one of the explanatory variables up into groups essentially turns a numeric variable into a categorical one. We did this just to make it easier to visualise how a relationship changes across the values of another variable, because we can imagine a separate line for the relationship between SCS and DASS-21 scores for each of the groups of neuroticism. However, in grouping a numeric variable like this we lose information. Neuroticism is measured on a continuous scale, and we want to capture how the relationship between SCS and DASS-21 changes across that continuum (rather than cutting it into chunks).\nWe could imagine cutting it into more and more chunks (see Figure @ref(fig:reglinescut)), until what we end up with is a an infinite number of lines - i.e., a three-dimensional plane/surface (recall that in for a multiple regression model with 2 explanatory variables, we can think of the model as having three-dimensions). The inclusion of the interaction term simply results in this surface no longer being necessarily flat. You can see this in Figure @ref(fig:3dint).\n\n\n\n\n\nSeparate regression lines DASS ~ SCS for neuroticism when cut into 4 (left) or 6 (center) or 12 (right) groups\n\n\n\n\n\n\n\n\n3D plot of regression surface with interaction. You can explore the plot in the figure below from different angles by moving it around with your mouse.\n\n\n\n\nQuestion C11\n\n\nFit your model using lm().\n\n\n\n\n Solution \n\n\n\ndass_mdl <- lm(dass ~ 1 + scs*zn, data = scs_study)\nsummary(dass_mdl)\n\n\nCall:\nlm(formula = dass ~ 1 + scs * zn, data = scs_study)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.301  -3.825  -0.173   3.733  45.777 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 60.80887    2.45399  24.780  < 2e-16 ***\nscs         -0.44391    0.06834  -6.495 1.64e-10 ***\nzn          20.12813    2.35951   8.531  < 2e-16 ***\nscs:zn      -0.51861    0.06552  -7.915 1.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.123 on 652 degrees of freedom\nMultiple R-squared:  0.1825,    Adjusted R-squared:  0.1787 \nF-statistic:  48.5 on 3 and 652 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nQuestion C12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n60.81\n2.454\n24.78\n5.005e-96\n\n\nscs\n-0.4439\n0.06834\n-6.495\n1.643e-10\n\n\nzn\n20.13\n2.36\n8.531\n1.017e-16\n\n\nscs:zn\n-0.5186\n0.06552\n-7.915\n1.063e-14\n\n\n\n\n\nRecall that the coefficients zn and scs from our model now reflect the estimated change in the outcome associated with an increase of 1 in the explanatory variables, when the other variable is zero.\nThink - what is 0 in each variable? what is an increase of 1? Are these meaningful? Would you suggest recentering either variable?\n\n\n\n\n Solution \n\n\nThe neuroticism variable zn is Z-scored, which means that 0 is the mean (it is mean-centered), and 1 is a standard deviation.\nThe Social Comparison Scale variable scs is the raw-score. Looking back at the description of the variables, we can work out that the minimum possible score is 11 (if people respond 1 for each of the 11 questions) and the maximum is 55 (if they respond 5 for all questions). Is it meaningful/useful to talk about estimated effects for people who score 0? Not really.\nBut we can make it so that zero represents something else, such as the minimum score, or the mean score. For instance, scs_study$scs - 11 will subtract 11 from the scores, making zero the minimum possible score on the scale.\n\n\n\n\nQuestion C13\n\n\nRecenter one or both of your explanatory variables to ensure that 0 is a meaningful value\n\n\n\n\n Solution \n\n\nWe’re going to mean-center the scores on the SCS. Think about what someone who now scores zero on the zn variable and zero on the mean-centered SCS?\n\nscs_study <-\n  scs_study %>%\n  mutate(\n    scs_mc = scs - mean(scs)\n  )\n\n\n\n\n\nQuestion C14\n\n\nWe’ll now re-fit the model using mean-centered SCS scores instead of the original variable. Here are the parameter estimates:\n\ndass_mdl2 <- lm(dass ~ 1 + scs_mc * zn, data = scs_study)\n\n# pull out the coefficients from the summary():\nsummary(dass_mdl2)$coefficients\n\n              Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) 44.9324476 0.24052861 186.807079 0.000000e+00\nscs_mc      -0.4439065 0.06834135  -6.495431 1.643265e-10\nzn           1.5797687 0.24086372   6.558766 1.105118e-10\nscs_mc:zn   -0.5186142 0.06552100  -7.915236 1.063297e-14\n\n\nFill in the blanks in the statements below.\n\n\nFor those of average neuroticism and who score average on the SCS, the estimated DASS-21 Score is ???\n\nFor those who who score ??? on the SCS, an increase of ??? in neuroticism is associated with a change of 1.58 in DASS-21 Scores\nFor those of average neuroticism, an increase of ??? on the SCS is associated with a change of -0.44 in DASS-21 Scores\n\nFor every increase of ??? in neuroticism, the change in DASS-21 associated with an increase of ??? on the SCS is asjusted by ???\nFor every increase of ??? in SCS, the change in DASS-21 associated with an increase of ??? in neuroticism is asjusted by ???\n\n\n\n\n\n\n Solution \n\n\n\n\nFor those of average neuroticism and who score average on the SCS, the estimated DASS-21 Score is 44.93\n\nFor those who who score average (mean) on the SCS, an increase of 1 standard deviation in neuroticism is associated with a change of 1.58 in DASS-21 Scores\nFor those of average neuroticism, an increase of 1 on the SCS is associated with a change of -0.44 in DASS-21 Scores\n\nFor every increase of 1 standard deviation in neuroticism, the change in DASS-21 associated with an increase of 1 on the SCS is asjusted by -0.52\nFor every increase of 1 in SCS, the change in DASS-21 associated with an increase of 1 standard deviation in neuroticism is asjusted by -0.52\n\n\n\n\n\n\nQuestion C15\n\n\nWhat do we get when we use the plot_model() function from sjPlot to plot this continuous*continuous interaction?\n\n\n\n\n Solution \n\n\nplot_model() will choose two values of zn at which to plot the effect of scs_mc on dass. Specifically, it will choose the minimum and the maximum:\n\nplot_model(dass_mdl2, type=\"int\")\n\n\n\n\n\n\n\n\nWe might want to choose some other values, such as the mean of zn and \\(\\pm 1\\) standard deviation from the mean. We can do this fairly easily:\n\nplot_model(dass_mdl2, type = \"pred\", terms = c(\"scs_mc\", \"zn [-1, 0, 1]\"))\n\n\n\n\n\n\n\n\n\n\n\n\n References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Univariate Statistics and Methodology in R",
    "section": "",
    "text": "Univariate Statistics and Methodology in R (USMR) is a semester long crash-course aimed at providing Masters students in psychology with a competence in standard statistical methodologies and data analysis using R. Typically the analyses taught in this course are relevant for when there is just one source of variation - i.e. when we are interested in a single outcome measured across a set of independent observations. The first half of the course covers the fundamentals of statistical inference using a simulation-based approach, and introduces students to working with R & RStudio. The latter half of the course focuses on the general linear model, emphasising the fact that many statistical methods are simply special cases of this approach. This course introduces students to statistical modelling and empowers them with tools to analyse richer data and answer a broader set of research questions."
  }
]