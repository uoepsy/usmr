[
  {
    "objectID": "zz_wt.html",
    "href": "zz_wt.html",
    "title": "- WalkThrough: Advanced Data Wrangling",
    "section": "",
    "text": "In many projects (both in academic research & in other sectors), more time will be spent cleaning and organising data than will actually be spent conducting statistical analyses (a well designed study with a well-thought through data collection process can go a long way to remedy this!).\nFor this reason, we’re going to take a little detour away from statistics to get some more practice wrangling and cleaning data in R. Don’t worry about the trying to remember all of the new R functions introduced in this topic - there are a lot. Use them as a means of learning about some of the different ways of doing things in R.\n\nStudy Background & Data\nThe data we’re going to look at now is from an experiment on language comprehension, looking at whether people perceive blinking as a sign of lying.\n\nResearch Question: Is the rate of blinking during speech interpreted as a sign of dishonesty (in the context of a lie-detection game)?\n\nParticipants were informed that they were going to take part in a lie-detection game. They were presented with audiovisual recordings of a speaker stating behind which of two objects (displayed on screen) there was hidden treasure. Utterances took the form of “The treasure is behind the [target name]”.\nOver 20 trials, participants were tasked with using the mouse to click on the object they believed the treasure to be behind. They were told that the speaker in the video was attempting to mislead them, meaning that sometimes they told the truth, and sometimes they lied. Crucially, in the videos presented of the speaker producing the utterances, we manipulated the number of times the speaker blinked (from 1 to 10 times). Participants eyes were tracked for the duration of the experiment, with the time spent looking at either object taken as an implicit indication of perceiving a truthful utterance (in which the participant looks at and clicks on the ‘target object’ (the one identified by the speaker as hiding the treasure)) or a dishonest one (in which the participant would look at and click on the alternative ‘distractor’ object).\n\nblink_setup.csv\n\nThe data from the experimental design are available at https://uoepsy.github.io/data/blink_setup.csv. In this data, each participant is a row, and the information about what video is presented in each trial are presented in separate columns for each trial. The first bit of the data looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n1\nsubject_1\n/files/vids/blinks_1.mp4\n/files/vids/blinsk_8.mp4\n…\n\n\n2\nsubject_2\n/files/vids/blinks_2.mp4\n/files/vids/blinks_4.mp4\n…\n\n\n3\nsubject_3\n/files/vids/blinks_4.mp4\n/files/vids/blinks_5.mp4\n…\n\n\n4\nsubject_4\n/files/vids/blinks_4.mp4\n/files/vids/blinks_7.mp4\n…\n\n\n5\nsubject_5\n/files/vids/blinks_1.mp4\n/files/vids/blinks_4.mp4\n…\n\n\n6\nsubject_6\n/files/vids/blinks_2.mp4\n/files/vids/blinks_3.mp4\n…\n\n\n…\n…\n…\n…\n…\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\nblink_eyegaze.xlsx\n\nThe data from the eye-tracker, which has been processed to show the proportion of time spent looking at the distractor object in each trial, can be found at https://uoepsy.github.io/data/blink_eyegaze.xlsx. In contrast to the blink_setup.csv data, in this data each trial is a row, so we have 20 rows per participant.\n\n\n\n\n\n\n\n\n\nvariable_names\ndescription\n\n\n\n\nsub\nParticipant number\n\n\ntrial_no\nTrial number\n\n\ndistractor_fix\nTime spent looking at distractor object (measured in milliseconds from onset of noun phrase)\n\n\nrt\nTime taken to click on an object (measured in milliseconds from the onset of the noun phrase\n\n\n\n\n\nThe top of the data looks like this:\n\n\n\n\n\nsub\ntrial_no\ndistractor_fix\nrt\n\n\n\n\n1\n1\n503.990657311976\n2812\n\n\n1\n2\n2810.1367654\n2974\n\n\n1\n3\n706.739099152984\n2257\n\n\n1\n4\nNA\nNA\n\n\n1\n5\n223.327680772201\n4546\n\n\n1\n6\nNA\nNA\n\n\n…\n…\n…\n…\n\n\n…\n…\n…\n…\n\n\n\n\n\n\n\n\n\n\nDifferent Data Formats\nData can come in lots of different formats, meaning that we need lots of different ways to read data into R. Below is some information on some of the more common functions for reading and writing different types of data.\nText based files\n\n\n\n\n\n\n\n\n\nfiletype\ndescription\nreading\nwriting\n\n\n\n\n.csv\ncomma separated values\ntidyverse - read_csv()read.csv()read.table(..., sep = \",\")\ntidyverse - write_csv()write.csv()write.table(..., sep=\",\")\n\n\n.tsv\ntab separated values\ntidyverse - read_tsv()read.table(..., sep = \"\\t\")\ntidyverse - write_tsv()write.table(..., sep = \"\\t\")\n\n\n.txt\nanything-separated values!\nread.table(..., sep = ...)\nwrite.table(..., sep = ...)\n\n\n\nR files\n\n\n\n\n\n\n\n\n\nfiletype\ndescription\nreading\nwriting\n\n\n\n\n.RDS\n1 file = a single R object\nreadRDS()\nsaveRDS()\n\n\n.RData\n1 file = a collection of R objects\nload()\nsave()save.image() - to save all objects in the environment)\n\n\n\nExcel files\nThe package readxl provides a variety of functions for reading in different types of Microsoft Excel spreadsheet, such as read_excel(), read_xls(), read_xlsx().\nOther software\nThe package haven provides functions for files which have been saved from other statistical software, for instance with read_spss()/read_sav() and read_sas() for files from SPSS and SAS.\nGoogle sheets\nThe googlesheets4 package can read in data directly from a spreadsheet stored on google drive. You simply find the id of the sheet (it’s the big long string of numbers & letters in the url of your google sheet), and pass it to read_sheet().\nIt will prompt you to authenticate your account via your browser, but it’s really easy!\n\nQuestion 1\n\n\nRead in the two data-sets. Take care to look at the file extension (e.g., .csv, .tsv, .xlsx) as indicators of what function to try.\nMake sure you assign them identifiable names.\nOnce you’ve loaded the data-set, take a look at them using functions like summary(), str(), dim()/nrow(), or viewing them by clicking on them in the environment.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nSome functions like read_excel() don’t allow you to download directly from a url, like we have been doing with .csv files.\n\nSolution 1:\n\nDownload the data to your computer\nupload to the rstudio server if you are using it\nDirect the function to read it from the place you stored it.\n\nSolution 2:\n\nMake R download the data directly to somewhere in your working directory (see download.file()).\n\n\nDo both the data-sets have column names? By default R will assume the first row is the name of the column. Look in the help documentation to see how to stop this from happening.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndownload.file('https://uoepsy.github.io/data/blink_eyegaze.xlsx', 'blink_eyegaze.xlsx', mode=\"wb\")\neyedata &lt;- read_excel(path = 'blink_eyegaze.xlsx')\n\nsetupdata &lt;- read_csv(\"https://uoepsy.github.io/data/blink_setup.csv\", col_names = FALSE)\n\n\n\n\n\n\n\n\n\nRenaming Columns\nYou can access the column names from a data-set using names() or colnames().\n\nnames(data)\ncolnames(data)\n\nAnd we can easily rename these using indexing:\n\n#name the third column \"peppapig\"\nnames(data)[3]&lt;-\"peppapig\"\n\nOr in tidyverse, using rename():\n\ndata |&gt;\n  rename(newname = currentname)\n\n\nQuestion 2\n\n\nProblem\nThe blink_setup.csv file doesn’t have any column names!\nWe know that there are 20 trials for each participant, and we can see that the 2nd column has information about which subject it is.\nColumns 3:22 are trials 1 to 20.\n\nhead(setupdata)\n\n\n\n   X1        X2                       X3 ...\n1   1 subject_1 /files/vids/blinks_1.mp4 ...\n2   2 subject_2 /files/vids/blinks_2.mp4 ...\n3   3 subject_3 /files/vids/blinks_4.mp4 ...\n4   4 subject_4 /files/vids/blinks_4.mp4 ...\n5   5 subject_5 /files/vids/blinks_1.mp4 ...\n6   6 subject_6 /files/vids/blinks_2.mp4 ...\n7 ...       ...                      ... ...\n8 ...       ...                      ... ...\n\n\nTask\n\nRemove the first column\nRename columns 2 to 22 with sensible names.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nnames(setupdata) # what are the names\nnames(setupdata)[2] # what is the 2nd name\nnames(setupdata) &lt;- c(\"...\", \"...\", \"...\",..) # set the names\n\n\nc(\"kermit\", paste(\"peppapig\", 1:3, sep=\"_\"))\n\n[1] \"kermit\"     \"peppapig_1\" \"peppapig_2\" \"peppapig_3\"\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. remove the first column\n\nsetupdata &lt;- setupdata[,-1]\n\nSet the names\n\nnames(setupdata) &lt;- c(\"sub\",paste(\"trial\", 1:20, sep = \"_\"))\n\nCheck:\n\nhead(setupdata)\n\n\n\n        sub                  trial_1 ...\n1 subject_1 /files/vids/blinks_1.mp4 ...\n2 subject_2 /files/vids/blinks_2.mp4 ...\n3 subject_3 /files/vids/blinks_4.mp4 ...\n4 subject_4 /files/vids/blinks_4.mp4 ...\n5 subject_5 /files/vids/blinks_1.mp4 ...\n6 subject_6 /files/vids/blinks_2.mp4 ...\n7       ...                      ... ...\n8       ...                      ... ...\n\n\n\n\n\n\n\n\n\n\nReshaping data\nPivot!\nOne of the more confusing things to get to grips with is the idea of reshaping a dataframe.\nFor different reasons, you might sometimes want to have data in wide, or in long format.\n\n\n\n\n\nSource: https://fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/\n\n\n\n\nWhen the data is wide, we can make it long using pivot_longer(). When we make data longer, we’re essentially making lots of columns into 2 longer columns. Above, in the animation, the wide variable x, y and z go into a new longer column called name that specifies which (x/y/z) it came from, and the values get put into the val column.\nThe animation takes a shortcut in the code it displays above, but you could also use pivot_longer(c(x,y,z), names_to = \"name\", values_to = \"val\"). To reverse this, and put it back to being wide, we tell R which columns to take the names and values from: pivot_wider(names_from = name, values_from = val).\n\nQuestion 3\n\n\nProblem\nThe blink_setup.csv file has the data in a different shape to the blink_eyegaze.xlsx file.\n\nblink_setup.csv : one row per participant\n\nblink_eyegaze.xlsx : one row per trial\n\nTask\nReshape the data to make it so that there is one row per trial.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nin the tidyverse functions, you can specify all columns between column x and column z by using the colon, x:z.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. (Note that this will depend on what you called your columns in the previous question - we just called them “trial_1”, … , “trial_20”).\n\nsetuplong &lt;- \n  setupdata |&gt;\n  pivot_longer(trial_1:trial_20, names_to = \"trial_number\", values_to = \"video\")\n\nsetuplong\n\n# A tibble: 460 × 3\n   sub       trial_number video                    \n   &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;                    \n 1 subject_1 trial_1      /files/vids/blinks_1.mp4 \n 2 subject_1 trial_2      /files/vids/blinsk_8.mp4 \n 3 subject_1 trial_3      /files/vids/blinks_1.mp4 \n 4 subject_1 trial_4      /files/vids/blinks_5.mp4 \n 5 subject_1 trial_5      /files/vids/blinks_4.mp4 \n 6 subject_1 trial_6      /files/vids/blinks_10.mp4\n 7 subject_1 trial_7      /files/vids/blinks_1.mp4 \n 8 subject_1 trial_8      /files/vids/blinks_5.mp4 \n 9 subject_1 trial_9      /files/vids/blinks_6.mp4 \n10 subject_1 trial_10     /files/vids/blinks_4.mp4 \n# ℹ 450 more rows\n\n\n\n\n\n\n\n\n\n\nDealing with character strings\nThere are loads of functions we can use to do various things with character strings in R.\nHere are a few examples:\n\n\n\n\n\n\ngsub() - substitute a string of characters for another string\n\n\n\n\n\n\ngsub(\"don't like\",\"love\", \"i really really really don't like statistics!\")\n\n[1] \"i really really really love statistics!\"\n\n\n\n\n\n\n\n\n\n\n\nseparate() - separate a column into multiple columns by splitting at a set of characters\n\n\n\n\n\n\nmupsimp &lt;- read_csv(\"https://uoepsy.github.io/data/muppet_simp.csv\")\nmupsimp\n\n# A tibble: 18 × 1\n   show_name                     \n   &lt;chr&gt;                         \n 1 simpsons_Marge Simpson        \n 2 muppets_Scooter               \n 3 muppets_Rowlf the Dog         \n 4 muppets_Fozzie Bear           \n 5 simpsons_Abraham Simpson      \n 6 muppets_Walter                \n 7 muppets_Pepe the King Prawn   \n 8 muppets_Gonzo                 \n 9 simpsons_Santa's Little Helper\n10 simpsons_Snowball II/V        \n11 simpsons_Maggie Simpson       \n12 simpsons_Lisa Simpson         \n13 simpsons_Bart Simpson         \n14 muppets_Animal                \n15 simpsons_Homer Simpson        \n16 muppets_Miss Piggy            \n17 muppets_Rizzo the Rat         \n18 muppets_Kermit the Frog       \n\nmupsimp |&gt; \n  separate(show_name, into = c(\"show\",\"name\"), sep = \"_\")\n\n# A tibble: 18 × 2\n   show     name                 \n   &lt;chr&gt;    &lt;chr&gt;                \n 1 simpsons Marge Simpson        \n 2 muppets  Scooter              \n 3 muppets  Rowlf the Dog        \n 4 muppets  Fozzie Bear          \n 5 simpsons Abraham Simpson      \n 6 muppets  Walter               \n 7 muppets  Pepe the King Prawn  \n 8 muppets  Gonzo                \n 9 simpsons Santa's Little Helper\n10 simpsons Snowball II/V        \n11 simpsons Maggie Simpson       \n12 simpsons Lisa Simpson         \n13 simpsons Bart Simpson         \n14 muppets  Animal               \n15 simpsons Homer Simpson        \n16 muppets  Miss Piggy           \n17 muppets  Rizzo the Rat        \n18 muppets  Kermit the Frog      \n\n\n\n\n\n\n\n\n\n\n\nsubstr() - extract or replace substrings in a character vector\n\n\n\n\n\n\n# get the first 3 letters\nsubstr(mupsimp$show_name, 1, 3)\n\n [1] \"sim\" \"mup\" \"mup\" \"mup\" \"sim\" \"mup\" \"mup\" \"mup\" \"sim\" \"sim\" \"sim\" \"sim\"\n[13] \"sim\" \"mup\" \"sim\" \"mup\" \"mup\" \"mup\"\n\n\nCan be combined with functions like nchar() (to find the number of characters in each string). Additionally, can be used in tidyverse easily:\n\nmupsimp |&gt;\n  mutate(\n    first3 = substr(show_name, 1, 3),\n    last3 = substr(show_name, nchar(show_name)-2, nchar(show_name))\n  )\n\n# A tibble: 18 × 3\n   show_name                      first3 last3\n   &lt;chr&gt;                          &lt;chr&gt;  &lt;chr&gt;\n 1 simpsons_Marge Simpson         sim    son  \n 2 muppets_Scooter                mup    ter  \n 3 muppets_Rowlf the Dog          mup    Dog  \n 4 muppets_Fozzie Bear            mup    ear  \n 5 simpsons_Abraham Simpson       sim    son  \n 6 muppets_Walter                 mup    ter  \n 7 muppets_Pepe the King Prawn    mup    awn  \n 8 muppets_Gonzo                  mup    nzo  \n 9 simpsons_Santa's Little Helper sim    per  \n10 simpsons_Snowball II/V         sim    I/V  \n11 simpsons_Maggie Simpson        sim    son  \n12 simpsons_Lisa Simpson          sim    son  \n13 simpsons_Bart Simpson          sim    son  \n14 muppets_Animal                 mup    mal  \n15 simpsons_Homer Simpson         sim    son  \n16 muppets_Miss Piggy             mup    ggy  \n17 muppets_Rizzo the Rat          mup    Rat  \n18 muppets_Kermit the Frog        mup    rog  \n\n\n\n\n\n\n\n\n\n\n\npaste() - quickly combine two character vectors\n\n\n\n\n\n\npaste(\"hello\",\"everyone\",sep=\" \")\n\n[1] \"hello everyone\"\n\n\nYou can also use it to collapse a vector into a single string:\n\npaste(mupsimp$show_name, collapse=\" \")\n\n[1] \"simpsons_Marge Simpson muppets_Scooter muppets_Rowlf the Dog muppets_Fozzie Bear simpsons_Abraham Simpson muppets_Walter muppets_Pepe the King Prawn muppets_Gonzo simpsons_Santa's Little Helper simpsons_Snowball II/V simpsons_Maggie Simpson simpsons_Lisa Simpson simpsons_Bart Simpson muppets_Animal simpsons_Homer Simpson muppets_Miss Piggy muppets_Rizzo the Rat muppets_Kermit the Frog\"\n\n\nand paste0() is a quick shortcut for using sep=\"\":\n\npaste0(\"hello\",\"everyone\")\n\n[1] \"helloeveryone\"\n\n\n\n\n\n\nQuestion 4\n\n\nProblem\nIf you look at what data was captured by the software to indicate which video was used in each trial, there is a lot of unnecessary data there. The number of the filename indicates how many blinks are in the video. This is the only bit of information we want.\n\nhead(setuplong$video)\n\n[1] \"/files/vids/blinks_1.mp4\"  \"/files/vids/blinsk_8.mp4\" \n[3] \"/files/vids/blinks_1.mp4\"  \"/files/vids/blinks_5.mp4\" \n[5] \"/files/vids/blinks_4.mp4\"  \"/files/vids/blinks_10.mp4\"\n\n\nTask\n\nIn your (now reshaped to long) blink_setup.csv data, make a new, or edit an existing column, which is a numeric variable containing the number of blinks presented in the video in each trial\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nthere are lots of different ways you could do this.\n\nyou can substitute out multiple different strings by separating them with the | symbol:\n\n\n  gsub(\"dog|cat\", \"horse\", \"I have a dog and a cat and the dogs name is Graham\")\n\n[1] \"I have a horse and a horse and the horses name is Graham\"\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nsetuplong &lt;- setuplong |&gt;\n  mutate(\n    nr_blinks = as.numeric(gsub(\"/files/vids/|blinks_|blinsk_|.mp4\",\"\",video))\n  )\n\nsetuplong\n\n# A tibble: 460 × 4\n   sub       trial_number video                     nr_blinks\n   &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;                         &lt;dbl&gt;\n 1 subject_1 trial_1      /files/vids/blinks_1.mp4          1\n 2 subject_1 trial_2      /files/vids/blinsk_8.mp4          8\n 3 subject_1 trial_3      /files/vids/blinks_1.mp4          1\n 4 subject_1 trial_4      /files/vids/blinks_5.mp4          5\n 5 subject_1 trial_5      /files/vids/blinks_4.mp4          4\n 6 subject_1 trial_6      /files/vids/blinks_10.mp4        10\n 7 subject_1 trial_7      /files/vids/blinks_1.mp4          1\n 8 subject_1 trial_8      /files/vids/blinks_5.mp4          5\n 9 subject_1 trial_9      /files/vids/blinks_6.mp4          6\n10 subject_1 trial_10     /files/vids/blinks_4.mp4          4\n# ℹ 450 more rows\n\n\n\n\n\n\n\n\n\n\nJoining/merging\nNow comes a fun bit.\nRecall that the research question is interested in the relationship between the number of times the speaker was seen to blink, and the time the participants spent looking at the distractor object (indicating perceived dishonesty).\nYou may have noticed that these variables are currently in different data-sets! The blink_setup.csv contains information about the numbers of blinks in the videos, and the blink_eyegaze.xlsx contains the data on the fixations.\nSolution: we need to join them together!\nNote that because both data-sets contain information on participant number and trial number, which uniquely identifies each observation, we can join them together matching on these variables!\nThere are lots of different ways to join data-sets, depending on whether we want to keep rows from one data-set or the other, or keep only those in both data-sets etc.\n\n\n\n\n\nCheck out the help documentation for them all using ?full_join.\n\n\n\n\n\nQuestion 5\n\n\nProblem\nVariables are in different data-sets.\nTask\n\nJoin the two data-sets (the reshaped-to-long blink_setup.csv data, and the blink_eyegaze.xlsx data) together, and store the joined data in a new object (you can use your own name, but the solutions will use the name blinks_full).\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe want to match the observations based on two columns which are present in each data-set, indicating which participant, and which trial.\n\nRemember that R doesn’t have your intelligence - it doesn’t know that in one data-set the variable is called e.g., trial_no and in the other it is called trial_number.\n\nAnother thing which R doesn’t know is that “subject_1” in setup data is the same participant as “1” in the eye gaze data. It needs to match the same symbols, and what is more, it needs the variables to be the same type (character, numeric, factor etc).\n\nyou might want to make use of the skills you learned for manipulating character strings.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. In this solution, let’s build up a sequence step by step. Work through the steps, adding lines of code each time. Between each step, run the code to quickly see what the output looks like at each step.\n\nFirst, let’s see how we can remove the “subject_” from “subject_1” etc..\n\n\nsetuplong |&gt;\n  mutate(\n    sub = gsub(\"subject_\",\"\",sub)\n  )\n\n\nBut we also want it to be numeric, to match the sub variable in the eyegaze data, so let’s edit it to:\n\n\nsetuplong |&gt;\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub))\n  )\n\n\nWe’ll also need to do the same for the trial_number variable, so let’s add that line too:\n\n\nsetuplong |&gt;\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  )\n\n\nAnd then, we’ll note that we need to have the same name for variables indicating trial number in both data-sets, so lets rename it:\n\n\nsetuplong |&gt;\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  ) |&gt;\n  rename(trial_no = trial_number)\n\n\nAnd now… add the join!\n\n\nsetuplong |&gt;\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  ) |&gt;\n  rename(trial_no = trial_number) |&gt;\n  full_join(x = _, y = eyedata)\n\nNOTE the solution has x = _, y = eyedata to make it clear that we are ‘piping in’ (using |&gt;) the thing coming out of the previous lines of code, and putting it where the _ is. .... |&gt; full_join(eyedata) would do the same.\nWe use full_join here because we want to keep all the data, but left_join would do the same. right_join would be slightly different, because there are 3 observations in the setup data (when reshaped to long, n = 460) which aren’t in the eye gaze data (n = 457). You can see which ones they are by using anti_join.\n\nFinally - we need to give the whole output a name to store it in our environment!\n\n\nblinks_full &lt;- \n  setuplong |&gt;\n  mutate(\n    sub = as.numeric(gsub(\"subject_\",\"\",sub)),\n    trial_number = as.numeric(gsub(\"trial_\",\"\",trial_number))\n  ) |&gt;\n  rename(trial_no = trial_number) |&gt;\n  full_join(x = _, y = eyedata)\n\n\n\n\n\n\n\n\n\nImpossible Values\nIt’s important to check that there are no values in the data which are impossible, given what you know about how the data was measured. This is where exploratory plots and descriptive statistics come in handy.\n\nhist(as.numeric(blinks_full$distractor_fix))\n\n\n\n\n\n\n\n\nIn some trials, participants spent less that 0ms fixating on the distractor object!?!?!?\nWe have a couple of options as to how to deal with them.\n\nDelete the entire row\nChange the specific entry/s in that variable to be NA (Not Applicable) - this has the benefit of keeping the rows should we consider those row to have a valid observation in other variables (for instance the rt - reaction time?)\n\nSome of the tools we learned in the Chapter 2 will come in handy here.\n\nQuestion 6\n\n\nProblem\nSome impossible values in the distractor_fix variable.\nTask\n- Assign the entries of the distractor_fix variable which are &lt; 0 to be NA.\n- Are there any other impossible values (or combinations of values) in the data?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWhile you’re there, why not convert any variables to the right type (numeric, factor, etc).\n\nWe might not have come across this before, but there is a really useful function called ifelse().\nPlay around with the below code to learn:\n\n\ntibble(x = 1:10) |&gt;\n  mutate(\n    new_variable = ifelse(x&gt;5,1,0),\n    another_new_variable = ifelse(x&gt;5,\"peppapig\",\"kermit\"),\n    morevariables = ifelse(another_new_variable == \"kermit\",\"kermit the frog\", another_new_variable)\n  )\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. Below we’ve taken similar steps for both the distractor_fix and rt variables. Neither can be &lt;0 or &gt;5000.\nHowever, we know that the distractor_fix variable has no entries &gt;5000 (because of the histogram above).\n\nblinks_full &lt;- \n  blinks_full |&gt;\n  mutate(\n    distractor_fix = as.numeric(distractor_fix),\n    distractor_fix = ifelse(distractor_fix&lt;0, NA, distractor_fix),\n    rt = ifelse(as.numeric(rt)&gt;5000 | as.numeric(rt)&lt;0, NA, as.numeric(rt))\n  )\n\nNote how two steps (making it numeric, and replacing values with NAs) are combined for the rt variable. Note also how we have specified that we replace with NAs entries which meet either on condition (&gt;5000) or (using |) another (&lt;0).\n\n\n\n\n\n\n\n\nMissing Data in R\nMissing data can be a big problem for statistics. For those of you thinking of taking Multivariate Statistics & Methodology in R next semester, you can look forward to discussions around this sort of issue.\nHere, however, we are simply going to discuss the practicalities of how to make R code work when some of your values are NAs.\nConsider:\n\nvec &lt;- c(1,2,3,4,NA)\nmean(vec)\n\n[1] NA\n\n\nThink about why this is: \\[\n\\text{mean(vec)} = \\frac{1+2+3+4+\\text{NA}}{5} = \\frac{\\text{??}}{5} = \\text{??}\n\\] There are numerous different ways that functions in R cope with missing values, but if you’re ever in doubt, try na.rm = TRUE. This will basically tell R to “remove the NAs before doing the calculation”.\n\nmean(vec, na.rm=T)\n\n[1] 2.5\n\n\nOther functions include na.omit(), which remove any row with has an NA anywhere in it:\n\ncomplete_data &lt;- na.omit(data)\n\n\n\n\n\n\nOutliers\nOutliers are the extreme - but plausible - values in variables. There is no one way to identify what is extreme enough to consider and outlier, nor is there one way to handle them.\nSome outliers could be considered important observations which we would not want to exclude. However, being an outlier can (but not always) result in an observation exerting too great an influence on our analysis.\n\nSome common approaches to identifying outliers:\n\nobservations which are \\(&gt; 3\\) (sometimes \\(&gt; 2.5\\)) standard deviations away from the mean.\nobservations greater than \\(1.5 \\times IQR\\) below the first quartile \\(Q_1\\) or above the third quartile \\(Q_3\\).\n\nSome common approaches to handling outliers:\n\nExclude now - for instance, set as NA\n“Winsorize” - set to a specified percentile. For example, all observations below the 5th percentile set to the 5th percentile, and all observations above the 95th percentile set to the 95th percentile\nExclude from analysis later, based on measures of influence (we’ll learn about this in future topics)\n\n\n\nQuestion 7\n\n\nMake a bloxplot of the distractor_fix variable. Does it look like there might be any outliers?\n\n\n\n\n\nSolution\n\n\n\nSolution 7. The last line of this is there just because I personally don’t like the default look of geom_boxplot where it is really wide, so this line changes the limits of the x-axis (and also removes the ticks).\n\nggplot(data = blinks_full, aes(y = distractor_fix)) +\n  geom_boxplot()+\n  scale_x_continuous(limits = c(-2,2), breaks = NULL)\n\n\n\n\n\n\n\n\nIt looks like there are possibly some outliers at the upper end of the distribution. One of them looks really quite anomalous!\n\n\n\n\n\n\n\n\nCustom Functions\n\nQuestion 8\n\n\nWriting your own function\nWe already saw some custom functions in the first week, where we made some called dice() and wdice().\nCan you write a function which, given a vector, returns TRUE if it is an outlier and FALSE if it is not, based on the criterion of being \\(&gt;3\\) sd away from the mean.\n\noutliers &lt;- function(obs){\n ...\n ...\n ...\n}\n\n\n\n\n\n\nSolution Part 1 - Working out the internal code\n\n\n\nSolution 8. Let’s do the calculation on a little vector, keeping it all outside of a function first:\n\n# a random vector (length = 20)\nvec &lt;- rnorm(n = 20, mean = 0, sd = 1)\n# pick two random entries and make them outliers (one in each direction)\nvec[3] &lt;- 150\nvec[16] &lt;- -150\nvec\n\n [1]   -1.01500872   -0.07963674  150.00000000   -0.81726793    0.77209084\n [6]   -0.16561194    0.97287443    1.71653398    0.25523700    0.36658112\n[11]    1.18078924    0.64319207    1.29532187    0.18791807    1.59120510\n[16] -150.00000000    0.83847112    0.15937013    0.62595440    0.63358473\n\n# deviations from each point to mean\nvec - mean(vec)\n\n [1]   -1.47308865   -0.53771668  149.54192006   -1.27534787    0.31401090\n [6]   -0.62369187    0.51479449    1.25845404   -0.20284293   -0.09149882\n[11]    0.72270930    0.18511213    0.83724193   -0.27016186    1.13312516\n[16] -150.45807994    0.38039118   -0.29870981    0.16787446    0.17550479\n\n# and three times the standard deviation\n3 * sd(vec)\n\n[1] 146.0152\n\n# but this won't work because some are below, rather than above the mean. \n(vec - mean(vec)) &gt; (3 * sd(vec))\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# instead we want the ABSOLUTE value \nabs(vec - mean(vec)) &gt; (3 * sd(vec))\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n\nSolution Part 2 - Writing it as a function\n\n\n\nSolution 9. Okay, now that we’ve worked out the code, we want to make this a function. The template function in the question had an input called obs:\n\noutliers &lt;- function(obs){\n\n}\n\nSo we would want to add our code to the function, but change it to use obs (which is whatever we give the function)\n\noutliers &lt;- function(obs){\n  abs(obs - mean(obs)) &gt; (3 * sd(obs))\n}\n\n\n\n\n\n\nSolution Part 3 - Testing the function\n\n\n\nSolution 10. we can test it on the vec object we created earlier.\n\noutliers(obs = vec)\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\nWe can use it to access and edit those entries:\n\nvec[outliers(vec)]\n\n[1]  150 -150\n\nvec[outliers(vec)] &lt;- NA\n\n\n\n\n\n\nExtra - Adding more arguments\n\n\n\nSolution 11. We could edit the function so that we can also vary how many standard deviations away we are wanting to identify!\n\noutliers &lt;- function(obs, x = 3){\n  abs(obs - mean(obs)) &gt; (x * sd(obs))\n}\n\nthe x = 3 means that the function will default to looking 3 standard deviations away, but if we wanted to use outliers(obs = vec, x = 2) we could identify those which are 2 away!\n:::\n\n\n\n\nQuestion 9\n\n\nLook through the solutions to the question above, and make sure that you are comfortable with how writing a function works.\nCan you edit the outliers() function you wrote to make it work with vectors which include NAs?\n\n\n\n\n\nSolution\n\n\n\nSolution 12. \n\noutliers &lt;- function(obs, x = 3){\n  abs(obs - mean(obs, na.rm=TRUE)) &gt; (x * sd(obs, na.rm=TRUE))\n}\n\n\n\n\n\nQuestion 10\n\n\nProblem\nPossible outliers in the distractor_fix variable.\nTask\n\nReplace any values of the distractor_fix variable which are \\(&gt;3\\) standard deviations from the mean with NA.\n\nMake a new boxplot of the variable\n\n\n\n\n\n\n\nIf you skipped questions 8 and 9\n\n\n\n\n\nIf you skipped the last couple of questions, then copy and run this code into your document.\nIt will give you a function which takes a vector and returns TRUEs and FALSEs based on whether each entry is greater than 3 standard deviations from the mean.\n\noutliers &lt;- function(obs, x = 3){\n  abs(obs - mean(obs, na.rm=TRUE)) &gt; (x * sd(obs, na.rm=TRUE))\n}\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 13. \n\nblinks_full$distractor_fix[outliers(blinks_full$distractor_fix)]&lt;- NA\n\nggplot(data = blinks_full, aes(y = distractor_fix)) +\n  geom_boxplot()+\n  scale_x_continuous(limits = c(-2,2), breaks = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary plots\nSo where are we now? We have a dataset that has one row per trial per subject, and it contains information on the number of blinks, and on the time spent looking at the distractor:\n\nhead(blinks_full)\n\n# A tibble: 6 × 6\n    sub trial_no video                     nr_blinks distractor_fix    rt\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1     1        1 /files/vids/blinks_1.mp4          1           504.  2812\n2     1        2 /files/vids/blinsk_8.mp4          8            NA   2974\n3     1        3 /files/vids/blinks_1.mp4          1           707.  2257\n4     1        4 /files/vids/blinks_5.mp4          5            NA     NA\n5     1        5 /files/vids/blinks_4.mp4          4           223.  4546\n6     1        6 /files/vids/blinks_10.mp4        10            NA     NA\n\n\nThinking way back to the top of this page, we recall that our research question is concerned with whether perceived lying (as implied by more looking at the distractor) increases with the number of blinks seen.\nSo we might start by plotting those two variables:\n\nggplot(blinks_full, aes(x=nr_blinks, y = distractor_fix))+\n  geom_point()\n\n\n\n\n\n\n\n\nIt’s hard to see a pattern clearly here, so we’re going to introduce a handy part of ggplot. The stat_summary() function allows us to plot summarised data (i.e. the mean of the y-variable), rather than the data itself. If we choose to plot it as a “pointrange”, it gives us the mean and standard error for \\(y\\) across each level of \\(x\\):\n\nggplot(blinks_full, aes(x = nr_blinks, y = distractor_fix)) +\n  stat_summary(geom = \"pointrange\")\n\n\n\n\n\n\n\n\nThis is the same as doing some grouping and summarising first, and then giving those summarised values to the plot:\n\nblinks_full |&gt;\n  group_by(nr_blinks) |&gt;\n  summarise(mean = mean(distractor_fix, na.rm=TRUE),\n            se = sd(distractor_fix, na.rm=TRUE)/sqrt(n()),\n            lwr = mean-se,\n            upr = mean+se\n            ) |&gt;\n  ggplot(aes(x = nr_blinks, y = mean, ymin = lwr, ymax = upr)) +\n  geom_pointrange()\n\n\n\n\n\n\n\n\n\n\nBuild a model!\nWe’re now finally getting to the analysis. As we said earlier, this can sometimes be very straightforward in comparison to the amount of effort involved in cleaning data.\nRecall that we’re interested in whether the perception of whether or not a speaker is lying about the location of some hidden treasure (as measured by the pattern of eye fixations towards the object not referred to by the speaker) is influenced by the number of times the speaker is seen to blink while producing the utterance.\n\nQuestion 11\n\n\nFit the linear model specified below to the data using the lm() function and store the output in the environment as an object named blinks_mdl.\n\\[\n\\begin{align}\n& \\text{Fixation time to distractor} = b_0 + b_1 \\ \\text{Number of blinks} + \\epsilon \\\\\n\\quad \\\\\n& \\text{where} \\quad \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n\\]\n\n\n\n\n\nSolution\n\n\n\nSolution 14. \n\nblinks_mdl &lt;- lm(distractor_fix ~ 1 + nr_blinks, data=blinks_full)\n\nsummary(blinks_mdl)\n\n\nCall:\nlm(formula = distractor_fix ~ 1 + nr_blinks, data = blinks_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-944.47 -253.55    5.58  217.74 1033.81 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  517.296     36.076   14.34   &lt;2e-16 ***\nnr_blinks     64.128      5.808   11.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 355.1 on 447 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.2143,    Adjusted R-squared:  0.2126 \nF-statistic: 121.9 on 1 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nQuestion 12\n\n\nThe \\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\) bit of our model is an assumption we have to make. It concerns the errors (the deviations from observations to our line). Our model assumes that these are normally distributed and centered on 0. We can plot the distribution of residuals to check how well our assumption holds:\n\nhist(residuals(blinks_mdl))\n\n\n\n\n\n\n\n\nHowever, we also make the assumption that the errors are independent - i.e. they are not related to one another.\nFor us, this is not the case, and so we should not be using this simple linear regression here.\nIn what way are we violating the assumption of independence?\n\n\n\n\n\nSolution\n\n\n\nSolution 15. The dataset to which we are fitting our model does not contain independent observations. We have multiple observations from each participant. i.e. Subject 1 makes up 20 rows, and subject 2 makes up 20 rows.\nIndependence of observations is actually an assumption that is relied upon by all the tests that we cover in this course. So we can’t actually perform an analysis on this data as it is!! We will learn about how to deal with this sort of study design next semester, in the Multivariate Statistics & Methodoligy using R (MSMR) course.\nOne option open to us now is to simply “aggregate up”, so that we remove the dependence from our rows in our dataset. However, this means reducing the number of rows. This is sub-optimal (and we’ll see why next semester!), but we can calculate the average distractor_fix for each level of nr_blinks, and then use those in our model.\nWe can do this the same way as we saw with our summary plots - with group_by() and summarise()!\n\nblinks_agg &lt;- blinks_full |&gt;\n  group_by(nr_blinks) |&gt;\n  summarise(\n    meanDF = mean(distractor_fix, na.rm=TRUE)\n  )\nhead(blinks_agg)\n\n# A tibble: 6 × 2\n  nr_blinks meanDF\n      &lt;dbl&gt;  &lt;dbl&gt;\n1         1   648.\n2         2   607.\n3         3   695.\n4         4   720.\n5         5   789.\n6         6   898.\n\n\nOne obvious disadvantage here is that we are going from loads of datapoints to only 10! Furthermore, because of some of the missing data, each of these 10 datapoints is estimated from a slightly different number of trials, and from a slightly different set of participants. Our final model is really just a very simple line fitted to 10 datapoints, but this is pretty far away from modelling how these values actually came to arise in the real world!\n\nggplot(blinks_agg, aes(x = nr_blinks, y = meanDF)) +\n  geom_point() +\n  geom_smooth(method=lm)\n\n\n\n\n\n\n\n\nBut we can still get some useful linear approximation of the pattern - avg look at the distractor increase by 64ms for every additional blink in the video!\n\nmod &lt;- lm(meanDF ~ nr_blinks, data = blinks_agg)\n\nsummary(mod)\n\n\nCall:\nlm(formula = meanDF ~ nr_blinks, data = blinks_agg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.780 -45.383  -8.233  35.998  78.532 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  514.501     36.354   14.15 6.04e-07 ***\nnr_blinks     64.584      5.859   11.02 4.08e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53.22 on 8 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9382,    Adjusted R-squared:  0.9305 \nF-statistic: 121.5 on 1 and 8 DF,  p-value: 4.083e-06"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Models",
    "section": "",
    "text": "These readings and walkthroughs take you from complete basics of R and statistics up to the linear model, one of the most foundational methods in statistics. We will see how we can visualise and describe patterns in sample data, before discussing how we can infer from those samples to talk about the broader population of interest. Following examples of the more common basic statistical tests to describe relationships between two variables, we then move on to the main course of the linear model, before finishing up with a taster of how this method can be extended to other scenarios.\nReadings and walkthroughs are presented with accompanying R code."
  },
  {
    "objectID": "06_inference2.html",
    "href": "06_inference2.html",
    "title": "6: Practical Inference",
    "section": "",
    "text": "This reading:\n\nHow does hypothesis testing work in practice?\n\nHow do we do all this in R?\n\nspoiler: it’s easier than you think\n\nWhat are some basic hypothesis tests that we can conduct?\n\nTests of a single continuous variable\nTests of the relationship between a continuous variable and a binary categorical variable\nIn Chapter 5 we learned about the logic of Null Hypothesis Significance Testing (NHST), allowing us to draw perform inferentials tests about parameters in the population, based on statistics computed on the sample that we have collected.\nWhile in practice NHST follows the logic described above, there is something important that we have been sweeping under the carpet.\nIn our estimation of the standard error we have used the formula that includes \\(\\sigma\\), which refers to the population standard deviation. However, we never know this value (because we don’t have data for the population), so we have been using the sample standard deviation \\(s\\) instead. This is an approximation, and might be okay when we have a very large \\(n\\) (meaning \\(s\\) provides an accurate estimate of \\(\\sigma\\)), but in practice this is not always feasible. \\[\nSE = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}\n\\]",
    "crumbs": [
      "6: Practical Inference"
    ]
  },
  {
    "objectID": "06_inference2.html#one-sample-t-test",
    "href": "06_inference2.html#one-sample-t-test",
    "title": "6: Practical Inference",
    "section": "One sample t-test",
    "text": "One sample t-test\n\nPurpose\nThe one sample t-test is what we have already seen above. We use it to test whether the mean is different from/greater than/less than some hypothesised value.\n\nExamples:\n\nIs the mean age of USMR students different from 20?\nIs the mean IQ different from 100?\n\nDo people read more than 250 words per minute?\n\n\nAssumptions:\n\nThe data are continuous (not discrete)\nThe data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)\nThe data are normally distributed (can be relaxed somewhat if the sample size is “large enough” (rule-of-thumb n = 30) and the data are not strongly skewed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Do people read more than 250 words per minute?\n\nFifty participants were recruited and tasked with reading a passage of text that was 2000 words long. Their reading times (in words per minute) was recorded, and these are accessible at https://uoepsy.github.io/data/usmr_tread.csv.\n\nwpmtime &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_tread.csv\")\nhead(wpmtime)\n\n# A tibble: 6 × 2\n  id      wpm\n  &lt;chr&gt; &lt;dbl&gt;\n1 ppt_1   307\n2 ppt_2   265\n3 ppt_3   205\n4 ppt_4   300\n5 ppt_5   207\n6 ppt_6   300\n\n\n\n\n\n\n\n\n\n\n\nDescriptives and Assumptions\n\n\n\n\n\nBelow are some quick descriptives.\n\nmean(wpmtime$wpm)\n\n[1] 258.36\n\nsd(wpmtime$wpm)\n\n[1] 32.08646\n\nhist(wpmtime$wpm)\n\n\n\n\n\n\n\n\nOur histogram looks roughly normally distributed. We can (if we like), test this using the Shapiro-Wilk test.\n\nshapiro.test(wpmtime$wpm)\n\n\n    Shapiro-Wilk normality test\n\ndata:  wpmtime$wpm\nW = 0.9636, p-value = 0.1258\n\n\nThe \\(p\\)-value of 0.126 is \\(&gt;.05\\), so we fail to reject the null hypothesis that the data come from a normal distribution. In other words, we have no reason to consider our assumption to be violated.\n\n\n\n\n\n\n\n\n\nQuick and easy t.test()\n\n\n\n\n\nPaying careful attention to the research question (“Do people read more than 250 words per minute?”), our null hypothesis here is that reading time is \\(\\leq 250\\) words per minute (wpm), and our alternative hypothesis is that it is \\(&gt;250\\) wpm.\nThis means that we will reject our null hypothesis if we get a test statistic indicating the mean is \\(&gt;250\\). We won’t reject it if the mean is \\(&lt;250\\).\nWe specify the direction of the alternative in the t.test() function:\n\nt.test(wpmtime$wpm, mu = 250, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  wpmtime$wpm\nt = 1.8423, df = 49, p-value = 0.03574\nalternative hypothesis: true mean is greater than 250\n95 percent confidence interval:\n 250.7523      Inf\nsample estimates:\nmean of x \n   258.36 \n\n\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nOur test-statistic is calculated as \\[\nt =  \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\nThere’s a lot of brackets in the code below, so go through it piece by piece if you are unsure of how it matches to the formula above\n\n(mean(wpmtime$wpm) - 250 ) / (sd(wpmtime$wpm) / sqrt(nrow(wpmtime)))\n\n[1] 1.842338\n\n\nThe test we are performing is against the null hypothesis that \\(\\mu_0 \\leq 250\\). So we will only reject the null hypothesis if we get a test statistic indicating the mean is \\(&gt;250\\). This means that our p-value will be just the one tail of the \\(t\\)-distribution:\n\npt(1.842338, df = 49, lower.tail = FALSE)\n\n[1] 0.0357404\n\n\n\n\n\n\n\n\n\n\n\nWrite-up\n\n\n\n\n\n\nA one-sample t-test was conducted in order to determine if the average reading time was significantly (\\(\\alpha=.05\\)) higher than 250 words per minute (wpm).\nThe sample of 50 participants read on average at 258 words per minute (Mean=258, SD=32). This was significantly above 250 (\\(t(49)=1.84, p = .036\\), one-tailed).",
    "crumbs": [
      "6: Practical Inference"
    ]
  },
  {
    "objectID": "06_inference2.html#two-sample-t-test",
    "href": "06_inference2.html#two-sample-t-test",
    "title": "6: Practical Inference",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nPurpose\nThe two sample t-test is used to test whether the mean of one group is different from/greater than/less than the mean of another.\n\nExamples:\n\nIs the mean age of cat people different from the mean age of dog people?\nDo people who identify as “morning people” have a higher average rating of sleep quality than those who identify as “evening people”?\nIs the average reaction time different between people who do and don’t drink caffeinated drinks?\n\n\nAssumptions:\n\nThe data are continuous (not discrete)\nThe data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)\nThe data are normally distributed for each group (can be relaxed somewhat if the sample size is “large enough” (rule-of-thumb n = 30) and the data are not strongly skewed)\nThe variance is equal across groups*.\n\n*We can relax this assumption by using an adjusted test called the “Welch \\(t\\)-test”, which calculates the standard error slightly differently, and estimates the degrees of freedom differently too. This is actually the default in R, and we change this easily in R using t.test(...., var.equal = FALSE/TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Is the average reaction time different between people who do and don’t drink caffeinated drinks?\n\nOne hundred participants were recruited and completed a simple reaction time task. They were also surveyed on whether they regularly drank caffeine in any form. The data are accessible at https://uoepsy.github.io/data/usmr_tcaff.csv.\n\ntcaff &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_tcaff.csv\")\nhead(tcaff)\n\n# A tibble: 6 × 2\n     rt caff \n  &lt;dbl&gt; &lt;chr&gt;\n1  482. yes  \n2  389. yes  \n3  484. no   \n4  601. no   \n5  409. yes  \n6  368. no   \n\n\n\n\n\n\n\n\n\n\n\nDescriptives and Assumptions\n\n\n\n\n\nFirst some quick descriptive stats. We’ll calculate the mean and standard deviation of reaction times for each group:\n\ntcaff |&gt; \n  group_by(caff) |&gt;\n  summarise(\n    m = mean(rt),\n    s = sd(rt)\n  )\n\n# A tibble: 2 × 3\n  caff      m     s\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 no     408.  88.9\n2 yes    465. 109. \n\n\nAnd we can make a plot here:\n\nggplot(tcaff, aes(x = rt)) +\n  geom_histogram() + \n  facet_wrap(~caff)\n\n\n\n\n\n\n\n\nThe data look fairly close to normally distributed for each group here. One thing to note is that the variances look like they may be different between the two groups. The caffeine drinkers’ reaction time’s have a standard deviation of 109ms, and the non-caffeine drinkers have an sd of only 89ms.\nAs before, we can (if we are so inclined) rely on specific tests of these assumptions, such as using shapiro.test() for the distribution in each group separately.\nSimilarly, the var.test() function performs a test to compare two variances (the null hypothesis of this test being that they are equal). However, it is more common to simply perform the Welch test straight away, and thus not have to worry about this assumption.\n\n\n\n\n\n\n\n\n\nQuick and easy t.test()\n\n\n\n\n\nWe can give R the two sets of data in two ways. Either by extracting the relevant entries:\n\nt.test(x = tcaff$rt[tcaff$caff==\"no\"], \n       y = tcaff$rt[tcaff$caff==\"yes\"])\n\nOr using the formula notation, with the ~ (“tilde”) symbol. In R, you can interpret y ~ x as “y is modeled as a function of x”. By splitting the numeric values (rt variable) by the categories of the caff variable, we can conduct a \\(t\\)-test using:\n\nt.test(rt ~ caff, data = tcaff)\n\n\n    Welch Two Sample t-test\n\ndata:  rt by caff\nt = -2.8497, df = 93.971, p-value = 0.005377\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -96.20205 -17.19423\nsample estimates:\n mean in group no mean in group yes \n         408.0505          464.7486 \n\n\nNote that the default behaviour of t.test() is to perform the Welch test - so we don’t have to assume equal variances. If we want to override this, we can use t.test(rt ~ caff, data = tcaff, var.equal = TRUE).\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nOur test statistic here is:4\n\\[\n\\begin{align}\n& t =  \\frac{\\bar x_1 - \\bar x_2}{SE}\\\\\n\\ \\\\\n& \\text{where:} \\\\\n& \\bar x_1 : \\text{sample mean group 1} \\\\\n& \\bar x_2 : \\text{sample mean group 2} \\\\\n& SE : \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\\\\n& s_1 : \\text{sample standard deviation of group 1} \\\\\n& s_2 : \\text{sample standard deviation of group 2} \\\\\n& n_1 : \\text{sample size group 1} \\\\\n& n_2 : \\text{sample size group 2} \\\\\n\\end{align}\n\\]\nWe can calculate each part:\n\ntcaff |&gt;\n  group_by(caff) |&gt;\n  summarise(\n    xbar = mean(rt),\n    s = sd(rt),\n    s2 = var(rt),\n    n = n()\n  )\n\n# A tibble: 2 × 5\n  caff   xbar     s     s2     n\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 no     408.  88.9  7906.    40\n2 yes    465. 109.  11892.    60\n\n\nplugging these bits in gives us: \\[\n\\begin{align}\nSE & = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} = \\sqrt{\\frac{7906}{40} + \\frac{11892}{60}} = \\sqrt{395.85} \\\\\n\\qquad \\\\\n& = 19.9\n\\end{align}\n\\] and \\[\n\\begin{align}\nt & =  \\frac{\\bar x_1 - \\bar x_2}{SE} = \\frac{408.1 - 464.8}{19.9} \\\\\n\\qquad \\\\\n& = -2.849 \\\\\n\\end{align}\n\\]\nOur \\(p\\)-value is determined against a \\(t\\)-distribution with a specific number of degrees of freedom. We are estimating two means here, the standard two-sample t-test uses \\(df = n-2\\). However, the Welch t-test, which we performed quickly with t.test(), where we don’t assume equal variances, makes the calculation of the degrees of freedom much more complicated.5\nUsing the same degrees of freedom as was used in the quick use of t.test() above, we get out our same p-value (or thereabouts - we have some rounding error):\n\n2*pt(abs(-2.849), df = 93.971, lower.tail = FALSE)\n\n[1] 0.005388563\n\n\n\n\n\n\n\n\n\n\n\nWrite-up\n\n\n\n\n\n\nA Welch two sample t-test was used to assess whether the mean reaction time of people who regularly drink caffeine (\\(n = 60\\)) was different to that of people who do not (\\(n=40\\)). There was a significant difference in average reaction time between the caffeine (Mean=465; SD=109) and non-caffeine (Mean=408; SD=89) groups (\\(t(93.97)=-2.85, p = 0.005\\), two-tailed). Therefore, we reject the null hypothesis that there is no difference in reaction times between caffeine drinkers and non-caffeine drinkers.\n\n\nCode\nggplot(tcaff, aes(x = caff, y = rt)) +\n  geom_boxplot()+\n  labs(x=\"drinks caffeine\",y=\"reaction time (ms)\")",
    "crumbs": [
      "6: Practical Inference"
    ]
  },
  {
    "objectID": "06_inference2.html#paired-sample-t-test",
    "href": "06_inference2.html#paired-sample-t-test",
    "title": "6: Practical Inference",
    "section": "Paired sample t-test",
    "text": "Paired sample t-test\n\nPurpose\nThe paired sample t-test is used to test whether the mean difference between two sets of paired observations is different from 0.\n\nExamples:\n\nIs the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?\n\nAre scores on test 1 different on average from scores on test 2 (with participants completing both tests).\n\n\nAssumptions:\n\nThe data are continuous (not discrete)\nThe differences are independent (i.e. the value of a the difference for one pair does not depend on the values of another pair in any way)\nThe differences are normally distributed OR the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Is the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?\n\nAddenbrooke’s Cognitive Examination-III (ACE-III) is a brief cognitive test that assesses five cognitive domains: attention, memory, verbal fluency, language and visuospatial abilities. The total score is 100 with higher scores indicating better cognitive functioning. A research project is examining changes in cognitive functioning with age, and administers the ACE-III to a set of participants at age 60, then again at age 70. The data is accessible at https://uoepsy.github.io/data/usmr_tcaff.csv.\n\nacedata &lt;- read_csv(\"https://uoepsy.github.io/data/acedata.csv\")\nhead(acedata)\n\n# A tibble: 6 × 3\n  participant ace_60 ace_70\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 sub1            93     85\n2 sub2            95     92\n3 sub3            93     90\n4 sub4            93     95\n5 sub5            96     88\n6 sub6            91     85\n\n\n\n\n\n\n\n\n\n\n\nThe paired t test is the one sample t test in disguise\n\n\n\n\n\nWe can either perform this with the data exactly as it is:\n\nt.test(x = acedata$ace_60, y = acedata$ace_70, \n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  acedata$ace_60 and acedata$ace_70\nt = 2.2542, df = 24, p-value = 0.03359\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.2093364 4.7506636\nsample estimates:\nmean difference \n           2.48 \n\n\nOr we can compute the differences, and perform a one sample test on the mean of those differences being different from 0.\nIt’s just the same result:\n\nacedata &lt;- acedata |&gt;\n  mutate(diff_score = ace_60 - ace_70)\n\nt.test(acedata$diff_score, mu = 0)\n\n\n    One Sample t-test\n\ndata:  acedata$diff_score\nt = 2.2542, df = 24, p-value = 0.03359\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.2093364 4.7506636\nsample estimates:\nmean of x \n     2.48",
    "crumbs": [
      "6: Practical Inference"
    ]
  },
  {
    "objectID": "06_inference2.html#footnotes",
    "href": "06_inference2.html#footnotes",
    "title": "6: Practical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that confidence intervals provide a range of plausible values for the population mean. In this case, zero is a plausible value.↩︎\nThis is because with smaller samples we have less certainty in the estimate of the population standard deviation, and our estimates of mean and standard deviation are more dependent on one another. The bottom part of \\(\\frac{\\bar x - \\mu}{SE}\\) has a greater chance of being smaller than the top part, meaning that our resulting our test statistics will tend to be slightly bigger. To better represent this greater chance of seeing bigger test statistics from small samples, our \\(t\\)-distributions have heavier tails.↩︎\nThis is because, practically speaking, what we really need in order to make useful, defensible conclusions, is not that the population itself is normally distributed, but that the sampling distribution of the statistic is close enough to the \\(t\\)-distribution. This can often be the case when we have a large sample without much skew.↩︎\nThe formula here is for the Welch test.\nFor a standard two sample t-test that assumes equal variances, we first calculate the “pooled standard deviation” - \\(s_p = \\sqrt\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\\).\nWe then use this to calculate the standard error - \\(SE_{(\\bar{x}_1 - \\bar{x}_2)} = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)↩︎\nIf you really want it, the formula is: \\(\\text{df}=\\frac{\\left(\\dfrac{s_1^2}{n_1}+\\dfrac{s_2^2}{n_2}\\right)^2}{\\dfrac{\\left(\\dfrac{s_1^2}{n_1}\\right)^2}{n_1-1}+\\dfrac{\\left(\\dfrac{s_2^2}{n_2}\\right)^2}{n_2-1}}\\)↩︎",
    "crumbs": [
      "6: Practical Inference"
    ]
  },
  {
    "objectID": "04_sampling.html",
    "href": "04_sampling.html",
    "title": "4: Curves & Sampling",
    "section": "",
    "text": "This reading:\n\nWhat are probability distributions and why are they relevant?\n\nHow does using a sample to approximate a population lead to uncertainty?\nHow can we quantify uncertainty due to sampling?",
    "crumbs": [
      "4: Curves & Sampling"
    ]
  },
  {
    "objectID": "04_sampling.html#the-standard-normal-distribution",
    "href": "04_sampling.html#the-standard-normal-distribution",
    "title": "4: Curves & Sampling",
    "section": "The Standard Normal Distribution",
    "text": "The Standard Normal Distribution\nNote that if we translate our “IQ &gt;120” to being in terms of standard deviations - \\(\\frac{120 - 100}{15} = 1\\frac{1}{3}\\) - then we can perform the same computations as we have done above, but comparing against against a normal distribution with mean of 0 and standard deviation of 1 (which are the defaults for the pnorm() function):\n\npnorm((120-100)/15, lower.tail = FALSE)\n\n[1] 0.09121122\n\n\n\n\n\n\n\n\n\n\nFigure 7: pnorm() with the ‘standard normal distribution’: the normal distribution with mean = 0 and sd = 1\n\n\n\n\n\nWhat we’re doing here is re-expressing the observed distribution into one which has mean of 0 and standard deviation of 1 - we are standardising them. This idea will become incredibly useful. For one thing it makes comparisons possible, for example, consider the two statements below:\n\n“I am 15 IQ points higher than average, and 24cm taller than average”\n“I am 1 standard deviation above the average IQ, and 2 standard deviations above average height”\n\nThe standard normal distribution - the normal distribution with mean = 0, sd = 1, is going to be seen a lot more frequently.",
    "crumbs": [
      "4: Curves & Sampling"
    ]
  },
  {
    "objectID": "04_sampling.html#the-relevance-of-the-normal-distribution",
    "href": "04_sampling.html#the-relevance-of-the-normal-distribution",
    "title": "4: Curves & Sampling",
    "section": "The relevance of the normal distribution?",
    "text": "The relevance of the normal distribution?\nWe can motivate the relevance of the normal distribution in various ways. For instance, when we take a measurement of something such as the length of a stick, then we always have a bit of imprecision - our measurements will vary a bit. Assuming that our measurement tool is unbiased and this imprecision is purely random, we would expect the measurements of the stick to be ‘normally distributed’ around the true length of the stick (Figure 8).\n\n\n\n\n\n\n\n\nFigure 8: Snapshots from 21/22 lecture slides on measurement\n\n\n\n\n\nIn this way, the normal distribution captures the idea of random deviations around a central point. As we will see below, this becomes extremely relevant for statistics because we tend to collect data on a random sample of people, and all of the samples we could have taken will randomly deviate a bit in how well they represents the bigger group that we take them from.",
    "crumbs": [
      "4: Curves & Sampling"
    ]
  },
  {
    "objectID": "04_sampling.html#footnotes",
    "href": "04_sampling.html#footnotes",
    "title": "4: Curves & Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nremember that standard deviation is \\(\\sqrt{\\text{variance}}\\)↩︎\nOften in neuropsychological testing, a set of “normative values” are provided in order to define “what is expected” (sometimes in reference to a specific population).↩︎\nand the statements may not hold for all individuals - for certain people, some drugs just won’t work! but what is important for a healthcare system deciding on whether or not to purchase supplies of a drug is the average treatment effect.↩︎\nIf you’re working along with this, yours will be different, because it’s random!↩︎\nImagine a bag full of coloured marbles. If we sample with replacement, then we take a marble out, record its colour, and put it back. Then we take a marble out, record its colour, and put it back. And so on. This means we might get the same marble more than once.↩︎\nusing the formula \\(\\frac{\\sigma}{\\sqrt{n}}\\) for standard error↩︎",
    "crumbs": [
      "4: Curves & Sampling"
    ]
  },
  {
    "objectID": "02_data.html",
    "href": "02_data.html",
    "title": "2: More R - Basic Data Skills",
    "section": "",
    "text": "This reading:\n\nHow does R store data?\n\nWhat can R do with data?\n\nHow can we use R to access and manipulate data?",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#accessing-subsets-of-data",
    "href": "02_data.html#accessing-subsets-of-data",
    "title": "2: More R - Basic Data Skills",
    "section": "Accessing subsets of data",
    "text": "Accessing subsets of data\nWhat if we want to extract certain subsections of our dataset, such as specific observational units or variables? This is where we learn about two important bits of R code used to access parts of data - the dollar sign $, and the square brackets [].\n\nThe dollar sign $\nThe dollar sign allows us to extract a specific variable from a dataframe. For instance, we can pull out the variable named “eye_color” in the data, by using $eye_color after the name that we gave our dataframe.\nRemember that each variable in a dataframe is a vector (a set of values). Once extracted, we will have a vector and not a dataframe.\n\nstarwars2$eye_color\n\n [1] \"blue\"          \"yellow\"        \"red\"           \"yellow\"       \n [5] \"brown\"         \"blue\"          \"blue\"          \"red\"          \n [9] \"brown\"         \"blue-gray\"     \"blue\"          \"blue\"         \n[13] \"blue\"          \"brown\"         \"black\"         \"orange\"       \n[17] \"hazel\"         \"blue\"          \"yellow\"        \"brown\"        \n[21] \"red\"           \"brown\"         \"blue\"          \"orange\"       \n[25] \"blue\"          \"brown\"         \"black\"         \"red\"          \n[29] \"blue\"          \"orange\"        \"orange\"        \"orange\"       \n[33] \"yellow\"        \"orange\"        NA              \"brown\"        \n[37] \"yellow\"        \"pink\"          \"hazel\"         \"yellow\"       \n[41] \"black\"         \"orange\"        \"brown\"         \"yellow\"       \n[45] \"black\"         \"brown\"         \"blue\"          \"orange\"       \n[49] \"yellow\"        \"black\"         \"blue\"          \"brown\"        \n[53] \"brown\"         \"blue\"          \"yellow\"        \"blue\"         \n[57] \"blue\"          \"brown\"         \"brown\"         \"brown\"        \n[61] \"brown\"         \"yellow\"        \"yellow\"        \"black\"        \n[65] \"black\"         \"blue\"          \"unknown\"       \"unknown\"      \n[69] \"gold\"          \"black\"         \"green, yellow\" \"blue\"         \n[73] \"brown\"         \"black\"         NA             \n\n\n\n\nThe square brackets []\nSquare brackets are used to do what is known as indexing (finding specific entries in your data).\nWe can retrieve bits of data by identifying the \\(i^{th}\\) entry(s) inside the square brackets, for instance:\n\n# assign the numbers 10, 20 ... 100 to the name \"somevalues\"\nsomevalues &lt;- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n\n# pull out the 3rd entry\nsomevalues[3]\n\n[1] 30\n\n\nIn the above example, we have a vector (a single sequence of values), and so we can retrieve entries with the syntax:\n\nvector[entry]\n\n In a dataframe we have an extra dimension - we have rows and columns. Using square brackets with a dataframe needs us to specify both:\n\n\ndataframe[rows, columns]\n\n\nLet’s look at some examples:\n\n\nExamples of Indexing\n\nSpecifying row number and column number:\n\n\n# first row, fourth column\nstarwars2[1, 4]\n# tenth row, first column\nstarwars2[10, 1]\n\n\nIf we leave either rows or columns blank, then we will get out all of them:\n\n\n# tenth row, all columns\nstarwars2[10, ]\n# all rows, 2nd column\nstarwars2[ , 2]\n\n\nThere are is another way to identify column - we can use the name in quotation marks:\n\n\n# first row, \"species\" column\nstarwars2[1, \"species\"]\n\n\nWe can also ask for multiple rows, or multiple columns, or both! To do that, we use c():\n\n\n# the 1st AND the 6th rows, and the 1st AND 3rd columns\nstarwars2[c(1,6), c(1,3)] \n\n\nAnd we can specify a sequence using the colon, from:to: 2\n\n\n# FROM the 1st TO the 6th row, all columns\nstarwars2[1:6, ] \n\n\nWe can even use the two accessors in combination:3\n\n\n# extract the variable called \"name\" and show the 20th entry\nstarwars2$name[20]  \n\n\n\nShow me the output\n\nSpecifying row number and column number:\n\n\n# first row, fourth column\nstarwars2[1, 4]\n\n[1] \"blue\"\n\n# tenth row, first column\nstarwars2[10, 1]\n\n[1] \"Obi-Wan Kenobi\"\n\n\n\nIf we leave either rows or columns blank, then we will get out all of them:\n\n\n# tenth row, all columns\nstarwars2[10, ]\n\n             name height    hair_color eye_color homeworld species\n10 Obi-Wan Kenobi    182 auburn, white blue-gray   Stewjon   Human\n\n# all rows, 2nd column\nstarwars2[ , 2]\n\n [1] 172 167  96 202 150 178 165  97 183 182 188 180 228 180 173 175 170 180 170\n[20] 183 190 177 175 180 150  88 160 191 170 196 224 206 137 112 170 163 175 180\n[39] 178  94 122 163 188 198 196 171 184 188 264 188 196 185 157 183 183 170 166\n[58] 165 193 191 183 168 198 229 213 167  79 193 191 178 216 234 188 206 180\n\n\n\nThere are is another way to identify column - we can use the name in quotation marks:\n\n\n# first row, \"species\" column\nstarwars2[1, \"species\"]\n\n[1] \"Human\"\n\n\n\nWe can also ask for multiple rows, or multiple columns, or both! To do that, we use c():\n\n\n# the 1st AND the 6th rows, and the 1st AND 3rd columns\nstarwars2[c(1,6), c(1,3)] \n\n            name  hair_color\n1 Luke Skywalker       blond\n6      Owen Lars brown, grey\n\n\n\nAnd we can specify a sequence using the colon, from:to:\n\n\n# FROM the 1st TO the 6th row, all columns\nstarwars2[1:6, ] \n\n            name height  hair_color eye_color homeworld species\n1 Luke Skywalker    172       blond      blue  Tatooine   Human\n2          C-3PO    167        &lt;NA&gt;    yellow  Tatooine   Human\n3          R2-D2     96        &lt;NA&gt;       red     Naboo   Droid\n4    Darth Vader    202        none    yellow  Tatooine   Human\n5    Leia Organa    150       brown     brown  Alderaan   Human\n6      Owen Lars    178 brown, grey      blue  Tatooine   Human\n\n\n\nWe can even use the two accessors in combination:\n\n\n# extract the variable called \"name\" and show the 20th entry\nstarwars2$name[20]  \n\n[1] \"Boba Fett\"\n\n\n\n\n\nThe dollar sign $\nUsed to extract a variable from a dataframe:\n\ndataframe$variable\n\nThe square brackets []\nUsed to extract parts of an R object by identifying rows and/or columns, or more generally, “entries”. Left blank will return all.\n\nvector[entries]\ndataframe[rows, columns]",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#accessing-by-a-condition",
    "href": "02_data.html#accessing-by-a-condition",
    "title": "2: More R - Basic Data Skills",
    "section": "Accessing by a condition",
    "text": "Accessing by a condition\nWe can also do something really useful, which is to access all the entries in the data for which a specific condition is true.\nLet’s take a simple example to start:\n\nsomevalues &lt;- c(10, 10, 0, 20, 15, 40, 10, 40, 50, 35)\n\nTo only select values which are greater than 20, we can use:\n\nsomevalues[somevalues &gt; 20]\n\n[1] 40 40 50 35\n\n\n\nUnpacking: somevalues[somevalues &gt; 20]\n First, let’s look at what somevalues &gt; 20 does. It returns TRUE for the entries of somevalues which are greater than 20, and FALSE for the entries of somevalues that are not (that is, which are less than, or equal to, 20.\nThis statement somevalues &gt; 20 is called the condition.\n\nsomevalues &gt; 20\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\nNow consider putting that sequence of TRUEs and FALSEs inside the square brackets in somevalues[]. This returns only the entries of somevalues for which the condition is TRUE.\n\nsomevalues[c(FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE)]\n\n[1] 40 40 50 35\n\n\nSo what we’re doing is using a condition inside the square brackets to return all the values for which that condition is TRUE.\nAnd we’re being super efficient, because we don’t want to write out TRUEs and FALSEs all day, so we just give the conditional question inside the brackets directly:\n\nsomevalues[somevalues &gt; 20]\n\n[1] 40 40 50 35\n\n\n\n We can extend this same logic to a dataframe. Let’s suppose we want to access all the entries in our Star Wars data who have the value “Droid” in the species variable. To work out how to do this, we first need a line of code which defines our condition - one which returns TRUE for each entry of the species variable which is “Droid”, and FALSE for those that are not “Droid”.\nWe can use the dollar sign to pull out the species variable:\n\nstarwars2$species\n\n [1] \"Human\"        \"Human\"        \"Droid\"        \"Human\"        \"Human\"       \n [6] \"Human\"        \"Human\"        \"Droid\"        \"Human\"        \"Human\"       \n[11] \"Human\"        \"Human\"        \"Wookiee\"      \"Human\"        \"Rodian\"      \n[16] \"Hutt\"         \"Human\"        \"Human\"        \"Human\"        \"Human\"       \n[21] \"Trandoshan\"   \"Human\"        \"Human\"        \"Mon Calamari\" \"Human\"       \n[26] \"Ewok\"         \"Sullustan\"    \"Neimodian\"    \"Human\"        \"Gungan\"      \n[31] \"Gungan\"       \"Gungan\"       \"Toydarian\"    \"Dug\"          \"unknown\"     \n[36] \"Human\"        \"Zabrak\"       \"Twi'lek\"      \"Twi'lek\"      \"Vulptereen\"  \n[41] \"Xexto\"        \"Toong\"        \"Human\"        \"Cerean\"       \"Nautolan\"    \n[46] \"Zabrak\"       \"Tholothian\"   \"Iktotchi\"     \"Quermian\"     \"Kel Dor\"     \n[51] \"Chagrian\"     \"Human\"        \"Human\"        \"Human\"        \"Geonosian\"   \n[56] \"Mirialan\"     \"Mirialan\"     \"Human\"        \"Human\"        \"Human\"       \n[61] \"Human\"        \"Clawdite\"     \"Besalisk\"     \"Kaminoan\"     \"Kaminoan\"    \n[66] \"Human\"        \"Aleena\"       \"Skakoan\"      \"Muun\"         \"Togruta\"     \n[71] \"Kaleesh\"      \"Wookiee\"      \"Human\"        \"Pau'an\"       \"unknown\"     \n\n\nAnd we can ask R whether each value is equal to “Droid”\n\nRemember: in R, we ask whether something is equal to something else by using a double-equals, ==. A single equal sign would be wrong, as it denotes assignment.\n\n\nstarwars2$species == \"Droid\"\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE\n\n\nFinally, we can use this condition inside our square brackets to access the entries of the data for which this condition is TRUE:\n\n# I would read the code below as: \n# \"In the starwars2 dataframe, give me all the rows \n# for which the condition starwars2$species==\"Droid\"\n# is TRUE, and give me all the columns.\"\n\nstarwars2[starwars2$species == \"Droid\", ]\n\n   name height hair_color eye_color homeworld species\n3 R2-D2     96       &lt;NA&gt;       red     Naboo   Droid\n8 R5-D4     97       &lt;NA&gt;       red  Tatooine   Droid",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#more-complex-conditions",
    "href": "02_data.html#more-complex-conditions",
    "title": "2: More R - Basic Data Skills",
    "section": "More complex conditions",
    "text": "More complex conditions\nThinking back to Chapter 1 when we first introduced R, we talked briefly about “logical operators”. Specifically, the operators &, |, and ! (for “and”, “or”,” and “not”), will come in handy now.\nFor instance, we can now extract all those in the dataset which are humans and taller than 190cm:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$species==\"Human\" AND starwars2$height &gt; 190 are TRUE, \n# and give me all the columns.\"\nstarwars2[starwars2$species == \"Human\" & starwars2$height &gt; 190, ]\n\n                  name height hair_color eye_color homeworld species\n4          Darth Vader    202       none    yellow  Tatooine   Human\n59               Dooku    193      white     brown   Serenno   Human\n60 Bail Prestor Organa    191      black     brown  Alderaan   Human\n\n\nOr we can extract all those in the dataset which are either droids or ewoks:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$species==\"Droid\" OR starwars2$species==\"Ewok\" is TRUE, \n# and give me all the columns.\"\nstarwars2[starwars2$species == \"Droid\" | starwars2$species == \"Ewok\", ]\n\n                    name height hair_color eye_color homeworld species\n3                  R2-D2     96       &lt;NA&gt;       red     Naboo   Droid\n8                  R5-D4     97       &lt;NA&gt;       red  Tatooine   Droid\n26 Wicket Systri Warrick     88      brown     brown     Endor    Ewok",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#editing-specific-entries",
    "href": "02_data.html#editing-specific-entries",
    "title": "2: More R - Basic Data Skills",
    "section": "Editing specific entries",
    "text": "Editing specific entries\nNow that we’ve seen a few ways of accessing sections of data, we can learn how to edit them! One of the most common reasons you will need to modify entries in your data is in data cleaning. This is the process of identifying incorrect / incomplete / irrelevant data, and replacing / modifying / deleting them.\nAbove, we looked at the subsection of the data where the species variable had the entry “Droid”. Some of you may have noticed earlier that we had some data on C3PO. Are they not also a droid?\n\n\n\n(Looks pretty Droid-y to me! disclaimer: I know nothing about Star Wars 🙂 )\nJust as we saw above how to access specific entries, e.g.:\n\n# 2nd row, all columns\nstarwars2[2, ]\n\n   name height hair_color eye_color homeworld species\n2 C-3PO    167       &lt;NA&gt;    yellow  Tatooine   Human\n\n# 2nd row, 6th column (the \"species\" column)\nstarwars2[2,6]\n\n[1] \"Human\"\n\n\nWe can change these by assigning them a new value (remember the &lt;- symbol). In doing so, we overwrite4 the entry in the 2nd row and 6th column of the data (starwars2[2,6]) with the value “Droid”.\n\n# C3PO is a droid, not a human\nstarwars2[2,6] &lt;- \"Droid\"\n# Look at the 2nd row now -\n# the entry in the \"species\" column has changed:\nstarwars2[2, ]\n\n   name height hair_color eye_color homeworld species\n2 C-3PO    167       &lt;NA&gt;    yellow  Tatooine   Droid",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#editing-entries-via-a-condition",
    "href": "02_data.html#editing-entries-via-a-condition",
    "title": "2: More R - Basic Data Skills",
    "section": "Editing entries via a condition",
    "text": "Editing entries via a condition\nWe saw above how to access parts of data by means of a condition, with code such as:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$homeworld==\"Naboo\" is TRUE, and give me all the columns.\"\nstarwars2[starwars2$homeworld==\"Naboo\", ]\n\n            name height hair_color eye_color homeworld species\n3          R2-D2     96       &lt;NA&gt;       red     Naboo   Droid\n19     Palpatine    170       grey    yellow     Naboo   Human\n30 Jar Jar Binks    196       none    orange     Naboo  Gungan\n31  Roos Tarpals    224       none    orange     Naboo  Gungan\n32    Rugor Nass    206       none    orange     Naboo  Gungan\n52  Gregar Typho    185      black     brown     Naboo   Human\n53         Cordé    157      brown     brown     Naboo   Human\n58         Dormé    165      brown     brown     Naboo   Human\n\n\nWhat if we wanted to modify it so that every character from “Naboo” was actually of species “Nabooian”?\nWe can do that in a number of ways, all of which do the same thing - namely, they access parts of the data and assign them the new value “Nabooian”.\nThe lines of code below all do exactly that, in different ways. We’ve also tried to give a way of interepreting each line.\n\n# In the starwars2 data, give the rows for which condition \n# starwars2$homeworld==\"Naboo\" is TRUE, and select only the \"species\" column. \n# Assign to these selected entries the value \"Nabooian\".\nstarwars2[starwars2$homeworld==\"Naboo\", \"species\"] &lt;- \"Nabooian\"\n\n\n# In the starwars2 data, give the rows for which condition \n# starwars2$homeworld==\"Naboo\" is TRUE, and select only the 6th column. \n# Assign to these selected entries the value \"Nabooian\".\nstarwars2[starwars2$homeworld==\"Naboo\", 6] &lt;- \"Nabooian\"\n\n\n# Extract the species variable from the starwars2 data (it's a vector).\n# Pick the entries for which the condition starwars2$homeworld==\"Naboo\" is TRUE.\n# Assign to these selected entries the value \"Nabooian\".\nstarwars2$species[starwars2$homeworld==\"Naboo\"] &lt;- \"Nabooian\"\n\n\n\n\n\n\n\noptional: a little extra…\n\n\n\n\n\nIn a similar way, we could have changed C-3PO to a Droid without ever having to know what row of the data they were in!\n\n# for the row(s) where the name variable in starwars2\n# is equal to \"C-3PO\", in the species variable we assign\n# that entry to be \"Droid\"\nstarwars2[starwars2$name==\"C-3PO\", \"species\"] &lt;- \"Droid\"",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#addingchanging-a-variable",
    "href": "02_data.html#addingchanging-a-variable",
    "title": "2: More R - Basic Data Skills",
    "section": "Adding/Changing a variable",
    "text": "Adding/Changing a variable\nAnother thing we might want to do is change a whole variable (a whole column) in some way.\nThe logic is exactly the same, for instance, we can take the variable “height” from the dataframe “starwars2”, dividing it by 100 via starwars2$height / 100, and then assign the result to the same variable name in the data, i.e. we overwrite the column:\n\nstarwars2$height &lt;- starwars2$height / 100\n\nWe could instead have added a new column named “height_m” with those values if we did not want to overwrite “height”:\n\nstarwars2$height_m &lt;- starwars2$height / 100\n\nThis would have left the “height” variable as-is, and created a new one called “height2” which was the values in “height” divided by 100.",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#removing-rows-or-columns",
    "href": "02_data.html#removing-rows-or-columns",
    "title": "2: More R - Basic Data Skills",
    "section": "Removing rows or columns",
    "text": "Removing rows or columns\nLastly, we might want to change the data by removing a row or a column. Again, the logic remains the same, in that we use &lt;- to assign the edited data to a name (either a new name, thus creating a new object, or an existing name, thereby overwriting that object).\nFor instance, notice that the 35th and 75th rows of our data probably aren’t a valid observation - I’m reasonably sure that Marge and Homer Simpson never appeared in Star Wars:\n\nstarwars2[c(35,75), ]\n\n            name height hair_color eye_color   homeworld species\n35 Marge Simpson    1.7       Blue      &lt;NA&gt; Springfield unknown\n75 Homer Simpson    1.8       &lt;NA&gt;      &lt;NA&gt; Springfield unknown\n\n\nWe can remove a certain row(s) by using a minus sign - inside the square brackets\n\n# everything minus the 75th row\nstarwars2[-75, ]\n# everything minus the (35th and 75th rows)\nstarwars2[-c(35, 75), ]\n\nAnd we can simply re-use the name “starwars2” to overwrite the data and make this change take effect (rather than just print out the result, which the code above did):\n\nstarwars2 &lt;- starwars2[-c(35, 75), ]\n\n(now, in the environment pane of Rstudio, the object named “starwars2” will say 73 observations, rather than 75, which it had before - we’ve removed the 2 rows)\n The same logic applies for columns:\n\n# Create a new object called \"anonymous_starwars2\" and assign it \n# to the values which are the \"starwars2\" dataframe minus the \n# 1st column (the \"name\" column):\nanonymous_starwars2 &lt;- starwars2[, -1]\n# dimensions of our initial data\ndim(starwars2)\n\n[1] 73  6\n\n# the data we just assigned has one fewer columns\ndim(anonymous_starwars2)\n\n[1] 73  5",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "02_data.html#footnotes",
    "href": "02_data.html#footnotes",
    "title": "2: More R - Basic Data Skills",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo save as .csv in Microsoft Excel, we go to File &gt; Save as, and then in the Save as Type box, choose to save the file as CSV (Comma delimited)↩︎\nThe colon operator, from:to, creates a vector from the value from to the value to in steps of 1.\nFor instance, 1:6 is the same as c(1,2,3,4,5,6).↩︎\nNote: When we do this, we don’t have the comma inside the square brackets.\nWhen we use the $ to pull out a variable, such as starwars2$name, we no longer have a dataframe.\nstarwars2$name doesn’t have rows and columns, it just has a series of values - it’s a vector!\nSo when you are using [] with a vector (1 dimension) rather than a dataframe (2 dimensions), you don’t specify [rows, columns], but simply [entry].↩︎\nyou could think of this as replacing / overwriting / reassigning the entry↩︎",
    "crumbs": [
      "2: More R - Basic Data Skills"
    ]
  },
  {
    "objectID": "00activetest.html#introduction",
    "href": "00activetest.html#introduction",
    "title": "Test of learnr + webassemby",
    "section": "Introduction",
    "text": "Introduction\nwe have content.\nread this.\n\n\n\n\n\n\n\n\nabove here is a webr chunk with include: false. Because the webr stuff is basically just evaluated in a single env for the page, it might be worth having an initial chunk to pre-populate the page with loading the relevant libraries & data etc.\nhere is something you can play with:\ncan you guess what the coefficient will be?\n\n\n\n\n\n\n\n\nWhat about this one? where is df1 from? the author has made it for you already!\n\n\n\n\n\n\n\n\nblah blah.\nhere’s a question.. what is 3+4?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\n\n\n3+?\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n3+4\n3+4"
  },
  {
    "objectID": "01_R.html",
    "href": "01_R.html",
    "title": "1: A first look at R & RStudio",
    "section": "",
    "text": "This reading:\n\nHow does R work?\nHow do we use the Rstudio interface?\n\n\nThe best way to learn R is to use it. Try following along with these readings by typing the code into your R script and running them. You will hopefully get the same output as is presented on this page below each bit of code. If you get errors and warnings, don’t panic - read them!\n\n\n\nR is a calculator\nWhen we first open RStudio, we should see something which looks more or less like the image in Figure 1, where there are several little windows. We are going to explore what each of these little windows offer by just diving in and starting to do things.\n\n\n\n\n\n\n\n\nFigure 1: RStudio, freshly opened\n\n\n\n\n\nStarting in the left-hand window, you’ll notice the blue sign &gt;.\nThis is where we R code gets executed.\nType 2+2, and hit Enter ↵.\nYou should discover that R is a calculator - R responds by telling us the answer (4).\nLet’s work through some basic operations (adding, subtracting, etc). For instance, can you work out what R will give you for each of these operations?\n\n\nArithmetic operations\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n2 + 5\n\n\n\n10 - 4\n\n\n\n2 * 5\n\n\n\n10 - (2 * 5)\n\n\n\n(10 - 2) * 5\n\n\n\n10 / 2\n\n\n\n3^2\n(the ^ symbol is “to the power of”)\n\n\n\n\n\nShow me the output\n\n\n\nCode\nOutput\n\n\n\n\n2 + 5\n7\n\n\n10 - 4\n6\n\n\n2 * 5\n10\n\n\n10 - (2 * 5)\n0\n\n\n(10 - 2) * 5\n40\n\n\n10 / 2\n5\n\n\n3^2\n9(the ^ symbol is “to the power of”)\n\n\n\n\n\n\nR can get stuck\nWhenever you see the blue sign &gt;, it means R is ready and waiting for you to provide a command.\nIf you type 10 + and press Enter, you’ll see that instead of &gt; you are left with +. This means that R is waiting for more.\nEither give it more (finish the command), or cancel the command by pressing the Escape key on your keyboard.\n\nAs well as performing arithmetic calculations, we can ask R things for which the answer is TRUE or FALSE, such as “Is 3 less than 5?”. If we type 3 &lt; 5 and press Enter, then R should tell us that the statement we gave it is TRUE.\nThese computations don’t return numbers, but instead return logical values. There are few operators that we need to learn about here:\n\nEquality/Inequality: We use the symbols == to mean “is equal to”, and the symbols != for “is not equal to”.\n\nLess Than/Greater Than: To determine whether a value is less/greater than another value, we have our typical symbols &lt; and &gt;. We also have &lt;= and &gt;= when we want to include “less/greater than or equal to”.\n\nWe can combine these with & for “and”, | for “or”, and ! for “not”, to ask R all sorts of things.\nTry and work out what R should give you for each of these (or try it out yourself!):\n\n\nLogical operations\n\n\n\nCode\nOutput\n\n\n\n\n3 &gt; 5\n\n\n\n3 &lt;= 5\n\n\n\n3 &gt;= 3\n\n\n\n3 == 5\n\n\n\n(2 * 5) == 10\n\n\n\n(2 * 5) != 11\n\n\n\n(2 == 2) & (3 == 4)\n\n\n\n(2 == 2) | (3 == 4)\n\n\n\n(2 == 2) & !(3 == 4)\nTRUE\n\n\n\n\n\nShow me the output\n\n\n\nCode\nOutput\n\n\n\n\n3 &gt; 5\nFALSE\n\n\n3 &lt;= 5\nTRUE\n\n\n3 &gt;= 3\nTRUE\n\n\n3 == 5\nFALSE\n\n\n(2 * 5) == 10\nTRUE\n\n\n(2 * 5) != 11\nTRUE\n\n\n(2 == 2) & (3 == 4)\nFALSE\n\n\n(2 == 2) | (3 == 4)\nTRUE\n\n\n(2 == 2) & !(3 == 4)\nTRUE\n\n\n\n\n\n\nFALSE and TRUE as 0 and 1\nIt will become useful to think of these logical values (TRUE and FALSE) as also having intrinsic numeric values of 0 and 1.\nThis is how R will treat them if you ask it to do something that requires the values to be numeric.\nFor example, the code TRUE + 3 will return 4, and FALSE + 3 will return 3.\n\n\n\n\n\n\nR has a memory\nWe can also store things in R’s memory, and to do that we just need to give them a name.\nType x &lt;- 5 and press Enter.\nWhat has happened? We don’t get given an answer like we did with calculations such as 2 + 4. What we’ve done is stored in R’s memory something named x which has the value 5. We can now refer to the name and it will give us the value!\nIf we now type x and press Enter, it gives us whatever we assigned to the name “x”. So it gives us the number 5.\n\nx\n\n[1] 5\n\n\nWhat is going to happen when we type x * 3? It will give us 15!\n\nAssigning names to things in R\nThe &lt;- symbol, pronounced arrow, is what we use to assign a value to a named object:\n\nname &lt;- value\n\n\nNote, there are a few rules about names in R:\n\nNo spaces - spaces inside a name are not allowed (the spaces around the &lt;- don’t matter):\nlucky_number &lt;- 5 ✔ lucky number &lt;- 5 ❌\nNames must start with a letter:\nlucky_number &lt;- 5 ✔ 1lucky_number &lt;- 5 ❌\nCase sensitive:\nlucky_number is different from Lucky_Number\nhere is a set of words you can’t use as names, including: if, else, for, in, TRUE, FALSE, NULL, NA, NaN, function\n(Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these).\n\n\n\n\n\n\nThe Console and The Environment\nIf you are working along with us, you might have also noticed that something else happened when we executed the code x &lt;- 5. The thing we named x with a value of 5 suddenly appeared in the top-right window. This is known as the environment (Figure 2), and it shows everything that we store in R.\n\n\n\n\n\n\n\n\nFigure 2: Assigning names to objects stores them in R’s environment.\n\n\n\n\n\nSo we’ve now made use of two of the panes that we see in RStudio:\nWhere code is run: When we’ve been writing and running R code (e.g. typing 2+5 or x&lt;-5 and pressing Enter), we’ve been doing it in the “console”.\nThe console is where R code gets executed (i.e. where our coded instruction to R code is interpreted and acted on), but as we’ll see below, it isn’t where all R code gets written.\nWhere things get stored: We’ve also been learning about how we can store things in R’s memory (the environment) by assigning a name to them using the &lt;- operator. The top-right pane of RStudio shows us the environment, where we can see everything that we have stored in R. Note that this also means we can keep track of what objects we have saved that are available for our use. If we never stored an object named “peppapig”, then R will give us an error when we do something like:\n\n2*peppapig\n\nError: object 'peppapig' not found\n\n\nNow that we have an idea of what the console and the environment are for, we are well on our way. If you want a silly analogy, the console is like R’s mouth, where we feed it things, and the environment is just its memory, where it remembers what things are what. We can see these in Figure 3. Note however, that the console has been moved down to the bottom-left, as we are introducing a new pane above it. This is where we move to next.\n\n\n\n\n\n\n\n\nFigure 3: RStudio panes: Code is executed in the console, and objects are stored in the environment.\n\n\n\n\n\n\n\n\n\n\nR Scripts and the Editor\nWhat if we want to edit our code? Whatever we write in the console just disappears upwards. What if we want to change things we did earlier on?\nWhile the console is where code gets executed, it doesn’t have to be where code gets written.. We can write and edit our code in a separate place before we then send it to the console to be executed!!\nThe standard place to write and edit things is in an R scipt. We can open one by doing File &gt; New File &gt; R script, and a new file will open in the top-left pane of RStudio. The console will be shoved down to the bottom-left.\nIn the R script, we can write code. For instance, we might write:\n\nx &lt;- 210\ny &lt;- 15\nx / y\n\nNotice that nothing happens when we write each line. It’s not like writing in the console where R tells us the answers. This is because this code is not yet being executed. We haven’t actually fed it to R.\nThere are a couple of useful ways we can send the code to R.\n\nPosition your text-cursor (blinking vertical line) on the line of code we wish to run and press Ctrl+Enter (Windows) or Cmd+Enter (MacOS)\n\nPosition your text-cursor (blinking vertical line) on the line of code we wish to run and press the “Run” button in the top right of the script.\n\nWhen we do this, the line of code will be sent down to the console, where it will be executed, and R will do it’s thing.\nFor example, if we had sent the line x &lt;- 210 down to the console, R would then store the number 210 with the name x in our environment (as in Figure 4). Additionally, it will move the text-cursor to the next line, so we can just press Ctrl+Enter again to run the next line (and so on.).\n\n\n\n\n\n\n\n\nFigure 4: Code written in the script can be sent down to the console, where it is executed. In this example, the execution of the code stores an object in the environment.\n\n\n\n\n\nBy writing our code in a file such as an R script before sending it to the console we can edit, save, and share our code. This makes it so much more useful than just using the console (which is more like writing on scratch paper, where we can’t keep anything).\nFor instance, let’s say we made a mistake earlier, and instead of “x” being 210, it should have been 211. Well, we can just edit the script, and re-run it.\n\nRegularly save your scripts!\nTo save an R script that is open, we just\n\nFile &gt; Save (or Ctrl+S)\nLocate to the folder where we want to save the file.\n\ngive it an appropriate name, and click save.\n\nNOTE: When you save R script files, they terminate with a .R extension.\n\n\n\n\n\n\n\nLooking ahead to RMarkdown\n\n\n\n\n\n\nIn addition to R scripts, there is another type of document we can create, known as “Rmarkdown”.\nRmarkdown documents combine the analytical power of R and the utility of a text-processor. We can have one document which contains all of our analysis as well as our written text, and can be compiled into a nicely formatted report. This saves us doing analysis in R and copying results across to Microsoft Word. It ensures our report accurately reflects our analysis. Everything that you’re reading now has all been written in Rmarkdown!\n\n\n\n\n\n\nFigure 5: An example RMarkdown document\n\n\n\nWe’re going to learn more about Rmarkdown documents and how to write them later on, but the broad idea is that we can writing normal text interspersed with “code-chunks” (i.e., chunks of code!). RMarkdown documents looks much like an R script, only the code is contained within the grey-boxes, and text is written in between (see Figure 5). RMarkdown documents can then be compiled to create a lovely .pdf, .html, or .docx file.\n\n\n\n\n\n\n\n\nFigure 6: RMarkdown Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Four RStudio Panes\nWe’ve now seen almost all the different panes in RStudio:\n\n\nThe console is where R code gets executed\nThe environment is R’s memory, you can assign something a name and store it here, and then refer to it by name in your code.\nThe editor is where you can write and edit R code in R scripts and Rmarkdown documents. You can then send this to the console for it to be executed.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: The Four Panes of RStudio\n\n\n\n\n\n\nWe are yet to use the bottom-right window, but this is an easy one to explain. It is where we can see any plots that we create, where we can browse our files, and where we can ask R for some help documentation. We’ll make more use of this later on, but for now try typing plot(x = 4, y = 2) into the console and seeing what happens.\n\n\nProjects and file organisation\nWe’re not going to speak too much about this here but one key thing to remember is that R is working from a specific place in your computer. You can find out where by typing getwd() into the console.\nAn easy way to keep things organised is to set up an “R project”. This basically associates a specific folder on your computer with your working in R, and it means it will automatically look for things in that folder.\nWe recommend that you start a project for this course (call it something like “usmr”). This will the be project that you open whenever you work on this course (RStudio will usually re-open the previous project you were working on when you closed it).\nWith that project open, we suggest that you start a new script for each week, in which you complete your exercises, and which you then remember to save!\nIf you haven’t already, we suggest you start an R project by using (in the top menu of RStudio), File &gt; New Project and following the instructions. It will create a folder on your computer somewhere of your choosing, and you will now notice that if you click in the “Files” tab in the bottom right pane of RStudio, you can see the project folder!\n\n\n\n\n\n\nGood Habits\nAlong with regular saving of work and organising your files, it will be very useful in the long-run if we get used to always “starting fresh” when we open R.\nWe need to start thinking of the code that we write in an R script as a set of consecutive instructions that we can give to R in order to achieve our goal. It’s just a blank slate on which we write (in language R understands) “do this. now do this. now do this..” and so on.\nThis means that the script contains all the information needed.\nSo we can now:\n\nEmpty our environment\nRestart R\nRun all the code in our script (highlight multiple lines of code to run them all at once)\n\nand we’re back to where we are! This is great for when we make mistakes (we’re going to make many many mistakes!), because we can just clear everything, start at the top of our script, and work downwards to figure out what has gone wrong.\n\nTidying up\n\nTo empty our environment, we can click on the little broomstick icon: .\nTo restart the R Session (not always necessary, but good practice) in the top menu, we choose Session &gt; Restart R (or press Ctrl+Shift+F10).\n\n\nThe other very useful thing that we can do in a script is to write comments for ourselves or for others. By starting a line with a #, R will know that that entire line is not code, and so it won’t try to do anything with it. For instance, if we write these lines in our script, and send them both down to the console, nothing happens for the first line:\n\n\nComments\n\n# The line below will add 5 to 2. \n2+5\n\n[1] 7\n\n\n\n\nIf we forget the #\n\nThe line below will add 5 to 2. \n2+5\n\n\nError: unexpected symbol in “The line”\n\n\n\n\n\n\n\n\nUseful Settings\nBelow are a couple of our recommended settings for you to change as you begin your journey in R. After you’ve changed them, take a 5 minute break before moving on to the next chapter.\n\n1. Clean environments\nAs you use R more, you will store lots of things with different names. Throughout this course alone, you’ll probably name hundreds of different things. This could quickly get messy within our project.\nWe can make it so that we have a clean environment each time you open RStudio. This will be really handy.\n\nIn the top menu, click Tools &gt; Global Options…\nThen, untick the box for “Restore .RData into workspace at startup”, and change “Save workspace to .RData on exit” to Never:\n\n\n\n\n2. Wrapping code\nIn the editor, you might end up with a line of code which is really long, but you can make RStudio ‘wrap’ the line, so that you can see it all, without having to scroll:\n\nx &lt;- 1+2+3+6+3+45+8467+356+8565+34+34+657+6756+456+456+54+3+78+3+3476+8+4+67+456+567+3+34575+45+2+6+9+5+6\n\n\nIn the top menu, click Tools &gt; Global Options…\nIn the left menu of the box, click “Code”\nTick the box for “Soft-wrap R source files”",
    "crumbs": [
      "1: A first look at R & RStudio"
    ]
  },
  {
    "objectID": "03_measurement.html",
    "href": "03_measurement.html",
    "title": "3: Measurement & Distributions",
    "section": "",
    "text": "This reading:\n\nWhat different types of data can we collect?\nHow can we summarise and visualise distributions of different types of data?\n\nAlso:\n\nThe “tidyverse”: a different style of coding in R",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#categorical",
    "href": "03_measurement.html#categorical",
    "title": "3: Measurement & Distributions",
    "section": "Categorical",
    "text": "Categorical\n\nCategorical variables tell us what group or category each individual belongs to. Each distinct group or category is called a level of the variable.\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nNominal (Unordered categorical)\nA categorical variable with no intrinsic ordering among the levels.\nSpecies: Dog, Cat, Parrot, Horse, …\n\n\nOrdinal (Ordered categorical)\nA categorical variable which levels possess some kind of order\nLevel: Low, Medium, High\n\n\nBinary categorical\nA special case of categorical variable with only 2 possible levels\nisDog: Yes or No.\n\n\n\n\nIf we want to summarise a categorical variable into a single number, then the simplest approach is to use the mode:\n\nMode: The most frequent value (the value that occurs the greatest number of times).\n\nWhen we have ordinal variables, there is another option, and that is to use the median:\n\nMedian: This is the value for which 50% of observations are lower and 50% are higher. It is the mid-point of the values when they are rank-ordered. (Note that “lower” and “higher” requires our values to have an order to them)\n\nWhen we use the median as our measure of central tendency (i.e. the middle of the distribution) and we want to discuss how spread out the spread are around it, then we will want to use quartiles. The Inter-Quartile Range (IQR) is obtained by rank-ordering all the data, and finding the points at which 25% (one quarter) and 75% (three quarters) of the data falls below (this makes the median the “2nd quartile”).\nIn our dataset on passwords, we have various categorical variables, such as the type of password (categories like “animal”, “fluffy” etc).\nThere are various ways we might want to summarise categorical variables like this. We have already seen the code to do this in our example of the dice simulation - we can simply counting the frequencies in each level:\n\ntable(pwords$type)\n\n\n             animal          cool-macho              fluffy                food \n                 29                  79                  44                  11 \n               name           nerdy-pop    password-related     rebellious-rude \n                183                  30                  15                  11 \nsimple-alphanumeric               sport \n                 61                  37 \n\n\nThis shows us that the mode (most common) is “name” related passwords.\nWe could also convert these to proportions, by dividing each of these by the total number of observations. For instance, here are the percentages of passwords of each type1:\n\ntable(pwords$type) / sum(table(pwords$type)) * 100\n\n\n             animal          cool-macho              fluffy                food \n                5.8                15.8                 8.8                 2.2 \n               name           nerdy-pop    password-related     rebellious-rude \n               36.6                 6.0                 3.0                 2.2 \nsimple-alphanumeric               sport \n               12.2                 7.4 \n\n\n\nOften, if the entries in a variable are characters (letters), then many functions in R (like table()) will treat it the same as if it is a categorical variable.\nHowever, this is not always the case, so it is good to tell R specifically that each variable is a categorical variable.\nThere is a special way that we tell R that a variable is categorical - we set it to be a “factor”. Note what happens when we make the “type” and “strength_cat” variables to be a factor:\n\npwords$type &lt;- factor(pwords$type)\npwords$strength_cat &lt;- factor(pwords$strength_cat)\nsummary(pwords)\n\n      rank         password                          type        cracked      \n Min.   :  1.0   Length:500         name               :183   Min.   : 1.290  \n 1st Qu.:125.8   Class :character   cool-macho         : 79   1st Qu.: 3.430  \n Median :250.5   Mode  :character   simple-alphanumeric: 61   Median : 3.720  \n Mean   :250.5                      fluffy             : 44   Mean   : 5.603  \n 3rd Qu.:375.2                      sport              : 37   3rd Qu.: 3.720  \n Max.   :500.0                      nerdy-pop          : 30   Max.   :92.270  \n                                    (Other)            : 66                   \n    strength      strength_cat\n Min.   : 1.000   medium:402  \n 1st Qu.: 6.000   strong: 25  \n Median : 7.000   weak  : 73  \n Mean   : 6.768               \n 3rd Qu.: 8.000               \n Max.   :10.000               \n                              \n\n\nR now recognises that there a set number of possible response options, or “levels”, for these variables. We can see what they are using:\n\nlevels(pwords$strength_cat)\n\n[1] \"medium\" \"strong\" \"weak\"  \n\n\nThe “strength_cat” variable specifically has an ordering to the levels, so we might be better off also telling R about this ordering. We do this like so:\n\npwords$strength_cat &lt;- factor(pwords$strength_cat, ordered = TRUE, levels = c(\"weak\",\"medium\",\"strong\"))\n\n\nSometimes, we might have a variable that we know is categorical, but we might want to treat it as a set of numbers instead. A very common example in psychological research is Likert data (questions measured on scales such as “Strongly Disagree”&gt;&gt;“Disagree”&gt;&gt;…&gt;&gt;“Strongly Agree”).\nIt is often useful to have these responses as numbers (e.g. 1 = “Strongly Disagree” to 5 = “Strongly Agree”), as this allows us to use certain functions and analyses more easily. For instance, the median() and IQR() functions require the data to be numbers.\nThis will not work:\n\nmedian(pwords$strength_cat)\n\nError in median.default(pwords$strength_cat): need numeric data\n\n\nWhen we ask R to convert a factor to a numeric variable, it will give turn the first category into 1, the second category to 2, and so on. As R knows that our strength_cat variable is the ordered categories “weak”&gt;&gt;“medium”&gt;&gt;“strong”, then as.numeric(pwords$strength_cat) will turn these to 1s, 2s, and 3s.\n\nmedian(as.numeric(pwords$strength_cat))\n\n[1] 2\n\n\n\nConverting between types of data:\nIn R, we can use various functions to convert between different types of data, such as:\n\nfactor() / as.factor() - to turn a variable into a factor\nas.numeric() - to turn a variable into numbers\nas.character() - to turn a variable into letters\n\nand we can check what type of data something is coded as, by using is.factor(), is.numeric(), is.character().\n\n\n\n\n\n\nbe careful with conversions\n\n\n\n\n\nStudy the code below and the output.\nThink carefully about why this happens:\n\nvec &lt;- c(1,2,4,7)\nas.numeric(as.factor(vec))\n\n[1] 1 2 3 4\n\n\nWhy is the output different here?\n\nas.numeric(as.character(as.factor(vec)))\n\n[1] 1 2 4 7",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#numeric",
    "href": "03_measurement.html#numeric",
    "title": "3: Measurement & Distributions",
    "section": "Numeric",
    "text": "Numeric\n\nNumeric (or quantitative) variables consist of numbers, and represent a measurable quantity. Operations like adding and averaging make sense only for numeric variables.\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nContinuous\nVariables which can take any real number within the specified range of measurement\nHeight: 172, 165.2, 183, …\n\n\nDiscrete\nVariables which can only take integer number values. For instance, a counts can only take positive integer values (0, 1, 2, 3, etc.)\nNumber_of_siblings: 0, 1, 2, 3, 4, …\n\n\n\n\nOne of the most frequently used measures of central tendency for numeric data is the mean. The mean is calculated by summing all of the observations together and then dividing by the total number of obervations (\\(n\\)).\n\nMean: \\(\\bar{x}\\)\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the mean is:\n\\[\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\]\n\n\n\n\n\n\nHelp reading mathematical formulae\n\n\n\n\n\nThis might be the first mathematical formula you have seen in a while, so let’s unpack it.\nThe \\(\\sum\\) symbol is used to denote a series of additions - a “summation”.\nWhen we include the bits around it: \\(\\sum\\limits_{i = 1}^{n}x_i\\) we are indicating that we add together all the terms \\(x_i\\) for values of \\(i\\) between \\(1\\) and \\(n\\): \\[\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n\\]\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the \\(1^{st}\\) to the \\(n^{th}\\) (where \\(n\\) is the total number of values), and we divide that by \\(n\\).\n\n\n\n\nIf we are using the mean as our as our measure of central tendency, we can think of the spread of the data in terms of the deviations (distances from each value to the mean).\nRecall that the mean is denoted by \\(\\bar{x}\\). If we use \\(x_i\\) to denote the \\(i^{th}\\) value of \\(x\\), then we can denote deviation for \\(x_i\\) as \\(x_i - \\bar{x}\\).\nThe deviations can be visualised by the red lines in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Deviations from the mean\n\n\n\n\n\n\nThe sum of the deviations from the mean, \\(x_i - \\bar x\\), is always zero\n\\[\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n\\]\nThe mean is like a center of gravity - the sum of the positive deviations (where \\(x_i &gt; \\bar{x}\\)) is equal to the sum of the negative deviations (where \\(x_i &lt; \\bar{x}\\)).\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider squared deviations.\nSquaring the deviations makes them all positive. Observations far away from the mean in either direction will have large, positive squared deviations. The average squared deviation is known as the variance, and denoted by \\(s^2\\)\n\nVariance: \\(s^2\\)\nThe variance is calculated as the average of the squared deviations from the mean.\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the variance is:\n\\[s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\noptional: why n minus 1?\n\n\n\n\n\nThe top part of the equation \\(\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\\) can be expressed in \\(n-1\\) terms, so we divide by \\(n-1\\) to get the average.\n Example: If we only have two observations \\(x_1\\) and \\(x_2\\), then we can write out the formula for variance in full quite easily. The top part of the equation would be: \\[\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\]\nThe mean for only two observations can be expressed as \\(\\bar{x} = \\frac{x_1 + x_2}{2}\\), so we can substitute this in to the formula above. \\[\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\] becomes: \\[\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2\n\\] Which simplifies down to one value: \\[\n\\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n\\]  So although we have \\(n=2\\) datapoints, \\(x_1\\) and \\(x_2\\), the top part of the equation for the variance has 1 fewer units of information. In order to take the average of these bits of information, we divide by \\(n-1\\).\n\n\n\n\nOne difficulty in interpreting variance as a measure of spread is that it is in units of squared deviations. It reflects the typical squared distance from a value to the mean.\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the standard deviation.\n\nStandard Deviation: \\(s\\)\nThe standard deviation, denoted by \\(s\\), is a rough estimate of the typical distance from a value to the mean.\nIt is the square root of the variance (the typical squared distance from a value to the mean).\n\\[\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n\\]\n\nIn the passwords dataset, we only have one continuous variable, and that is the “cracked” variable, which if we recall is the “Time to crack by online guessing”. You might be questioning whether the “strength” variable, which ranges from 1 to 10 is numeric? This depends on whether we think that statements like “a password of strength 10 is twice as strong as a password of strength 5”.\nFor now, we’ll just look at the “cracked” variable.\nTo calculate things like means and standard deviations in R is really easy, because there are functions that do them all for us.\nFor instance, we can do the calculation by summing the cracked variable, and dividing by the number of observations (in our case we have 500 passwords):\n\n# get the values in the \"cracked\" variable from the \"pwords\" dataframe, and\n# sum them all together. Then divide this by 500\nsum(pwords$cracked)/500\n\n[1] 5.60266\n\n\nOr, more easily, we can use the mean() function:\n\nmean(pwords$cracked)\n\n[1] 5.60266\n\n\nWe can get R to calculate the variance and standard deviation with the var() and sd() functions:\n\nvar(pwords$cracked)\n\n[1] 71.16618\n\nsd(pwords$cracked)\n\n[1] 8.436005\n\n\nand just to prove to ourselves:\n\nsd(pwords$cracked)^2 == var(pwords$cracked)\n\n[1] TRUE\n\n\n\nIf a column of our dataset contains only numbers, R will typically just interpret it as a numeric variable. However, we should still be careful; remember what happens if we have just one erroneous entry in there - they can all change to be characters (surrounded by quotation marks):\n\nc(1,3,6,\"peppapig\",3)\n\n[1] \"1\"        \"3\"        \"6\"        \"peppapig\" \"3\"       \n\n\nWe can force a variable to be numeric by using as.numeric(), which will also coerce any non-numbers to be NA (not applicable):\n\nas.numeric(c(1,3,6,\"peppapig\",3))\n\n[1]  1  3  6 NA  3\n\n\nIf there is an NA in the variable, many functions like mean(), var() and sd() will not compute:\n\nx &lt;- c(1, 3, 6, NA, 3)\nmean(x)\n\n[1] NA\n\n\nHowever, we can ask these functions to remove the NAs prior to the computation:\n\nmean(x, na.rm = TRUE)\n\n[1] 3.25\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Artwork by @allison_horst",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#this-is-a-pipe",
    "href": "03_measurement.html#this-is-a-pipe",
    "title": "3: Measurement & Distributions",
    "section": "This is a pipe!",
    "text": "This is a pipe!\nWe have seen already seen a few examples of code such as:\n\ndim(somedata)\ntable(somedata$somevariable)\n\n\n\nAnd we have seen how we might wrap functions inside functions:\n\nbarplot(table(somedata$somevariable))\n\nThis sort of writing (functions inside functions inside functions) involves R evaluating code from the inside out. But a lot of us don’t intuitively think that way, and actually find it easier to think in terms of a sequence. The code barplot(table(somedata$somevariable)) could be read as “take this variable, then make a table of it, then make a barplot of that table”.\nWe can actually write code that better maps to this way of reading, using a nice little symbol called a “pipe”:\n\nPiping\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write sequentially:\n\nNotice that what we are doing is using a new symbol: |&gt;\nThis symbol takes the output of whatever is on it’s left-hand side, and uses it as an input for whatever is on the right-hand side.\nThe |&gt; symbol gets called a “pipe”.\n\nLet’s see it in action with the passwords dataset we’ve been using.\n\n\ninside-out\nThe typical way of writing code is requires reading from the inside-out:\n\nbarplot(table(pwords$type))\n\n\n\n\n\n\n\n\n\n\npiped\nWhen we pipe code, we can read it from left to right:\n\npwords$type |&gt;\n    table() |&gt;\n    barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther pipes: |&gt; and %&gt;%\n\n\n\n\n\nThe |&gt; pipe is a relatively recent addition to R, but will likely be replacing the older %&gt;% pipe that was in a specific set of packages, and has been around since about 2014.\nThese two pipes do basically the same thing\nThere are some subtle differences between the two that only become apparent in very specific situations, none of which are likely to arise on this course.\nHowever, it’s important to be aware of them both, because you will like see them both in resources/online forums etc. You can usually just use them interchangeably.\n\n\n# for %&gt;% we need the tidyverse\nlibrary(tidyverse)\n1:10 %&gt;% mean()\n\n\n\n\n# the new base R pipe\n1:10 |&gt; mean()",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#the-tidyverse",
    "href": "03_measurement.html#the-tidyverse",
    "title": "3: Measurement & Distributions",
    "section": "The Tidyverse",
    "text": "The Tidyverse\nWe’re going to use pipes a lot throughout this course, and it pairs really well with a group of functions in the tidyverse packages, which were designed to be used in conjunction with a pipe:\n\nselect() extracts columns\n\nfilter() subsets data based on conditions\n\nmutate() adds new variables\n\ngroup_by() group related rows together\n\nsummarise()/summarize() reduces values down to a single summary\n\nTypically, the tidyverse means that we no longer have to keep telling R in which dataframe to look for the variable. The tidyverse functions are designed to make things is a bit easier. The examples below show how.\nYou’ll notice that the code has lots of indentations to make it more readable, which RStudio does for you when you press enter!\nBefore anything else, however, we need to load the tidyverse package:\n\nlibrary(tidyverse)\n\n\nselect()\nWe know about using $ to extract a column from a dataframe. The select() function is a little bit like that - it allows us to choose certain columns in a dataframe. It will return all rows.\nBecause we can select multiple columns this way, it doesn’t return us a vector (in the way dataframe$variable does), but returns a dataframe:\n\n# take the data\n# and select the \"variable1\" and \"variable2\" columns\ndata |&gt;\n  select(variable1, variable2)\n\n\n\nTidyverse\n\npwords |&gt;\n  select(type, strength)\n\n\n\nBase R\n\npwords[, c(\"type\",\"strength\")]\n\n\n\n\n\nfilter()\nThe filter() function is a bit like the [] to choose rows that meet certain conditios - it allows us to filter a dataframe down to those rows which meet a given condition. It will return all columns.\n\n# take the data\n# and filter it to only the rows where the \"variable1\" column is \n# equal to \"value1\". \ndata |&gt; \n  filter(variable1 == value1)\n\n\n\nTidyverse\n\npwords |&gt;\n    filter(strength_cat == \"strong\")\n\n\n\nBase R\n\npwords[pwords$strength_cat == \"strong\", ]\n\n\n\n\n\nmutate()\nThe mutate() function is used to add or modify variables to data.\n\n# take the data\n# |&gt;\n# mutate it, such that there is a variable called \"newvariable\", which\n# has the values of a variable called \"oldvariable\" multiplied by two.\ndata |&gt;\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\nTo ensure that our additions/modifications of variables are stored in R’s environment (rather than simply printed out), we need to reassign the name of our dataframe:\n\ndata &lt;- \n  data |&gt;\n  mutate(\n    ...\n  )\n\nNote: Inside functions like mutate(), we don’t have to keep using the dollar sign $, as we have already told it what data to look for variables in.\n\n\nTidyverse\n\npwords &lt;- pwords |&gt; \n    mutate(\n        cracked_min = cracked / 60\n    )\n\n\n\nBase R\n\npwords$cracked_min &lt;- pwords$cracked / 60\n\n\n\n\n\nsummarise()\nThe summarise() function is used to reduce variables down to a single summary value.\n\n# take the data |&gt;\n# summarise() it, such that there is a value called \"summary_value\", which\n# is the sum() of \"variable1\" column, and a value called \n# \"summary_value2\" which is the mean() of the \"variable2\" column.\ndata |&gt;\n  summarise(\n    summary_value = sum(variable1),\n    summary_value2 = mean(variable2)\n  )\n\n\n\nTidyverse\n\npwords |&gt; \n    summarise(\n        mean_cracked = mean(cracked),\n        sd_cracked = sd(cracked),\n        nr_strong = sum(strength_cat == \"strong\")\n    )\n\n\n\nBase R\nTo store these all in the same object (like the tidyverse way) we would have to create a data.frame() and add these as variables.\n\nmean(pwords$cracked)\nsd(pwords$cracked)\nsum(pwords$strength_cat == \"strong\")\n\n\n\n\n\ngroup_by()\nThe group_by() function is often used as an intermediate step in order to do something. For instance, if we want to summarise a variable by calculating its mean, but we want to do that for several groups, then we first group_by() and then summarise():\n\n# take the data |&gt; \n# and, grouped by the levels of the \"mygroups\" variable,\n# summarise() it so that there is a column called \"summary_col\", which\n# is the mean of the \"variable1\" column for each group. \ndata |&gt;\n    group_by(mygroups) |&gt;\n    summarise(\n        summary_col = mean(variable1)\n    )\n\n\n\nTidyverse\n\npwords |&gt; \n    group_by(strength_cat) |&gt;\n    summarise(\n        mean_cracked = mean(cracked)\n    )\n\n\n\nBase R\nThis is less easy. There are functions in Base R that can do similar things, but we’re not going to teach those here. You could envisage getting all the same values by doing:\n\nmean(pwords$cracked[pwords$strength_cat == \"weak\"])\nmean(pwords$cracked[pwords$strength_cat == \"medium\"])\nmean(pwords$cracked[pwords$strength_cat == \"strong\"])",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#boxplots",
    "href": "03_measurement.html#boxplots",
    "title": "3: Measurement & Distributions",
    "section": "Boxplots",
    "text": "Boxplots\nBoxplots provide a useful way of visualising the interquartile range (IQR). You can see what each part of the boxplot represents in Figure Figure 4.\n\n\n\n\n\n\n\n\nFigure 4: Anatomy of a boxplot\n\n\n\n\n\nWe can create a boxplot of our age variable using the following code:\n\n# Notice, we put strength on the x axis, making the box plot vertical. \n# If we had set aes(y = strength) instead, then it would simply be rotated 90 degrees \nggplot(data = pwords, aes(x = strength)) +\n  geom_boxplot()",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#histograms",
    "href": "03_measurement.html#histograms",
    "title": "3: Measurement & Distributions",
    "section": "Histograms",
    "text": "Histograms\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these map to how visualisations of numeric variables look.\nWe can visualise numeric data using a histogram, which shows the frequency of values which fall within bins of an equal width.\nTo do this, we’re going to use some new data, on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests. The data are available at https://uoepsy.github.io/data/wechsler.csv\n\nwechsler &lt;- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n\n\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\nWe can specifiy the width of the bins:\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\n\nLet’s take a look at the means and standard deviations of participants’ scores on the other tests (the test1 and test2 variables).\nNote how nicely we can do this with our newfound tidyverse skills!\n\nwechsler |&gt; \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n\n# A tibble: 1 × 4\n  mean_test1 sd_test1 mean_test2 sd_test2\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1       49.3     7.15       51.2     14.4\n\n\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1.\n\n\n\n\n\n\n\n\n\n\nDefining moments\nThe “moments” of a distribution are the metrics that relate to the shape of that distribution. We’ve already seen the primary two moments that define the shapes of these distributions: the mean and the variance. The mean moves the distribution right or left, and the variance makes the distribution wider or narrower.\nThere are two more, “skewness” and “kurtosis” which tend to be of less focus of investigation (the questions we ask tend to be mainly concerned with means and variances). Skewness is a measure of asymmetry in a distribution. Distributions can be positively skewed or negatively skewed, and this influences our measures of central tendency and of spread to different degrees. The kurtosis is a measure of how “pointy” vs “rounded” the shape of a distribution is.",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#density",
    "href": "03_measurement.html#density",
    "title": "3: Measurement & Distributions",
    "section": "Density",
    "text": "Density\nIn addition to grouping numeric data into bins in order to produce a histogram, we can also visualise a density curve.\nBecause there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, …), we could group the data into infinitely many bins. This is essentially what we are doing with a density curve.\nYou can think of “density” as a bit similar to the notion of “relative frequency” (or “proportion”), in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\nArea under the curve\nThink about the barplots we have been looking at in the exercises where we simulate dice rolling :\n\n# our function to simulate the roll of a die/some dice\ndice &lt;- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n# simulate 1000 rolls of a single die\nroll1000 &lt;- replicate(1000, dice(1))\n# tabulate and plot:\ntable(roll1000) |&gt;\n  barplot(ylab=\"count\")\n\n\n\n\n\n\n\n\nTo think about questions like “what proportion of 1000 rolls does the die land on 6?”, we are simply interested in the count of 6s divided by the count of all rolls:\n\ntab1000 &lt;- table(roll1000)\ntab1000\n\nroll1000\n  1   2   3   4   5   6 \n162 169 167 189 152 161 \n\ntab1000[6] / sum(tab1000)\n\n    6 \n0.161 \n\n\nSo Another way of thinking of this is that we are just dividing the count in each category by the total number. Or, Put another way, imagine we divide the area of each bar by the total area. The area now sums to 1, and our question is asking about the ratio of the red area to the total area (grey + red):\n\n\n\n\n\n\n\n\n\nNothing really changes with a density curve! If we want to ask what proportion of our distribution of IQ scores is &gt;120, then we are asking about the area under the curve that is to the right of 120:\n\n\n\n\n\n\n\n\n\nIt looks like about a third, maybe a little less. Let’s calculate this proportion directly:\n\nsum(wechsler$iq&gt;110) / length(wechsler$iq)\n\n[1] 0.2\n\n\nIt might seem a little odd to think about area under the curve when we are asking about “what proportion of the data is …?”. If we have the data, then we can just calculate the answer (like we did above). However, a lot of statistics is really concerned with the probability of events. When we discuss probability, we move from talking about a specific set of observed data to thinking about a theoretical/mathematical model that defines the way in which data is generated. This where it becomes more useful to think about distributions in a more abstract sense.\nFor instance, with a fair six-sided die, we have a probability distribution (Figure 5) in which each side is given the probability \\(\\frac{1}{6}\\): \\[\n\\begin{gather*}\nP(x) = \\begin{cases}\n  \\frac{1}{6} & \\text{if $x \\in \\{1,2,3,4,5,6\\}$}\\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n\\end{gather*}\n\\] Instead of rolling a die, suppose that we are picking a person off the street and measuring their IQ. Given that IQ scales are designed to have a mean of 100 and standard deviation of 15, what is the probability that we pick a person with an IQ of greater than 110?\n\n\n\n\n\n\n\n\nFigure 5: Left: Discrete probability distribution of a fair six-sided die. Right: Continuous probability distribution of IQ scores",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "03_measurement.html#footnotes",
    "href": "03_measurement.html#footnotes",
    "title": "3: Measurement & Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthink about what sum(table(pwords$type)) is doing. it’s counting all the values in the table. so it’s going to give us the total↩︎",
    "crumbs": [
      "3: Measurement & Distributions"
    ]
  },
  {
    "objectID": "05_inference.html",
    "href": "05_inference.html",
    "title": "5: Foundations of Inference",
    "section": "",
    "text": "This reading:\n\nHow do we quantify uncertainty due to sampling?\n\nHow can we make decisions (what to believe/how to act, etc) that take uncertainty into account?\n\nHow likely are we to make the wrong decision?\nWe use statistics primarily to estimate parameters in a population. Whether we are polling people to make predictions about the proportion of people who will vote for a certain party in the next election, or conducting a medical trial and assessing the change in blood pressure for patients given drug X vs those given a placebo in order to decide whether to put the drug into circulation in health service.\nWe have seen this already last week: We observed a sample of peoples’ life satisfaction ratings (scale 0-100), and we wanted to use these to make some statement about the wider population, such as “the average life-satisfaction rating is ?? out of 100”. So we use the mean of our sample, as an estimate of the mean of the population.",
    "crumbs": [
      "5: Foundations of Inference"
    ]
  },
  {
    "objectID": "05_inference.html#test-statistics-p-values",
    "href": "05_inference.html#test-statistics-p-values",
    "title": "5: Foundations of Inference",
    "section": "Test-statistics & p-values",
    "text": "Test-statistics & p-values\nThe p-value is a formal way of testing a statistic against a null hypothesis. To introduce the p-value, instead of thinking first about what we have observed in our sample, we need to think about what we would expect to observe if our null hypothesis is true.\nWith our Stroop Task example, our null hypothesis is that there is no difference between matching and mismatching conditions (\\(H_0: \\mu = 0\\)). Under \\(H_0\\), the average ‘mismatching-matching’ score in the population is zero, and we would expect most of the samples we might take to have a mean score of close to this (not exactly 0, but centered around 0). We saw above that we could express the sampling distribution of means taken from samples of size \\(n=131\\) using the standard error. Under \\(H_0\\) we would expect the samples of \\(n=131\\) we might take to have means that follow something like the distribution in Figure 3. We can think of this as the sampling distribution of \\(\\bar{x}\\), but centered on our null hypothesis (in this case, \\(\\mu = 0\\)). We call this the ‘null distribution’.\n\n\n\n\n\n\n\n\nFigure 3: Sampling distribution for mean of sample size 131, assuming population mean = 0. Observed sample mean shown in red\n\n\n\n\n\n\nTest-statistic\nThe first step now is to create a test-statistic. That is, a statistic that tell us, in some standardised units, how big our observed effect is from the null hypothesis (i.e. in this case, how far from \\(\\mu=0\\) our sample mean is).\nThe straightforward way to do this is to express how far away from \\(\\mu=0\\) our sample mean is in terms of standard errors. We’ll call our test statistic \\(Z\\):\n\\[\nZ = \\frac{\\text{estimate}-\\text{null}}{SE}\n\\]\nOur mean and standard error are:\n\nmean(stroopdata$diff)\n\n[1] 2.402977\n\nsd(stroopdata$diff) / sqrt(nrow(stroopdata))\n\n[1] 0.4382302\n\n\nSo our test-statistic is \\[\nZ = \\frac{2.40 - 0}{0.438} = 5.479\n\\]\n\n\np-value\nWe can now calculate how likely it is to see values at least as extreme as our observed test-statistic, if the null is true.\nIf the null hypothesis is true (there was no ‘mismatching-matching’ difference) then we would expect Z-statistics to be normally distributed with a mean of 0 and a standard deviation of 1.\nWe have seen the process of how we might calculate a probability from a distribution like this already: the pnorm() function gives us the area of a distribution to the one side of a given value:\n\npnorm(??, mean = 0, sd = 1, lower.tail = FALSE)\n\n\n\n\n\n\n\n\n\nFigure 4: pnorm() provides us with a p-value for a z-statistic\n\n\n\n\n\nRemember, our Z-statistic we calculated above is 5.479. If the null hypothesis were true then the probability that we would see a sample (\\(n=131\\)) with a Z-statistic at least that large is:\n\npnorm(5.479, lower.tail = FALSE)\n\n[1] 2.138682e-08\n\n\nwhich is R’s way of printing 0.00000002138682.\nThere is one last thing, and that the direction of our hypotheses. Recall from earlier that we stated \\(H_0: \\mu = 0\\) and \\(H_1: \\mu \\neq 0\\). This means that we are interested in the probability of getting results this far away from 0 in either direction.\nWe are interested in both tails:\n\n\n\n\n\n\n\n\nFigure 5: 2*pnorm gives the two tails\n\n\n\n\n\n\n2 * pnorm(5.479, lower.tail = FALSE)\n\n[1] 4.277364e-08\n\n\nor \\(p =\\) 0.00000004277364.\n\np-value\nThe p-value is the probability4 that we observe a test statistic at least as extreme as the one we observed, assuming the null hypothesis \\(H_0\\) to be true.",
    "crumbs": [
      "5: Foundations of Inference"
    ]
  },
  {
    "objectID": "05_inference.html#making-decisions",
    "href": "05_inference.html#making-decisions",
    "title": "5: Foundations of Inference",
    "section": "Making Decisions",
    "text": "Making Decisions\nNow that we have our p-value of 0.00000004277364, we need to use it to make a decision about our hypotheses.\nTypically, we pre-specify the probability level at which we will consider results to be so unlikely to have arisen from the null distribution that we will take them as evidence to reject the null hypothesis. This pre-specified level is commonly referred to as \\(\\alpha\\) (“alpha”). Setting \\(\\alpha = 0.05\\) means that we will reject \\(H_0\\) when we get a result which is extreme enough to only occur 0.05 (5%) of the time or less if the \\(H_0\\) is true.\nIn our case, 0.00000004277364 \\(&lt; 0.05\\), so we reject the null hypothesis that there is no difference in the mismatching/matching conditions of the Stroop Task.\n\nThere’s a lot of convention to how we talk about NHST, but the typical process is as follows:\n\nClearly specify the null and alternative hypotheses.\n\nSpecify \\(\\alpha\\)\nCalculate statistic\nCompute p-value\n\nIf \\(p&lt;\\alpha\\), then reject the null hypothesis.\nIf \\(p\\geq\\alpha\\), then fail to reject* the null hypothesis.\n\n\n*Note, we don’t “accept” anything, we just “reject” or “fail to reject” the null hypothesis. Think of it like a criminal court, and we are trying the null hypothesis - \\(H_0\\) is “innocent until proven guilty”.",
    "crumbs": [
      "5: Foundations of Inference"
    ]
  },
  {
    "objectID": "05_inference.html#making-mistakes",
    "href": "05_inference.html#making-mistakes",
    "title": "5: Foundations of Inference",
    "section": "Making Mistakes",
    "text": "Making Mistakes\nWhether our eventual decision is a) reject the null hypothesis, or b) fail to reject the null hypothesis, there’s always a chance that we might be making a mistake. There are actually two different types of mistakes we might make. An often used analogy (Figure 6) is the idea of criminal trials in which an innocent person can be wrongfully convicted, or a guilty person can be set free.\n\n\n\n\n\n\n\n\nFigure 6: Making errors in NHST is like a criminal court making errors in its decision on the defendent\n\n\n\n\n\nWe can actually quantify the chance that we’re making errors in our different decisions. Thinking back to the definition of a p-value, it is the probability of seeing our results if the null hypothesis is true. If we make a decision to reject the null hypothesis based on whether \\(p&lt;\\alpha\\), then the probability that this decision is a mistake is \\(\\alpha\\).\nThe probability that we the other sort of error (failing to reject the null hypothesis when the null hypothesis is actually false), we denote with \\(\\beta\\).\nDoing statistics is partly a matter of balancing these possibilities. If we used a very low \\(\\alpha\\)-level (e.g. we reject when \\(p&lt;.0001\\) rather than \\(p&lt;.05\\)) then we increase the probability of making a type II error.\n\nTypes of Errors in NHST\n\n\n\n\n\n\n\n\nFigure 7: Probabilities of making different errors in NHST\n\n\n\n\n\n\n\nPower (\\(1-\\beta\\))\nA key notion in conducting studies is “statistical power”. Studies want to increase the probability of correctly rejecting the null hypothesis (i.e. correctly identifying that there is something more than chance going on).\nThis is the bottom right cell of the tables in Figure 6 and Figure 7. We know that this will depend on the \\(\\alpha\\)-level that we choose, but there are other important factors that influence \\(1-\\beta\\):\n\npower increases as sample size increases\n\ne.g. it’s easier to determine that cats weigh less than dogs if we measure 100 animals vs if we measure only 10 animals\n\npower increases the farther away the true value is from the null hypothesis value\n\ne.g. it’s easier to determine that cats weigh less than elephants than it is to determine that cats weigh less than dogs",
    "crumbs": [
      "5: Foundations of Inference"
    ]
  },
  {
    "objectID": "05_inference.html#footnotes",
    "href": "05_inference.html#footnotes",
    "title": "5: Foundations of Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html↩︎\n For other intervals, such as a 90% interval, we need to know the point at which 5% is either side of a normal distribution (i.e., giving us the middle 90%). qnorm(c(0.05,0.95)) will give us 1.64, which we then put into our construction of the interval: \\(90\\%\\, CI = \\bar{x} \\pm 1.64 \\times SE\\).↩︎\nThink about an example where our question is about whether there is a difference in variable \\(Y\\) between groups A, B, C and D. Around what should we construct our interval? Around the difference \\(\\bar{Y}_A - \\bar{Y}_B\\) (difference between A and B’s average scores on \\(Y\\)), or \\(\\bar{Y}_A - \\bar{Y}_C\\), or \\(\\bar{Y}_B - \\bar{Y}_D\\)?↩︎\nWhat we have been seeing is that probabilities in NHST are defined as the relative frequency of an event over many trials (as “many” \\(\\to \\infty\\)). This requires assuming some features of the data generating process which guides what the “many trials” would look like (e.g., that there is no effect). The \\(p\\)-value is the probability of observing results as or more extreme than the data, if the data were really generated by a hypothesised chance process.↩︎",
    "crumbs": [
      "5: Foundations of Inference"
    ]
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "Notes for Wizards\n\n\n\n\n\nhere’s a note!\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\n\n\n\n\nlearning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n\nSolution\n\n\n\nSolution 1. solution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin flip. For a fair coin, the chance of getting a heads/tails for any given flip is 0.5.\nWe can simulate the number of “heads” in a single fair coin flip with the following code (because it is a single flip, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\n\nWe can simulate the number of “heads” in 8 fair coin flips with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin flips? Answer: 4! Doing another 8 flips:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin flips, but to do 1000 sets of 8 flips:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  4  27 128 211 277 199 116  35   3"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nSo what is the probability of observing \\(k\\) heads in \\(n\\) flips of a fair coin?\nAs coin flips are independent, we can calculate probability using the product rule (\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent).\nSo the probability of observing 2 heads in 2 flips is \\(0.5 \\cdot 0.5 = 0.25\\)\nWe can get to this probability using dbinom():\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 flips, those two heads could occur in various ways:\n\n\n\n\n\n\n\n\nWays to get 2 heads in 8 flips\n\n\n\n\nHTHTTTTT\n\n\nTHTTHTTT\n\n\nTHHTTTTT\n\n\nTHTTTHTT\n\n\nTTHTTHTT\n\n\nTTTTHTTH\n\n\nTTTTHHTT\n\n\nTTTTTHHT\n\n\nTHTTTTTH\n\n\nTTTTTTHH\n\n\n...\n\n\n\n\n\n\n\nAs it happens, there are 28 different ways this could happen.2\nThe probability of getting 2 heads in 8 flips of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin flips (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given flip)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin flips is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nSo how does likelihood differ?\nFor likelihood, we are interested in hypotheses about or models of our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?). Do we think it is biased to land on heads 60% of the time? or 30% of the time? All of these are different ‘models’.\nTo consider these hypotheses, we need to observe some data - we need to have a given number of flips, and the resulting number of heads.\nWhereas when discussing probability, we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given flip, for the likelihood we are fixing the number of heads observed, and can make statements about different possible parameters that might govern the coin’s behaviour.\nFor example, let’s suppose we did observe 2 heads in 8 flips, what is the probability of seeing this data given various parameters?\nHere, our parameter (the probability that we think the coin lands on heads) can take any real number between from 0 to 1, but let’s do it for a selection:\n\npossible_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, possible_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin flips, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). The idea that the coin is fair (0.5 probability) is more likely. The most likely parameter is 0.25 (because \\(\\frac{2}{8}=0.25\\)).\nYou can visualise this below:"
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nE.g. above we were talking about \\(P(\\text{2 heads in 8 flips}\\vert \\text{fair coin})\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\).\nGiven that we know that if we have a specific value for \\(\\theta\\), then \\(P(d \\vert \\theta)\\) will give us the probability of observing \\(d\\), we can ask “what value of \\(\\theta)\\) will maximise the probability of observing \\(d\\)?”.\nThis will sometimes get written as \\(\\mathcal{L}(\\theta \\vert d)\\) as the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. There are other ways to do statistics (not covered in this course) - e.g., in Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎\nIf you really want to see them all, try running combn(8, 2) in your console.↩︎"
  }
]