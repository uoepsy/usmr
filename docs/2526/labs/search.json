[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Univariate Statistics and Methodology in R Workbook",
    "section": "",
    "text": "This site contains weekly exercises for the USMR course.\nAt the end of each week, solutions (where these are not already available) will be made visible directly beneath each question.\n\nAbout USMR\nUnivariate Statistics and Methodology in R (USMR) is a semester long crash-course aimed at providing Masters students in psychology with a competence in standard statistical methodologies and data analysis using R. Typically the analyses taught in this course are relevant for when there is just one source of variation - i.e. when we are interested in a single outcome measured across a set of independent observations. The first half of the course covers the fundamentals of statistical inference using a simulation-based approach, and introduces students to working with R & RStudio. The latter half of the course focuses on the general linear model, emphasising the fact that many statistical methods are simply special cases of this approach. This course introduces students to statistical modelling and empowers them with tools to analyse richer data and answer a broader set of research questions."
  },
  {
    "objectID": "08_ex.html",
    "href": "08_ex.html",
    "title": "W8: Scaling | Categorical Predictors",
    "section": "",
    "text": "neuronews.csv\nThis dataset is from a study1 looking at the influence of presenting scientific news with different pictures on how believable readers interpret the news.\n120 participants took part in the study. Participation involved reading a short article about some research in neuroscience, and then rating how credible they found the research. Participants were randomly placed into one of three conditions, in which the article was presented a) in text alone, b) with a picture of a brain, or c) with a picture of a brain and a fancy looking (but unrelated to the research) graph. They rated credibility using a sliding scale from 0 to 100, with higher values indicating more credibility.\nThe data is available at https://uoepsy.github.io/data/usmr_neuronews.csv.\n\n\n\n\n\n\nusmr_neuronews.csv data dictionary\n\n\nvariable\ndescription\n\n\n\n\npid\nParticipant ID\n\n\nname\nParticipant Name\n\n\ncondition\nCondition (text-only / text+brain / text+brain+graph)\n\n\ncredibility\nCredibility rating (0-100)\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and take a look around (this is almost always the first thing to do!)\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\nnndat &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_neuronews.csv\")\n\nhist(nndat$credibility)\n\n\n\n\n\n\n\n# geom jitter is a way of randomly 'jittering' points so that the don't overlap\n# i want them to be the right height, so no jitter in height, but i'll give them a little width jitter\nggplot(nndat, aes(x=condition,y=credibility)) +\n   geom_jitter(height = 0, width = .2)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nFit a model examining whether credibility of the research article differs between conditions.\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nmod1 &lt;- lm(credibility ~ condition, data = nndat)\n\n\n\n\n\nQuestion 3\n\n\nExamine the summary output of the model. Do conditions differ in credibility ratings? If so, how?\nNote that these questions are sort of the same, but sort of different. Which bits of the summary output answer each question?\n\n\n\n\n\n\nHints\n\n\n\n\n\n“Do conditions differ”? is an overall question, not asking about differences between specific levels. You can find a way to test this question either at the bottom of the summary() output, or by comparing it with a model without condition differences in it (see Chapter 14 #testing-group-differences).\n“How do conditions differ?” is more specific, and will require us to look at something that tests between specific groups (Chapter 14 #testing-differences-between-specific-groups).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. Traditional stats (where we had to do stuff by hand) would do something called “ANOVA” which gets at the overall question “do groups differ?”\nAs it happens, this is is actually just a linear model in disguise. The info is also at the bottom of the summary output. Because we have just one predictor in the model (condition) it means that the overall model fit is essentially asking “does condition make a difference?”:\n\nsummary(mod1)\n\nF-statistic: 3.246 on 2 and 117 DF,  p-value: 0.04245\n\nConditions significantly differed in credibility ratings \\(F(2,117) = 3.25, p = .0425\\)\n\nIt’s exactly the same thing as if we did:\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: credibility\n           Df Sum Sq Mean Sq F value  Pr(&gt;F)  \ncondition   2    674  337.02  3.2464 0.04245 *\nResiduals 117  12146  103.81                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr frame it as a model comparison:\n\nmod0 &lt;- lm(credibility ~ 1, data = nndat)\nanova(mod0, mod1)\n\nAnalysis of Variance Table\n\nModel 1: credibility ~ 1\nModel 2: credibility ~ condition\n  Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    119 12820                              \n2    117 12146  2    674.03 3.2464 0.04245 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is then the coefficients that are telling us how the conditions differ:\n\nsummary(mod1)\n\n\nCall:\nlm(formula = credibility ~ condition, data = nndat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.4476  -6.6086  -0.3778   6.5657  26.8589 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 59.663      1.611  37.035   &lt;2e-16 ***\nconditiontext+brain          4.909      2.278   2.155   0.0332 *  \nconditiontext+brain+graph    5.138      2.278   2.255   0.0260 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.19 on 117 degrees of freedom\nMultiple R-squared:  0.05258,   Adjusted R-squared:  0.03638 \nF-statistic: 3.246 on 2 and 117 DF,  p-value: 0.04245\n\n\n\nCompared to when presented in text-only, conditions where the article was presented alongside a picture of a brain, or alongside both a brain-picture and a graph, resulted in higher average credibility ratings. Including a picture of a brain was associated with a 4.91 increase in credibility over the text-only article (\\(b = 4.91\\), \\(t(117)=2.15\\), \\(p=0.033\\)), and including both a brain-picture and a graph was associated with a 5.14 higher average credibility rating (\\(b = 5.14\\), \\(t(117)=2.26\\), \\(p=0.026\\)).\n\n\n\n\n\nQuestion 4\n\n\nLet’s prove something to ourselves.\nBecause we have no other predictors in the model, it should be possible to see how the coefficients from our model map exactly to the group means.\nCalculate the mean credibility for each condition, and compare with your model coefficients.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo calculate group means, we can use group_by() and summarise()!\n\n\n\n\n\n\n\n\nSolution Part 1 - calculate group means\n\n\n\nSolution 4. \n\nnndat |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    meancred = mean(credibility)\n  )\n\n# A tibble: 3 × 2\n  condition        meancred\n  &lt;chr&gt;               &lt;dbl&gt;\n1 text+brain           64.6\n2 text+brain+graph     64.8\n3 text-only            59.7\n\n\n\n\n\n\n\nSolution Part 2 - compare to the coefficients\n\n\n\nSolution 5. Here are our model coefficients:\n\ncoef(mod1)\n\n              (Intercept)       conditiontext+brain conditiontext+brain+graph \n                59.662662                  4.909058                  5.138197 \n\n\nAnd here are our means:\n\nnndat |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    meancred = mean(credibility)\n  )\n\n# A tibble: 3 × 2\n  condition        meancred\n  &lt;chr&gt;               &lt;dbl&gt;\n1 text+brain           64.6\n2 text+brain+graph     64.8\n3 text-only            59.7\n\n\nWe can see that the intercept is the mean of the text-only group.\nThe next coefficient - named “conditiontext+brain” is the difference from the text-only group to the text+brain group.\nThe final coefficient - named “conditiontext+brain+graph” is the difference from the text-only group to the text+brain+graph group.\nSo this means we can get to our group means by using:\n\ncoef(mod1)[c(1,2)] |&gt; sum() # the mean of text+brain\n\n[1] 64.57172\n\ncoef(mod1)[c(1,3)] |&gt; sum() # the mean of text+brain+graph\n\n[1] 64.80086"
  },
  {
    "objectID": "08_ex.html#footnotes",
    "href": "08_ex.html#footnotes",
    "title": "W8: Scaling | Categorical Predictors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnot a real one, but inspired very loosely by this one↩︎\napparently coughing is a method of immediately lowering heart rate!↩︎"
  },
  {
    "objectID": "05_ex.html",
    "href": "05_ex.html",
    "title": "W5: Covariance, Correlation & Linear Regression",
    "section": "",
    "text": "Question 1\n\n\nGo to http://guessthecorrelation.com/ and play the “guess the correlation” game for a little while to get an idea of what different strengths and directions of \\(r\\) can look like.\n\n\n\n\n\n\nData: Sleep levels and daytime functioning\nA researcher is interested in the relationship between hours slept per night and self-rated effects of sleep on daytime functioning. She recruited 50 healthy adults, and collected data on the Total Sleep Time (TST) over the course of a seven day period via sleep-tracking devices.\nAt the end of the seven day period, participants completed a Daytime Functioning (DTF) questionnaire. This involved participants rating their agreement with ten statements (see Table 1). Agreement was measured on a scale from 1-5. An overall score of daytime functioning can be calculated by:\n\nreversing the scores for items 4,5 and 6 (because those items reflect agreement with positive statements, whereas the other ones are agreement with negative statement);\nsumming the scores on each item; and\nsubtracting the sum score from 50 (the max possible score). This will make higher scores reflect better perceived daytime functioning.\n\nThe data is available at https://uoepsy.github.io/data/sleepdtf.csv\n\n\n\n\nTable 1: Daytime Functioning Questionnaire\n\n\n\n\n\n\n\n\n\nItem\nStatement\n\n\n\n\nItem_1\nI often felt an inability to concentrate\n\n\nItem_2\nI frequently forgot things\n\n\nItem_3\nI found thinking clearly required a lot of effort\n\n\nItem_4\nI often felt happy\n\n\nItem_5\nI had lots of energy\n\n\nItem_6\nI worked efficiently\n\n\nItem_7\nI often felt irritable\n\n\nItem_8\nI often felt stressed\n\n\nItem_9\nI often felt sleepy\n\n\nItem_10\nI often felt fatigued\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nLoad the required libraries (probably just tidyverse for now), and read in the data.\nCalculate the overall daytime functioning score, following the criteria outlined above, and make this a new column in your dataset.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo reverse items 4, 5 and 6, we we need to make all the scores of 1 become 5, scores of 2 become 4, and so on… What number satisfies all of these equations: ? - 5 = 1, ? - 4 = 2, ? - 3 = 3?\nTo quickly sum across rows, you might find the rowSums() function useful (you don’t have to use it though)\nIf my items were in columns between 4 to 15:\n\ndataframe$sumscore = rowSums(dataframe[, 4:15])\n\n\n\n\n\n\n\n\n\nSolution Part 1 - Reversing Items\n\n\n\nSolution 1. \n\nsleepdtf &lt;- read_csv(\"https://uoepsy.github.io/data/sleepdtf.csv\")\nsummary(sleepdtf)\n\n      TST             item_1         item_2         item_3         item_4    \n Min.   : 4.900   Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00  \n 1st Qu.: 7.225   1st Qu.:1.00   1st Qu.:2.00   1st Qu.:1.25   1st Qu.:1.00  \n Median : 7.900   Median :1.00   Median :2.00   Median :2.00   Median :1.00  \n Mean   : 8.004   Mean   :1.58   Mean   :2.46   Mean   :2.38   Mean   :1.26  \n 3rd Qu.: 9.025   3rd Qu.:2.00   3rd Qu.:3.00   3rd Qu.:3.00   3rd Qu.:1.00  \n Max.   :11.200   Max.   :3.00   Max.   :5.00   Max.   :5.00   Max.   :3.00  \n     item_5         item_6         item_7         item_8        item_9    \n Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.0   Min.   :1.00  \n 1st Qu.:2.00   1st Qu.:2.00   1st Qu.:1.00   1st Qu.:2.0   1st Qu.:2.00  \n Median :2.00   Median :3.00   Median :2.00   Median :2.5   Median :3.00  \n Mean   :2.36   Mean   :2.78   Mean   :2.04   Mean   :2.5   Mean   :2.96  \n 3rd Qu.:3.00   3rd Qu.:4.00   3rd Qu.:3.00   3rd Qu.:3.0   3rd Qu.:4.00  \n Max.   :4.00   Max.   :5.00   Max.   :4.00   Max.   :4.0   Max.   :5.00  \n    item_10    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :3.00  \n Mean   :2.54  \n 3rd Qu.:3.00  \n Max.   :5.00  \n\n\nTo reverse the items, we can simply do 6 minus the score:\n\nsleepdtf &lt;- \n  sleepdtf |&gt; mutate(\n    item_4=6-item_4,\n    item_5=6-item_5,\n    item_6=6-item_6\n  ) \n\n\n\n\n\n\nSolution Part 2 - Creating a scale score\n\n\n\nSolution 2. We can now do one of two things.\nEither, we use rowSums(), and subtract the sum scores from from 50 (the max score):\n\nsleepdtf$dtf &lt;- 50-rowSums(sleepdtf[, 2:11])\n\nOr, the tidyverse way to do this would be:\n\nsleeptdf &lt;- \n  sleepdtf |&gt; \n  mutate(\n    dtf = 50 - (item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10)\n  )\n\n\n\n\n\nQuestion 3\n\n\nCalculate the correlation between the total sleep time (TST) and the overall daytime functioning score calculated in the previous question.\nConduct a test to establish the probability of observing a correlation this strong in a sample of this size assuming the true correlation to be 0.\nWrite a sentence or two summarising the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can do this all with one function, see Chapter 9 #correlation-test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\ncor.test(sleepdtf$TST, sleepdtf$dtf)\n\n\n    Pearson's product-moment correlation\n\ndata:  sleepdtf$TST and sleepdtf$dtf\nt = 6.244, df = 48, p-value = 1.062e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4807039 0.7989417\nsample estimates:\n      cor \n0.6694741 \n\n\n\nThere was a strong positive correlation between total sleep time and self-reported daytime functioning score (\\(r\\) = 0.67, \\(t(48)\\) = 6.24, \\(p &lt; .001\\)) in the current sample. As total sleep time increased, levels of self-reported daytime functioning increased.\n\n\n\n\n\nQuestion 4 (open-ended)\n\n\nThink about this relationship in terms of causation.\n Claim: Less sleep causes poorer daytime functioning.\n Why might it be inappropriate to make the claim above based on these data alone? Think about what sort of study could provide stronger evidence for such a claim.\n\n\n\n\n\n\nThings to think about:\n\n\n\n\n\n\ncomparison groups.\n\nrandom allocation.\n\nmeasures of daytime functioning.\n\nmeasures of sleep time.\n\nother (unmeasured) explanatory variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: Education SIMD Indicators\nThe Scottish Government regularly collates data across a wide range of societal, geographic, and health indicators for every “datazone” (small area) in Scotland.\nThe dataset at https://uoepsy.github.io/data/simd20_educ.csv contains some of the education indicators (see Table 2).\n\n\n\n\nTable 2: Education indicators from the 2020 SIMD data\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nintermediate_zone\nAreas of scotland containing populations of between 2.5k-6k household residents\n\n\nattendance\nAverage School pupil attendance\n\n\nattainment\nAverage attainment score of School leavers (based on Scottish Credit and Qualifications Framework (SCQF))\n\n\nuniversity\nProportion of 17-21 year olds entering university\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nConduct a test of whether there is a correlation between school attendance and school attainment in Scotland.\nPresent and write up the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe readings have not included an example write-up for you to follow. Try to follow the logic of those for t-tests and \\(\\chi^2\\)-tests.\n\ndescribe the relevant data\nexplain what test was conducted and why\npresent the relevant statistic, degrees of freedom (if applicable), statement on p-value, etc.\nstate the conclusion.\n\nBe careful figuring out how many observations your test is conducted on. cor.test() includes only the complete observations.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nsimd &lt;- read_csv(\"https://uoepsy.github.io/data/simd20_educ.csv\")\n\nHere are the means of the two variables. We should remember that these calculations will include some observations which have missing data on the other variable.\n\nsimd |&gt; \n  summarise(\n    m_attendance = mean(attendance, na.rm = TRUE),\n    m_attainment = mean(attainment, na.rm = TRUE)\n)\n\n# A tibble: 1 × 2\n  m_attendance m_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;\n1        0.811         5.53\n\n\nInstead, to match with our analysis, we might be inclined to filter our data to complete data:\n\nsimd_comp &lt;- simd |&gt; \n  filter(!is.na(attendance) & !is.na(attainment))\n\nsimd_comp |&gt;\n  summarise(\n    m_attendance = mean(attendance),\n    m_attainment = mean(attainment),\n    sd_attendance = sd(attendance),\n    sd_attainment = sd(attainment)\n)\n\n# A tibble: 1 × 4\n  m_attendance m_attainment sd_attendance sd_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.811         5.53        0.0696         0.386\n\n\n\ncor.test(simd_comp$attendance, simd_comp$attainment)\n\n\n    Pearson's product-moment correlation\n\ndata:  simd_comp$attendance and simd_comp$attainment\nt = 51.495, df = 1242, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8066637 0.8421951\nsample estimates:\n      cor \n0.8252442 \n\n\n\nA correlation test was conducted to assess whether there is a relationship between an area’s average school attendance, and its average school attainment level. A total of 1244 geographical areas were included in the analysis, with a mean school attendance of 0.81 (SD = 0.07) and a mean school attainment score of 5.53 (SD = 0.39).\nThere was a strong positive correlation between a geographical area’s level of school attendance and its school attainment (\\(r\\) = 0.83, \\(t(1242)\\) = 51.5, \\(p &lt; 0.001\\)). We therefore reject the null hypothesis that there is no correlation between an area’s school attendance and attainment. Figure 1 provides a visualisation of the relationship.\n\n\nCode\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  labs(x = \"School attendance\",\n       y = \"School attainment\")\n\n\n\n\n\n\n\n\nFigure 1: Positive relationship between geographical areas’ level of school attendance and school attainment\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: some extra plotting bits\n\n\n\n\n\nSometimes we may want to highlight certain parts of a plot. We can do that using the gghighlight package, and giving it a set of conditions (like we do for filter()) in order for it to decide which points to highlight.\nYou can see an example below.\nWe have also created the title by referring to the cor() function, and ’paste’ing it together to “r =”\n\nlibrary(gghighlight)\n\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  gghighlight( (attainment&gt;6 & attendance&lt;.75) | \n               attendance &gt; .95 | \n               (attendance &gt; .82 & attainment&lt;5),\n               label_key = intermediate_zone) + \n  labs(x = \"School attendance\",\n       y = \"School attainment\",\n       title = paste0(\"r = \",\n                       round(\n                         cor(simd_comp$attendance,\n                                  simd_comp$attainment),\n                         2)\n                       ))"
  },
  {
    "objectID": "05_ex.html#sleepy-time",
    "href": "05_ex.html#sleepy-time",
    "title": "W5: Covariance, Correlation & Linear Regression",
    "section": "",
    "text": "Data: Sleep levels and daytime functioning\nA researcher is interested in the relationship between hours slept per night and self-rated effects of sleep on daytime functioning. She recruited 50 healthy adults, and collected data on the Total Sleep Time (TST) over the course of a seven day period via sleep-tracking devices.\nAt the end of the seven day period, participants completed a Daytime Functioning (DTF) questionnaire. This involved participants rating their agreement with ten statements (see Table 1). Agreement was measured on a scale from 1-5. An overall score of daytime functioning can be calculated by:\n\nreversing the scores for items 4,5 and 6 (because those items reflect agreement with positive statements, whereas the other ones are agreement with negative statement);\nsumming the scores on each item; and\nsubtracting the sum score from 50 (the max possible score). This will make higher scores reflect better perceived daytime functioning.\n\nThe data is available at https://uoepsy.github.io/data/sleepdtf.csv\n\n\n\n\nTable 1: Daytime Functioning Questionnaire\n\n\n\n\n\n\n\n\n\nItem\nStatement\n\n\n\n\nItem_1\nI often felt an inability to concentrate\n\n\nItem_2\nI frequently forgot things\n\n\nItem_3\nI found thinking clearly required a lot of effort\n\n\nItem_4\nI often felt happy\n\n\nItem_5\nI had lots of energy\n\n\nItem_6\nI worked efficiently\n\n\nItem_7\nI often felt irritable\n\n\nItem_8\nI often felt stressed\n\n\nItem_9\nI often felt sleepy\n\n\nItem_10\nI often felt fatigued\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nLoad the required libraries (probably just tidyverse for now), and read in the data.\nCalculate the overall daytime functioning score, following the criteria outlined above, and make this a new column in your dataset.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo reverse items 4, 5 and 6, we we need to make all the scores of 1 become 5, scores of 2 become 4, and so on… What number satisfies all of these equations: ? - 5 = 1, ? - 4 = 2, ? - 3 = 3?\nTo quickly sum across rows, you might find the rowSums() function useful (you don’t have to use it though)\nIf my items were in columns between 4 to 15:\n\ndataframe$sumscore = rowSums(dataframe[, 4:15])\n\n\n\n\n\n\n\n\n\nSolution Part 1 - Reversing Items\n\n\n\nSolution 1. \n\nsleepdtf &lt;- read_csv(\"https://uoepsy.github.io/data/sleepdtf.csv\")\nsummary(sleepdtf)\n\n      TST             item_1         item_2         item_3         item_4    \n Min.   : 4.900   Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00  \n 1st Qu.: 7.225   1st Qu.:1.00   1st Qu.:2.00   1st Qu.:1.25   1st Qu.:1.00  \n Median : 7.900   Median :1.00   Median :2.00   Median :2.00   Median :1.00  \n Mean   : 8.004   Mean   :1.58   Mean   :2.46   Mean   :2.38   Mean   :1.26  \n 3rd Qu.: 9.025   3rd Qu.:2.00   3rd Qu.:3.00   3rd Qu.:3.00   3rd Qu.:1.00  \n Max.   :11.200   Max.   :3.00   Max.   :5.00   Max.   :5.00   Max.   :3.00  \n     item_5         item_6         item_7         item_8        item_9    \n Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.0   Min.   :1.00  \n 1st Qu.:2.00   1st Qu.:2.00   1st Qu.:1.00   1st Qu.:2.0   1st Qu.:2.00  \n Median :2.00   Median :3.00   Median :2.00   Median :2.5   Median :3.00  \n Mean   :2.36   Mean   :2.78   Mean   :2.04   Mean   :2.5   Mean   :2.96  \n 3rd Qu.:3.00   3rd Qu.:4.00   3rd Qu.:3.00   3rd Qu.:3.0   3rd Qu.:4.00  \n Max.   :4.00   Max.   :5.00   Max.   :4.00   Max.   :4.0   Max.   :5.00  \n    item_10    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :3.00  \n Mean   :2.54  \n 3rd Qu.:3.00  \n Max.   :5.00  \n\n\nTo reverse the items, we can simply do 6 minus the score:\n\nsleepdtf &lt;- \n  sleepdtf |&gt; mutate(\n    item_4=6-item_4,\n    item_5=6-item_5,\n    item_6=6-item_6\n  ) \n\n\n\n\n\n\nSolution Part 2 - Creating a scale score\n\n\n\nSolution 2. We can now do one of two things.\nEither, we use rowSums(), and subtract the sum scores from from 50 (the max score):\n\nsleepdtf$dtf &lt;- 50-rowSums(sleepdtf[, 2:11])\n\nOr, the tidyverse way to do this would be:\n\nsleeptdf &lt;- \n  sleepdtf |&gt; \n  mutate(\n    dtf = 50 - (item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10)\n  )\n\n\n\n\n\nQuestion 3\n\n\nCalculate the correlation between the total sleep time (TST) and the overall daytime functioning score calculated in the previous question.\nConduct a test to establish the probability of observing a correlation this strong in a sample of this size assuming the true correlation to be 0.\nWrite a sentence or two summarising the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can do this all with one function, see Chapter 9 #correlation-test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\ncor.test(sleepdtf$TST, sleepdtf$dtf)\n\n\n    Pearson's product-moment correlation\n\ndata:  sleepdtf$TST and sleepdtf$dtf\nt = 6.244, df = 48, p-value = 1.062e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4807039 0.7989417\nsample estimates:\n      cor \n0.6694741 \n\n\n\nThere was a strong positive correlation between total sleep time and self-reported daytime functioning score (\\(r\\) = 0.67, \\(t(48)\\) = 6.24, \\(p &lt; .001\\)) in the current sample. As total sleep time increased, levels of self-reported daytime functioning increased.\n\n\n\n\n\nQuestion 4 (open-ended)\n\n\nThink about this relationship in terms of causation.\n Claim: Less sleep causes poorer daytime functioning.\n Why might it be inappropriate to make the claim above based on these data alone? Think about what sort of study could provide stronger evidence for such a claim.\n\n\n\n\n\n\nThings to think about:\n\n\n\n\n\n\ncomparison groups.\n\nrandom allocation.\n\nmeasures of daytime functioning.\n\nmeasures of sleep time.\n\nother (unmeasured) explanatory variables."
  },
  {
    "objectID": "05_ex.html#attendance-and-attainment",
    "href": "05_ex.html#attendance-and-attainment",
    "title": "W5: Covariance, Correlation & Linear Regression",
    "section": "",
    "text": "Data: Education SIMD Indicators\nThe Scottish Government regularly collates data across a wide range of societal, geographic, and health indicators for every “datazone” (small area) in Scotland.\nThe dataset at https://uoepsy.github.io/data/simd20_educ.csv contains some of the education indicators (see Table 2).\n\n\n\n\nTable 2: Education indicators from the 2020 SIMD data\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nintermediate_zone\nAreas of scotland containing populations of between 2.5k-6k household residents\n\n\nattendance\nAverage School pupil attendance\n\n\nattainment\nAverage attainment score of School leavers (based on Scottish Credit and Qualifications Framework (SCQF))\n\n\nuniversity\nProportion of 17-21 year olds entering university\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nConduct a test of whether there is a correlation between school attendance and school attainment in Scotland.\nPresent and write up the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe readings have not included an example write-up for you to follow. Try to follow the logic of those for t-tests and \\(\\chi^2\\)-tests.\n\ndescribe the relevant data\nexplain what test was conducted and why\npresent the relevant statistic, degrees of freedom (if applicable), statement on p-value, etc.\nstate the conclusion.\n\nBe careful figuring out how many observations your test is conducted on. cor.test() includes only the complete observations.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nsimd &lt;- read_csv(\"https://uoepsy.github.io/data/simd20_educ.csv\")\n\nHere are the means of the two variables. We should remember that these calculations will include some observations which have missing data on the other variable.\n\nsimd |&gt; \n  summarise(\n    m_attendance = mean(attendance, na.rm = TRUE),\n    m_attainment = mean(attainment, na.rm = TRUE)\n)\n\n# A tibble: 1 × 2\n  m_attendance m_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;\n1        0.811         5.53\n\n\nInstead, to match with our analysis, we might be inclined to filter our data to complete data:\n\nsimd_comp &lt;- simd |&gt; \n  filter(!is.na(attendance) & !is.na(attainment))\n\nsimd_comp |&gt;\n  summarise(\n    m_attendance = mean(attendance),\n    m_attainment = mean(attainment),\n    sd_attendance = sd(attendance),\n    sd_attainment = sd(attainment)\n)\n\n# A tibble: 1 × 4\n  m_attendance m_attainment sd_attendance sd_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.811         5.53        0.0696         0.386\n\n\n\ncor.test(simd_comp$attendance, simd_comp$attainment)\n\n\n    Pearson's product-moment correlation\n\ndata:  simd_comp$attendance and simd_comp$attainment\nt = 51.495, df = 1242, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8066637 0.8421951\nsample estimates:\n      cor \n0.8252442 \n\n\n\nA correlation test was conducted to assess whether there is a relationship between an area’s average school attendance, and its average school attainment level. A total of 1244 geographical areas were included in the analysis, with a mean school attendance of 0.81 (SD = 0.07) and a mean school attainment score of 5.53 (SD = 0.39).\nThere was a strong positive correlation between a geographical area’s level of school attendance and its school attainment (\\(r\\) = 0.83, \\(t(1242)\\) = 51.5, \\(p &lt; 0.001\\)). We therefore reject the null hypothesis that there is no correlation between an area’s school attendance and attainment. Figure 1 provides a visualisation of the relationship.\n\n\nCode\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  labs(x = \"School attendance\",\n       y = \"School attainment\")\n\n\n\n\n\n\n\n\nFigure 1: Positive relationship between geographical areas’ level of school attendance and school attainment\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: some extra plotting bits\n\n\n\n\n\nSometimes we may want to highlight certain parts of a plot. We can do that using the gghighlight package, and giving it a set of conditions (like we do for filter()) in order for it to decide which points to highlight.\nYou can see an example below.\nWe have also created the title by referring to the cor() function, and ’paste’ing it together to “r =”\n\nlibrary(gghighlight)\n\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  gghighlight( (attainment&gt;6 & attendance&lt;.75) | \n               attendance &gt; .95 | \n               (attendance &gt; .82 & attainment&lt;5),\n               label_key = intermediate_zone) + \n  labs(x = \"School attendance\",\n       y = \"School attainment\",\n       title = paste0(\"r = \",\n                       round(\n                         cor(simd_comp$attendance,\n                                  simd_comp$attainment),\n                         2)\n                       ))"
  },
  {
    "objectID": "05_ex.html#monkey-exploration",
    "href": "05_ex.html#monkey-exploration",
    "title": "W5: Covariance, Correlation & Linear Regression",
    "section": "Monkey Exploration",
    "text": "Monkey Exploration\n\nData: monkeyexplorers.csv\nArcher, Winther & Gandolfi (2024)1 have conducted a study on monkeys! They were interested in whether younger monkeys tend to be more inquisitive about new things than older monkeys do. They sampled 108 monkeys ranging from 1 to 24 years old. Each monkey was given a novel object, the researchers recorded the time (in minutes) that each monkey spent exploring the object.\nFor this week, we’re going to be investigating the research question:\n\nDo older monkeys spend more/less time exploring novel objects?\n\nThe data is available at https://uoepsy.github.io/data/monkeyexplorers.csv and contains the variables described in Table 3\n\n\n\n\nTable 3: Data dictionary for monkeyexplorers.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nname\nMonkey Name\n\n\nage\nAge of monkey in years\n\n\nexploration_time\nTime (in minutes) spent exploring the object\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nFor this week, we’re going to be investigating the following research question:\n\nDo older monkeys spend more/less time exploring novel objects?\n\nRead in the data to your R session, then visualise and describe the marginal distributions of those variables which are of interest to us. These are the distribution of each variable (time spent exploring, and monkey age) without reference to the values of the other variables.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nYou could use, for example, geom_density() for a density plot or geom_histogram() for a histogram.\nLook at the shape, center and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\nDo you notice any extreme observations?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. \n\nmonkeyexp &lt;- read_csv(\"https://uoepsy.github.io/data/monkeyexplorers.csv\")\nhead(monkeyexp)\n\n# A tibble: 6 × 3\n  name            age exploration_time\n  &lt;chr&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 Ed Sheeran        8             13.1\n2 Vince Gill       17              6.8\n3 Jewel            13             12.8\n4 Jaden Smith      18              8.9\n5 Reba McEntire    16              1.1\n6 Metallica        13              5.7\n\n\nWe can plot the marginal distribution of these two continuous variables as density curves, and add a boxplot underneath to check for the presence of outliers. The width of the geom_boxplot() is always quite wide, so I want to make it narrower so that we can see it at the same time as the density plot. Deciding on the exact value for the width here is just trial and error:\n\nlibrary(patchwork)\n# the patchwork library allows us to combine plots together\nggplot(data = monkeyexp, aes(x = age)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Age (in years)\", \n       y = \"Probability density\") +\n\nggplot(data = monkeyexp, aes(x = exploration_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/175) +\n  labs(x = \"Time spent exploring a\\n novel object (in minutes)\", \n       y = \"Probability density\")\n\n\n\n\n\n\n\nFigure 2: Density plot and boxplot of monkey’s age and their time spent exploring novel objects\n\n\n\n\n\nThe plots suggests that the distributions of monkeys’ ages and the time they spend exploring novel objects are both unimodal. Most of the monkeys are between roughly 8 and 18 years old, and most of them spent between 7 and 12 minutes exploring the objects. The boxplots suggest an outlier in the distribution of exploration-times, with one monkeys spending more than \\(1.5 \\times IQR\\) beyond the 3rd quartile.\nTo further summarize a distribution, it is typical to compute and report numerical summary statistics such as the mean and standard deviation.\nAs we have seen, in earlier weeks, one way to compute these values is to use the summarise()/summarize() function from the tidyverse library:\n\nmonkeyexp |&gt; \n  summarize(\n    mean_age = mean(age), \n    sd_age = sd(age),\n    mean_exptime = mean(exploration_time),\n    sd_exptime = sd(exploration_time)\n    )\n\n# A tibble: 1 × 4\n  mean_age sd_age mean_exptime sd_exptime\n     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1     13.5   5.92         9.40       4.50\n\n\n\nThe marginal distribution of age is unimodal with a mean of 13.5 years, and a standard deviation of 5.9.\nThe marginal distribution of time-spent-exploring is unimodal with a mean of 9.4 years, and a standard deviation of 4.5.\n\n\n\n\n\nQuestion 7\n\n\nAfter we’ve looked at the marginal distributions of the variables of interest in the analysis, we typically move on to examining relationships between the variables.\nVisualise and describe the relationship between age and exploration-time among the monkeys in the sample.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about:\n\nDirection of association\nForm of association (can it be summarised well with a straight line?)\n\nStrength of association (how closely do points fall to a recognizable pattern such as a line?)\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. Because we are investigating how time-spent-exploring varies with monkeys’ ages, the exploration-time here is the dependent variable (on the y-axis), and age is the independent variable (on the x-axis).\n\nggplot(data = monkeyexp, aes(x = age, y = exploration_time)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Age (in years)\", \n       y = \"Time spent exploring a\\n novel object (in minutes)\")\n\n\n\n\n\n\n\nFigure 3: The relationship between monkeys’ age and time-spent-exploring.\n\n\n\n\n\nThere appears to be a moderate negative linear relationship between age and exploration time in these monkeys. Older monkeys appear to spend less time, on average, exploring a novel object. The scatterplot does highlight that there is one one young monkey who is behaving a bit weirdly, and spent quite a long time exploring the object!\nTo comment numerically on the strength of the linear association we might compute the correlation coefficient that we were introduced to in Chapter 9: Covariance & Correlation\n\nmonkeyexp |&gt;\n  select(age, exploration_time) |&gt;\n  cor()\n\n                        age exploration_time\nage               1.0000000       -0.2885495\nexploration_time -0.2885495        1.0000000\n\n\nthat is, \\(r_{\\text{age, exploration-time}} = -0.29\\)\n\n\n\n\nQuestion 8\n\n\nUsing the lm() function, fit a linear model to the sample data, in which time that monkeys spend exploring novel objects is explained by age. Assign it to a name to store it in your environment.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can see how to fit linear models in R using lm() in Chapter 10 #fitting-linear-models-in-r\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. As the variables are in the monkeyexp dataframe, we would write:\n\nmodel1 &lt;- lm(exploration_time ~ 1 + age, data = monkeyexp)\n\n\n\n\n\nQuestion 9\n\n\nInterpret the estimated intercept and slope in the context of the question of interest.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe saw how to extract lots of information on our model using summary() (see Chapter 10 #model-summary), but there are lots of other functions too.\nIf we called our linear model object “model1” in the environment, then we can use:\n\ntype model1, i.e. simply invoke the name of the fitted model;\ntype model1$coefficients;\nuse the coef(model1) function;\nuse the coefficients(model1) function;\nuse the summary(model1)$coefficients to extract just that part of the summary.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\ncoef(model1)\n\n(Intercept)         age \n   12.36253    -0.21897 \n\n\nFrom this, we get that the fitted line is: \\[\n\\widehat{\\text{ExplorationTime}} = 12.36 - 0.22 \\cdot \\text{Age} \\\\\n\\]\nWe can interpret the estimated intercept as:\n\nThe estimated average time spent exploring novel objects for a monkey of age zero is 12.36 minutes.\n\nFor the estimated slope we get:\n\nA one year increase in age is associated with an estimated decrease of -0.22 minutes (or -13 seconds) that a monkey will spend exploring a novel object.\n\n\n\n\n\nQuestion 10\n\n\nTest the hypothesis that the population slope is zero — that is, that there is no linear association between exploration time and age in the population.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou don’t need to do anything for this, you can find all the necessary information in summary() of your model.\nSee Chapter 10 #inference-for-regression-coefficients.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. The information is already contained in the row corresponding to the variable “age” in the output of summary(), which reports the t-statistic under t value and the p-value under Pr(&gt;|t|):\n\nsummary(model1)\n\n\nCall:\nlm(formula = exploration_time ~ 1 + age, data = monkeyexp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3056  -2.3461   0.3841   2.1936  21.8323 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.36253    1.04263  11.857  &lt; 2e-16 ***\nage         -0.21897    0.07057  -3.103  0.00246 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.324 on 106 degrees of freedom\nMultiple R-squared:  0.08326,   Adjusted R-squared:  0.07461 \nF-statistic: 9.627 on 1 and 106 DF,  p-value: 0.002458\n\n\nRecall that very small p-values such as the one for the intercept 2e-16 in the Pr(&gt;|t|) column simply means \\(2 \\times 10^{-16}\\), or 0.0000000000000002. Conventions such as the APA guidelines give rules on how to report these numbers (see, e.g. APA’s number and stats guide). For the p-values in this summary (the one for the intercept and the one for the slope) we could report them as “&lt;.001” and “0.003” respectively.\n\nA significant association was found between age (in years) and time spent exploring novel objects, with exploration time decreasing on average by -0.22 minutes (or -13 seconds) for every additional year of age (\\(b = -0.22\\), \\(SE = 0.071\\), \\(t(106)=-3.103\\), \\(p=.003\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\nCreate a visualisation of the estimated association between age and exploration time.\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor our simple regression model, there is a really easy way to do this with geom_smooth(). Check Chapter 10 #example, which shows an example.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\nggplot(data = monkeyexp, aes(x = age, y = exploration_time)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method=lm) +\n  labs(x = \"Age (in years)\", \n       y = \"Time spent exploring a\\n novel object (in minutes)\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12 - Optional\n\n\nConsider the following:\n\nIn fitting a linear regression model, we make the assumption that the errors around the line are normally distributed around zero (this is the \\(\\epsilon \\sim N(0, \\sigma)\\) bit.)\n\nAbout 95% of values from a normal distribution fall within two standard deviations of the centre.\n\nWe can obtain the estimated standard deviation of the errors (\\(\\hat \\sigma\\)) from the fitted model using sigma() and giving it the name of our model.\nWhat does this tell us?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Chapter 10 #the-error.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 11. The estimated standard deviation of the errors can be obtained by:\n\nsigma(model1)\n\n[1] 4.324399\n\n\nFor any particular age, the time monkeys spend exploring novel objects is estimated as being distributed above and below the regression line with a standard deviation of \\(\\hat \\sigma = 4.32\\). Since \\(1.96 \\hat \\sigma = 1.96 \\times 4.32 = \\text{approx} 8.6\\), we would expect most (about 95%) of the monkeys’ exploration times to be within about 8.6 minutes from the regression line.\n\n\n\n\nQuestion 13 - Optional\n\n\nCompute the model-predicted exploration-time for a monkey that is 1 year old.\n\n\n\n\n\n\nHints\n\n\n\n\n\nGiven that you know the intercept and slope, you can calculate this algebraically. However, try to also use the predict() function (see Chapter 10 #model-predictions).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 12. Using predict(), we need to give it our model, plus some new data which contains a monkey that has 1 in the age column. First we make a new dataframe with an age variable, with one entry which has the value 1, and then we give that to predict():\n\nage_query &lt;- data.frame(age = c(1))\npredict(model1, newdata = age_query)\n\n       1 \n12.14356 \n\n\nGiven that our fitted model takes the form:\n\\[\n\\widehat{\\text{ExplorationTime}} = 12.36 - 0.22 \\cdot \\text{Age}\n\\]\nWe are asking what the predicted exploration time is for a monkey with 1 year of age. So we can substitute in “1” for the Age variable: \\[\n\\begin{align}\n\\text{ExplorationTime} &= 12.36 - 0.22 \\cdot \\text{Age} \\\\\n\\text{ExplorationTime} &= 12.36 - 0.22 \\cdot 1 \\\\\n\\text{ExplorationTime} &= 12.36 - 0.22\\\\\n\\text{ExplorationTime} &= 12.14\\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\nInfluential Monkeys\n\nQuestion 14\n\n\nTake a look at the assumption plots (see Chapter 10 #assumptions) for your model.\n\nThe trick to looking at assumption plots in linear regression is to look for “things that don’t look random”.\nAs well as looking for patterns, these plots can also higlight individual datapoints that might be skewing the results. Can you figure out if there are any unusual monkeys in our dataset? Can you re-fit the model without that monkey? When you do so, do your conclusions change?\n\n\n\n\n\n\nSolution\n\n\n\nSolution 13. \n\nplot(model1)\n\n\n\n\n\n\n\n\nFrom these plots, we can see that the 57th observation is looking a bit influential. It is the one datapoint that is looking weird in all of the plots.\nLet’s look at them:\n\nmonkeyexp[57, ]\n\n# A tibble: 1 × 3\n  name       age exploration_time\n  &lt;chr&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n1 Lil Fizz     5             33.1\n\n\nThis is a young monkey (5 years old) called “Lil Fizz”, who seems to have spent quite a long time exploring the toy. It’s important to remember that this monkey is a valuable datapoint, despite being a bit different from the general pattern.\nHowever, it would be nice to know how much Lil Fizz is affecting our conclusions, so let’s re-fit the model on everybody except that one monkey\n\nmodel1a &lt;- lm(exploration_time ~ age, data = monkeyexp[-57, ])\nsummary(model1a)\n\n\nCall:\nlm(formula = exploration_time ~ age, data = monkeyexp[-57, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.558  -2.321   0.427   2.386   9.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.46135    0.92155  12.437  &lt; 2e-16 ***\nage         -0.16781    0.06212  -2.701  0.00805 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.769 on 105 degrees of freedom\nMultiple R-squared:  0.06498,   Adjusted R-squared:  0.05608 \nF-statistic: 7.297 on 1 and 105 DF,  p-value: 0.008053\n\n\nOur conclusions haven’t changed - we still have a significant association.\nWhat we have just done is called a “sensitivity analysis” - we’ve asked if our analyses are sensitive to a specific decision we could make (whether or not we include/exclude this monkey).\n\n\nCode\nggplot(monkeyexp, aes(x=age,y=exploration_time))+\n  geom_point()+\n  geom_smooth(method=lm, fullrange=TRUE)+\n  ylim(0,34) + \n  labs(title=\"With monkey 57\") +\n\nggplot(monkeyexp[-57,], aes(x=age,y=exploration_time))+\n  geom_point()+\n  geom_smooth(method=lm, fullrange=TRUE)+\n  ylim(0,34) + \n  labs(title=\"Without monkey 57\")\n\n\n\n\n\n\n\n\n\nWe now have a decision to make. Do we continue with the monkey removed, or do we keep them in? There’s not really a right answer here, but it’s worth noting a practical issue - our assumption plots look considerably better for our model without this monkey.\nWhatever we do, when writing up the analysis we need to mention clearly if and why we exclude any observations, and how that decision has/hasn’t influenced our conclusions.\n\n\n\n\n\nMonkey Exploration in Adulthood\nLet’s suppose instead of having measured monkeys’ ages in years, researchers simply recorded whether each monkey was an adult or a juvenile (species like Capuchins and Rhesus Macaques reach adulthood at about 8 years old).\nThe code below creates a this new variable for us:\n\nmonkeyexp &lt;- monkeyexp |&gt; \n  mutate(\n    isAdult = ifelse(age &gt;= 8, \"yes\",\"no\")\n  )\n\n\nQuestion 15\n\n\nFit the following model, and interpret the coefficients.\n\\[\n\\text{ExplorationTime} = b_0 + b_1 \\cdot \\text{isAdult} + \\varepsilon\n\\]\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor help interpreting the coefficients, see Chapter 10 #binary-predictors.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 14. \n\nmodel2 &lt;- lm(exploration_time ~ 1 + isAdult, data = monkeyexp)\nsummary(model2)\n\n\nCall:\nlm(formula = exploration_time ~ 1 + isAdult, data = monkeyexp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2556  -2.3694  -0.0444   2.5556  21.4444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   11.656      1.037  11.239   &lt;2e-16 ***\nisAdultyes    -2.711      1.136  -2.386   0.0188 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.4 on 106 degrees of freedom\nMultiple R-squared:  0.05099,   Adjusted R-squared:  0.04204 \nF-statistic: 5.695 on 1 and 106 DF,  p-value: 0.01878\n\n\n\n(Intercept) = the estimated exploration time of juvenile monkeys (11.7 minutes)\nisAdultyes = the estimated change in exploration time from juvenile monkeys to adult monkeys (-2.7 minutes)\n\n\nggplot(monkeyexp, aes(x = isAdult, y = exploration_time)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 16\n\n\nWe’ve actually already seen a way to analyse questions of this sort (“is the average exploration-time different between juveniles and adults?”)\nRun the following t-test, and consider the statistic, p value etc. How does it compare to the model in the previous question?\n\nt.test(exploration_time ~ isAdult, data = monkeyexp, var.equal = TRUE)\n\n\n\n\n\n\nSolution\n\n\n\nSolution 15. \n\nt.test(exploration_time ~ isAdult, data = monkeyexp, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  exploration_time by isAdult\nt = 2.3865, df = 106, p-value = 0.01878\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n 0.4588049 4.9634174\nsample estimates:\n mean in group no mean in group yes \n        11.655556          8.944444 \n\n\nIt is identical! the \\(t\\)-statistics are the same, the p-values are the same, the degrees of freedom. Everything!\nThe two sample t-test is actually just a special case of the linear model, where we have a numeric outcome variable and a binary predictor!\nAnd… the one-sample t-test is the linear model without any predictors, so just with an intercept.\n\nt.test(monkeyexp$exploration_time, mu = 0)\n\n\n    One Sample t-test\n\ndata:  monkeyexp$exploration_time\nt = 21.722, df = 107, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  8.538785 10.253807\nsample estimates:\nmean of x \n 9.396296 \n\ninterceptonly_model &lt;- lm(exploration_time ~ 1, data = monkeyexp)\n\nsummary(interceptonly_model)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 9.396296  0.4325657 21.72224 5.060938e-41"
  },
  {
    "objectID": "05_ex.html#footnotes",
    "href": "05_ex.html#footnotes",
    "title": "W5: Covariance, Correlation & Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot a real study!↩︎"
  },
  {
    "objectID": "03_ex.html",
    "href": "03_ex.html",
    "title": "W3: T-tests",
    "section": "",
    "text": "Question 1\n\n\nAt the end of last week’s exercises, we estimated the mean sleep-quality rating, and computed a confidence interval, using the formula below.\n\\[\n\\begin{align}\n\\text{95\\% CI: }& \\bar x \\pm 1.96 \\times SE \\\\\n\\end{align}\n\\]\nCan you use R to show where the 1.96 comes from?\n\n\n\n\n\n\nHints\n\n\n\n\n\nqnorm! (see the end of Chapter 5 #uncertainty-due-to-sampling)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. The 1.96 comes from 95% of the normal distribution falling within 1.96 standard deviations of the mean:\n\nqnorm(c(0.025, 0.975))\n\n[1] -1.959964  1.959964\n\n\n\n\n\n\nQuestion 2\n\n\nAs we learned in Chapter 6 #t-distributions, the sampling distribution of a statistic has heavier tails the smaller the size of the sample it is derived from. In practice, we are better using \\(t\\)-distributions to construct confidence intervals and perform statistical tests.\nThe code below creates a dataframe that contains the number of books read by 7 people in 2024.\n(Note tibble is just a tidyverse version of data.frame):\n\nbookdata &lt;- \n  tibble(\n    person = c(\"Martin\",\"Umberto\",\"Monica\",\"Emma\",\"Josiah\",\"Dan\",\"Aja\"),\n    books_read = c(12,19,9,11,8,28,13)\n  )\n\nCalculate the mean number of books read in 2024, and construct an appropriate 95% confidence interval.\n\n\n\n\n\nSolution\n\n\n\nSolution 2. Here is our estimated average number of books read:\n\nmean(bookdata$books_read)\n\n[1] 14.28571\n\n\nAnd our standard error is still \\(\\frac{s}{\\sqrt{n}}\\):\n\nsd(bookdata$books_read)/sqrt(nrow(bookdata))\n\n[1] 2.652171\n\n\nWith \\(n = 7\\) observations, and estimating 1 mean, we are left with \\(6\\) degrees of freedom.\nFor our 95% confidence interval, the \\(t^*\\) in the formula below is obtained via:1\n\nqt(0.975, df = 6)\n\n[1] 2.446912\n\n\nOur confidence interval is therefore:\n\\[\n\\begin{align}\n\\text{CI} &= \\bar{x} \\pm t^* \\times SE \\\\\n\\text{95\\% CI} &= 14.286 \\pm 2.447 \\times 2.652 \\\\\n\\text{95\\% CI} &= [7.80,\\, 20.78] \\\\\n\\end{align}\n\\]\n\n\n\n\nQuestion 3\n\n\nWill a 90% confidence interval be wider or narrow?\nCalculate it and see.\n\n\n\n\n\nSolution\n\n\n\nSolution 3. A 90% confidence interval will be narrower:\n\nqt(0.95, df = 6)\n\n[1] 1.94318\n\n\n\\[\n\\begin{align}\n\\text{CI} &= \\bar{x} \\pm t^* \\times SE \\\\\n\\text{90\\% CI} &= 14.286 \\pm 1.943 \\times 2.652 \\\\\n\\text{90\\% CI} &= [9.13,\\, 19.44] \\\\\n\\end{align}\n\\]\nThe intuition behind this is that our level of confidence is inversely related to the width of the interval.\nTake it to the extremes:\n\nI have 100% confidence that the interval \\([-Infinity, +Infinity]\\) contains the true population mean.\nIf I want an narrower interval, then I have to sacrifice confidence. e.g. a 10% CI: \\([13.94, 14.63]\\)\n\nImagine playing a game of ringtoss. A person throwing a 2-meter diameter hoop will have much more confidence that they are going to get it over the pole than a person throwing a 10cm diameter ring."
  },
  {
    "objectID": "03_ex.html#footnotes",
    "href": "03_ex.html#footnotes",
    "title": "W3: T-tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Why 97.5? and not 95? We want the middle 95%, and \\(t\\)-distributions are symmetric, so we want to split that 5% in half, so that 2.5% is on either side. We could have also used qt(0.025, df = 6), which will just give us the same number but negative: -2.4469119)↩︎"
  },
  {
    "objectID": "01_ex.html",
    "href": "01_ex.html",
    "title": "W1: Intro R",
    "section": "",
    "text": "First things\nThe very first things to do are to open RStudio and get a blank script ready for writing your code!\n\n\nOur recommendation is that you have an R project for this course, and use a new script for each week of work. See the tip about “R projects” in Chapter 1.\n\n\nPet Data\n\nWe’re going to play with some data on a sample of licensed pets from the city of Seattle, USA. It can be downloaded (or read directly into R) from https://uoepsy.github.io/data/pets_seattle.csv. It contains information on the license ID, year of issue, as well as the species, breeds and weights of each pet. You can find a data dictionary in Table 1\n\n\n\n\nTable 1: Seattle Pets: Data dictionary\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nlicense_year\nYear in which license was issued\n\n\nlicense_number\nUnique license ID number\n\n\nanimals_name\nFull name of pet\n\n\nspecies\nSpecies of pet\n\n\nprimary_breed\nPrimary breed of pet\n\n\nsecondary_breed\nSecondary breed of pet\n\n\nweight_kg\nWeight in kilograms\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nWrite a line of code that reads in the data to your R session. Then examine the dimensions of the dataset, and take a look at the first few lines.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll need the read.csv() function. Remember to assign it a name to store it in your environment.\nChapter 2 #basic-data-wrangling contains an example of reading in data from a URL. You’ll then want to play with functions like dim() and head().\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. We’re going to call it petdata in our environment here. Don’t forget the quotation marks around the url (otherwise R will look for an object in your environment called https://..., which isn’t there).\n\npetdata&lt;-read.csv(\"https://uoepsy.github.io/data/pets_seattle.csv\")\ndim(petdata)\n\n[1] 1956    7\n\n\nWe can see there are 1956 rows and 7 columns.\nAnd we can see the first few rows here:\n\nhead(petdata)\n\n  license_year license_number  animals_name species         primary_breed\n1         2018      LNS150171        Norman     Dog                 Boxer\n2         2017        LN20666         Henry     Dog          Bichon Frise\n3         2018      LN8000658 Vega Williams     Dog                   Mix\n4         2018       LN730940         Molly     Dog   Australian Shepherd\n5         2016       LN964607         Gremy     Dog Chihuahua, Short Coat\n6         2018      LNS117115        Shadow     Dog   Retriever, Labrador\n  secondary_breed weight_kg\n1             Mix     29.15\n2        Havanese     23.70\n3         Unknown     21.13\n4             Mix     18.70\n5         Terrier     20.36\n6         Unknown     11.51\n\n\n\n\n\n\nQuestion 2\n\n\nWhat are the names of the 47th and the 200th animals in the dataset? (use R code to find out)\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll probably want to make use of the square brackets data[rows, columns].\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. There are lots of different ways to do this. We can get out the entire rows, either individually:\n\npetdata[47,]\npetdata[200,]\n\nOr together:\n\npetdata[c(47,200),]\n\n    license_year license_number animals_name species       primary_breed\n47          2018      LNS140233     Hooligan     Dog Retriever, Labrador\n200         2017       LN584186  Maple Syrup     Cat  Domestic Shorthair\n    secondary_breed weight_kg\n47          Unknown     12.27\n200         Unknown      4.66\n\n\nOr we can extract the names only:\n\n# These all do the same\npetdata[c(47,200),\"animals_name\"]\npetdata[c(47,200),3]\npetdata$animals_name[c(47,200)]\n\nThe will all give us these names:\n\n\n[1] \"Hooligan\"    \"Maple Syrup\"\n\n\nIn the last one, we use the $ to access the animals_name variable. In this case, we don’t need to specify [rows, columns] inside the square brackets, because it’s a single variable - there are no columns.\n\ndataframe[rows, columns]\n\nvariable[entries]\n\n\n\n\n\nQuestion 3\n\n\nSubset the data to only the animals which are dogs, and store this subset as another named object in your environment.\nDo the same for the cats.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll want to think about how we access data via asking for those entries that meet a specific condition (see Chapter 2 #accessing-by-a-condition)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. We can ask “which entries of species variable are equal to ‘Dog’?” by using pet$species==\"Dog\".\nThis will give us a TRUE for each dog, and a FALSE for each non-dog.\nWe can then use this set of TRUEs and FALSEs to access those rows for which it is TRUE in our data:\n\ndogdata &lt;- petdata[petdata$species==\"Dog\", ]\ncatdata &lt;- petdata[petdata$species==\"Cat\", ]\n\n\n\n\n\nQuestion 4\n\n\nFind the name and weight of the heaviest cat, and of the lightest dog.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou could do this using the original data you read in from question 1, or use the subsets you created in question 3. You’ll again want to supply a condition within square brackets data[?==?]. That condition may well have something to do with being equal to the min() or the max() of some variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. We can use min() and max() to return the minimum and maximum of a variable:\n\nmin(dogdata$weight_kg)\n\n[1] 0.39\n\nmax(catdata$weight_kg)\n\n[1] 5.48\n\n\nWe could then ask for each entry “is this cat’s weight the maximum cat’s weight?” with catdata$weight_kg == max(catdata$weight_kg) and then use that condition to access the rows in our dataset where the weight_kg variable is at its maximum:\n\ncatdata[catdata$weight_kg == max(catdata$weight_kg), ]\n\n    license_year license_number animals_name species      primary_breed\n414         2018      LNS101014       Smokey     Cat Domestic Shorthair\n    secondary_breed weight_kg\n414             Mix      5.48\n\ndogdata[dogdata$weight_kg == min(dogdata$weight_kg), ]\n\n     license_year license_number animals_name species  primary_breed\n1126         2017      LNS139134       Claire     Dog Great Pyrenees\n     secondary_breed weight_kg\n1126         Unknown      0.39\n\n\n\n\n\n\nQuestion 5\n\n\nDoes the original dataset contain only dogs and cats?\n\n\n\n\n\n\nHints\n\n\n\n\n\nGiven what you did in question 3, you might be able to answer this by just looking at your environment.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. In the environment, we can see that the entire dataset has 1956 observations, the Dog’s data frame has 1322, and the Cat’s has 632.\nSo there are 2 missing!\n\n\n\n\nQuestion 6\n\n\nExtract the entries of the original dataset for which the species is neither “Dog” nor “Cat”?\nWhat are the names and species of these animals?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is a slightly complex one. Chapter 2 #more-complex-conditions might help you here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. As always, there are lots of different ways.\nHere are three:\n\n\n“not a dog AND not a cat”\nWe can ask if something is not a dog by using petdata$species != \"Dog\". But we want the rows where the species is not a dog and it’s not a cat. So it’s two conditions:\n\npetdata[petdata$species != \"Cat\" & petdata$species != \"Dog\", ]\n\n     license_year license_number     animals_name species primary_breed\n1505         2018      LNS147013    Billy the Kid    Goat     Miniature\n1655         2018      LNS132953 Vincent Van Goat    Goat     Miniature\n     secondary_breed weight_kg\n1505         Unknown    103.48\n1655         Unknown     73.96\n\n\n\n\n“not (dog OR cat)”\nWe could also do this in other ways, such as asking for all the entries which are either “Dog” or “Cat”, and then negating them:\n\npetdata[!(petdata$species == \"Cat\" | petdata$species == \"Dog\"), ]\n\n     license_year license_number     animals_name species primary_breed\n1505         2018      LNS147013    Billy the Kid    Goat     Miniature\n1655         2018      LNS132953 Vincent Van Goat    Goat     Miniature\n     secondary_breed weight_kg\n1505         Unknown    103.48\n1655         Unknown     73.96\n\n\n\n\n“not one of [Dog, Cat]”\nAnother clever little operator is the %in% operator, which asks whether something is in a set of things. Unfortunately, we can’t use !%in% to mean “not in”, so we need to put the ! right at the start of the condition:\n\npetdata[!petdata$species %in% c(\"Cat\",\"Dog\"), ]\n\n     license_year license_number     animals_name species primary_breed\n1505         2018      LNS147013    Billy the Kid    Goat     Miniature\n1655         2018      LNS132953 Vincent Van Goat    Goat     Miniature\n     secondary_breed weight_kg\n1505         Unknown    103.48\n1655         Unknown     73.96\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nCreate a new variable in the data, which contains the weights of all the animals, but rounded to the nearest kg.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTry looking up the help documentation for the function round(). Try playing with it in the console, e.g. round(c(3.5, 4.257, 1.1111)). You may find it helpful to look back at Chapter 2 #adding/changing-a-variable.\n\n“to the nearest kg” would mean we want no decimal points. Note that round() has a digits argument. e.g. round(22.324, digits = 2) and round(22.324, digits = 1) do different things.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. We’re wanting this variable as a new column in the data, so don’t forget the dataframe$newvariable &lt;- ...... bit.\n\npetdata$weight_rounded &lt;- round(petdata$weight_kg)\n\n\n\n\n\nQuestion 8\n\n\nTry giving the dataset to the function summary(). You’ll get out some information on each of the variables. It is likely that you’ll get more useful information for the variables containing information on the animal’s weights than for those containing their names, breeds etc because these variables are vectors of “characters”. We’ll start to look more about different types of data next week.\n\n\n\n\n\nSolution\n\n\n\nSolution 8. Easy to do!\n\nsummary(petdata)\n\n  license_year  license_number     animals_name         species         \n Min.   :2015   Length:1956        Length:1956        Length:1956       \n 1st Qu.:2017   Class :character   Class :character   Class :character  \n Median :2018   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2018                                                           \n 3rd Qu.:2018                                                           \n Max.   :2018                                                           \n primary_breed      secondary_breed      weight_kg       weight_rounded \n Length:1956        Length:1956        Min.   :  0.390   Min.   :  0.0  \n Class :character   Class :character   1st Qu.:  4.707   1st Qu.:  5.0  \n Mode  :character   Mode  :character   Median : 16.630   Median : 17.0  \n                                       Mean   : 15.312   Mean   : 15.3  \n                                       3rd Qu.: 22.500   3rd Qu.: 22.0  \n                                       Max.   :103.480   Max.   :103.0  \n\n\n\n\n\n\n\n\n\n\nSimulating Dice\n\nQuestion 9\n\n\nCopy the code from the lecture which creates a custom function called dice() (copied below).\nBe sure to run the code (highlight it all with your cursor, and hit “run” in the top right, or press Ctrl/Cmd+Enter).\n\ndice &lt;- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n\n\n\n\n\nWhat did that code do?\nIn a sense, this code does nothing: It won’t give you any output when you run it. What it is actually doing, though, is defining a function called dice(). If you look at your environment panel (top right), you’ll see dice appear when you run the code.\nTo produce some output, we have to call the function dice() (by writing it into code: dice(4), for example). dice() wants to be supplied with some information (in the argument num). If no information is supplied, num will take a default value of 1. (So writing dice() is equivalent to writing dice(1)).\nWhat does dice() do with num? It calls another function, sample(), with 3 arguments. We didn’t write sample(): it’s a function that’s “supplied with” R. To find out more about what sample() does:\n\nclick inside the brackets just after sample() in your R script;\npress TAB (⇥), then F1\nyou should see some help appear in the bottom right-hand panel of RStudio.\n\nYou will find that “sample() takes a sample … from the elements of x …” If you compare the code in RStudio to the code under “Usage” you’ll see that where the help has x, we have 1:6. So what does 1:6 mean? One way to find out is to open the console in RStudio (bottom left) and just type stuff in. What happens when you type 1:6? What about 2:17? (What about 6:1?)\nRemember: The console is the place to “try stuff out” (don’t worry, you can’t break it).\nWhat you will discover is that 1:6 creates a vector (list of similar things, in this case numbers) of the numbers 1-6. The next bit of the sample() function is size. In the dice() function, the num passes down to the size of the sample(): Looking through the help, size is the number of items to choose. So sample(1:6, 1) would choose one number from the numbers 1-6 at random; sample(1:6, 3) would choose 3, and so on. The last argument, replace=TRUE, tells sample() what to do with a number once it’s been picked: Does it go ‘back into the bag’ to be picked again (TRUE) or not? (FALSE)?\nAround the outside is sum() which simply sums the numbers on however many (num) dice you “rolled”.\nPutting it all together, our dice() function “throws num dice” by sample()ing from the numbers 1-6 num times, replaceing each number when it’s been picked, and sums the numbers of all the dice.\n\n\nQuestion 10\n\n\nUse the function you just made to ‘roll a die’ a few times. Check that it works like you expect.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou just need to run dice() a few times. A single die means num = 1, which is the default.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. \n\ndice()\n\n[1] 5\n\ndice()\n\n[1] 3\n\ndice()\n\n[1] 4\n\ndice()\n\n[1] 2\n\n\n\n\n\n\nQuestion 11\n\n\nLook up the function replicate(). We can use it to do something in R lots of times! For instance, replicate(20, 1+1) will evaluate 1+1 twenty times.\nUse replicate() to simulate 20 rolls of a single die, and store the results in an object in your environment. Give it an easily identifiable name.\nWhat does each value in this object represent?\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\nrolls20 &lt;- replicate(20, dice(num = 1))\nrolls20\n\n [1] 6 5 4 5 4 4 4 1 6 5 1 6 1 2 2 5 2 1 3 6\n\n\nEach value in rolls20 represents the simulated roll of a single die. We roll our die, and get a 6, we roll it again and get 5, the third roll we get 4, and so on..\n\n\n\n\nQuestion 12\n\n\nCreate a barplot showing the frequency with which each number was landed on in the 20 rolls.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe functions table() and barplot() were used to do this in the lecture.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 11. Your plots will look slightly different to these, because all of our dice are random!\n\n# We can get the frequency table using table()\ntable(rolls20)\n\nrolls20\n1 2 3 4 5 6 \n4 3 1 4 4 4 \n\n# Which we can then pass to the barplot() function:\nbarplot(table(rolls20))\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 13\n\n\nDo the same for 100 rolls, and then for 1,000. What do you notice?\n\n\n\n\n\nSolution\n\n\n\nSolution 12. \n\nmorerolls &lt;- replicate(100, dice(1))\nbarplot(table(morerolls))\n\n\n\n\n\n\n\nmorerolls2 &lt;- replicate(1000, dice(1))\nbarplot(table(morerolls2))\n\n\n\n\n\n\n\n\nThe more rolls we do of the dice, the flatter the graph becomes. This is because there is an equal probability of the die landing on any of the responses - there is a uniform probability.\n\n\n\n\nQuestion 14\n\n\nCopy the code below into your script and run it. It creates a new function called wdice() which simulates the rolling of num dice which are slightly weighted.\nRoll a single weighted die 20 times and plot the frequency distribution. Do the same for 100 and 1,000 rolls of a single die. Does a pattern emerge? At how many rolls?\n\nwdice &lt;- function(num = 1){\n    sum(sample(1:6, num, replace=TRUE, prob = c(0.15,0.15,0.15,0.15,0.15,0.25)))\n}\n\n\n\n\n\n\nSolution\n\n\n\nSolution 13. \n\nwdice &lt;- function(num = 1){\n    sum(sample(1:6, num, replace=TRUE, prob = c(0.15,0.15,0.15,0.15,0.15,0.25)))\n}\n\nwd &lt;- replicate(20, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\nwd &lt;- replicate(1000, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\nwd &lt;- replicate(10000, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\n\nThe die is clearly weighted towards landing on 6. However, is 20 rolls enough to reliably observe this? In our 20 rolls above, it landed on 3 quite a bit too (yours will be different)! The pattern becomes clearer after 1000 rolls.\n\n\n\n\nQuestion 15\n\n\nRemember, wdice() and dice() are really just relying on different functions, like sample(). Try playing around with sample() in the console again - what does the prob = c(....) bit do?\n\n\n\n\n\nSolution\n\n\n\nSolution 14. The prob bit is defining the probabilities of observing each outcome - i.e. there is a 25% chance of rolling a 6.\n\n\n\n\nQuestion 16\n\n\nLet’s try to modify the wdice() function. Edit the code for wdice() so that 50% of the time it lands on number 6.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nTo test out your modified function, you will need to re-run the code which defines the function. When we use wdice() we use the function which is in our environment. If we want to edit the function, we need to then overwrite (or “replace”/“reassign”) the object in our environment.\n\nWe need to be careful to remember that the probability of different outcomes should sum to 1 (i.e., it’s not possible to “50% of the time land on 6” as well as “70% of the time land on 5”!).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 15. \n\nwdice &lt;- function(num = 1){\n    sum(sample(1:6, num, replace=TRUE, prob = c(0.1,0.1,0.1,0.1,0.1,0.5)))\n}\n\n\n\n\n\nQuestion 17\n\n\nCan you observe the weighting in your new die (the one which 50% of the time lands on number 6) in only 20 rolls?\n\n\n\n\n\nSolution\n\n\n\nSolution 16. \n\nwd &lt;- replicate(20, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\n\nThe die is very clearly weighted to land on 6. We can see this in just 20 rolls. Presumably it will become even clearer if we increased how many times we roll it.\n\n\n\n\nQuestion 18\n\n\nConceptually, what can we learn from this toy example?\n\n\n\n\n\nSolution\n\n\n\nSolution 17. The more highly weighted a die is, the less we have to roll it in order to observe that weighting."
  },
  {
    "objectID": "02_ex.html",
    "href": "02_ex.html",
    "title": "W2: More R; Estimates & Intervals",
    "section": "",
    "text": "Question 0"
  },
  {
    "objectID": "02_ex.html#optional-extras",
    "href": "02_ex.html#optional-extras",
    "title": "W2: More R; Estimates & Intervals",
    "section": "Optional Extras",
    "text": "Optional Extras\n\nOptional Extra\n\n\nNote that the confidence interval from the previous question is concerned with describing the abstract and theoretical distribution of “what the mean sleep quality rating would look like from all possible samples of this size that I could take”. In order to do this we used a formula to describe the spread of this distribution, and in doing so had to assume that the standard deviation of our sample is a good approximation of the standard deviation of the population, and that the population is normally distributed.\nWe can also avoid ever using the standard deviation of our sample (sd(usmr2022on$sleeprating)), and instead approximate the sampling distribution of the mean by “bootstrapping” - taking repeated resamples with replacement from the original sample (see Chapter 4 #standard-error.\n\nbootstrap_means &lt;- replicate(1000, mean(sample(observed_sample, replace = TRUE)))\n\n\nCreate an object that contains the 10,000 means from 10,000 resamples of our sleep ratings.\n\nThe distribution of resample means is the ‘bootstrap distribution’. Plot a histogram of it. What is the standard deviation? How does it compare to the standard error you calculated in the previous question with the formula?\n\nAt what values does the middle 95% of the bootstrap distribution fall?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor 3, look up quantile(). We saw this in Chapter 4 #confidence-intervals.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 14. \n\n\nResample means\nHere is our sample of sleep ratings:\n\nsleeprates &lt;- usmr2022on$sleeprating\n\nAnd we can get rid of the NA’s:\n\nsleeprates &lt;- sleeprates[!is.na(sleeprates)]\n\nWe can resample with replacement from this set of numbers by using the replace = TRUE argument in the sample() function.\nNote, we’re leaving size = blank, which means it will stop at the same length as the original vector we give it.\n\nsample(sleeprates, replace = TRUE)\n\nand the mean of a given resample is calculated by wrapping mean() around the above code:\n\nmean(sample(sleeprates, replace = TRUE))\n\n[1] 66.32558\n\n\nfinally, we’ll do it lots and lots of times, using replicate():\n\nBSmeans &lt;- replicate(10000, mean(sample(sleeprates, replace = TRUE)))\n\n\n\nBootstrap Distribution\nHere’s the histogram of the bootstrap distribution:\n\nhist(BSmeans)\n\n\n\n\n\n\n\n\nAnd here’s the standard deviation of that distribution. This is a bootstrapped estimate of the standard error.\n\nsd(BSmeans)\n\n[1] 1.525482\n\n\nRecall our standard error calculated using \\(\\frac{s}{\\sqrt{n}}\\) from the previous question was 1.52\n\n\nPercentiles\nWe can get the 2.5% and 97.5% percentiles (i.e. getting the middle 95%), using the code below. Recall our confidence intervals that we computed analytically were 63.38 and 69.34.\n\nquantile(BSmeans, c(.025,.975))\n\n    2.5%    97.5% \n63.31151 69.27907 \n\n\n\n\n\n\n\n\n\n\n\nbootstraps\n\n\n\n\n\nBootstrapping is a great way to learn about sampling variability because it allows us to actually plot, summarise and describe what would otherwise be an abstract conceptual distribution.\nIt can also be a useful tool in practice, but it doesn’t come without its own problems/complexities. One important thing to note is that it often works worse than traditional methods for small samples, especially skewed samples (i.e. bootstrapping a “95% CI” for a small sample will often be too narrow and &lt;95%)."
  },
  {
    "objectID": "04_ex.html",
    "href": "04_ex.html",
    "title": "W4: Binomial & Chi-Square Tests",
    "section": "",
    "text": "Research Question: Is the probability that a USMR student in 2025 prefers Dogs over Cats greater than 50%?\n\n\nData: USMR 2025! It’s YOU!!\nWe’ve used the survey data from previous years for a few weeks now, but let’s start using the data that you provided!\nYou can find the results from the 2025 survey at https://uoepsy.github.io/data/usmr2025.csv\n\nusmr25 &lt;- \n  read_csv(\"https://uoepsy.github.io/data/usmr2025.csv\")\n\n\n\nQuestion 1\n\n\nCalculate the proportion of students who chose cats vs dogs in the survey. If the probability that a student prefers dogs to cats is 50%, what would we expect to see?\n\n\n\n\n\n\nHints\n\n\n\n\n\ntable() |&gt; prop.table() might be the quickest way here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. I sometimes like adding a |&gt; print() |&gt; in the middle of these sequences of pipes to print out the intermediary output as well:\n\ntable(usmr25$catdog) |&gt;\n  print() |&gt;\n  prop.table()\n\n\ncat dog \n 42  35 \n\n\n\n      cat       dog \n0.5454545 0.4545455 \n\n\nIf the probability was 50%, or 0.5, then we would expect the numbers to be equal. We have 77 responses here, so we would expect 38.5 to be team cats, and 38.5 to be team dogs!\n\n\n\n\nQuestion 2\n\n\n\nResearch Question: Is the probability that a student prefers Dogs over Cats greater than 50%?\n\nConduct a test to address the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nNote the “tailedness” of the question. Just like t.test(), the function we use here should allow us to specify our hypothesis.\n\nYou can see an example of this test in Chapter 7 #binomials.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. We’re going to want to use the binom.test() function. This is just like the reading, where we tested the proportion of our sample who were left-handed.\nWe can either type in the numbers of “Dog” people out of our total number:\n\nbinom.test(35, 77, p = 0.5, alternative = \"greater\")\n\n\n    Exact binomial test\n\ndata:  35 and 77\nnumber of successes = 35, number of trials = 77, p-value = 0.819\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.3573548 1.0000000\nsample estimates:\nprobability of success \n             0.4545455 \n\n\nOr give the function a table.\nBUT we need to make sure that it is picking up the right value as “successes”.\nNotice that this gives us a different result:\n\n\n\n    Exact binomial test\n\ndata:  table(usmr25$catdog)\nnumber of successes = 42, number of trials = 77, p-value = 0.2472\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.4455646 1.0000000\nsample estimates:\nprobability of success \n             0.5454545 \n\n\nThis is because it is taking the first entry as the number of successes. So it is saying that Cat people = 1 and Dog people = 0. But we want it the other way around! You could either create a different table (or switch its order), or simply switch to the alternative being “less” (because in this unrealistic binary world, the question “is the probability of being Dog person &gt;0.5?” is the same thing as “is probability of being Cat person &lt;0.5?”)\n\n\n\n    Exact binomial test\n\ndata:  table(usmr25$catdog)\nnumber of successes = 42, number of trials = 77, p-value = 0.819\nalternative hypothesis: true probability of success is less than 0.5\n95 percent confidence interval:\n 0.0000000 0.6426452\nsample estimates:\nprobability of success \n             0.5454545"
  },
  {
    "objectID": "04_ex.html#cats-n-dogs",
    "href": "04_ex.html#cats-n-dogs",
    "title": "W4: Binomial & Chi-Square Tests",
    "section": "",
    "text": "Research Question: Is the probability that a USMR student in 2025 prefers Dogs over Cats greater than 50%?\n\n\nData: USMR 2025! It’s YOU!!\nWe’ve used the survey data from previous years for a few weeks now, but let’s start using the data that you provided!\nYou can find the results from the 2025 survey at https://uoepsy.github.io/data/usmr2025.csv\n\nusmr25 &lt;- \n  read_csv(\"https://uoepsy.github.io/data/usmr2025.csv\")\n\n\n\nQuestion 1\n\n\nCalculate the proportion of students who chose cats vs dogs in the survey. If the probability that a student prefers dogs to cats is 50%, what would we expect to see?\n\n\n\n\n\n\nHints\n\n\n\n\n\ntable() |&gt; prop.table() might be the quickest way here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. I sometimes like adding a |&gt; print() |&gt; in the middle of these sequences of pipes to print out the intermediary output as well:\n\ntable(usmr25$catdog) |&gt;\n  print() |&gt;\n  prop.table()\n\n\ncat dog \n 42  35 \n\n\n\n      cat       dog \n0.5454545 0.4545455 \n\n\nIf the probability was 50%, or 0.5, then we would expect the numbers to be equal. We have 77 responses here, so we would expect 38.5 to be team cats, and 38.5 to be team dogs!\n\n\n\n\nQuestion 2\n\n\n\nResearch Question: Is the probability that a student prefers Dogs over Cats greater than 50%?\n\nConduct a test to address the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nNote the “tailedness” of the question. Just like t.test(), the function we use here should allow us to specify our hypothesis.\n\nYou can see an example of this test in Chapter 7 #binomials.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. We’re going to want to use the binom.test() function. This is just like the reading, where we tested the proportion of our sample who were left-handed.\nWe can either type in the numbers of “Dog” people out of our total number:\n\nbinom.test(35, 77, p = 0.5, alternative = \"greater\")\n\n\n    Exact binomial test\n\ndata:  35 and 77\nnumber of successes = 35, number of trials = 77, p-value = 0.819\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.3573548 1.0000000\nsample estimates:\nprobability of success \n             0.4545455 \n\n\nOr give the function a table.\nBUT we need to make sure that it is picking up the right value as “successes”.\nNotice that this gives us a different result:\n\n\n\n    Exact binomial test\n\ndata:  table(usmr25$catdog)\nnumber of successes = 42, number of trials = 77, p-value = 0.2472\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.4455646 1.0000000\nsample estimates:\nprobability of success \n             0.5454545 \n\n\nThis is because it is taking the first entry as the number of successes. So it is saying that Cat people = 1 and Dog people = 0. But we want it the other way around! You could either create a different table (or switch its order), or simply switch to the alternative being “less” (because in this unrealistic binary world, the question “is the probability of being Dog person &gt;0.5?” is the same thing as “is probability of being Cat person &lt;0.5?”)\n\n\n\n    Exact binomial test\n\ndata:  table(usmr25$catdog)\nnumber of successes = 42, number of trials = 77, p-value = 0.819\nalternative hypothesis: true probability of success is less than 0.5\n95 percent confidence interval:\n 0.0000000 0.6426452\nsample estimates:\nprobability of success \n             0.5454545"
  },
  {
    "objectID": "04_ex.html#birth-months",
    "href": "04_ex.html#birth-months",
    "title": "W4: Binomial & Chi-Square Tests",
    "section": "Birth-Months",
    "text": "Birth-Months\n\nResearch Question: Are USMR 2025 students more likely to be born in certain months than others?\n\n\nData: USMR 2025! It’s YOU!!\nWe’ve used the survey data from previous years for a few weeks now, but let’s start using the data that you provided!\nYou can find the results from the 2025 survey at https://uoepsy.github.io/data/usmr2025.csv\n\nusmr25 &lt;- \n  read_csv(\"https://uoepsy.github.io/data/usmr2025.csv\")\n\n\n\nQuestion 3\n\n\nWhat is your intuition about the distribution of all students’ birth-months?\nDo you think they will be spread uniformly across all months of the year (like a fair 12-sided dice), or do you think people are more likely to be born in certain months more than others?\nPlot the distribution and get an initial idea of how things are looking.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can do this quickly with barplot() and table(), or you could create try using ggplot() and looking into geom_bar().\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. The quick and dirty way to plot:\n\nbarplot(table(usmr25$birthmonth))\n\n\n\n\n\n\n\n\nA ggplot option:\n\nggplot(data = usmr25, aes(x = birthmonth)) +\n    geom_bar() +\n    labs(x = \"- Birth Month -\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nWe’re going to perform a statistical test to assess the extent to which our data conforms to the hypothesis that people are no more likely to be born on one month than another.\nUnder this hypothesis, what would be the proportional breakdown of observed births in each of the months?\n\n\n\n\n\nSolution\n\n\n\nSolution 4. If people are no more likely to be born in one month than another, then we would expect the same proportion of observed births in each month.\nThere are 12 months, so we would expect \\(\\frac{1}{12}\\) observations in each month.\nWe can write these as: \\[\n\\begin{align}\n& p_{jan} = 1/12 \\\\\n& p_{feb} = 1/12 \\\\\n& ... \\\\\n& p_{dec} = 1/12 \\\\\n\\end{align}\n\\]\n\n\n\n\nQuestion 5\n\n\nHow many observations in our sample would we expect to find with a birthday in January? And in February? … and so on?\n\n\n\n\n\n\nHints\n\n\n\n\n\nHow many responses (i.e. not missing values) do we have for this question?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. There are 77 people who have non-NA values (sum(!is.na(usmr25$birthmonth))).\nUnder the null hypothesis, we would expect \\(\\frac{1}{12} \\times\\) 77 = 6.42 observations born in each month.\n\n\n\n\nQuestion 6\n\n\nThe code below creates counts for each month. Before doing that, it removes the rows which have an NA in them for birthmonth:\n\nusmr25 |&gt;\n  filter(!is.na(birthmonth)) |&gt;\n  group_by(birthmonth) |&gt;\n  summarise(\n      observed = n()\n  )\n\n(A shortcut for this would be usmr25 |&gt; filter(!is.na(birthmonth)) |&gt; count(birthmonth))\nAdd to the code above to create columns showing:\n\nthe expected counts \\(E_i\\)\nobserved-expected (\\(O_i - E_i\\))\nthe squared differences \\((O_i - E_i)^2\\)\nthe standardised square differences \\(\\frac{(O_i - E_i)^2}{E_i}\\)\n\nThen calculate the \\(\\chi^2\\) statistic (the sum of the standardised squared differences).\nIf your observed counts matched the expected counts perfectly, what would the \\(\\chi^2\\) statistic be?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis was all done in the step-by-step example of a \\(\\chi^2\\) test in Chapter 7 #chi2-goodness-of-fit-test\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\nchi_table &lt;- \n    usmr25 |&gt;\n    filter(!is.na(birthmonth)) |&gt;\n    group_by(birthmonth) |&gt;\n    summarise(\n        observed = n(),\n        expected = sum(!is.na(usmr25$birthmonth))/12,\n        diff = observed-expected,\n        sq_diff = diff^2,\n        std_sq_diff = sq_diff / expected\n    )\nchi_table\n\n# A tibble: 12 × 6\n   birthmonth observed expected   diff sq_diff std_sq_diff\n   &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 apr               8     6.42  1.58    2.51       0.391 \n 2 aug               3     6.42 -3.42   11.7        1.82  \n 3 dec               8     6.42  1.58    2.51       0.391 \n 4 feb               5     6.42 -1.42    2.01       0.313 \n 5 jan               5     6.42 -1.42    2.01       0.313 \n 6 jul               6     6.42 -0.417   0.174      0.0271\n 7 jun               5     6.42 -1.42    2.01       0.313 \n 8 mar               7     6.42  0.583   0.340      0.0530\n 9 may               9     6.42  2.58    6.67       1.04  \n10 nov               1     6.42 -5.42   29.3        4.57  \n11 oct               8     6.42  1.58    2.51       0.391 \n12 sep              12     6.42  5.58   31.2        4.86  \n\n\nAnd we can calculate our \\(\\chi^2\\) test statistic by simply summing the values in the last column we created:\n\nsum(chi_table$std_sq_diff)\n\n[1] 14.48052\n\n\nIf all our observed counts are equal to our expected counts, then the diff column above will be all \\(0\\), and \\(0^2=0\\), and \\(\\frac{0}{E_i}\\) will be \\(0\\). So \\(\\chi^2\\) will be \\(0\\).\n\n\n\n\nQuestion 7\n\n\nYou can see the distribution of \\(\\chi^2\\) statistics with different degrees of freedom below.\n\n\n\n\n\n\n\n\nFigure 1: Chi-Square Distributions\n\n\n\n\n\nWe can find out the proportion of the distribution which falls to either side of a given value of \\(\\chi^2\\) using pchisq(). We need to give it our calculated \\(\\chi^2\\) statistic, our degrees of freedom (df), which is equal to the number of categories minus 1. We also need to specify whether we want the proportion to the left (lower.tail=TRUE) or to the right (lower.tail=FALSE).\n\nUsing pchisq(), calculate the probability of observing a \\(\\chi^2\\) statistic as least as extreme as the one we have calculated.\n\nCheck that these results match with those provided by R’s built-in function: chisq.test(table(usmr25$birthmonth)) (the table function will ignore NAs by default, so we don’t need to do anything extra for this).\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\nsum(chi_table$std_sq_diff)\n\n[1] 14.48052\n\npchisq(sum(chi_table$std_sq_diff), df = 11, lower.tail = FALSE)\n\n[1] 0.2075348\n\n\n\nchisq.test(table(usmr25$birthmonth))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(usmr25$birthmonth)\nX-squared = 14.481, df = 11, p-value = 0.2075\n\n\n\n\n\n\nQuestion 8\n\n\nWhich months of year had the highest contributions to the chi-square test statistic?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about your standardised squared deviations.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. Standardized squared deviations\nOne possible way to answer this question is to look at the individual contribution of each category to the \\(\\chi^2\\) statistic. We computed these values in an earlier question.\n\nchi_table |&gt;\n  select(birthmonth, std_sq_diff)\n\n# A tibble: 12 × 2\n   birthmonth std_sq_diff\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 apr             0.391 \n 2 aug             1.82  \n 3 dec             0.391 \n 4 feb             0.313 \n 5 jan             0.313 \n 6 jul             0.0271\n 7 jun             0.313 \n 8 mar             0.0530\n 9 may             1.04  \n10 nov             4.57  \n11 oct             0.391 \n12 sep             4.86  \n\n\nFrom the barplot we created earlier on, we can see which months make up higher/lower proportions than expected:\n\nggplot(chi_table, aes(x = birthmonth, y = observed/nrow(usmr25))) +\n  geom_col(fill = 'lightblue') +\n  geom_hline(yintercept = 1/12, color = 'red') +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nPearson residuals\nEquivalently, you could answer by looking at Pearson residuals:\n\nchisq.test(table(usmr25$birthmonth))$residuals\n\n\n       apr        aug        dec        feb        jan        jul        jun \n 0.6250541 -1.3488010  0.6250541 -0.5592589 -0.5592589 -0.1644879 -0.5592589 \n       mar        may        nov        oct        sep \n 0.2302831  1.0198251 -2.1383430  0.6250541  2.2041382 \n\n\nThe greatest absolute values are for sep and nov, showing that for these months the deviations from expected to observed were the greatest."
  },
  {
    "objectID": "04_ex.html#childrens-favourite-colours",
    "href": "04_ex.html#childrens-favourite-colours",
    "title": "W4: Binomial & Chi-Square Tests",
    "section": "Children’s Favourite Colours",
    "text": "Children’s Favourite Colours\n\nResearch Question: Do childrens’ favourite colours correspond to the those suggested by the internet?\n\n\nAccording to one part of the internet, 30% of children have red as their favourite colour, 20% have blue, 15% yellow, 11% purple, 9% green, and 15% prefer some other colour.\nWe collected data from 50 children aged between 2 and 5, and got them to choose one of a set of objects that were identical apart from colour. You can see the data in Table 1\n\n\n\n\nTable 1: Colour preferences of 50 children aged between 2 and 5\n\n\n\n\n\n\n\n\n\ncolour\nFreq\n\n\n\n\nblue\n10\n\n\ngreen\n6\n\n\nother\n3\n\n\npurple\n8\n\n\nred\n8\n\n\nyellow\n15\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nPerform a \\(\\chi^2\\) goodness of fit test to assess the extent to which our sample of children conform to this theorised distribution of colour preferences.\nNo need to do this manually - once is enough. Just go straight to using the chisq.test() function.\nHowever, we will need to get the numbers into R somehow..\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can make a table from scratch using, for example: as.table(c(1,2,3,4,5)).\nFor the test, try using chisq.test(..., p = c(?,?,?,...) ).\nWe saw the use of chisq.test() in the example goodness of fit test, Chapter 7 #chi2-goodness-of-fit-test\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. Let’s get the data in:\n\nchildcols &lt;- as.table(c(10,6,3,8,8,15))\nnames(childcols) &lt;- c(\"blue\",\"green\",\"other\",\"purple\",\"red\",\"yellow\")\nchildcols\n\n  blue  green  other purple    red yellow \n    10      6      3      8      8     15 \n\n\nOur theoretical probabilities of different colours must match the order in the table which we give chisq.test(). They must also always sum to 1.\n\nchisq.test(childcols, p = c(.20,.09,.15,.11,.30,.15))\n\nWarning in chisq.test(childcols, p = c(0.2, 0.09, 0.15, 0.11, 0.3, 0.15)):\nChi-squared approximation may be incorrect\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  childcols\nX-squared = 15.103, df = 5, p-value = 0.009931\n\n\nNote, we get a warning here of “Chi-squared approximation may be incorrect”. This is because some of the expected cell counts are &lt;5.\n\nchisq.test(childcols, \n           p = c(.20,.09,.15,.11,.30,.15))$expected\n\n  blue  green  other purple    red yellow \n  10.0    4.5    7.5    5.5   15.0    7.5 \n\n\nThere are a couple of options here, but the easiest is to use the functionality of chisq.test() that allows us to compute the p-value by using a simulation (similar to the idea we saw in Chapter 4 #sampling-&-sampling-distributions), rather than by comparing it to a theoretical \\(\\chi^2\\) distribution. We can do this by using:\n\nchisq.test(childcols, p = c(.20,.09,.15,.11,.30,.15),\n           simulate.p.value = TRUE)\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 2000 replicates)\n\ndata:  childcols\nX-squared = 15.103, df = NA, p-value = 0.01249\n\n\n\n\n\n\nQuestion 10\n\n\nWhat are the observed proportions of children who prefer each colour?\n\n\n\n\n\n\nHints\n\n\n\n\n\nLook up the prop.table() function?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. From the help documentation (?prop.table()), we see that we can pass prop.table() the argument x, which needs to be a table.\n\nprop.table(childcols)*100\n\n  blue  green  other purple    red yellow \n    20     12      6     16     16     30 \n\n\n\nbarplot(prop.table(childcols)*100)"
  },
  {
    "objectID": "04_ex.html#jokes-and-tips",
    "href": "04_ex.html#jokes-and-tips",
    "title": "W4: Binomial & Chi-Square Tests",
    "section": "Jokes and Tips",
    "text": "Jokes and Tips\n\nData: TipJokes\n\nResearch Question: Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?\n\nA study published in the Journal of Applied Social Psychology1 investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. The waiter randomly assigned coffee-ordering customers to one of three groups. When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all.\nThe data are available at https://uoepsy.github.io/data/TipJoke.csv\nThe dataset contains the variables:\n\nCard: None, Joke, Ad.\nTip: 1 = The customer left a tip, 0 = The customer did not leave tip.\n\n\n\nQuestion 11\n\n\nProduce a plot and a table to display the relationship between whether or not the customer left a tip, and what (if any) card they received alongside the bill.\nDon’t worry about making it all pretty. Mosaic plots in R are a bit difficult.\n\n\n\n\n\n\nHints\n\n\n\n\n\nplot(table(...)) will give you something. You can see one in the example \\(\\chi^2\\) test of independence,Chapter 7 #chi2-test-of-independence.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 11. \n\ntipjoke &lt;- read_csv('https://uoepsy.github.io/data/TipJoke.csv')\n\ntable(tipjoke$Card, tipjoke$Tip)\n\n      \n        0  1\n  Ad   60 14\n  Joke 42 30\n  None 49 16\n\nplot(table(tipjoke$Card, tipjoke$Tip))\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\nWhat would you expect the cell counts to look like if there were no relationship between what the waiter left and whether or not the customer tipped?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about what proportion of customers tipped. Then work out how many customers got each type of card. If there were no relationship, then the proportions would be the same in each group.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 12. In total, 60 customers tipped (14+30+16), and 151 did not. So overall, 0.28 (\\(\\frac{60}{(60+151)}\\)) of customers tip.\n74 customers got an Ad card, 72 customers got a Joke, and 65 got None. If this were independent of whether or not they left a tip, we would expect equal proportions of tippers in each group.\nSo we would expect 0.28 of each group to leave a tip.\n\n\n\n\n\n\nsome calculations\n\n\n\n\n\nYou can think about observed vs expected by looking at the two-way table along with the marginal row and column totals given:\n\n\n\n\n\n\n0\n1\n\n\n\n\n\nAd\n\n\n74\n\n\nJoke\n\n\n72\n\n\nNone\n\n\n65\n\n\n\n151\n60\n211\n\n\n\n\n\n\n\nFor a given cell of the table we can calculate the expected count as \\(\\text{row total} \\times \\frac{\\text{column total}}{\\text{samplesize}}\\):\nExpected:\n\n\n\n\n\n\n0\n1\n\n\n\n\n\nAd\n52.96\n21.04\n74\n\n\nJoke\n51.53\n20.47\n72\n\n\nNone\n46.52\n18.48\n65\n\n\n\n151.00\n60.00\n211\n\n\n\n\n\n\n\nIf you’re wondering how we do this in R.. here’s our table:\n\nt &lt;- tipjoke |&gt;\n  select(Card, Tip) |&gt; table()\nt\n\n      Tip\nCard    0  1\n  Ad   60 14\n  Joke 42 30\n  None 49 16\n\n\nHere are the row totals:\n\nrowSums(t)\n\n  Ad Joke None \n  74   72   65 \n\n\nand column totals divided by total:\n\ncolSums(t) / sum(t)\n\n        0         1 \n0.7156398 0.2843602 \n\n\nthere’s a complicated bit of code using %o% which could do this for us. You don’t need to remember %o%, it’s very rarely used):\n\ne &lt;- rowSums(t) %o% colSums(t) / sum(t)\ne\n\n            0        1\nAd   52.95735 21.04265\nJoke 51.52607 20.47393\nNone 46.51659 18.48341\n\n\nOr, alternatively, do it one by one:\n\nrowSums(t) * (colSums(t) / sum(t))[1]\n\n      Ad     Joke     None \n52.95735 51.52607 46.51659 \n\nrowSums(t) * (colSums(t) / sum(t))[2]\n\n      Ad     Joke     None \n21.04265 20.47393 18.48341 \n\n\n\n\n\n\n\n\n\nQuestion 13\n\n\nJust like we gave the chisq.test() function a table of observed frequencies when we conducted a goodness of fit test in earlier exercises, we can give it a two-way table of observed frequencies to conduct a test of independence.\nTry it now.\n\n\n\n\n\nSolution\n\n\n\nSolution 13. \n\nchisq.test(table(tipjoke$Card, tipjoke$Tip))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(tipjoke$Card, tipjoke$Tip)\nX-squared = 9.9533, df = 2, p-value = 0.006897"
  },
  {
    "objectID": "04_ex.html#footnotes",
    "href": "04_ex.html#footnotes",
    "title": "W4: Binomial & Chi-Square Tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. Journal of Applied Social Psychology, 32(9), 1955-1963.↩︎"
  },
  {
    "objectID": "07_ex.html#footnotes",
    "href": "07_ex.html#footnotes",
    "title": "W7: Multiple Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnother fake study!↩︎\napparently coughing is a method of immediately lowering heart rate!↩︎"
  },
  {
    "objectID": "09_ex.html",
    "href": "09_ex.html",
    "title": "W9: Interactions!",
    "section": "",
    "text": "Processing a hangover\n\nDataset: hangover_speed.csv\nHow is hours of sleep associated with processing speed? Is this dependent upon whether or not alcohol was consumed the previous night? 107 participants completed the Symbol Digit Modalities Task (SDMT), a measure of processing speed. Participants also recorded how many hours they had slept the previous night (to the nearest 15 mins), and whether or not they had consumed alcohol.\nThe dataset is available at https://uoepsy.github.io/data/hangover_speed.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nhrs_sleep\nhours slept the previous night (to the nearest 15 mins)\n\n\nalc\nwas alcohol consumed the previous evening? ('y'=yes, 'n'=no)\n\n\nsdmt\nscore on the Symbol Digit Modalities Task (SDMT), a measure of processing speed (range 0 to 100)\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and provide some simple descriptives.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWhat is the mean score on the SDMT, what is the variability in scores?\n\nHow many people had alcohol the previous night?\n\nHow many hours did people sleep on average? Did this vary between the drinkers and the non-drinkers?\n\n\n\n\n\n\n\n\nHere’s our data. Everything looks within plausible ranges for our two continuous variables hrs_sleep and sdmt.\n\nhodat &lt;- read_csv(\"https://uoepsy.github.io/data/hangover_speed.csv\")\n\nWe can get the means and SDs for hours slept and the SDMT:\n\nlibrary(psych)\ndescribe(hodat |&gt; select(-alc))\n\n          vars   n  mean    sd median trimmed   mad   min  max range skew\nhrs_sleep    1 107  7.20  1.20   7.25    7.18  1.11  4.25 10.5  6.25 0.20\nsdmt         2 107 54.59 12.13  53.00   54.23 10.38 22.00 86.0 64.00 0.28\n          kurtosis   se\nhrs_sleep     0.07 0.12\nsdmt          0.08 1.17\n\n\nWe can see that 57% of participants didn’t drink, and 43% of them did:\n\ntable(hodat$alc)\n\n\n n  y \n61 46 \n\n\nAnd it doesn’t look like they differed very much in their sleep times:\n\nhodat |&gt; group_by(alc) |&gt;\n  summarise(\n    slept = mean(hrs_sleep)\n  )\n\n# A tibble: 2 × 2\n  alc   slept\n  &lt;chr&gt; &lt;dbl&gt;\n1 n      7.12\n2 y      7.30\n\n\n\n\n\n\nQuestion 2\n\n\nMake a plot of SDMT predicted by hours slept, and colour the points by whether or not the participants had drank alcohol.\nCan you plot a separate lm line on the graph using geom_smooth for each group (alcohol v no alcohol)?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nto make geom_smooth() fit a linear model (lm), remember to use geom_smooth(method=lm).\n\nif you have the grouping in the aes(), then when you add geom_smooth() it should make a different one for each group!\n\n\n\n\n\n\n\n\n\nggplot(hodat, aes(x = hrs_sleep, y = sdmt, col = alc)) +\n  geom_point() +\n  geom_smooth(method=lm)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nAdding a different geom_smooth(method=lm) for each group is just fitting a different model to each groups’ data - i.e. a slope of sdmt~hrs_sleep for the drinkers and a slope for the non-drinkers.\nBut we actually want to test if the two slopes are different, and for that we need to create one model that includes the appropriate interaction term.\nFit a model to examine whether the association between hrs_sleep and sdmt is different depending upon alcohol consumption.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is the same logic as the air-pollution & APOE-4 example in Chapter 15 #it-depends.\n\n\n\n\n\n\n\n\nmod_int &lt;- lm(sdmt ~ hrs_sleep * alc, data = hodat)\nsummary(mod_int)\n\n\nCall:\nlm(formula = sdmt ~ hrs_sleep * alc, data = hodat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3384  -5.2919  -0.2911   6.6458  19.7389 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.1496     6.9631   0.452   0.6520    \nhrs_sleep        7.6499     0.9643   7.933 2.71e-12 ***\nalcy            17.7151    10.6874   1.658   0.1004    \nhrs_sleep:alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.914 on 103 degrees of freedom\nMultiple R-squared:  0.4753,    Adjusted R-squared:   0.46 \nF-statistic:  31.1 on 3 and 103 DF,  p-value: 2.136e-14\n\n\n\n\n\n\nQuestion 4\n\n\nInterpret each coefficient from your model.\n\n\n\n\n\n\nHints\n\n\n\n\n\nOur interaction involves a continuous variable (hrs_sleep) and a binary variable (alc). An interpretation of a similar example is in Chapter 15 #interpretation.\n\n\n\n\n\n\n\n\nsummary(mod_int)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.1496     6.9631   0.452   0.6520    \nhrs_sleep        7.6499     0.9643   7.933 2.71e-12 ***\nalcy            17.7151    10.6874   1.658   0.1004    \nhrs_sleep:alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\n\n\n\n\n\n\n\n\ncoefficient\nestimate\ninterpretation\n\n\n\n\n(Intercept)\n3.15\nA non-drinker who slept 0 hours is estimated to have an SDMT score of 3.1 - note this is not significantly different from zero\n\n\nhrs_sleep\n7.65\nFor every additional hour slept, non-drinkers scores on SDMT are estimated to increase by 7.6\n\n\nalcy\n17.72\ndrinkers who slept zero hours are estimated to have 17.7 higher scores on SDMT than non-drinkers who slept zero hours - note this is not significantly different from zero\n\n\nhrs_sleep:alcy\n-3.59\nFor every additional hour's sleep, drinkers scores on SDMT increase by 3.6 *less* than they do for non-drinkers\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nConstruct a plot of the model estimated associations between hours-slept and SDMT for drinkers and non-drinkers.\nBecause we have nothing else in our model, this should end up looking exactly the same as our initial plot in Question 2!\n\n\n\n\n\n\nHints\n\n\n\n\n\nIt all follows the same logic as we have used before:\n\nmake a dataframe of the values of the predictors that we wish to plot across\nusing augment(), add to that the predicted values of the model, and the associated confidence intervals\nshove it all in ggplot!\n\nBecause we are wanting to plot across multiple predictors (i.e. we want to plot across a range of hrs_slept and both values of alc), try using expand_grid().\nPlay around with this to see what it does:\n\nexpand_grid(\n  continuous = 1:5,\n  binary = c(\"dog\",\"cat\",\"parrot\")\n)\n\n# A tibble: 15 × 2\n   continuous binary\n        &lt;int&gt; &lt;chr&gt; \n 1          1 dog   \n 2          1 cat   \n 3          1 parrot\n 4          2 dog   \n 5          2 cat   \n 6          2 parrot\n 7          3 dog   \n 8          3 cat   \n 9          3 parrot\n10          4 dog   \n11          4 cat   \n12          4 parrot\n13          5 dog   \n14          5 cat   \n15          5 parrot\n\n\nIf you get stuck, a very similar example is in Chapter 15 #visualisation.\n\n\n\n\n\n\n\n\n# plot data\nplotdat &lt;- expand_grid(\n  hrs_sleep = 0:14,\n  alc = c(\"n\",\"y\")\n)\n# plot\nbroom::augment(mod_int, newdata = plotdat, interval=\"confidence\") |&gt;\n  ggplot(aes(x= hrs_sleep, y = .fitted, \n             col = alc, fill = alc)) + \n  geom_line() +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nNo one in our dataset has slept zero hours, and i’m probably not interested in differences between drinkers/non-drinkers who never sleep.\nRefit the model to adjust the intercept to a more meaningful value. Everyone always goes on about 8 hours of sleep being the minimum?\nHow has the interpretation of your coefficient(s) changed?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Ch 15 #mean-centering for an example of mean-centering a predictor in the interaction. Remember that there are multiple ways to do this - you could make a new variable first, or you could do it all inside the model.\n\n\n\n\n\n\n\n\nSolution Part 1 - recentering and refitting\n\n\n\nSolution 1. If we want to recenter our hrs_sleep variable on 8 hours, we could either create a new variable which is hrs_sleep - 8 (which would therefore make everyone who slept 8 hours be given a value of 0 in the variable), or we could do it in the model:\n\n\nre-centering first\n\nhodat &lt;- \n  hodat |&gt;\n    mutate(\n      hrs_8 = hrs_sleep - 8\n    )\n\nmod_int2 &lt;- lm(sdmt ~ hrs_8 * alc, data = hodat)\n\n\n\ndoing it all in the model\n\nmod_int2 &lt;- lm(sdmt ~ I(hrs_sleep-8) * alc, data = hodat)\n\n\n\n\n\n\n\n\nSolution Part 2 - interpreting coefficients\n\n\n\nSolution 2. \n\nsummary(mod_int2)\n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            64.3487     1.4206  45.297  &lt; 2e-16 ***\nI(hrs_sleep - 8)        7.6499     0.9643   7.933 2.71e-12 ***\nalcy                  -10.9786     2.0800  -5.278 7.27e-07 ***\nI(hrs_sleep - 8):alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\n\n\n\n\n\n\n\n\ncoefficient\nestimate\ninterpretation\n\n\n\n\n(Intercept)\n64.35\nA non-drinker who slept 8 hours is estimated to have an SDMT score of 64.3\n\n\nI(hrs_sleep - 8)\n7.65\nFor every additional hour slept, non-drinkers scores on SDMT are estimated to increase by 7.6\n\n\nalcy\n-10.98\ndrinkers who slept 8 hours are estimated to have 11 lower scores on SDMT than non-drinkers who slept 8 hours\n\n\nI(hrs_sleep - 8):alcy\n-3.59\nFor every additional hour's sleep, drinkers scores on SDMT increase by 3.6 *less* than they do for non-drinkers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe monkeys are back!\n\nData: ctmtoys.csv\nSo far, we have analysed the data for two studies (not real!) of the inquisitive nature of monkeys. Initially (week 5 exercises), Archer, Winther & Gandolfi (2024a) investigated age differences in exploration of novel objects, and found that older monkeys spend on average less time playing with a novel object than their younger counterparts (we looked at this with both with the linear effect of age in years, and by comparing adults to juveniles). Following this, in our week 7 exercises Bolton, Archer, Peng, Winther & Gandolfi (2024b) wanted to see if monkeys showed a preference for different types of object (i.e. ones with moving parts vs ones that are soft). They found that, after accounting for differences due to age, monkeys showed a significant preference for toys with moving parts in comparison to soft toys.\nThe same team of researchers are again asking for our help, and this time with a bigger study, of 107 monkeys. They are interested in whether the preference for mechanical toys over soft toys is different for different species of monkey. Both the previous studies were conducted on Rhesus Macaques (a species that have adapted very well to human dominated landscapes), so this study has re-run the same experiment on 0 Capuchin monkeys, 0 Tamarin monkeys and 0 Macaques.\nThe aim of this study is to investigate the following question:\n\nAre preferences between soft toys vs mechanical toys different for different species of monkey?\n\nThe data is available at https://uoepsy.github.io/data/ctmtoys.csv and contains the variables described in Table 1\n\n\n\n\nTable 1: Data dictionary for ctmtoys.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nname\nMonkey Name\n\n\nage\nAge of monkey in years\n\n\nspecies\nSpecies (capuchin, macaque, tamarin)\n\n\nobj_type\nType of novel object given (mechanical / soft)\n\n\nobj_colour\nMain colour of object (red / green / blue)\n\n\nobj_size\nSize of object in cm (length of largest dimension of the object)\n\n\nexptime\nTime (in minutes) spent exploring the object\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nAs always, begin by reading in your data and making some exploratory plots to get an idea of the distributions we’re dealing with.\n\n\n\n\nEverything looks okay in terms of our variable ranges here:\n\nctmtoys &lt;- read_csv(\"https://uoepsy.github.io/data/ctmtoys.csv\")\nsummary(ctmtoys)\n\n     name                age          species            obj_type        \n Length:216         Min.   : 1.00   Length:216         Length:216        \n Class :character   1st Qu.: 8.00   Class :character   Class :character  \n Mode  :character   Median :13.00   Mode  :character   Mode  :character  \n                    Mean   :12.92                                        \n                    3rd Qu.:18.00                                        \n                    Max.   :25.00                                        \n  obj_colour           obj_size        exptime     \n Length:216         Min.   : 5.00   Min.   : 4.20  \n Class :character   1st Qu.:36.00   1st Qu.:11.00  \n Mode  :character   Median :49.00   Median :13.45  \n                    Mean   :49.06   Mean   :13.66  \n                    3rd Qu.:62.25   3rd Qu.:16.75  \n                    Max.   :94.00   Max.   :25.10  \n\n\nLet’s shove it in pairs.panels:\n\nctmtoys |&gt; \n  select(age, obj_size, exptime) |&gt;\n  psych::pairs.panels()\n\n\n\n\n\n\n\n\nAnd let’s tabulate obj_type and species:\n\nctmtoys |&gt;\n  select(obj_type, species) |&gt;\n  table()\n\n            species\nobj_type     capuchin macaque tamarin\n  mechanical       33      43      32\n  soft             36      33      39\n\n\n\n\n\n\nQuestion 8\n\n\nTry making some initial exploratory plots of the relationships in the data that are relevant to the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’re wanting to plot exploration_time and obj_type here, but we’re also wanting to show it for each species. This means we’ll need things like colours, facets, etc.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. From initial exploration - it looks like capuchins are generally spending more time playing with the objects (of both types).\nThe slight preference for mechanical toys seems to be present Capuchins and Macaques, but it’s less clear in the Tamarins.\n\nggplot(ctmtoys, aes(x = obj_type, y = exptime, \n                    col = obj_type)) +\n  geom_violin() + \n  geom_jitter(width=.2, alpha=.4) + \n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nAll of this is just initial speculation, however - we haven’t considered lots of things such as the distributions of ages between the species, or whether they all saw objects of similar sizes and colours. These things could make these plots appear to show relationships that are actually driven by other things. They could also make these plots hide relationships that are present once these things are controlled for.\n\n\n\n\nQuestion 9\n\n\nFit an appropriate model to address the research question.\nThink about what we already know from previous studies - we’ll likely want to control for age and for other aspects of objects like size and colour.\nThen think about the specific research question and what is needed to test it.\n\n\n\n\n\nmodelmonkey &lt;- lm(exptime ~ age + obj_size + obj_colour +\n                obj_type * species, data = ctmtoys)\n\n\n\n\n\nQuestion 10\n\n\nNow that you’ve fitted your model, test the interaction to get an overall answer to “Do species differ in their preference for different types of object?”\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about how the question is worded - there’s no “how”/“what” etc, it’s just “are there differences?” (this is just the same as we did last week, and in Chapter 14 #testing-group-differences - try a model comparison?).\n\n\n\n\n\n\n\nLet’s compare a reduced model without the interaction to our model with the interaction:\n\nmodelmonkey0 &lt;- lm(exptime ~ age + obj_size + obj_colour +\n                obj_type + species, data = ctmtoys)\n\nanova(modelmonkey0, modelmonkey)\n\nAnalysis of Variance Table\n\nModel 1: exptime ~ age + obj_size + obj_colour + obj_type + species\nModel 2: exptime ~ age + obj_size + obj_colour + obj_type * species\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    208 2339.2                              \n2    206 2257.0  2    82.136 3.7483 0.02518 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlternatively, we could use the shortcut and just give one model to anova() and take a look at the last line. They should be the same:\n\nanova(modelmonkey)\n\nAnalysis of Variance Table\n\nResponse: exptime\n                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nage                1  293.67  293.67 26.8034 5.340e-07 ***\nobj_size           1    0.42    0.42  0.0381   0.84546    \nobj_colour         2    8.91    4.46  0.4067   0.66636    \nobj_type           1  177.27  177.27 16.1798 8.085e-05 ***\nspecies            2  892.98  446.49 40.7517 1.225e-15 ***\nobj_type:species   2   82.14   41.07  3.7483   0.02518 *  \nResiduals        206 2257.02   10.96                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSpecies significantly differed in the extent to which exploration time varied between different types of object (\\(F(2, 206)=3.75, p=0.0252\\)).\n\n\n\n\n\nQuestion 11\n\n\nWe’re almost always going to want to know how species differ in their preferences for different types of object (if they do differ).\nTo get this information, we can just look at the model coefficients.\nTake a look at the interaction coefficients - notice anything that appears to contradict the previous question?\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nsummary(modelmonkey)\n\n\nCall:\nlm(formula = exptime ~ age + obj_size + obj_colour + obj_type * \n    species, data = ctmtoys)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.959 -2.348 -0.045  2.015  9.745 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 19.33098    1.03529  18.672  &lt; 2e-16 ***\nage                         -0.16464    0.03961  -4.157 4.73e-05 ***\nobj_size                     0.01122    0.01252   0.896   0.3712    \nobj_colourgreen             -0.14963    0.55815  -0.268   0.7889    \nobj_colourred               -0.19467    0.55887  -0.348   0.7279    \nobj_typesoft                -1.96193    0.80839  -2.427   0.0161 *  \nspeciesmacaque              -3.90720    0.77012  -5.073 8.70e-07 ***\nspeciestamarin              -5.08712    0.82839  -6.141 4.16e-09 ***\nobj_typesoft:speciesmacaque -1.51124    1.11855  -1.351   0.1782    \nobj_typesoft:speciestamarin  1.53986    1.15128   1.338   0.1825    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.31 on 206 degrees of freedom\nMultiple R-squared:  0.392, Adjusted R-squared:  0.3655 \nF-statistic: 14.76 on 9 and 206 DF,  p-value: &lt; 2.2e-16\n\n\nIn our model coefficients, both interaction terms are non-significant.\nBut we just said before that in the previous question that there is an interaction (we got our F test for it).\nThe coefficients above are telling us that, compared to the reference species (capuchins), neither macaques nor tamarins show significant differences in their preference for mechanical toys vs soft toys.\nBut while macaques and tamarins may not be different from capuchins, it is entirely possible that macaques and tamarins could be different from each other?\n\n\n\n\nQuestion 12\n\n\nModel coefficients are always relative to some reference point (i.e. capuchins with mechanical toys).\n\nChange the reference point to Macaques with soft toys.\n\nRefit the model.\nInterpret the coefficients.\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. We always get to choose which level we want as a reference. Sometimes, this will be easy because on level represents the norm/status quo. In this context, Macaques is possibly a more useful reference, given that the study description earlier mentioned the previous studies were conducted on Macaques.\nRe-levelling obj_type to have “soft” as the reference won’t make much difference - it will just swap the signs of the coefficients (comparing mechanical to soft, as opposed to comparing soft to mechanical).\n\nctmtoys &lt;- \n  ctmtoys |&gt;\n  mutate(\n    species = fct_relevel(species, \"macaque\"),\n    obj_type = fct_relevel(obj_type, \"soft\")\n  )\n\nlm(exptime ~ age + obj_size + obj_colour +\n                obj_type * species, data = ctmtoys) |&gt;\n  summary()\n\n\nCall:\nlm(formula = exptime ~ age + obj_size + obj_colour + obj_type * \n    species, data = ctmtoys)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.959 -2.348 -0.045  2.015  9.745 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        11.95061    1.01699  11.751  &lt; 2e-16 ***\nage                                -0.16464    0.03961  -4.157 4.73e-05 ***\nobj_size                            0.01122    0.01252   0.896  0.37122    \nobj_colourgreen                    -0.14963    0.55815  -0.268  0.78890    \nobj_colourred                      -0.19467    0.55887  -0.348  0.72794    \nobj_typemechanical                  3.47317    0.76957   4.513 1.07e-05 ***\nspeciescapuchin                     5.41843    0.80728   6.712 1.83e-10 ***\nspeciestamarin                      1.87117    0.80555   2.323  0.02116 *  \nobj_typemechanical:speciescapuchin -1.51124    1.11855  -1.351  0.17816    \nobj_typemechanical:speciestamarin  -3.05110    1.11480  -2.737  0.00674 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.31 on 206 degrees of freedom\nMultiple R-squared:  0.392, Adjusted R-squared:  0.3655 \nF-statistic: 14.76 on 9 and 206 DF,  p-value: &lt; 2.2e-16\n\n\nThese coefficients show us that Macaques tend to spend 3.5 more minutes playing with mechanical toys than soft toys, and that Capuchins do not significantly differ in this respect, but Tamarins do. Relative to Macaques, Tamarins spend 3.05 less additional time with mechanical toys compared to soft toys.\n\n\n\n\nQuestion 13\n\n\nThe interpretation of interaction coefficients tends to be quite confusing, and invariably it helps to tie these to a visualisation. We’re going to do it manually here, because it’s a very useful learning exercise.\nBelow are our coefficients of interest from the model, when the reference level for obj_type is “soft” and for species is “macaque”.\n...\nobj_typemechanical                  3.47317    0.76957   4.513 1.07e-05 ***\nspeciescapuchin                     5.41843    0.80728   6.712 1.83e-10 ***\nspeciestamarin                      1.87117    0.80555   2.323  0.02116 *  \nobj_typemechanical:speciescapuchin -1.51124    1.11855  -1.351  0.17816    \nobj_typemechanical:speciestamarin  -3.05110    1.11480  -2.737  0.00674 ** \nGrab a piece of paper, and draw the points for each species & obj_type combination, relative to the reference point.\nStart with the plot below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 1 - coefficient: obj_typemechanical\n\n\n\nSolution 6. The coefficient “obj_typemechanical” tells us the difference between soft and mechanical toys when species is zero - i.e., when it is at the reference level (macaques).\nSo we know that macaques spend 3.5 more minutes with mechanical toys compared to soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 2 - coefficient: speciescapuchin\n\n\n\nSolution 7. The coefficient “speciescapuchin” tells us the difference between capuchins and macaques, when obj_type is zero - i.e., when it is at the reference level (soft).\nSo we know that capuchins spend 5.4 more minutes with soft toys compared to macaques with soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 3 - coefficient: speciestamarin\n\n\n\nSolution 8. The coefficient “speciestamarin” tells us the difference between tamarins and macaques, when obj_type is zero - i.e., when it is at the reference level (soft).\nSo we know that tamarins spend 1.9 more minutes with soft toys compared to macaques with soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 4 - coefficient: obj_typemechanical:speciescapuchin\n\n\n\nSolution 9. The coefficient “obj_typemechanical:speciescapuchin” tells us the difference between capuchins and macaques’ soft-vs-mechanical differences.\ni.e. when moving from macaques to capuchins, the soft-to-mechanical difference changes by this much.\nSo we know that whereas macaques spend 3.5 more minutes with mechanical toys compared to soft toys, for capuchins this is 1.5 minutes less.\nSo rather than going up 3.5, we go up \\(3.5-1.5=2\\).\nHowever, this is non-significant. So the difference could easily just be 0 - i.e. they could increase by the same 3.5 minutes as Macaques do. So we know that the uncertainty here should also capture as if Capuchins and Macaques have the same increases from soft to mechanical.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 5 - coefficient: obj_typemechanical:speciestamarin\n\n\n\nSolution 10. The coefficient “obj_typemechanical:speciestamarin” tells us the difference between tamarins and macaques’ soft-vs-mechanical differences.\ni.e. when moving from macaques to tamarins, the soft-to-mechanical difference changes by this much.\nSo we know that whereas macaques spend 3.5 more minutes with mechanical toys compared to soft toys, for tamarins this is 3.05 minutes less.\nSo rather than going up 3.5, we go up \\(3.5-3.05=0.45\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 14\n\n\nOkay, now let’s make a plot in R.\nTry running this code in pieces to see what each bit does, and then running it all at once to get the plot.\nDoes it match with what you sketched in the previous question?\n\nlibrary(effects)\neffect(\"obj_type*species\",modelmonkey) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    obj_type = fct_relevel(factor(obj_type), \"soft\")\n  ) |&gt;\n  ggplot(aes(x=obj_type, y=fit, col=species)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper))\n\n\n\n\n\n\n\nWhy use the effects package?\n\n\n\n\n\nUp to now, when we’ve been plotting our associations of interest we’ve been choosing to construct our plots at the mean of our other predictors.\nHowever, in our current monkey model, we’ve also got a categorical covariate (obj_colour) in our model. What should we do with that?\n\nplotdat &lt;- expand_grid(\n  age = mean(ctmtoys$age),\n  obj_size = mean(ctmtoys$obj_size),\n  obj_colour = ???\n  obj_type = c(\"soft\",\"mechanical\"), # of interest\n  species = c(\"macaque\",\"capuchin\",\"tamarin\") # of interest\n)\n\nWe could:\n\nchoose just one colour to plot it at\nmake separate plots to each colour\nplot the association of interest holding the colours at their proportions\n\nTo achieve a) or b), we can use the strategy we have been using already (make a little dataframe, use augment etc).\nHowever, to achieve c), it is easiest to use something like the effect() function from the effects package. This will also come in handy next semester, as we will use it for plotting effects from more complex models.\n\n\n\n\n\n\n\n\nlibrary(effects)\neffect(\"obj_type*species\",modelmonkey) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    obj_type = fct_relevel(factor(obj_type), \"soft\")\n  ) |&gt;\n  ggplot(aes(x=obj_type, y=fit, col=species)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeamwork and Communication\n\nDataset: teamprod.csv\nA company has recently decided to move towards having more structured team-based projects for its employees, rather than giving individual projects. They want better understanding what makes teams work well together. Specifically, they want to know whether the amount of communication in a team is associated with better quality work, and if this is different depending upon the teams’ ‘collective efficacy’ (their belief in their ability to do the work)?\nThey have collected data on 80 teams. Prior to starting the projects, each team completed a questionnaire measuring ‘collective efficacy’ (the teams’ belief in their ability to succeed at their project), and ‘collective experience’ (a measure of how much relevant experience the team has). At the end of the projects, each team’s work was rated across various measures (timeliness, quality, relevance etc) to provide a metric of ‘work quality’. In addition, information was gathered on the volume of each teams’ communication (via the company’s workspace chat platform).\nThe data is available at https://uoepsy.github.io/data/teamqual.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nteam_id\nTeam ID\n\n\ncommvol\nVolume of communication (avg messages per day)\n\n\nexp\nPrior experience (Z-scored)\n\n\ncolleff\nCollective Efficacy measure of team's belief in ability to succeed (range 0 - 70)\n\n\nwork_qual\nWork Quality measure (metric based on timeliness, quality, relevance etc). Ranges 0 to Infinity\n\n\n\n\n\n\n\n\n\nQuestion 15\n\n\nBelow, we have provided a regression table, a plot, and a written paragraph.\nThere are lots of mistakes in the writing (both mismatching numbers and errors in interpretation). Note down as many errors as you can find.\nFeel free to read in the data yourself to play around.\n\n\nTable\n\n\n\n\n\n \nwork qual\n\n\nPredictors\nEstimates\nstd. Error\nStatistic\np\n\n\n(Intercept)\n-25.29\n15.21\n-1.66\n0.100\n\n\nexp\n0.87\n2.05\n0.42\n0.674\n\n\ncommvol\n2.69\n0.45\n6.04\n&lt;0.001\n\n\ncolleff\n1.60\n0.44\n3.63\n0.001\n\n\ncommvol × colleff\n-0.04\n0.01\n-2.63\n0.010\n\n\nObservations\n80\n\n\nR2 / R2 adjusted\n0.603 / 0.582\n\n\n\n\n\n\n\n\n\nPlot\n\n\n\n\n\n\n\n\n\n\n\nWriting\nWork quality was modelled using multiple regression. Team experience (Z-scored), Communication volume (messages per day) and Collective efficacy (Z-scored) were included as predictors, along with the interaction between communication and collective efficacy. The model explained 80% of the variance in work-quality scores. More experienced teams were found to produce significantly better quality work (\\(\\beta=0.82, t(74)=0.4, p&gt;.05\\)). Volume of communication was significantly associated with work quality (\\(\\beta=2.68, t(75)=6.17, p&lt;.001\\)), suggesting that teams that communicate more produced better quality work. Collective efficacy was also significantly associated with work quality (\\(\\beta=1.59, t(75)=3.64, p&lt;.001\\)), indicating that better quality work will be produced by a team that has collective efficacy (compared to those that do not). A significant interaction (\\(\\beta=-0.03, t=2.68, p &lt; .09\\)) was found between volume of communication and collective efficacy, suggesting that these two predictors are related. Overall these results suggest that for teams that have more collective efficacy, communication is more important in producing quality work."
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "Notes for Wizards\n\n\n\n\n\nhere’s a note!\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\n\n\n\n\nlearning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n\nSolution\n\n\n\nSolution 1. solution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  }
]