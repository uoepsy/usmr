[
  {
    "objectID": "01_ex.html",
    "href": "01_ex.html",
    "title": "Exercises: Intro R",
    "section": "",
    "text": "First things\nThe very first things to do are to open RStudio and get a blank script ready for writing your code!\n\n\nOur recommendation is that you have an R project for this course, and use a new script for each week of work. See the tip about “R projects” in Section 1A.\n\n\nPet Data\n\nWe’re going to play with some data on a sample of licensed pets from the city of Seattle, USA. It can be downloaded (or read directly into R) from https://uoepsy.github.io/data/pets_seattle.csv. It contains information on the license ID, year of issue, as well as the species, breeds and weights of each pet. You can find a data dictionary in Table 1\n\n\n\n\nTable 1: Seattle Pets: Data dictionary\n\n\nVariable\nDescription\n\n\n\n\nlicense_year\nYear in which license was issued\n\n\nlicense_number\nUnique license ID number\n\n\nanimals_name\nFull name of pet\n\n\nspecies\nSpecies of pet\n\n\nprimary_breed\nPrimary breed of pet\n\n\nsecondary_breed\nSecondary breed of pet\n\n\nweight_kg\nWeight in kilograms\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nWrite a line of code that reads in the data to your R session. Then examine the dimensions of the dataset, and take a look at the first few lines.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll need the read.csv() function. Remember to assign it a name to store it in your environment.\n1B #basic-data-wrangling contains an example of reading in data from a URL. You’ll then want to play with functions like dim() and head().\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe’re going to call it petdata in our environment here. Don’t forget the quotation marks around the url (otherwise R will look for an object in your environment called https://..., which isn’t there).\n\npetdata&lt;-read.csv(\"https://uoepsy.github.io/data/pets_seattle.csv\")\ndim(petdata)\n\n[1] 1956    7\n\n\nWe can see there are 1956 rows and 7 columns.\nAnd we can see the first few rows here:\n\nhead(petdata)\n\n  license_year license_number  animals_name species         primary_breed\n1         2018      LNS150171        Norman     Dog                 Boxer\n2         2017        LN20666         Henry     Dog          Bichon Frise\n3         2018      LN8000658 Vega Williams     Dog                   Mix\n4         2018       LN730940         Molly     Dog   Australian Shepherd\n5         2016       LN964607         Gremy     Dog Chihuahua, Short Coat\n6         2018      LNS117115        Shadow     Dog   Retriever, Labrador\n  secondary_breed weight_kg\n1             Mix     29.15\n2        Havanese     23.70\n3         Unknown     21.13\n4             Mix     18.70\n5         Terrier     20.36\n6         Unknown     11.51\n\n\n\n\n\n\nQuestion 2\n\n\nWhat are the names of the 47th and the 200th animals in the dataset? (use R code to find out)\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll probably want to make use of the square brackets data[rows, columns].\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are lots of different ways to do this. We can get out the entire rows, either individually:\n\npetdata[47,]\npetdata[200,]\n\nOr together:\n\npetdata[c(47,200),]\n\n    license_year license_number animals_name species       primary_breed\n47          2018      LNS140233     Hooligan     Dog Retriever, Labrador\n200         2017       LN584186  Maple Syrup     Cat  Domestic Shorthair\n    secondary_breed weight_kg\n47          Unknown     12.27\n200         Unknown      4.66\n\n\nOr we can extract the names only:\n\n# These all do the same\npetdata[c(47,200),\"animals_name\"]\npetdata[c(47,200),3]\npetdata$animals_name[c(47,200)]\n\nThe will all give us these names:\n\n\n[1] \"Hooligan\"    \"Maple Syrup\"\n\n\nIn the last one, we use the $ to access the animals_name variable. In this case, we don’t need to specify [rows, columns] inside the square brackets, because it’s a single variable - there are no columns.\n\ndataframe[rows, columns]\n\nvariable[entries]\n\n\n\n\n\nQuestion 3\n\n\nSubset the data to only the animals which are dogs, and store this subset as another named object in your environment.\nDo the same for the cats.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll want to think about how we access data via asking for those entries that meet a specific condition (see 1B #accessing-by-a-condition)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe can ask “which entries of species variable are equal to ‘Dog’?” by using pet$species==\"Dog\".\nThis will give us a TRUE for each dog, and a FALSE for each non-dog.\nWe can then use this set of TRUEs and FALSEs to access those rows for which it is TRUE in our data:\n\ndogdata &lt;- petdata[petdata$species==\"Dog\", ]\ncatdata &lt;- petdata[petdata$species==\"Cat\", ]\n\n\n\n\n\nQuestion 4\n\n\nFind the name and weight of the heaviest cat, and of the lightest dog.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou could do this using the original data you read in from question 1, or use the subsets you created in question 3. You’ll again want to supply a condition within square brackets data[?==?]. That condition may well have something to do with being equal to the min() or the max() of some variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe can use min() and max() to return the minimum and maximum of a variable:\n\nmin(dogdata$weight_kg)\n\n[1] 0.39\n\nmax(catdata$weight_kg)\n\n[1] 5.48\n\n\nWe could then ask for each entry “is this cat’s weight the maximum cat’s weight?” with catdata$weight_kg == max(catdata$weight_kg) and then use that condition to access the rows in our dataset where the weight_kg variable is at its maximum:\n\ncatdata[catdata$weight_kg == max(catdata$weight_kg), ]\n\n    license_year license_number animals_name species      primary_breed\n414         2018      LNS101014       Smokey     Cat Domestic Shorthair\n    secondary_breed weight_kg\n414             Mix      5.48\n\ndogdata[dogdata$weight_kg == min(dogdata$weight_kg), ]\n\n     license_year license_number animals_name species  primary_breed\n1126         2017      LNS139134       Claire     Dog Great Pyrenees\n     secondary_breed weight_kg\n1126         Unknown      0.39\n\n\n\n\n\n\nQuestion 5\n\n\nDoes the original dataset contain only dogs and cats?\n\n\n\n\n\n\nHints\n\n\n\n\n\nGiven what you did in question 3, you might be able to answer this by just looking at your environment.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIn the environment, we can see that the entire dataset has 1956 observations, the Dog’s data frame has 1322, and the Cat’s has 632.\nSo there are 2 missing!\n\n\n\n\nQuestion 6\n\n\nExtract the entries of the original dataset for which the species is neither “Dog” nor “Cat”?\nWhat are the names and species of these animals?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is a slightly complex one. 1B #more-complex-conditions might help you here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAs always, there are lots of different ways.\nHere are three:\n\n\n“not a dog AND not a cat”\nWe can ask if something is not a dog by using petdata$species != \"Dog\". But we want the rows where the species is not a dog and it’s not a cat. So it’s two conditions:\n\npetdata[petdata$species != \"Cat\" & petdata$species != \"Dog\", ]\n\n     license_year license_number     animals_name species primary_breed\n1505         2018      LNS147013    Billy the Kid    Goat     Miniature\n1655         2018      LNS132953 Vincent Van Goat    Goat     Miniature\n     secondary_breed weight_kg\n1505         Unknown    103.48\n1655         Unknown     73.96\n\n\n\n\n“not (dog OR cat)”\nWe could also do this in other ways, such as asking for all the entries which are either “Dog” or “Cat”, and then negating them:\n\npetdata[!(petdata$species == \"Cat\" | petdata$species == \"Dog\"), ]\n\n     license_year license_number     animals_name species primary_breed\n1505         2018      LNS147013    Billy the Kid    Goat     Miniature\n1655         2018      LNS132953 Vincent Van Goat    Goat     Miniature\n     secondary_breed weight_kg\n1505         Unknown    103.48\n1655         Unknown     73.96\n\n\n\n\n“not one of [Dog, Cat]”\nAnother clever little operator is the %in% operator, which asks whether something is in a set of things. Unfortunately, we can’t use !%in% to mean “not in”, so we need to put the ! right at the start of the condition:\n\npetdata[!petdata$species %in% c(\"Cat\",\"Dog\"), ]\n\n     license_year license_number     animals_name species primary_breed\n1505         2018      LNS147013    Billy the Kid    Goat     Miniature\n1655         2018      LNS132953 Vincent Van Goat    Goat     Miniature\n     secondary_breed weight_kg\n1505         Unknown    103.48\n1655         Unknown     73.96\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nCreate a new variable in the data, which contains the weights of all the animals, but rounded to the nearest kg.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTry looking up the help documentation for the function round(). Try playing with it in the console, e.g. round(c(3.5, 4.257, 1.1111)). You may find it helpful to look back at 1B #adding/changing-a-variable.\n\n“to the nearest kg” would mean we want no decimal points. Note that round() has a digits argument. e.g. round(22.324, digits = 2) and round(22.324, digits = 1) do different things.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe’re wanting this variable as a new column in the data, so don’t forget the dataframe$newvariable &lt;- ...... bit.\n\npetdata$weight_rounded &lt;- round(petdata$weight_kg)\n\n\n\n\n\nQuestion 8\n\n\nTry giving the dataset to the function summary(). You’ll get out some information on each of the variables. It is likely that you’ll get more useful information for the variables containing information on the animal’s weights than for those containing their names, breeds etc because these variables are vectors of “characters”. We’ll start to look more about different types of data next week.\n\n\n\n\n\nSolution\n\n\n\nEasy to do!\n\nsummary(petdata)\n\n  license_year  license_number     animals_name         species         \n Min.   :2015   Length:1956        Length:1956        Length:1956       \n 1st Qu.:2017   Class :character   Class :character   Class :character  \n Median :2018   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2018                                                           \n 3rd Qu.:2018                                                           \n Max.   :2018                                                           \n primary_breed      secondary_breed      weight_kg       weight_rounded \n Length:1956        Length:1956        Min.   :  0.390   Min.   :  0.0  \n Class :character   Class :character   1st Qu.:  4.707   1st Qu.:  5.0  \n Mode  :character   Mode  :character   Median : 16.630   Median : 17.0  \n                                       Mean   : 15.312   Mean   : 15.3  \n                                       3rd Qu.: 22.500   3rd Qu.: 22.0  \n                                       Max.   :103.480   Max.   :103.0  \n\n\n\n\n\n\n\n\n\n\nSimulating Dice\n\nQuestion 9\n\n\nCopy the code from the lecture which creates a custom function called dice() (copied below).\nBe sure to run the code (highlight it all with your cursor, and hit “run” in the top right, or press Ctrl/Cmd+Enter).\n\ndice &lt;- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n\n\n\n\n\nWhat did that code do?\nIn a sense, this code does nothing: It won’t give you any output when you run it. What it is actually doing, though, is defining a function called dice(). If you look at your environment panel (top right), you’ll see dice appear when you run the code.\nTo produce some output, we have to call the function dice() (by writing it into code: dice(4), for example). dice() wants to be supplied with some information (in the argument num). If no information is supplied, num will take a default value of 1. (So writing dice() is equivalent to writing dice(1)).\nWhat does dice() do with num? It calls another function, sample(), with 3 arguments. We didn’t write sample(): it’s a function that’s “supplied with” R. To find out more about what sample() does:\n\nclick inside the brackets just after sample() in your R script;\npress TAB (⇥), then F1\nyou should see some help appear in the bottom right-hand panel of RStudio.\n\nYou will find that “sample() takes a sample … from the elements of x …” If you compare the code in RStudio to the code under “Usage” you’ll see that where the help has x, we have 1:6. So what does 1:6 mean? One way to find out is to open the console in RStudio (bottom left) and just type stuff in. What happens when you type 1:6? What about 2:17? (What about 6:1?)\nRemember: The console is the place to “try stuff out” (don’t worry, you can’t break it).\nWhat you will discover is that 1:6 creates a vector (list of similar things, in this case numbers) of the numbers 1-6. The next bit of the sample() function is size. In the dice() function, the num passes down to the size of the sample(): Looking through the help, size is the number of items to choose. So sample(1:6, 1) would choose one number from the numbers 1-6 at random; sample(1:6, 3) would choose 3, and so on. The last argument, replace=TRUE, tells sample() what to do with a number once it’s been picked: Does it go ‘back into the bag’ to be picked again (TRUE) or not? (FALSE)?\nAround the outside is sum() which simply sums the numbers on however many (num) dice you “rolled”.\nPutting it all together, our dice() function “throws num dice” by sample()ing from the numbers 1-6 num times, replaceing each number when it’s been picked, and sums the numbers of all the dice.\n\n\nQuestion 10\n\n\nUse the function you just made to ‘roll a die’ a few times. Check that it works like you expect.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou just need to run dice() a few times. A single die means num = 1, which is the default.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ndice()\n\n[1] 6\n\ndice()\n\n[1] 1\n\ndice()\n\n[1] 4\n\ndice()\n\n[1] 5\n\n\n\n\n\n\nQuestion 11\n\n\nLook up the function replicate(). We can use it to do something in R lots of times! For instance, replicate(20, 1+1) will evaluate 1+1 twenty times.\nUse replicate() to simulate 20 rolls of a single die, and store the results in an object in your environment. Give it an easily identifiable name.\nWhat does each value in this object represent?\n\n\n\n\n\nSolution\n\n\n\n\nrolls20 &lt;- replicate(20, dice(num = 1))\nrolls20\n\n [1] 3 1 3 2 6 5 3 6 4 5 2 4 1 1 4 3 1 4 2 1\n\n\nEach value in rolls20 represents the simulated roll of a single die. We roll our die, and get a 3, we roll it again and get 1, the third roll we get 3, and so on..\n\n\n\n\nQuestion 12\n\n\nCreate a barplot showing the frequency with which each number was landed on in the 20 rolls.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe functions table() and barplot() were used to do this in the lecture.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nYour plots will look slightly different to these, because all of our dice are random!\n\n# We can get the frequency table using table()\ntable(rolls20)\n\nrolls20\n1 2 3 4 5 6 \n5 3 4 4 2 2 \n\n# Which we can then pass to the barplot() function:\nbarplot(table(rolls20))\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 13\n\n\nDo the same for 100 rolls, and then for 1,000. What do you notice?\n\n\n\n\n\nSolution\n\n\n\n\nmorerolls &lt;- replicate(100, dice(1))\nbarplot(table(morerolls))\n\n\n\n\n\n\n\nmorerolls2 &lt;- replicate(1000, dice(1))\nbarplot(table(morerolls2))\n\n\n\n\n\n\n\n\nThe more rolls we do of the dice, the flatter the graph becomes. This is because there is an equal probability of the die landing on any of the responses - there is a uniform probability.\n\n\n\n\nQuestion 14\n\n\nCopy the code below into your script and run it. It creates a new function called wdice() which simulates the rolling of num dice which are slightly weighted.\nRoll a single weighted die 20 times and plot the frequency distribution. Do the same for 100 and 1,000 rolls of a single die. Does a pattern emerge? At how many rolls?\n\nwdice &lt;- function(num = 1){\n    sum(sample(1:6, num, replace=TRUE, prob = c(0.15,0.15,0.15,0.15,0.15,0.25)))\n}\n\n\n\n\n\n\nSolution\n\n\n\n\nwdice &lt;- function(num = 1){\n    sum(sample(1:6, num, replace=TRUE, prob = c(0.15,0.15,0.15,0.15,0.15,0.25)))\n}\n\nwd &lt;- replicate(20, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\nwd &lt;- replicate(1000, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\nwd &lt;- replicate(10000, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\n\nThe die is clearly weighted towards landing on 6. However, is 20 rolls enough to reliably observe this? In our 20 rolls above, it landed on 3 quite a bit too (yours will be different)! The pattern becomes clearer after 1000 rolls.\n\n\n\n\nQuestion 15\n\n\nRemember, wdice() and dice() are really just relying on different functions, like sample(). Try playing around with sample() in the console again - what does the prob = c(....) bit do?\n\n\n\n\n\nSolution\n\n\n\nThe prob bit is defining the probabilities of observing each outcome - i.e. there is a 25% chance of rolling a 6.\n\n\n\n\nQuestion 16\n\n\nLet’s try to modify the wdice() function. Edit the code for wdice() so that 50% of the time it lands on number 6.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nTo test out your modified function, you will need to re-run the code which defines the function. When we use wdice() we use the function which is in our environment. If we want to edit the function, we need to then overwrite (or “replace”/“reassign”) the object in our environment.\n\nWe need to be careful to remember that the probability of different outcomes should sum to 1 (i.e., it’s not possible to “50% of the time land on 6” as well as “70% of the time land on 5”!).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nwdice &lt;- function(num = 1){\n    sum(sample(1:6, num, replace=TRUE, prob = c(0.1,0.1,0.1,0.1,0.1,0.5)))\n}\n\n\n\n\n\nQuestion 17\n\n\nCan you observe the weighting in your new die (the one which 50% of the time lands on number 6) in only 20 rolls?\n\n\n\n\n\nSolution\n\n\n\n\nwd &lt;- replicate(20, wdice(1))\nbarplot(table(wd))\n\n\n\n\n\n\n\n\nThe die is very clearly weighted to land on 6. We can see this in just 20 rolls. Presumably it will become even clearer if we increased how many times we roll it.\n\n\n\n\nQuestion 18\n\n\nConceptually, what can we learn from this toy example?\n\n\n\n\n\nSolution\n\n\n\nThe more highly weighted a die is, the less we have to roll it in order to observe that weighting."
  },
  {
    "objectID": "01a_R.html",
    "href": "01a_R.html",
    "title": "1A: A first look at R & RStudio",
    "section": "",
    "text": "This reading:\n\nHow does R work?\nHow do we use the Rstudio interface?\n\n\nThe best way to learn R is to use it. Try following along with these reading by typing the code into your R script and running them. You will hopefully get the same output as is presented on this page below each bit of code. If you get errors and warnings, don’t panic - read them!\n\n\n\nR is a calculator\nWhen we first open RStudio, we should see something which looks more or less like the image in Figure 1, where there are several little windows. We are going to explore what each of these little windows offer by just diving in and starting to do things.\n\n\n\n\n\nFigure 1: RStudio, freshly opened\n\n\n\n\nStarting in the left-hand window, you’ll notice the blue sign &gt;.\nThis is where we R code gets executed.\nType 2+2, and hit Enter ↵.\nYou should discover that R is a calculator - R responds by telling us the answer (4).\nLet’s work through some basic operations (adding, subtracting, etc). For instance, can you work out what R will give you for each of these operations?\n\n\nArithmetic operations\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n2 + 5\n\n\n\n10 - 4\n\n\n\n2 * 5\n\n\n\n10 - (2 * 5)\n\n\n\n(10 - 2) * 5\n\n\n\n10 / 2\n\n\n\n3^2\n(the ^ symbol is “to the power of”)\n\n\n\n\n\nShow me the output\n\n\n\nCode\nOutput\n\n\n\n\n2 + 5\n7\n\n\n10 - 4\n6\n\n\n2 * 5\n10\n\n\n10 - (2 * 5)\n0\n\n\n(10 - 2) * 5\n40\n\n\n10 / 2\n5\n\n\n3^2\n9(the ^ symbol is “to the power of”)\n\n\n\n\n\n\nR can get stuck\nWhenever you see the blue sign &gt;, it means R is ready and waiting for you to provide a command.\nIf you type 10 + and press Enter, you’ll see that instead of &gt; you are left with +. This means that R is waiting for more.\nEither give it more (finish the command), or cancel the command by pressing the Escape key on your keyboard.\n\nAs well as performing arithmetic calculations, we can ask R things for which the answer is TRUE or FALSE, such as “Is 3 less than 5?”. If we type 3 &lt; 5 and press Enter, then R should tell us that the statement we gave it is TRUE.\nThese computations don’t return numbers, but instead return logical values. There are few operators that we need to learn about here:\n\nEquality/Inequality: We use the symbols == to mean “is equal to”, and the symbols != for “is not equal to”.\n\nLess Than/Greater Than: To determine whether a value is less/greater than another value, we have our typical symbols &lt; and &gt;. We also have &lt;= and &gt;= when we want to include “less/greater than or equal to”.\n\nWe can combine these with & for “and”, | for “or”, and ! for “not”, to ask R all sorts of things.\nTry and work out what R should give you for each of these (or try it out yourself!):\n\n\nLogical operations\n\n\n\nCode\nOutput\n\n\n\n\n3 &gt; 5\n\n\n\n3 &lt;= 5\n\n\n\n3 &gt;= 3\n\n\n\n3 == 5\n\n\n\n(2 * 5) == 10\n\n\n\n(2 * 5) != 11\n\n\n\n(2 == 2) & (3 == 4)\n\n\n\n(2 == 2) | (3 == 4)\n\n\n\n(2 == 2) & !(3 == 4)\nTRUE\n\n\n\n\n\nShow me the output\n\n\n\nCode\nOutput\n\n\n\n\n3 &gt; 5\nFALSE\n\n\n3 &lt;= 5\nTRUE\n\n\n3 &gt;= 3\nTRUE\n\n\n3 == 5\nFALSE\n\n\n(2 * 5) == 10\nTRUE\n\n\n(2 * 5) != 11\nTRUE\n\n\n(2 == 2) & (3 == 4)\nFALSE\n\n\n(2 == 2) | (3 == 4)\nTRUE\n\n\n(2 == 2) & !(3 == 4)\nTRUE\n\n\n\n\n\n\nFALSE and TRUE as 0 and 1\nIt will become useful to think of these logical values (TRUE and FALSE) as also having intrinsic numeric values of 0 and 1.\nThis is how R will treat them if you ask it to do something that requires the values to be numeric.\nFor example, the code TRUE + 3 will return 4, and FALSE + 3 will return 3.\n\n\n\n\n\n\nR has a memory\nWe can also store things in R’s memory, and to do that we just need to give them a name.\nType x &lt;- 5 and press Enter.\nWhat has happened? We don’t get given an answer like we did with calculations such as 2 + 4. What we’ve done is stored in R’s memory something named x which has the value 5. We can now refer to the name and it will give us the value!\nIf we now type x and press Enter, it gives us whatever we assigned to the name “x”. So it gives us the number 5.\n\nx\n\n[1] 5\n\n\nWhat is going to happen when we type x * 3? It will give us 15!\n\nAssigning names to things in R\nThe &lt;- symbol, pronounced arrow, is what we use to assign a value to a named object:\n\nname &lt;- value\n\n\nNote, there are a few rules about names in R:\n\nNo spaces - spaces inside a name are not allowed (the spaces around the &lt;- don’t matter):\nlucky_number &lt;- 5 ✔ lucky number &lt;- 5 ❌\nNames must start with a letter:\nlucky_number &lt;- 5 ✔ 1lucky_number &lt;- 5 ❌\nCase sensitive:\nlucky_number is different from Lucky_Number\nhere is a set of words you can’t use as names, including: if, else, for, in, TRUE, FALSE, NULL, NA, NaN, function\n(Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these).\n\n\n\n\n\n\nThe Console and The Environment\nIf you are working along with us, you might have also noticed that something else happened when we executed the code x &lt;- 5. The thing we named x with a value of 5 suddenly appeared in the top-right window. This is known as the environment (Figure 2), and it shows everything that we store in R.\n\n\n\n\n\nFigure 2: Assigning names to objects stores them in R’s environment.\n\n\n\n\nSo we’ve now made use of two of the panes that we see in RStudio:\nWhere code is run: When we’ve been writing and running R code (e.g. typing 2+5 or x&lt;-5 and pressing Enter), we’ve been doing it in the “console”.\nThe console is where R code gets executed (i.e. where our coded instruction to R code is interpreted and acted on), but as we’ll see below, it isn’t where all R code gets written.\nWhere things get stored: We’ve also been learning about how we can store things in R’s memory (the environment) by assigning a name to them using the &lt;- operator. The top-right pane of RStudio shows us the environment, where we can see everything that we have stored in R. Note that this also means we can keep track of what objects we have saved that are available for our use. If we never stored an object named “peppapig”, then R will give us an error when we do something like:\n\n2*peppapig\n\nError in eval(expr, envir, enclos): object 'peppapig' not found\n\n\nNow that we have an idea of what the console and the environment are for, we are well on our way. If you want a silly analogy, the console is like R’s mouth, where we feed it things, and the environment is just its memory, where it remembers what things are what. We can see these in Figure 3. Note however, that the console has been moved down to the bottom-left, as we are introducing a new pane above it. This is where we move to next.\n\n\n\n\n\nFigure 3: RStudio panes: Code is executed in the console, and objects are stored in the environment.\n\n\n\n\n\n\n\n\n\nR Scripts and the Editor\nWhat if we want to edit our code? Whatever we write in the console just disappears upwards. What if we want to change things we did earlier on?\nWhile the console is where code gets executed, it doesn’t have to be where code gets written.. We can write and edit our code in a separate place before we then send it to the console to be executed!!\nThe standard place to write and edit things is in an R scipt. We can open one by doing File &gt; New File &gt; R script, and a new file will open in the top-left pane of RStudio. The console will be shoved down to the bottom-left.\nIn the R script, we can write code. For instance, we might write:\n\nx &lt;- 210\ny &lt;- 15\nx / y\n\nNotice that nothing happens when we write each line. It’s not like writing in the console where R tells us the answers. This is because this code is not yet being executed. We haven’t actually fed it to R.\nThere are a couple of useful ways we can send the code to R.\n\nPosition your text-cursor (blinking vertical line) on the line of code we wish to run and press Ctrl+Enter (Windows) or Cmd+Enter (MacOS)\n\nPosition your text-cursor (blinking vertical line) on the line of code we wish to run and press the “Run” button in the top right of the script.\n\nWhen we do this, the line of code will be sent down to the console, where it will be executed, and R will do it’s thing.\nFor example, if we had sent the line x &lt;- 210 down to the console, R would then store the number 210 with the name x in our environment (as in Figure 4). Additionally, it will move the text-cursor to the next line, so we can just press Ctrl+Enter again to run the next line (and so on.).\n\n\n\n\n\nFigure 4: Code written in the script can be sent down to the console, where it is executed. In this example, the execution of the code stores an object in the environment.\n\n\n\n\nBy writing our code in a file such as an R script before sending it to the console we can edit, save, and share our code. This makes it so much more useful than just using the console (which is more like writing on scratch paper, where we can’t keep anything).\nFor instance, let’s say we made a mistake earlier, and instead of “x” being 210, it should have been 211. Well, we can just edit the script, and re-run it.\n\nRegularly save your scripts!\nTo save an R script that is open, we just\n\nFile &gt; Save (or Ctrl+S)\nLocate to the folder where we want to save the file.\n\ngive it an appropriate name, and click save.\n\nNOTE: When you save R script files, they terminate with a .R extension.\n\n\n\n\n\n\n\nLooking ahead to RMarkdown\n\n\n\n\n\n\nIn addition to R scripts, there is another type of document we can create, known as “Rmarkdown”.\nRmarkdown documents combine the analytical power of R and the utility of a text-processor. We can have one document which contains all of our analysis as well as our written text, and can be compiled into a nicely formatted report. This saves us doing analysis in R and copying results across to Microsoft Word. It ensures our report accurately reflects our analysis. Everything that you’re reading now has all been written in Rmarkdown!\n\n\n\nFigure 5: An example RMarkdown document\n\n\nWe’re going to learn more about Rmarkdown documents and how to write them later on, but the broad idea is that we can writing normal text interspersed with “code-chunks” (i.e., chunks of code!). RMarkdown documents looks much like an R script, only the code is contained within the grey-boxes, and text is written in between (see Figure 5). RMarkdown documents can then be compiled to create a lovely .pdf, .html, or .docx file.\n\n\n\n\n\nFigure 6: RMarkdown Workflow\n\n\n\n\n\n\n\n\n\n\n\n\nThe Four RStudio Panes\nWe’ve now seen almost all the different panes in RStudio:\n\n\nThe console is where R code gets executed\nThe environment is R’s memory, you can assign something a name and store it here, and then refer to it by name in your code.\nThe editor is where you can write and edit R code in R scripts and Rmarkdown documents. You can then send this to the console for it to be executed.\n\n\n\n\n\n\n\n\nFigure 7: The Four Panes of RStudio\n\n\n\n\n\nWe are yet to use the bottom-right window, but this is an easy one to explain. It is where we can see any plots that we create, where we can browse our files, and where we can ask R for some help documentation. We’ll make more use of this later on, but for now try typing plot(x = 4, y = 2) into the console and seeing what happens.\n\n\nProjects and file organisation\nWe’re not going to speak too much about this here but one key thing to remember is that R is working from a specific place in your computer. You can find out where by typing getwd() into the console.\nAn easy way to keep things organised is to set up an “R project”. This basically associates a specific folder on your computer with your working in R, and it means it will automatically look for things in that folder.\nWe recommend that you start a project for this course (call it something like “usmr”). This will the be project that you open whenever you work on this course (RStudio will usually re-open the previous project you were working on when you closed it).\nWith that project open, we suggest that you start a new script for each week, in which you complete your exercises, and which you then remember to save!\nIf you haven’t already, we suggest you start an R project by using (in the top menu of RStudio), File &gt; New Project and following the instructions. It will create a folder on your computer somewhere of your choosing, and you will now notice that if you click in the “Files” tab in the bottom right pane of RStudio, you can see the project folder!\n\n\n\n\n\n\nGood Habits\nAlong with regular saving of work and organising your files, it will be very useful in the long-run if we get used to always “starting fresh” when we open R.\nWe need to start thinking of the code that we write in an R script as a set of consecutive instructions that we can give to R in order to achieve our goal. It’s just a blank slate on which we write (in language R understands) “do this. now do this. now do this..” and so on.\nThis means that the script contains all the information needed.\nSo we can now:\n\nEmpty our environment\nRestart R\nRun all the code in our script (highlight multiple lines of code to run them all at once)\n\nand we’re back to where we are! This is great for when we make mistakes (we’re going to make many many mistakes!), because we can just clear everything, start at the top of our script, and work downwards to figure out what has gone wrong.\n\nTidying up\n\nTo empty our environment, we can click on the little broomstick icon: .\nTo restart the R Session (not always necessary, but good practice) in the top menu, we choose Session &gt; Restart R (or press Ctrl+Shift+F10).\n\n\nThe other very useful thing that we can do in a script is to write comments for ourselves or for others. By starting a line with a #, R will know that that entire line is not code, and so it won’t try to do anything with it. For instance, if we write these lines in our script, and send them both down to the console, nothing happens for the first line:\n\n\nComments\n\n# The line below will add 5 to 2. \n2+5\n\n[1] 7\n\n\n\n\nIf we forget the #\n\nThe line below will add 5 to 2. \n2+5\n\n\nError: unexpected symbol in “The line”\n\n\n\n\n\n\n\n\nUseful Settings\nBelow are a couple of our recommended settings for you to change as you begin your journey in R. After you’ve changed them, take a 5 minute break before moving on to the next reading.\n\n1. Clean environments\nAs you use R more, you will store lots of things with different names. Throughout this course alone, you’ll probably name hundreds of different things. This could quickly get messy within our project.\nWe can make it so that we have a clean environment each time you open RStudio. This will be really handy.\n\nIn the top menu, click Tools &gt; Global Options…\nThen, untick the box for “Restore .RData into workspace at startup”, and change “Save workspace to .RData on exit” to Never:\n\n\n\n\n2. Wrapping code\nIn the editor, you might end up with a line of code which is really long, but you can make RStudio ‘wrap’ the line, so that you can see it all, without having to scroll:\n\nx &lt;- 1+2+3+6+3+45+8467+356+8565+34+34+657+6756+456+456+54+3+78+3+3476+8+4+67+456+567+3+34575+45+2+6+9+5+6\n\n\nIn the top menu, click Tools &gt; Global Options…\nIn the left menu of the box, click “Code”\nTick the box for “Soft-wrap R source files”"
  },
  {
    "objectID": "01b_data.html",
    "href": "01b_data.html",
    "title": "1B: More R - Basic Data Skills",
    "section": "",
    "text": "This reading:\n\nHow does R store data?\n\nWhat can R do with data?\n\nHow can we use R to access and manipulate data?"
  },
  {
    "objectID": "01b_data.html#accessing-subsets-of-data",
    "href": "01b_data.html#accessing-subsets-of-data",
    "title": "1B: More R - Basic Data Skills",
    "section": "Accessing subsets of data",
    "text": "Accessing subsets of data\nWhat if we want to extract certain subsections of our dataset, such as specific observational units or variables? This is where we learn about two important bits of R code used to access parts of data - the dollar sign $, and the square brackets [].\n\nThe dollar sign $\nThe dollar sign allows us to extract a specific variable from a dataframe. For instance, we can pull out the variable named “eye_color” in the data, by using $eye_color after the name that we gave our dataframe.\nRemember that each variable in a dataframe is a vector (a set of values). Once extracted, we will have a vector and not a dataframe.\n\nstarwars2$eye_color\n\n [1] \"blue\"          \"yellow\"        \"red\"           \"yellow\"       \n [5] \"brown\"         \"blue\"          \"blue\"          \"red\"          \n [9] \"brown\"         \"blue-gray\"     \"blue\"          \"blue\"         \n[13] \"blue\"          \"brown\"         \"black\"         \"orange\"       \n[17] \"hazel\"         \"blue\"          \"yellow\"        \"brown\"        \n[21] \"red\"           \"brown\"         \"blue\"          \"orange\"       \n[25] \"blue\"          \"brown\"         \"black\"         \"red\"          \n[29] \"blue\"          \"orange\"        \"orange\"        \"orange\"       \n[33] \"yellow\"        \"orange\"        NA              \"brown\"        \n[37] \"yellow\"        \"pink\"          \"hazel\"         \"yellow\"       \n[41] \"black\"         \"orange\"        \"brown\"         \"yellow\"       \n[45] \"black\"         \"brown\"         \"blue\"          \"orange\"       \n[49] \"yellow\"        \"black\"         \"blue\"          \"brown\"        \n[53] \"brown\"         \"blue\"          \"yellow\"        \"blue\"         \n[57] \"blue\"          \"brown\"         \"brown\"         \"brown\"        \n[61] \"brown\"         \"yellow\"        \"yellow\"        \"black\"        \n[65] \"black\"         \"blue\"          \"unknown\"       \"unknown\"      \n[69] \"gold\"          \"black\"         \"green, yellow\" \"blue\"         \n[73] \"brown\"         \"black\"         NA             \n\n\n\n\nThe square brackets []\nSquare brackets are used to do what is known as indexing (finding specific entries in your data).\nWe can retrieve bits of data by identifying the \\(i^{th}\\) entry(s) inside the square brackets, for instance:\n\n# assign the numbers 10, 20 ... 100 to the name \"somevalues\"\nsomevalues &lt;- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n\n# pull out the 3rd entry\nsomevalues[3]\n\n[1] 30\n\n\nIn the above example, we have a vector (a single sequence of values), and so we can retrieve entries with the syntax:\n\nvector[entry]\n\n In a dataframe we have an extra dimension - we have rows and columns. Using square brackets with a dataframe needs us to specify both:\n\n\ndataframe[rows, columns]\n\n\nLet’s look at some examples:\n\n\nExamples of Indexing\n\nSpecifying row number and column number:\n\n\n# first row, fourth column\nstarwars2[1, 4]\n# tenth row, first column\nstarwars2[10, 1]\n\n\nIf we leave either rows or columns blank, then we will get out all of them:\n\n\n# tenth row, all columns\nstarwars2[10, ]\n# all rows, 2nd column\nstarwars2[ , 2]\n\n\nThere are is another way to identify column - we can use the name in quotation marks:\n\n\n# first row, \"species\" column\nstarwars2[1, \"species\"]\n\n\nWe can also ask for multiple rows, or multiple columns, or both! To do that, we use c():\n\n\n# the 1st AND the 6th rows, and the 1st AND 3rd columns\nstarwars2[c(1,6), c(1,3)] \n\n\nAnd we can specify a sequence using the colon, from:to: 2\n\n\n# FROM the 1st TO the 6th row, all columns\nstarwars2[1:6, ] \n\n\nWe can even use the two accessors in combination:3\n\n\n# extract the variable called \"name\" and show the 20th entry\nstarwars2$name[20]  \n\n\n\nShow me the output\n\nSpecifying row number and column number:\n\n\n# first row, fourth column\nstarwars2[1, 4]\n\n[1] \"blue\"\n\n# tenth row, first column\nstarwars2[10, 1]\n\n[1] \"Obi-Wan Kenobi\"\n\n\n\nIf we leave either rows or columns blank, then we will get out all of them:\n\n\n# tenth row, all columns\nstarwars2[10, ]\n\n             name height    hair_color eye_color homeworld species\n10 Obi-Wan Kenobi    182 auburn, white blue-gray   Stewjon   Human\n\n# all rows, 2nd column\nstarwars2[ , 2]\n\n [1] 172 167  96 202 150 178 165  97 183 182 188 180 228 180 173 175 170 180 170\n[20] 183 190 177 175 180 150  88 160 191 170 196 224 206 137 112 170 163 175 180\n[39] 178  94 122 163 188 198 196 171 184 188 264 188 196 185 157 183 183 170 166\n[58] 165 193 191 183 168 198 229 213 167  79 193 191 178 216 234 188 206 180\n\n\n\nThere are is another way to identify column - we can use the name in quotation marks:\n\n\n# first row, \"species\" column\nstarwars2[1, \"species\"]\n\n[1] \"Human\"\n\n\n\nWe can also ask for multiple rows, or multiple columns, or both! To do that, we use c():\n\n\n# the 1st AND the 6th rows, and the 1st AND 3rd columns\nstarwars2[c(1,6), c(1,3)] \n\n            name  hair_color\n1 Luke Skywalker       blond\n6      Owen Lars brown, grey\n\n\n\nAnd we can specify a sequence using the colon, from:to:\n\n\n# FROM the 1st TO the 6th row, all columns\nstarwars2[1:6, ] \n\n            name height  hair_color eye_color homeworld species\n1 Luke Skywalker    172       blond      blue  Tatooine   Human\n2          C-3PO    167        &lt;NA&gt;    yellow  Tatooine   Human\n3          R2-D2     96        &lt;NA&gt;       red     Naboo   Droid\n4    Darth Vader    202        none    yellow  Tatooine   Human\n5    Leia Organa    150       brown     brown  Alderaan   Human\n6      Owen Lars    178 brown, grey      blue  Tatooine   Human\n\n\n\nWe can even use the two accessors in combination:\n\n\n# extract the variable called \"name\" and show the 20th entry\nstarwars2$name[20]  \n\n[1] \"Boba Fett\"\n\n\n\n\n\nThe dollar sign $\nUsed to extract a variable from a dataframe:\n\ndataframe$variable\n\nThe square brackets []\nUsed to extract parts of an R object by identifying rows and/or columns, or more generally, “entries”. Left blank will return all.\n\nvector[entries]\ndataframe[rows, columns]"
  },
  {
    "objectID": "01b_data.html#accessing-by-a-condition",
    "href": "01b_data.html#accessing-by-a-condition",
    "title": "1B: More R - Basic Data Skills",
    "section": "Accessing by a condition",
    "text": "Accessing by a condition\nWe can also do something really useful, which is to access all the entries in the data for which a specific condition is true.\nLet’s take a simple example to start:\n\nsomevalues &lt;- c(10, 10, 0, 20, 15, 40, 10, 40, 50, 35)\n\nTo only select values which are greater than 20, we can use:\n\nsomevalues[somevalues &gt; 20]\n\n[1] 40 40 50 35\n\n\n\nUnpacking: somevalues[somevalues &gt; 20]\n First, let’s look at what somevalues &gt; 20 does. It returns TRUE for the entries of somevalues which are greater than 20, and FALSE for the entries of somevalues that are not (that is, which are less than, or equal to, 20.\nThis statement somevalues &gt; 20 is called the condition.\n\nsomevalues &gt; 20\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\nNow consider putting that sequence of TRUEs and FALSEs inside the square brackets in somevalues[]. This returns only the entries of somevalues for which the condition is TRUE.\n\nsomevalues[c(FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE)]\n\n[1] 40 40 50 35\n\n\nSo what we’re doing is using a condition inside the square brackets to return all the values for which that condition is TRUE.\nAnd we’re being super efficient, because we don’t want to write out TRUEs and FALSEs all day, so we just give the conditional question inside the brackets directly:\n\nsomevalues[somevalues &gt; 20]\n\n[1] 40 40 50 35\n\n\n\n We can extend this same logic to a dataframe. Let’s suppose we want to access all the entries in our Star Wars data who have the value “Droid” in the species variable. To work out how to do this, we first need a line of code which defines our condition - one which returns TRUE for each entry of the species variable which is “Droid”, and FALSE for those that are not “Droid”.\nWe can use the dollar sign to pull out the species variable:\n\nstarwars2$species\n\n [1] \"Human\"        \"Human\"        \"Droid\"        \"Human\"        \"Human\"       \n [6] \"Human\"        \"Human\"        \"Droid\"        \"Human\"        \"Human\"       \n[11] \"Human\"        \"Human\"        \"Wookiee\"      \"Human\"        \"Rodian\"      \n[16] \"Hutt\"         \"Human\"        \"Human\"        \"Human\"        \"Human\"       \n[21] \"Trandoshan\"   \"Human\"        \"Human\"        \"Mon Calamari\" \"Human\"       \n[26] \"Ewok\"         \"Sullustan\"    \"Neimodian\"    \"Human\"        \"Gungan\"      \n[31] \"Gungan\"       \"Gungan\"       \"Toydarian\"    \"Dug\"          \"unknown\"     \n[36] \"Human\"        \"Zabrak\"       \"Twi'lek\"      \"Twi'lek\"      \"Vulptereen\"  \n[41] \"Xexto\"        \"Toong\"        \"Human\"        \"Cerean\"       \"Nautolan\"    \n[46] \"Zabrak\"       \"Tholothian\"   \"Iktotchi\"     \"Quermian\"     \"Kel Dor\"     \n[51] \"Chagrian\"     \"Human\"        \"Human\"        \"Human\"        \"Geonosian\"   \n[56] \"Mirialan\"     \"Mirialan\"     \"Human\"        \"Human\"        \"Human\"       \n[61] \"Human\"        \"Clawdite\"     \"Besalisk\"     \"Kaminoan\"     \"Kaminoan\"    \n[66] \"Human\"        \"Aleena\"       \"Skakoan\"      \"Muun\"         \"Togruta\"     \n[71] \"Kaleesh\"      \"Wookiee\"      \"Human\"        \"Pau'an\"       \"unknown\"     \n\n\nAnd we can ask R whether each value is equal to “Droid”\n\nRemember: in R, we ask whether something is equal to something else by using a double-equals, ==. A single equal sign would be wrong, as it denotes assignment.\n\n\nstarwars2$species == \"Droid\"\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE\n\n\nFinally, we can use this condition inside our square brackets to access the entries of the data for which this condition is TRUE:\n\n# I would read the code below as: \n# \"In the starwars2 dataframe, give me all the rows \n# for which the condition starwars2$species==\"Droid\"\n# is TRUE, and give me all the columns.\"\n\nstarwars2[starwars2$species == \"Droid\", ]\n\n   name height hair_color eye_color homeworld species\n3 R2-D2     96       &lt;NA&gt;       red     Naboo   Droid\n8 R5-D4     97       &lt;NA&gt;       red  Tatooine   Droid"
  },
  {
    "objectID": "01b_data.html#more-complex-conditions",
    "href": "01b_data.html#more-complex-conditions",
    "title": "1B: More R - Basic Data Skills",
    "section": "More complex conditions",
    "text": "More complex conditions\nThinking back to Reading 1A when we first introduced R, we talked briefly about “logical operators”. Specifically, the operators &, |, and ! (for “and”, “or”,” and “not”), will come in handy now.\nFor instance, we can now extract all those in the dataset which are humans and taller than 190cm:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$species==\"Human\" AND starwars2$height &gt; 190 are TRUE, \n# and give me all the columns.\"\nstarwars2[starwars2$species == \"Human\" & starwars2$height &gt; 190, ]\n\n                  name height hair_color eye_color homeworld species\n4          Darth Vader    202       none    yellow  Tatooine   Human\n59               Dooku    193      white     brown   Serenno   Human\n60 Bail Prestor Organa    191      black     brown  Alderaan   Human\n\n\nOr we can extract all those in the dataset which are either droids or ewoks:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$species==\"Droid\" OR starwars2$species==\"Ewok\" is TRUE, \n# and give me all the columns.\"\nstarwars2[starwars2$species == \"Droid\" | starwars2$species == \"Ewok\", ]\n\n                    name height hair_color eye_color homeworld species\n3                  R2-D2     96       &lt;NA&gt;       red     Naboo   Droid\n8                  R5-D4     97       &lt;NA&gt;       red  Tatooine   Droid\n26 Wicket Systri Warrick     88      brown     brown     Endor    Ewok"
  },
  {
    "objectID": "01b_data.html#editing-specific-entries",
    "href": "01b_data.html#editing-specific-entries",
    "title": "1B: More R - Basic Data Skills",
    "section": "Editing specific entries",
    "text": "Editing specific entries\nNow that we’ve seen a few ways of accessing sections of data, we can learn how to edit them! One of the most common reasons you will need to modify entries in your data is in data cleaning. This is the process of identifying incorrect / incomplete / irrelevant data, and replacing / modifying / deleting them.\nAbove, we looked at the subsection of the data where the species variable had the entry “Droid”. Some of you may have noticed earlier that we had some data on C3PO. Are they not also a droid?\n\n\n\n(Looks pretty Droid-y to me! disclaimer: I know nothing about Star Wars 🙂 )\nJust as we saw above how to access specific entries, e.g.:\n\n# 2nd row, all columns\nstarwars2[2, ]\n\n   name height hair_color eye_color homeworld species\n2 C-3PO    167       &lt;NA&gt;    yellow  Tatooine   Human\n\n# 2nd row, 6th column (the \"species\" column)\nstarwars2[2,6]\n\n[1] \"Human\"\n\n\nWe can change these by assigning them a new value (remember the &lt;- symbol). In doing so, we overwrite4 the entry in the 2nd row and 6th column of the data (starwars2[2,6]) with the value “Droid”.\n\n# C3PO is a droid, not a human\nstarwars2[2,6] &lt;- \"Droid\"\n# Look at the 2nd row now -\n# the entry in the \"species\" column has changed:\nstarwars2[2, ]\n\n   name height hair_color eye_color homeworld species\n2 C-3PO    167       &lt;NA&gt;    yellow  Tatooine   Droid"
  },
  {
    "objectID": "01b_data.html#editing-entries-via-a-condition",
    "href": "01b_data.html#editing-entries-via-a-condition",
    "title": "1B: More R - Basic Data Skills",
    "section": "Editing entries via a condition",
    "text": "Editing entries via a condition\nWe saw above how to access parts of data by means of a condition, with code such as:\n\n# \"In the starwars2 dataframe, give me all the rows for which the\n# condition starwars2$homeworld==\"Naboo\" is TRUE, and give me all the columns.\"\nstarwars2[starwars2$homeworld==\"Naboo\", ]\n\n            name height hair_color eye_color homeworld species\n3          R2-D2     96       &lt;NA&gt;       red     Naboo   Droid\n19     Palpatine    170       grey    yellow     Naboo   Human\n30 Jar Jar Binks    196       none    orange     Naboo  Gungan\n31  Roos Tarpals    224       none    orange     Naboo  Gungan\n32    Rugor Nass    206       none    orange     Naboo  Gungan\n52  Gregar Typho    185      black     brown     Naboo   Human\n53         Cordé    157      brown     brown     Naboo   Human\n58         Dormé    165      brown     brown     Naboo   Human\n\n\nWhat if we wanted to modify it so that every character from “Naboo” was actually of species “Nabooian”?\nWe can do that in a number of ways, all of which do the same thing - namely, they access parts of the data and assign them the new value “Nabooian”.\nThe lines of code below all do exactly that, in different ways. We’ve also tried to give a way of interepreting each line.\n\n# In the starwars2 data, give the rows for which condition \n# starwars2$homeworld==\"Naboo\" is TRUE, and select only the \"species\" column. \n# Assign to these selected entries the value \"Nabooian\".\nstarwars2[starwars2$homeworld==\"Naboo\", \"species\"] &lt;- \"Nabooian\"\n\n\n# In the starwars2 data, give the rows for which condition \n# starwars2$homeworld==\"Naboo\" is TRUE, and select only the 6th column. \n# Assign to these selected entries the value \"Nabooian\".\nstarwars2[starwars2$homeworld==\"Naboo\", 6] &lt;- \"Nabooian\"\n\n\n# Extract the species variable from the starwars2 data (it's a vector).\n# Pick the entries for which the condition starwars2$homeworld==\"Naboo\" is TRUE.\n# Assign to these selected entries the value \"Nabooian\".\nstarwars2$species[starwars2$homeworld==\"Naboo\"] &lt;- \"Nabooian\"\n\n\n\n\n\n\n\noptional: a little extra…\n\n\n\n\n\nIn a similar way, we could have changed C-3PO to a Droid without ever having to know what row of the data they were in!\n\n# for the row(s) where the name variable in starwars2\n# is equal to \"C-3PO\", in the species variable we assign\n# that entry to be \"Droid\"\nstarwars2[starwars2$name==\"C-3PO\", \"species\"] &lt;- \"Droid\""
  },
  {
    "objectID": "01b_data.html#addingchanging-a-variable",
    "href": "01b_data.html#addingchanging-a-variable",
    "title": "1B: More R - Basic Data Skills",
    "section": "Adding/Changing a variable",
    "text": "Adding/Changing a variable\nAnother thing we might want to do is change a whole variable (a whole column) in some way.\nThe logic is exactly the same, for instance, we can take the variable “height” from the dataframe “starwars2”, dividing it by 100 via starwars2$height / 100, and then assign the result to the same variable name in the data, i.e. we overwrite the column:\n\nstarwars2$height &lt;- starwars2$height / 100\n\nWe could instead have added a new column named “height_m” with those values if we did not want to overwrite “height”:\n\nstarwars2$height_m &lt;- starwars2$height / 100\n\nThis would have left the “height” variable as-is, and created a new one called “height2” which was the values in “height” divided by 100."
  },
  {
    "objectID": "01b_data.html#removing-rows-or-columns",
    "href": "01b_data.html#removing-rows-or-columns",
    "title": "1B: More R - Basic Data Skills",
    "section": "Removing rows or columns",
    "text": "Removing rows or columns\nLastly, we might want to change the data by removing a row or a column. Again, the logic remains the same, in that we use &lt;- to assign the edited data to a name (either a new name, thus creating a new object, or an existing name, thereby overwriting that object).\nFor instance, notice that the 35th and 75th rows of our data probably aren’t a valid observation - I’m reasonably sure that Marge and Homer Simpson never appeared in Star Wars:\n\nstarwars2[c(35,75), ]\n\n            name height hair_color eye_color   homeworld species\n35 Marge Simpson    1.7       Blue      &lt;NA&gt; Springfield unknown\n75 Homer Simpson    1.8       &lt;NA&gt;      &lt;NA&gt; Springfield unknown\n\n\nWe can remove a certain row(s) by using a minus sign - inside the square brackets\n\n# everything minus the 75th row\nstarwars2[-75, ]\n# everything minus the (35th and 75th rows)\nstarwars2[-c(35, 75), ]\n\nAnd we can simply re-use the name “starwars2” to overwrite the data and make this change take effect (rather than just print out the result, which the code above did):\n\nstarwars2 &lt;- starwars2[-c(35, 75), ]\n\n(now, in the environment pane of Rstudio, the object named “starwars2” will say 73 observations, rather than 75, which it had before - we’ve removed the 2 rows)\n The same logic applies for columns:\n\n# Create a new object called \"anonymous_starwars2\" and assign it \n# to the values which are the \"starwars2\" dataframe minus the \n# 1st column (the \"name\" column):\nanonymous_starwars2 &lt;- starwars2[, -1]\n# dimensions of our initial data\ndim(starwars2)\n\n[1] 73  6\n\n# the data we just assigned has one fewer columns\ndim(anonymous_starwars2)\n\n[1] 73  5"
  },
  {
    "objectID": "01b_data.html#footnotes",
    "href": "01b_data.html#footnotes",
    "title": "1B: More R - Basic Data Skills",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo save as .csv in Microsoft Excel, we go to File &gt; Save as, and then in the Save as Type box, choose to save the file as CSV (Comma delimited)↩︎\nThe colon operator, from:to, creates a vector from the value from to the value to in steps of 1.\nFor instance, 1:6 is the same as c(1,2,3,4,5,6).↩︎\nNote: When we do this, we don’t have the comma inside the square brackets.\nWhen we use the $ to pull out a variable, such as starwars2$name, we no longer have a dataframe.\nstarwars2$name doesn’t have rows and columns, it just has a series of values - it’s a vector!\nSo when you are using [] with a vector (1 dimension) rather than a dataframe (2 dimensions), you don’t specify [rows, columns], but simply [entry].↩︎\nyou could think of this as replacing / overwriting / reassigning the entry↩︎"
  },
  {
    "objectID": "02_ex.html",
    "href": "02_ex.html",
    "title": "Exercises: More R; Estimates & Intervals",
    "section": "",
    "text": "Question 0"
  },
  {
    "objectID": "02_ex.html#optional-extras",
    "href": "02_ex.html#optional-extras",
    "title": "Exercises: More R; Estimates & Intervals",
    "section": "Optional Extras",
    "text": "Optional Extras\n\nOptional Extra\n\n\nNote that the confidence interval from the previous question is concerned with describing the abstract and theoretical distribution of “what the mean sleep quality rating would look like from all possible samples of this size that I could take”. In order to do this we used a formula to describe the spread of this distribution, and in doing so had to assume that the standard deviation of our sample is a good approximation of the standard deviation of the population, and that the population is normally distributed.\nWe can also avoid ever using the standard deviation of our sample (sd(usmr2022$sleeprating)), and instead approximate the sampling distribution of the mean by “bootstrapping” - taking repeated resamples with replacement from the original sample (see 2B#standard-error.\n\nbootstrap_means &lt;- replicate(1000, mean(sample(observed_sample, replace = TRUE)))\n\n\nCreate an object that contains the 10,000 means from 10,000 resamples of our sleep ratings.\n\nThe distribution of resample means is the ‘bootstrap distribution’. Plot a histogram of it. What is the standard deviation? How does it compare to the standard error you calculated in the previous question with the formula?\n\nAt what values does the middle 95% of the bootstrap distribution fall?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor 3, look up quantile(). We saw this in 2B #confidence-intervals.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nResample means\nHere is our sample of sleep ratings:\n\nsleeprates &lt;- usmr2022$sleeprating\n\nAnd we can get rid of the NA’s:\n\nsleeprates &lt;- sleeprates[!is.na(sleeprates)]\n\nWe can resample with replacement from this set of numbers by using the replace = TRUE argument in the sample() function.\nNote, we’re leaving size = blank, which means it will stop at the same length as the original vector we give it.\n\nsample(sleeprates, replace = TRUE)\n\nand the mean of a given resample is calculated by wrapping mean() around the above code:\n\nmean(sample(sleeprates, replace = TRUE))\n\n[1] 70.32911\n\n\nfinally, we’ll do it lots and lots of times, using replicate():\n\nBSmeans &lt;- replicate(10000, mean(sample(sleeprates, replace = TRUE)))\n\n\n\nBootstrap Distribution\nHere’s the histogram of the bootstrap distribution:\n\nhist(BSmeans)\n\n\n\n\n\n\n\n\nAnd here’s the standard deviation of that distribution. This is a bootstrapped estimate of the standard error.\n\nsd(BSmeans)\n\n[1] 2.517222\n\n\nRecall our standard error calculated using \\(\\frac{s}{\\sqrt{n}}\\) from the previous question was 2.55\n\n\nPercentiles\nWe can get the 2.5% and 97.5% percentiles (i.e. getting the middle 95%), using the code below. Recall our confidence intervals that we computed analytically were 60.99 and 70.99.\n\nquantile(BSmeans, c(.025,.975))\n\n    2.5%    97.5% \n60.97437 70.78481 \n\n\n\n\n\n\n\n\n\n\n\nbootstraps\n\n\n\n\n\nBootstrapping is a great way to learn about sampling variability because it allows us to actually plot, summarise and describe what would otherwise be an abstract conceptual distribution.\nIt can also be a useful tool in practice, but it doesn’t come without its own problems/complexities. One important thing to note is that it often works worse than traditional methods for small samples, especially skewed samples (i.e. bootstrapping a “95% CI” for a small sample will often be too narrow and &lt;95%)."
  },
  {
    "objectID": "02a_measurement.html",
    "href": "02a_measurement.html",
    "title": "2A: Measurement & Distributions",
    "section": "",
    "text": "This reading:\n\nWhat different types of data can we collect?\nHow can we summarise and visualise distributions of different types of data?\n\nAlso:\n\nThe “Tidyverse” a different style of coding in R"
  },
  {
    "objectID": "02a_measurement.html#categorical",
    "href": "02a_measurement.html#categorical",
    "title": "2A: Measurement & Distributions",
    "section": "Categorical",
    "text": "Categorical\n\nCategorical variables tell us what group or category each individual belongs to. Each distinct group or category is called a level of the variable.\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nNominal (Unordered categorical)\nA categorical variable with no intrinsic ordering among the levels.\nSpecies: Dog, Cat, Parrot, Horse, …\n\n\nOrdinal (Ordered categorical)\nA categorical variable which levels possess some kind of order\nLevel: Low, Medium, High\n\n\nBinary categorical\nA special case of categorical variable with only 2 possible levels\nisDog: Yes or No.\n\n\n\n\nIf we want to summarise a categorical variable into a single number, then the simplest approach is to use the mode:\n\nMode: The most frequent value (the value that occurs the greatest number of times).\n\nWhen we have ordinal variables, there is another option, and that is to use the median:\n\nMedian: For ordinal variables only, this is the value for which 50% of observations are lower and 50% are higher. It is the mid-point of the values when they are rank-ordered.\n\nWhen we use the median as our measure of “central tendency” (i.e. the middle of the distribution) and we want to discuss how spread out the spread are around it, then we will want to use quartiles. The Inter-Quartile Range (IQR) is obtained by rank-ordering all the data, and finding the points at which 25% (one quarter) and 75% (three quarters) of the data falls below (this makes the median the “2nd quartile”).\nIn our dataset on passwords, we have various categorical variables, such as the type of password (categories like “animal”, “fluffy” etc).\nThere are various ways we might want to summarise categorical variables like this. We have already seen the code to do this in our example of the dice simulation - we can simply counting the frequencies in each level:\n\ntable(pwords$type)\n\n\n             animal          cool-macho              fluffy                food \n                 29                  79                  44                  11 \n               name           nerdy-pop    password-related     rebellious-rude \n                183                  30                  15                  11 \nsimple-alphanumeric               sport \n                 61                  37 \n\n\nThis shows us that the mode (most common) is “name” related passwords.\nWe could also convert these to proportions, by dividing each of these by the total number of observations. For instance, here are the percentages of passwords of each type1:\n\ntable(pwords$type) / sum(table(pwords$type)) * 100\n\n\n             animal          cool-macho              fluffy                food \n                5.8                15.8                 8.8                 2.2 \n               name           nerdy-pop    password-related     rebellious-rude \n               36.6                 6.0                 3.0                 2.2 \nsimple-alphanumeric               sport \n               12.2                 7.4 \n\n\n\nOften, if the entries in a variable are characters (letters), then many functions in R (like table()) will treat it the same as if it is a categorical variable.\nHowever, this is not always the case, so it is good to tell R specifically that each variable is a categorical variable.\nThere is a special way that we tell R that a variable is categorical - we set it to be a “factor”. Note what happens when we make the “type” and “strength_cat” variables to be a factor:\n\npwords$type &lt;- factor(pwords$type)\npwords$strength_cat &lt;- factor(pwords$strength_cat)\nsummary(pwords)\n\n      rank         password                          type        cracked      \n Min.   :  1.0   Length:500         name               :183   Min.   : 1.290  \n 1st Qu.:125.8   Class :character   cool-macho         : 79   1st Qu.: 3.430  \n Median :250.5   Mode  :character   simple-alphanumeric: 61   Median : 3.720  \n Mean   :250.5                      fluffy             : 44   Mean   : 5.603  \n 3rd Qu.:375.2                      sport              : 37   3rd Qu.: 3.720  \n Max.   :500.0                      nerdy-pop          : 30   Max.   :92.270  \n                                    (Other)            : 66                   \n    strength      strength_cat\n Min.   : 1.000   medium:402  \n 1st Qu.: 6.000   strong: 25  \n Median : 7.000   weak  : 73  \n Mean   : 6.768               \n 3rd Qu.: 8.000               \n Max.   :10.000               \n                              \n\n\nR now recognises that there a set number of possible response options, or “levels”, for these variables. We can see what they are using:\n\nlevels(pwords$strength_cat)\n\n[1] \"medium\" \"strong\" \"weak\"  \n\n\nThe “strength_cat” variable specifically has an ordering to the levels, so we might be better off also telling R about this ordering. We do this like so:\n\npwords$strength_cat &lt;- factor(pwords$strength_cat, ordered = TRUE, levels = c(\"weak\",\"medium\",\"strong\"))\n\n\nSometimes, we might have a variable that we know is categorical, but we might want to treat it as a set of numbers instead. A very common example in psychological research is Likert data (questions measured on scales such as “Strongly Disagree”&gt;&gt;“Disagree”&gt;&gt;…&gt;&gt;“Strongly Agree”).\nIt is often useful to have these responses as numbers (e.g. 1 = “Strongly Disagree” to 5 = “Strongly Agree”), as this allows us to use certain functions and analyses more easily. For instance, the median() and IQR() functions require the data to be numbers.\nThis will not work:\n\nmedian(pwords$strength_cat)\n\nError in median.default(pwords$strength_cat): need numeric data\n\n\nWhen we ask R to convert a factor to a numeric variable, it will give turn the first category into 1, the second category to 2, and so on. As R knows that our strength_cat variable is the ordered categories “weak”&gt;&gt;“medium”&gt;&gt;“strong”, then as.numeric(pwords$strength_cat) will turn these to 1s, 2s, and 3s.\n\nmedian(as.numeric(pwords$strength_cat))\n\n[1] 2\n\n\n\nConverting between types of data:\nIn R, we can use various functions to convert between different types of data, such as:\n\nfactor() / as.factor() - to turn a variable into a factor\nas.numeric() - to turn a variable into numbers\nas.character() - to turn a variable into letters\n\nand we can check what type of data something is coded as, by using is.factor(), is.numeric(), is.character().\n\n\n\n\n\n\nbe careful with conversions\n\n\n\n\n\nStudy the code below and the output.\nThink carefully about why this happens:\n\nvec &lt;- c(1,2,4,7)\nas.numeric(as.factor(vec))\n\n[1] 1 2 3 4\n\n\nWhy is the output different here?\n\nas.numeric(as.character(as.factor(vec)))\n\n[1] 1 2 4 7"
  },
  {
    "objectID": "02a_measurement.html#numeric",
    "href": "02a_measurement.html#numeric",
    "title": "2A: Measurement & Distributions",
    "section": "Numeric",
    "text": "Numeric\n\nNumeric (or quantitative) variables consist of numbers, and represent a measurable quantity. Operations like adding and averaging make sense only for numeric variables.\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nContinuous\nVariables which can take any real number within the specified range of measurement\nHeight: 172, 165.2, 183, …\n\n\nDiscrete\nVariables which can only take integer number values. For instance, a counts can only take positive integer values (0, 1, 2, 3, etc.)\nNumber_of_siblings: 0, 1, 2, 3, 4, …\n\n\n\n\nOne of the most frequently used measures of central tendency for numeric data is the mean. The mean is calculated by summing all of the observations together and then dividing by the total number of obervations (\\(n\\)).\n\nMean: \\(\\bar{x}\\)\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the mean is:\n\\[\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\]\n\n\n\n\n\n\nHelp reading mathematical formulae\n\n\n\n\n\nThis might be the first mathematical formula you have seen in a while, so let’s unpack it.\nThe \\(\\sum\\) symbol is used to denote a series of additions - a “summation”.\nWhen we include the bits around it: \\(\\sum\\limits_{i = 1}^{n}x_i\\) we are indicating that we add together all the terms \\(x_i\\) for values of \\(i\\) between \\(1\\) and \\(n\\): \\[\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n\\]\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the \\(1^{st}\\) to the \\(n^{th}\\) (where \\(n\\) is the total number of values), and we divide that by \\(n\\).\n\n\n\n\nIf we are using the mean as our as our measure of central tendency, we can think of the spread of the data in terms of the deviations (distances from each value to the mean).\nRecall that the mean is denoted by \\(\\bar{x}\\). If we use \\(x_i\\) to denote the \\(i^{th}\\) value of \\(x\\), then we can denote deviation for \\(x_i\\) as \\(x_i - \\bar{x}\\).\nThe deviations can be visualised by the red lines in Figure 1.\n\n\n\n\n\nFigure 1: Deviations from the mean\n\n\n\n\n\nThe sum of the deviations from the mean, \\(x_i - \\bar x\\), is always zero\n\\[\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n\\]\nThe mean is like a center of gravity - the sum of the positive deviations (where \\(x_i &gt; \\bar{x}\\)) is equal to the sum of the negative deviations (where \\(x_i &lt; \\bar{x}\\)).\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider squared deviations.\nSquaring the deviations makes them all positive. Observations far away from the mean in either direction will have large, positive squared deviations. The average squared deviation is known as the variance, and denoted by \\(s^2\\)\n\nVariance: \\(s^2\\)\nThe variance is calculated as the average of the squared deviations from the mean.\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the variance is:\n\\[s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\noptional: why n minus 1?\n\n\n\n\n\nThe top part of the equation \\(\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\\) can be expressed in \\(n-1\\) terms, so we divide by \\(n-1\\) to get the average.\n Example: If we only have two observations \\(x_1\\) and \\(x_2\\), then we can write out the formula for variance in full quite easily. The top part of the equation would be: \\[\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\]\nThe mean for only two observations can be expressed as \\(\\bar{x} = \\frac{x_1 + x_2}{2}\\), so we can substitute this in to the formula above. \\[\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\] becomes: \\[\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2\n\\] Which simplifies down to one value: \\[\n\\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n\\]  So although we have \\(n=2\\) datapoints, \\(x_1\\) and \\(x_2\\), the top part of the equation for the variance has 1 fewer units of information. In order to take the average of these bits of information, we divide by \\(n-1\\).\n\n\n\n\nOne difficulty in interpreting variance as a measure of spread is that it is in units of squared deviations. It reflects the typical squared distance from a value to the mean.\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the standard deviation.\n\nStandard Deviation: \\(s\\)\nThe standard deviation, denoted by \\(s\\), is a rough estimate of the typical distance from a value to the mean.\nIt is the square root of the variance (the typical squared distance from a value to the mean).\n\\[\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n\\]\n\nIn the passwords dataset, we only have one continuous variable, and that is the “cracked” variable, which if we recall is the “Time to crack by online guessing”. You might be questioning whether the “strength” variable, which ranges from 1 to 10 is numeric? This depends on whether we think that statements like “a password of strength 10 is twice as strong as a password of strength 5”.\nFor now, we’ll just look at the “cracked” variable.\nTo calculate things like means and standard deviations in R is really easy, because there are functions that do them all for us.\nFor instance, we can do the calculation by summing the cracked variable, and dividing by the number of observations (in our case we have 500 passwords):\n\n# get the values in the \"cracked\" variable from the \"pwords\" dataframe, and\n# sum them all together. Then divide this by 500\nsum(pwords$cracked)/500\n\n[1] 5.60266\n\n\nOr, more easily, we can use the mean() function:\n\nmean(pwords$cracked)\n\n[1] 5.60266\n\n\nWe can get R to calculate the variance and standard deviation with the var() and sd() functions:\n\nvar(pwords$cracked)\n\n[1] 71.16618\n\nsd(pwords$cracked)\n\n[1] 8.436005\n\n\nand just to prove to ourselves:\n\nsd(pwords$cracked)^2 == var(pwords$cracked)\n\n[1] TRUE\n\n\n\nIf a column of our dataset contains only numbers, R will typically just interpret it as a numeric variable. However, we should still be careful; remember what happens if we have just one erroneous entry in there - they can all change to be characters (surrounded by quotation marks):\n\nc(1,3,6,\"peppapig\",3)\n\n[1] \"1\"        \"3\"        \"6\"        \"peppapig\" \"3\"       \n\n\nWe can force a variable to be numeric by using as.numeric(), which will also coerce any non-numbers to be NA (not applicable):\n\nas.numeric(c(1,3,6,\"peppapig\",3))\n\n[1]  1  3  6 NA  3\n\n\nIf there is an NA in the variable, many functions like mean(), var() and sd() will not compute:\n\nx &lt;- c(1, 3, 6, NA, 3)\nmean(x)\n\n[1] NA\n\n\nHowever, we can ask these functions to remove the NAs prior to the computation:\n\nmean(x, na.rm = TRUE)\n\n[1] 3.25\n\n\n\n\n\n\n\n\nFigure 2: Artwork by @allison_horst"
  },
  {
    "objectID": "02a_measurement.html#this-is-a-pipe",
    "href": "02a_measurement.html#this-is-a-pipe",
    "title": "2A: Measurement & Distributions",
    "section": "This is a pipe!",
    "text": "This is a pipe!\nWe have seen already seen a few examples of code such as:\n\ndim(somedata)\ntable(somedata$somevariable)\n\n\n\n\nAnd we have seen how we might wrap functions inside functions:\n\nbarplot(table(somedata$somevariable))\n\nThis sort of writing (functions inside functions inside functions) involves R evaluating code from the inside out. But a lot of us don’t intuitively think that way, and actually find it easier to think in terms of a sequence. The code barplot(table(somedata$somevariable)) could be read as “take this variable, then make a table of it, then make a barplot of that table”.\nWe can actually write code that better maps to this way of reading, using a nice little symbol called a “pipe”:\n\nPiping\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write sequentially:\n\nNotice that what we are doing is using a new symbol: |&gt;\nThis symbol takes the output of whatever is on it’s left-hand side, and uses it as an input for whatever is on the right-hand side.\nThe |&gt; symbol gets called a “pipe”.\n\nLet’s see it in action with the passwords dataset we’ve been using.\n\n\ninside-out\nThe typical way of writing code is requires reading from the inside-out:\n\nbarplot(table(pwords$type))\n\n\n\n\n\n\n\n\n\n\npiped\nWhen we pipe code, we can read it from left to right:\n\npwords$type |&gt;\n    table() |&gt;\n    barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther pipes: |&gt; and %&gt;%\n\n\n\n\n\nThe |&gt; pipe is a relatively recent addition to R, but will likely be replacing the older %&gt;% pipe that was in a specific set of packages, and has been around since about 2014.\nThese two pipes do basically the same thing\nThere are some subtle differences between the two that only become apparent in very specific situations, none of which are likely to arise on this course.\nHowever, it’s important to be aware of them both, because you will like see them both in resources/online forums etc. You can usually just use them interchangeably.\n\n\n# for %&gt;% we need the tidyverse\nlibrary(tidyverse)\n1:10 %&gt;% mean()\n\n\n\n\n# the new base R pipe\n1:10 |&gt; mean()"
  },
  {
    "objectID": "02a_measurement.html#the-tidyverse",
    "href": "02a_measurement.html#the-tidyverse",
    "title": "2A: Measurement & Distributions",
    "section": "The Tidyverse",
    "text": "The Tidyverse\nWe’re going to use pipes a lot throughout this course, and it pairs really well with a group of functions in the tidyverse packages, which were designed to be used in conjunction with a pipe:\n\nselect() extracts columns\n\nfilter() subsets data based on conditions\n\nmutate() adds new variables\n\ngroup_by() group related rows together\n\nsummarise()/summarize() reduces values down to a single summary\n\nTypically, the tidyverse means that we no longer have to keep telling R in which dataframe to look for the variable. The tidyverse functions are designed to make things is a bit easier. The examples below show how.\nYou’ll notice that the code has lots of indentations to make it more readable, which RStudio does for you when you press enter!\nBefore anything else, however, we need to load the tidyverse package:\n\nlibrary(tidyverse)\n\n\nselect()\nWe know about using $ to extract a column from a dataframe. The select() function is a little bit like that - it allows us to choose certain columns in a dataframe. It will return all rows.\nBecause we can select multiple columns this way, it doesn’t return us a vector (in the way dataframe$variable does), but returns a dataframe:\n\n# take the data\n# and select the \"variable1\" and \"variable2\" columns\ndata |&gt;\n  select(variable1, variable2)\n\n\n\nTidyverse\n\npwords |&gt;\n  select(type, strength)\n\n\n\nBase R\n\npwords[, c(\"type\",\"strength\")]\n\n\n\n\n\nfilter()\nThe filter() function is a bit like the [] to choose rows that meet certain conditios - it allows us to filter a dataframe down to those rows which meet a given condition. It will return all columns.\n\n# take the data\n# and filter it to only the rows where the \"variable1\" column is \n# equal to \"value1\". \ndata |&gt; \n  filter(variable1 == value1)\n\n\n\nTidyverse\n\npwords |&gt;\n    filter(strength_cat == \"strong\")\n\n\n\nBase R\n\npwords[pwords$strength_cat == \"strong\", ]\n\n\n\n\n\nmutate()\nThe mutate() function is used to add or modify variables to data.\n\n# take the data\n# |&gt;\n# mutate it, such that there is a variable called \"newvariable\", which\n# has the values of a variable called \"oldvariable\" multiplied by two.\ndata |&gt;\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\nTo ensure that our additions/modifications of variables are stored in R’s environment (rather than simply printed out), we need to reassign the name of our dataframe:\n\ndata &lt;- \n  data |&gt;\n  mutate(\n    ...\n  )\n\nNote: Inside functions like mutate(), we don’t have to keep using the dollar sign $, as we have already told it what data to look for variables in.\n\n\nTidyverse\n\npwords &lt;- pwords |&gt; \n    mutate(\n        cracked_min = cracked / 60\n    )\n\n\n\nBase R\n\npwords$cracked_min &lt;- pwords$cracked / 60\n\n\n\n\n\nsummarise()\nThe summarise() function is used to reduce variables down to a single summary value.\n\n# take the data |&gt;\n# summarise() it, such that there is a value called \"summary_value\", which\n# is the sum() of \"variable1\" column, and a value called \n# \"summary_value2\" which is the mean() of the \"variable2\" column.\ndata |&gt;\n  summarise(\n    summary_value = sum(variable1),\n    summary_value2 = mean(variable2)\n  )\n\n\n\nTidyverse\n\npwords |&gt; \n    summarise(\n        mean_cracked = mean(cracked),\n        sd_cracked = sd(cracked),\n        nr_strong = sum(strength_cat == \"strong\")\n    )\n\n\n\nBase R\nTo store these all in the same object (like the tidyverse way) we would have to create a data.frame() and add these as variables.\n\nmean(pwords$cracked)\nsd(pwords$cracked)\nsum(pwords$strength_cat == \"strong\")\n\n\n\n\n\ngroup_by()\nThe group_by() function is often used as an intermediate step in order to do something. For instance, if we want to summarise a variable by calculating its mean, but we want to do that for several groups, then we first group_by() and then summarise():\n\n# take the data |&gt; \n# and, grouped by the levels of the \"mygroups\" variable,\n# summarise() it so that there is a column called \"summary_col\", which\n# is the mean of the \"variable1\" column for each group. \ndata |&gt;\n    group_by(mygroups) |&gt;\n    summarise(\n        summary_col = mean(variable1)\n    )\n\n\n\nTidyverse\n\npwords |&gt; \n    group_by(strength_cat) |&gt;\n    summarise(\n        mean_cracked = mean(cracked)\n    )\n\n\n\nBase R\nThis is less easy. There are functions in Base R that can do similar things, but we’re not going to teach those here. You could envisage getting all the same values by doing:\n\nmean(pwords$cracked[pwords$strength_cat == \"weak\"])\nmean(pwords$cracked[pwords$strength_cat == \"medium\"])\nmean(pwords$cracked[pwords$strength_cat == \"strong\"])"
  },
  {
    "objectID": "02a_measurement.html#boxplots",
    "href": "02a_measurement.html#boxplots",
    "title": "2A: Measurement & Distributions",
    "section": "Boxplots",
    "text": "Boxplots\nBoxplots provide a useful way of visualising the interquartile range (IQR). You can see what each part of the boxplot represents in Figure Figure 4.\n\n\n\n\n\nFigure 4: Anatomy of a boxplot\n\n\n\n\nWe can create a boxplot of our age variable using the following code:\n\n# Notice, we put strength on the x axis, making the box plot vertical. \n# If we had set aes(y = strength) instead, then it would simply be rotated 90 degrees \nggplot(data = pwords, aes(x = strength)) +\n  geom_boxplot()"
  },
  {
    "objectID": "02a_measurement.html#histograms",
    "href": "02a_measurement.html#histograms",
    "title": "2A: Measurement & Distributions",
    "section": "Histograms",
    "text": "Histograms\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these map to how visualisations of numeric variables look.\nWe can visualise numeric data using a histogram, which shows the frequency of values which fall within bins of an equal width.\nTo do this, we’re going to use some new data, on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests. The data are available at https://uoepsy.github.io/data/wechsler.csv\n\nwechsler &lt;- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n\n\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\nWe can specifiy the width of the bins:\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\n\nLet’s take a look at the means and standard deviations of participants’ scores on the other tests (the test1 and test2 variables).\nNote how nicely we can do this with our newfound tidyverse skills!\n\nwechsler |&gt; \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n\n# A tibble: 1 × 4\n  mean_test1 sd_test1 mean_test2 sd_test2\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1       49.3     7.15       51.2     14.4\n\n\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1.\n\n\n\n\n\n\n\n\n\n\nDefining moments\nThe “moments” of a distribution are the metrics that relate to the shape of that distribution. We’ve already seen the primary two moments that define the shapes of these distributions: the mean and the variance. The mean moves the distribution right or left, and the variance makes the distribution wider or narrower.\nThere are two more, “skewness” and “kurtosis” which tend to be of less focus of investigation (the questions we ask tend to be mainly concerned with means and variances). Skewness is a measure of asymmetry in a distribution. Distributions can be positively skewed or negatively skewed, and this influences our measures of central tendency and of spread to different degrees. The kurtosis is a measure of how “pointy” vs “rounded” the shape of a distribution is."
  },
  {
    "objectID": "02a_measurement.html#density",
    "href": "02a_measurement.html#density",
    "title": "2A: Measurement & Distributions",
    "section": "Density",
    "text": "Density\nIn addition to grouping numeric data into bins in order to produce a histogram, we can also visualise a density curve.\nBecause there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, …), we could group the data into infinitely many bins. This is essentially what we are doing with a density curve.\nYou can think of “density” as a bit similar to the notion of “relative frequency” (or “proportion”), in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\nArea under the curve\nThink about the barplots we have been looking at in the exercises where we simulate dice rolling :\n\n# our function to simulate the roll of a die/some dice\ndice &lt;- function(num = 1) {\n  sum(sample(1:6, num, replace=TRUE))\n}\n# simulate 1000 rolls of a single die\nroll1000 &lt;- replicate(1000, dice(1))\n# tabulate and plot:\ntable(roll1000) |&gt;\n  barplot(ylab=\"count\")\n\n\n\n\n\n\n\n\nTo think about questions like “what proportion of 1000 rolls does the die land on 6?”, we are simply interested in the count of 6s divided by the count of all rolls:\n\ntab1000 &lt;- table(roll1000)\ntab1000\n\nroll1000\n  1   2   3   4   5   6 \n162 169 167 189 152 161 \n\ntab1000[6] / sum(tab1000)\n\n    6 \n0.161 \n\n\nSo Another way of thinking of this is that we are just dividing the count in each category by the total number. Or, Put another way, imagine we divide the area of each bar by the total area. The area now sums to 1, and our question is asking about the ratio of the red area to the total area (grey + red):\n\n\n\n\n\n\n\n\n\nNothing really changes with a density curve! If we want to ask what proportion of our distribution of IQ scores is &gt;120, then we are asking about the area under the curve that is to the right of 120:\n\n\n\n\n\n\n\n\n\nIt looks like about a third, maybe a little less. Let’s calculate this proportion directly:\n\nsum(wechsler$iq&gt;110) / length(wechsler$iq)\n\n[1] 0.2\n\n\nIt might seem a little odd to think about area under the curve when we are asking about “what proportion of the data is …?”. If we have the data, then we can just calculate the answer (like we did above). However, a lot of statistics is really concerned with the probability of events. When we discuss probability, we move from talking about a specific set of observed data to thinking about a theoretical/mathematical model that defines the way in which data is generated. This where it becomes more useful to think about distributions in a more abstract sense.\nFor instance, with a fair six-sided die, we have a probability distribution (Figure 5) in which each side is given the probability \\(\\frac{1}{6}\\): \\[\n\\begin{gather*}\nP(x) = \\begin{cases}\n  \\frac{1}{6} & \\text{if $x \\in \\{1,2,3,4,5,6\\}$}\\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n\\end{gather*}\n\\] Instead of rolling a die, suppose that we are picking a person off the street and measuring their IQ. Given that IQ scales are designed to have a mean of 100 and standard deviation of 15, what is the probability that we pick a person with an IQ of greater than 110?\n\n\n\n\n\nFigure 5: Left: Discrete probability distribution of a fair six-sided die. Right: Continuous probability distribution of IQ scores"
  },
  {
    "objectID": "02a_measurement.html#footnotes",
    "href": "02a_measurement.html#footnotes",
    "title": "2A: Measurement & Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthink about what sum(table(pwords$type)) is doing. it’s counting all the values in the table. so it’s going to give us the total↩︎"
  },
  {
    "objectID": "02b_sampling.html",
    "href": "02b_sampling.html",
    "title": "2B: Curves & Sampling",
    "section": "",
    "text": "This reading:\n\nWhat are probability distributions and why are they relevant?\n\nHow does using a sample to approximate a population lead to uncertainty?\nHow can we quantify uncertainty due to sampling?"
  },
  {
    "objectID": "02b_sampling.html#the-standard-normal-distribution",
    "href": "02b_sampling.html#the-standard-normal-distribution",
    "title": "2B: Curves & Sampling",
    "section": "The Standard Normal Distribution",
    "text": "The Standard Normal Distribution\nNote that if we translate our “IQ &gt;120” to being in terms of standard deviations - \\(\\frac{120 - 100}{15} = 1\\frac{1}{3}\\) - then we can perform the same computations as we have done above, but comparing against against a normal distribution with mean of 0 and standard deviation of 1 (which are the defaults for the pnorm() function):\n\npnorm((120-100)/15, lower.tail = FALSE)\n\n[1] 0.09121122\n\n\n\n\n\n\n\nFigure 7: pnorm() with the ‘standard normal distribution’: the normal distribution with mean = 0 and sd = 1\n\n\n\n\nWhat we’re doing here is re-expressing the observed distribution into one which has mean of 0 and standard deviation of 1 - we are standardising them. This idea will become incredibly useful. For one thing it makes comparisons possible, for example, consider the two statements below:\n\n“I am 15 IQ points higher than average, and 24cm taller than average”\n“I am 1 standard deviation above the average IQ, and 2 standard deviations above average height”\n\nThe standard normal distribution - the normal distribution with mean = 0, sd = 1, is going to be seen a lot more frequently."
  },
  {
    "objectID": "02b_sampling.html#the-relevance-of-the-normal-distribution",
    "href": "02b_sampling.html#the-relevance-of-the-normal-distribution",
    "title": "2B: Curves & Sampling",
    "section": "The relevance of the normal distribution?",
    "text": "The relevance of the normal distribution?\nWe can motivate the relevance of the normal distribution in various ways. For instance, when we take a measurement of something such as the length of a stick, then we always have a bit of imprecision - our measurements will vary a bit. Assuming that our measurement tool is unbiased and this imprecision is purely random, we would expect the measurements of the stick to be ‘normally distributed’ around the true length of the stick (Figure 8).\n\n\n\n\n\nFigure 8: Snapshots from 21/22 lecture slides on measurement\n\n\n\n\nIn this way, the normal distribution captures the idea of random deviations around a central point. As we will see below, this becomes extremely relevant for statistics because we tend to collect data on a random sample of people, and each sample we could take will randomly deviate a bit in how well it represents the bigger group that we took it from."
  },
  {
    "objectID": "02b_sampling.html#footnotes",
    "href": "02b_sampling.html#footnotes",
    "title": "2B: Curves & Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nremember that standard deviation is \\(\\sqrt{\\text{variance}}\\)↩︎\nOften in neuropsychological testing, a set of “normative values” are provided in order to define “what is expected” (sometimes in reference to a specific population).↩︎\nand the statements may not hold for all individuals - for certain people, some drugs just won’t work! but what is important for a healthcare system deciding on whether or not to purchase supplies of a drug is the average treatment effect.↩︎\nIf you’re working along with this, yours will be different, because it’s random!↩︎\nImagine a bag full of coloured marbles. If we sample with replacement, then we take a marble out, record its colour, and put it back. Then we take a marble out, record its colour, and put it back. And so on. This means we might get the same marble more than once.↩︎\nusing the formula \\(\\frac{\\sigma}{\\sqrt{n}}\\) for standard error↩︎"
  },
  {
    "objectID": "03_ex.html",
    "href": "03_ex.html",
    "title": "Exercises: T-tests",
    "section": "",
    "text": "Question 1\n\n\nAt the end of last week’s exercises, we estimated the mean sleep-quality rating, and computed a confidence interval, using the formula below.\n\\[\n\\begin{align}\n\\text{95\\% CI: }& \\bar x \\pm 1.96 \\times SE \\\\\n\\end{align}\n\\]\nCan you use R to show where the 1.96 comes from?\n\n\n\n\n\n\nHints\n\n\n\n\n\nqnorm! (see the end of 03A #uncertainty-due-to-sampling)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe 1.96 comes from 95% of the normal distribution falling within 1.96 standard deviations of the mean:\n\nqnorm(c(0.025, 0.975))\n\n[1] -1.959964  1.959964\n\n\n\n\n\n\nQuestion 2\n\n\nAs we learned in 3B #t-distributions, the sampling distribution of a statistic has heavier tails the smaller the size of the sample it is derived from. In practice, we are better using \\(t\\)-distributions to construct confidence intervals and perform statistical tests.\nThe code below creates a dataframe that contains the number of books read by 7 people in 2022.\n(Note tibble is just a tidyverse version of data.frame):\n\nbookdata &lt;- \n  tibble(\n    person = c(\"Martin\",\"Umberto\",\"Monica\",\"Emma\",\"Josiah\",\"Dan\",\"Aja\"),\n    books_read = c(12,19,9,11,8,28,13)\n  )\n\nCalculate the mean number of books read in 2022, and construct an appropriate 95% confidence interval.\n\n\n\n\n\nSolution\n\n\n\nHere is our estimated average number of books read:\n\nmean(bookdata$books_read)\n\n[1] 14.28571\n\n\nAnd our standard error is still \\(\\frac{s}{\\sqrt{n}}\\):\n\nsd(bookdata$books_read)/sqrt(nrow(bookdata))\n\n[1] 2.652171\n\n\nWith \\(n = 7\\) observations, and estimating 1 mean, we are left with \\(6\\) degrees of freedom.\nFor our 95% confidence interval, the \\(t^*\\) in the formula below is obtained via:1\n\nqt(0.975, df = 6)\n\n[1] 2.446912\n\n\nOur confidence interval is therefore:\n\\[\n\\begin{align}\n\\text{CI} &= \\bar{x} \\pm t^* \\times SE \\\\\n\\text{95\\% CI} &= 14.286 \\pm 2.447 \\times 2.652 \\\\\n\\text{95\\% CI} &= [7.80,\\, 20.78] \\\\\n\\end{align}\n\\]\n\n\n\n\nQuestion 3\n\n\nWill a 90% confidence interval be wider or narrow?\nCalculate it and see.\n\n\n\n\n\nSolution\n\n\n\nA 90% confidence interval will be narrower:\n\nqt(0.95, df = 6)\n\n[1] 1.94318\n\n\n\\[\n\\begin{align}\n\\text{CI} &= \\bar{x} \\pm t^* \\times SE \\\\\n\\text{90\\% CI} &= 14.286 \\pm 1.943 \\times 2.652 \\\\\n\\text{90\\% CI} &= [9.13,\\, 19.44] \\\\\n\\end{align}\n\\]\nThe intuition behind this is that our level of confidence is inversely related to the width of the interval.\nTake it to the extremes:\n\nI have 100% confidence that the interval \\([-Infinity, +Infinity]\\) contains the true population mean.\nIf I want an narrower interval, then I have to sacrifice confidence. e.g. a 10% CI: \\([13.94, 14.63]\\)\n\nImagine playing a game of ringtoss. A person throwing a 2-meter diameter hoop will have much more confidence that they are going to get it over the pole than a person throwing a 10cm diameter ring."
  },
  {
    "objectID": "03_ex.html#footnotes",
    "href": "03_ex.html#footnotes",
    "title": "Exercises: T-tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Why 97.5? and not 95? We want the middle 95%, and \\(t\\)-distributions are symmetric, so we want to split that 5% in half, so that 2.5% is on either side. We could have also used qt(0.025, df = 6), which will just give us the same number but negative: -2.4469119)↩︎"
  },
  {
    "objectID": "03a_inference.html",
    "href": "03a_inference.html",
    "title": "3A: Foundations of Inference",
    "section": "",
    "text": "This reading:\n\nHow do we quantify uncertainty due to sampling?\n\nHow can we make decisions (what to believe/how to act, etc) that take uncertainty into account?\n\nHow likely are we to make the wrong decision?\nWe use statistics primarily to estimate parameters in a population. Whether we are polling people to make predictions about the proportion of people who will vote for a certain party in the next election, or conducting a medical trial and assessing the change in blood pressure for patients given drug X vs those given a placebo in order to decide whether to put the drug into circulation in health service.\nWe have seen this already last week: We observed a sample of peoples’ life satisfaction ratings (scale 0-100), and we wanted to use these to make some statement about the wider population, such as “the average life-satisfaction rating is ?? out of 100”. So we use the mean of our sample, as an estimate of the mean of the population."
  },
  {
    "objectID": "03a_inference.html#test-statistics-p-values",
    "href": "03a_inference.html#test-statistics-p-values",
    "title": "3A: Foundations of Inference",
    "section": "Test-statistics & p-values",
    "text": "Test-statistics & p-values\nThe p-value is a formal way of testing a statistic against a null hypothesis. To introduce the p-value, instead of thinking first about what we have observed in our sample, we need to think about what we would expect to observe if our null hypothesis is true.\nWith our Stroop Task example, our null hypothesis is that there is no difference between matching and mismatching conditions (\\(H_0: \\mu = 0\\)). Under \\(H_0\\), the average ‘mismatching-matching’ score in the population is zero, and we would expect most of the samples we might take to have a mean score of close to this (not exactly 0, but centered around 0). We saw above that we could express the sampling distribution of means taken from samples of size \\(n=131\\) using the standard error. Under \\(H_0\\) we would expect the samples of \\(n=131\\) we might take to have means that follow something like the distribution in Figure 3. We can think of this as the sampling distribution of \\(\\bar{x}\\), but centered on our null hypothesis (in this case, \\(\\mu = 0\\)). We call this the ‘null distribution’.\n\n\n\n\n\nFigure 3: Sampling distribution for mean of sample size 131, assuming population mean = 0. Observed sample mean shown in red\n\n\n\n\n\nTest-statistic\nThe first step now is to create a test-statistic. That is, a statistic that tell us, in some standardised units, how big our observed effect is from the null hypothesis (i.e. in this case, how far from \\(\\mu=0\\) our sample mean is).\nThe straightforward way to do this is to express how far away from \\(\\mu=0\\) our sample mean is in terms of standard errors. We’ll call our test statistic \\(Z\\):\n\\[\nZ = \\frac{\\text{estimate}-\\text{null}}{SE}\n\\]\nOur mean and standard error are:\n\nmean(stroopdata$diff)\n\n[1] 2.402977\n\nsd(stroopdata$diff) / sqrt(nrow(stroopdata))\n\n[1] 0.4382302\n\n\nSo our test-statistic is \\[\nZ = \\frac{2.40 - 0}{0.438} = 5.479\n\\]\n\n\np-value\nWe can now calculate how likely it is to see values at least as extreme as our observed test-statistic, if the null is true.\nIf the null hypothesis is true (there was no ‘mismatching-matching’ difference) then we would expect Z-statistics to be normally distributed with a mean of 0 and a standard deviation of 1.\nWe have seen the process of how we might calculate a probability from a distribution like this already: the pnorm() function gives us the area of a distribution to the one side of a given value:\n\npnorm(??, mean = 0, sd = 1, lower.tail = FALSE)\n\n\n\n\n\n\nFigure 4: pnorm() provides us with a p-value for a z-statistic\n\n\n\n\nRemember, our Z-statistic we calculated above is 5.479. If the null hypothesis were true then the probability that we would see a sample (\\(n=131\\)) with a Z-statistic at least that large is:\n\npnorm(5.479, lower.tail = FALSE)\n\n[1] 2.138682e-08\n\n\nwhich is R’s way of printing 0.00000002138682.\nThere is one last thing, and that the direction of our hypotheses. Recall from earlier that we stated \\(H_0: \\mu = 0\\) and \\(H_1: \\mu \\neq 0\\). This means that we are interested in the probability of getting results this far away from 0 in either direction.\nWe are interested in both tails:\n\n\n\n\n\nFigure 5: 2*pnorm gives the two tails\n\n\n\n\n\n2 * pnorm(5.479, lower.tail = FALSE)\n\n[1] 4.277364e-08\n\n\nor \\(p =\\) 0.00000004277364.\n\np-value\nThe p-value is the probability4 that we observe a test statistic at least as extreme as the one we observed, assuming the null hypothesis \\(H_0\\) to be true."
  },
  {
    "objectID": "03a_inference.html#making-decisions",
    "href": "03a_inference.html#making-decisions",
    "title": "3A: Foundations of Inference",
    "section": "Making Decisions",
    "text": "Making Decisions\nNow that we have our p-value of 0.00000004277364, we need to use it to make a decision about our hypotheses.\nTypically, we pre-specify the probability level at which we will consider results to be so unlikely to have arisen from the null distribution that we will take them as evidence to reject the null hypothesis. This pre-specified level is commonly referred to as \\(\\alpha\\) (“alpha”). Setting \\(\\alpha = 0.05\\) means that we will reject \\(H_0\\) when we get a result which is extreme enough to only occur 0.05 (5%) of the time or less if the \\(H_0\\) is true.\nIn our case, 0.00000004277364 \\(&lt; 0.05\\), so we reject the null hypothesis that there is no difference in the mismatching/matching conditions of the Stroop Task.\n\nThere’s a lot of convention to how we talk about NHST, but the typical process is as follows:\n\nClearly specify the null and alternative hypotheses.\n\nSpecify \\(\\alpha\\)\nCalculate statistic\nCompute p-value\n\nIf \\(p&lt;\\alpha\\), then reject the null hypothesis.\nIf \\(p\\geq\\alpha\\), then fail to reject* the null hypothesis.\n\n\n*Note, we don’t “accept” anything, we just “reject” or “fail to reject” the null hypothesis. Think of it like a criminal court, and we are trying the null hypothesis - \\(H_0\\) is “innocent until proven guilty”."
  },
  {
    "objectID": "03a_inference.html#making-mistakes",
    "href": "03a_inference.html#making-mistakes",
    "title": "3A: Foundations of Inference",
    "section": "Making Mistakes",
    "text": "Making Mistakes\nWhether our eventual decision is a) reject the null hypothesis, or b) fail to reject the null hypothesis, there’s always a chance that we might be making a mistake. There are actually two different types of mistakes we might make. An often used analogy (Figure 6) is the idea of criminal trials in which an innocent person can be wrongfully convicted, or a guilty person can be set free.\n\n\n\n\n\nFigure 6: Making errors in NHST is like a criminal court making errors in its decision on the defendent\n\n\n\n\nWe can actually quantify the chance that we’re making errors in our different decisions. Thinking back to the definition of a p-value, it is the probability of seeing our results if the null hypothesis is true. If we make a decision to reject the null hypothesis based on whether \\(p&lt;\\alpha\\), then the probability that this decision is a mistake is \\(\\alpha\\).\nThe probability that we the other sort of error (failing to reject the null hypothesis when the null hypothesis is actually false), we denote with \\(\\beta\\).\nDoing statistics is partly a matter of balancing these possibilities. If we used a very low \\(\\alpha\\)-level (e.g. we reject when \\(p&lt;.0001\\) rather than \\(p&lt;.05\\)) then we increase the probability of making a type II error.\n\nTypes of Errors in NHST\n\n\n\n\n\nFigure 7: Probabilities of making different errors in NHST\n\n\n\n\n\n\nPower (\\(1-\\beta\\))\nA key notion in conducting studies is “statistical power”. Studies want to increase the probability of correctly rejecting the null hypothesis (i.e. correctly identifying that there is something more than chance going on).\nThis is the bottom right cell of the tables in Figure 6 and Figure 7. We know that this will depend on the \\(\\alpha\\)-level that we choose, but there are other important factors that influence \\(1-\\beta\\):\n\npower increases as sample size increases\n\ne.g. it’s easier to determine that cats weigh less than dogs if we measure 100 animals vs if we measure only 10 animals\n\npower increases the farther away the true value is from the null hypothesis value\n\ne.g. it’s easier to determine that cats weigh less than elephants than it is to determine that cats weigh less than dogs"
  },
  {
    "objectID": "03a_inference.html#footnotes",
    "href": "03a_inference.html#footnotes",
    "title": "3A: Foundations of Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html↩︎\n For other intervals, such as a 90% interval, we need to know the point at which 5% is either side of a normal distribution (i.e., giving us the middle 90%). qnorm(c(0.05,0.95)) will give us 1.64, which we then put into our construction of the interval: \\(90\\%\\, CI = \\bar{x} \\pm 1.64 \\times SE\\).↩︎\nThink about an example where our question is about whether there is a difference in variable \\(Y\\) between groups A, B, C and D. Around what should we construct our interval? Around the difference \\(\\bar{Y}_A - \\bar{Y}_B\\) (difference between A and B’s average scores on \\(Y\\)), or \\(\\bar{Y}_A - \\bar{Y}_C\\), or \\(\\bar{Y}_B - \\bar{Y}_D\\)?↩︎\nWhat we have been seeing is that probabilities in NHST are defined as the relative frequency of an event over many trials (as “many” \\(\\to \\infty\\)). This requires assuming some features of the data generating process which guides what the “many trials” would look like (e.g., that there is no effect). The \\(p\\)-value is the probability of observing results as or more extreme than the data, if the data were really generated by a hypothesised chance process.↩︎"
  },
  {
    "objectID": "03b_inference2.html",
    "href": "03b_inference2.html",
    "title": "3B: Practical Inference",
    "section": "",
    "text": "This reading:\n\nHow does hypothesis testing work in practice?\n\nHow do we do all this in R?\n\nspoiler: it’s easier than you think\n\nWhat are some basic hypothesis tests that we can conduct?\n\nTests of a single continuous variable\nTests of the relationship between a continuous variable and a binary categorical variable\nIn 3A we learned about the logic of Null Hypothesis Significance Testing (NHST), allowing us to draw perform inferentials tests about parameters in the population, based on statistics computed on the sample that we have collected.\nWhile in practice NHST follows the logic described above, there is something important that we have been sweeping under the carpet.\nIn our estimation of the standard error we have used the formula that includes \\(\\sigma\\), which refers to the population standard deviation. However, we never know this value (because we don’t have data for the population), so we have been using the sample standard deviation \\(s\\) instead. This is an approximation, and might be okay when we have a very large \\(n\\) (meaning \\(s\\) provides an accurate estimate of \\(\\sigma\\)), but in practice this is not always feasible. \\[\nSE = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}\n\\]"
  },
  {
    "objectID": "03b_inference2.html#one-sample-t-test",
    "href": "03b_inference2.html#one-sample-t-test",
    "title": "3B: Practical Inference",
    "section": "One sample t-test",
    "text": "One sample t-test\n\nPurpose\nThe one sample t-test is what we have already seen above. We use it to test whether the mean is different from/greater than/less than some hypothesised value.\n\nExamples:\n\nIs the mean age of USMR students different from 20?\nIs the mean IQ different from 100?\n\nDo people read more than 250 words per minute?\n\n\nAssumptions:\n\nThe data are continuous (not discrete)\nThe data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)\nThe data are normally distributed (can be relaxed somewhat if the sample size is “large enough” (rule-of-thumb n = 30) and the data are not strongly skewed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Do people read more than 250 words per minute?\n\nFifty participants were recruited and tasked with reading a passage of text that was 2000 words long. Their reading times (in words per minute) was recorded, and these are accessible at https://uoepsy.github.io/data/usmr_tread.csv.\n\nwpmtime &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_tread.csv\")\nhead(wpmtime)\n\n# A tibble: 6 × 2\n  id      wpm\n  &lt;chr&gt; &lt;dbl&gt;\n1 ppt_1   307\n2 ppt_2   265\n3 ppt_3   205\n4 ppt_4   300\n5 ppt_5   207\n6 ppt_6   300\n\n\n\n\n\n\n\n\n\n\n\nDescriptives and Assumptions\n\n\n\n\n\nBelow are some quick descriptives.\n\nmean(wpmtime$wpm)\n\n[1] 258.36\n\nsd(wpmtime$wpm)\n\n[1] 32.08646\n\nhist(wpmtime$wpm)\n\n\n\n\n\n\n\n\nOur histogram looks roughly normally distributed. We can (if we like), test this using the Shapiro-Wilk test.\n\nshapiro.test(wpmtime$wpm)\n\n\n    Shapiro-Wilk normality test\n\ndata:  wpmtime$wpm\nW = 0.9636, p-value = 0.1258\n\n\nThe \\(p\\)-value of 0.126 is \\(&gt;.05\\), so we fail to reject the null hypothesis that the data come from a normal distribution. In other words, we have no reason to consider our assumption to be violated.\n\n\n\n\n\n\n\n\n\nQuick and easy t.test()\n\n\n\n\n\nPaying careful attention to the research question (“Do people read more than 250 words per minute?”), our null hypothesis here is that reading time is \\(\\leq 250\\) words per minute (wpm), and our alternative hypothesis is that it is \\(&gt;250\\) wpm.\nThis means that we will reject our null hypothesis if we get a test statistic indicating the mean is \\(&gt;250\\). We won’t reject it if the mean is \\(&lt;250\\).\nWe specify the direction of the alternative in the t.test() function:\n\nt.test(wpmtime$wpm, mu = 250, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  wpmtime$wpm\nt = 1.8423, df = 49, p-value = 0.03574\nalternative hypothesis: true mean is greater than 250\n95 percent confidence interval:\n 250.7523      Inf\nsample estimates:\nmean of x \n   258.36 \n\n\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nOur test-statistic is calculated as \\[\nt =  \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\nThere’s a lot of brackets in the code below, so go through it piece by piece if you are unsure of how it matches to the formula above\n\n(mean(wpmtime$wpm) - 250 ) / (sd(wpmtime$wpm) / sqrt(nrow(wpmtime)))\n\n[1] 1.842338\n\n\nThe test we are performing is against the null hypothesis that \\(\\mu_0 \\leq 250\\). So we will only reject the null hypothesis if we get a test statistic indicating the mean is \\(&gt;250\\). This means that our p-value will be just the one tail of the \\(t\\)-distribution:\n\npt(1.842338, df = 49, lower.tail = FALSE)\n\n[1] 0.0357404\n\n\n\n\n\n\n\n\n\n\n\nWrite-up\n\n\n\n\n\n\nA one-sample t-test was conducted in order to determine if the average reading time was significantly (\\(\\alpha=.05\\)) higher than 250 words per minute (wpm).\nThe sample of 50 participants read on average at 258 words per minute (Mean=258, SD=32). This was significantly above 250 (\\(t(49)=1.84, p = .036\\), one-tailed)."
  },
  {
    "objectID": "03b_inference2.html#two-sample-t-test",
    "href": "03b_inference2.html#two-sample-t-test",
    "title": "3B: Practical Inference",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nPurpose\nThe two sample t-test is used to test whether the mean of one group is different from/greater than/less than the mean of another.\n\nExamples:\n\nIs the mean age of cat people different from the mean age of dog people?\nDo people who identify as “morning people” have a higher average rating of sleep quality than those who identify as “evening people”?\nIs the average reaction time different between people who do and don’t drink caffeinated drinks?\n\n\nAssumptions:\n\nThe data are continuous (not discrete)\nThe data are independent (i.e. the value of a datapoint does not depend on the value of another datapoint in any way)\nThe data are normally distributed for each group (can be relaxed somewhat if the sample size is “large enough” (rule-of-thumb n = 30) and the data are not strongly skewed)\nThe variance is equal across groups*.\n\n*We can relax this assumption by using an adjusted test called the “Welch \\(t\\)-test”, which calculates the standard error slightly differently, and estimates the degrees of freedom differently too. This is actually the default in R, and we change this easily in R using t.test(...., var.equal = FALSE/TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Is the average reaction time different between people who do and don’t drink caffeinated drinks?\n\nOne hundred participants were recruited and completed a simple reaction time task. They were also surveyed on whether they regularly drank caffeine in any form. The data are accessible at https://uoepsy.github.io/data/usmr_tcaff.csv.\n\ntcaff &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_tcaff.csv\")\nhead(tcaff)\n\n# A tibble: 6 × 2\n     rt caff \n  &lt;dbl&gt; &lt;chr&gt;\n1  482. yes  \n2  389. yes  \n3  484. no   \n4  601. no   \n5  409. yes  \n6  368. no   \n\n\n\n\n\n\n\n\n\n\n\nDescriptives and Assumptions\n\n\n\n\n\nFirst some quick descriptive stats. We’ll calculate the mean and standard deviation of reaction times for each group:\n\ntcaff |&gt; \n  group_by(caff) |&gt;\n  summarise(\n    m = mean(rt),\n    s = sd(rt)\n  )\n\n# A tibble: 2 × 3\n  caff      m     s\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 no     408.  88.9\n2 yes    465. 109. \n\n\nAnd we can make a plot here:\n\nggplot(tcaff, aes(x = rt)) +\n  geom_histogram() + \n  facet_wrap(~caff)\n\n\n\n\n\n\n\n\nThe data look fairly close to normally distributed for each group here. One thing to note is that the variances look like they may be different between the two groups. The caffeine drinkers’ reaction time’s have a standard deviation of 109ms, and the non-caffeine drinkers have an sd of only 89ms.\nAs before, we can (if we are so inclined) rely on specific tests of these assumptions, such as using shapiro.test() for the distribution in each group separately.\nSimilarly, the var.test() function performs a test to compare two variances (the null hypothesis of this test being that they are equal). However, it is more common to simply perform the Welch test straight away, and thus not have to worry about this assumption.\n\n\n\n\n\n\n\n\n\nQuick and easy t.test()\n\n\n\n\n\nWe can give R the two sets of data in two ways. Either by extracting the relevant entries:\n\nt.test(x = tcaff$rt[tcaff$caff==\"no\"], \n       y = tcaff$rt[tcaff$caff==\"yes\"])\n\nOr using the formula notation, with the ~ (“tilde”) symbol. In R, you can interpret y ~ x as “y is modeled as a function of x”. By splitting the numeric values (rt variable) by the categories of the caff variable, we can conduct a \\(t\\)-test using:\n\nt.test(rt ~ caff, data = tcaff)\n\n\n    Welch Two Sample t-test\n\ndata:  rt by caff\nt = -2.8497, df = 93.971, p-value = 0.005377\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -96.20205 -17.19423\nsample estimates:\n mean in group no mean in group yes \n         408.0505          464.7486 \n\n\nNote that the default behaviour of t.test() is to perform the Welch test - so we don’t have to assume equal variances. If we want to override this, we can use t.test(rt ~ caff, data = tcaff, var.equal = TRUE).\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nOur test statistic here is:4\n\\[\n\\begin{align}\n& t =  \\frac{\\bar x_1 - \\bar x_2}{SE}\\\\\n\\ \\\\\n& \\text{where:} \\\\\n& \\bar x_1 : \\text{sample mean group 1} \\\\\n& \\bar x_2 : \\text{sample mean group 2} \\\\\n& SE : \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\\\\n& s_1 : \\text{sample standard deviation of group 1} \\\\\n& s_2 : \\text{sample standard deviation of group 2} \\\\\n& n_1 : \\text{sample size group 1} \\\\\n& n_2 : \\text{sample size group 2} \\\\\n\\end{align}\n\\]\nWe can calculate each part:\n\ntcaff |&gt;\n  group_by(caff) |&gt;\n  summarise(\n    xbar = mean(rt),\n    s = sd(rt),\n    s2 = var(rt),\n    n = n()\n  )\n\n# A tibble: 2 × 5\n  caff   xbar     s     s2     n\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 no     408.  88.9  7906.    40\n2 yes    465. 109.  11892.    60\n\n\nplugging these bits in gives us: \\[\n\\begin{align}\nSE & = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} = \\sqrt{\\frac{7906}{40} + \\frac{11892}{60}} = \\sqrt{395.85} \\\\\n\\qquad \\\\\n& = 19.9\n\\end{align}\n\\] and \\[\n\\begin{align}\nt & =  \\frac{\\bar x_1 - \\bar x_2}{SE} = \\frac{408.1 - 464.8}{19.9} \\\\\n\\qquad \\\\\n& = -2.849 \\\\\n\\end{align}\n\\]\nOur \\(p\\)-value is determined against a \\(t\\)-distribution with a specific number of degrees of freedom. We are estimating two means here, the standard two-sample t-test uses \\(df = n-2\\). However, the Welch t-test, which we performed quickly with t.test(), where we don’t assume equal variances, makes the calculation of the degrees of freedom much more complicated.5\nUsing the same degrees of freedom as was used in the quick use of t.test() above, we get out our same p-value (or thereabouts - we have some rounding error):\n\n2*pt(abs(-2.849), df = 93.971, lower.tail = FALSE)\n\n[1] 0.005388563\n\n\n\n\n\n\n\n\n\n\n\nWrite-up\n\n\n\n\n\n\nA Welch two sample t-test was used to assess whether the mean reaction time of people who regularly drink caffeine (\\(n = 60\\)) was different to that of people who do not (\\(n=40\\)). There was a significant difference in average reaction time between the caffeine (Mean=465; SD=109) and non-caffeine (Mean=408; SD=89) groups (\\(t(93.97)=-2.85, p = 0.005\\), two-tailed). Therefore, we reject the null hypothesis that there is no difference in reaction times between caffeine drinkers and non-caffeine drinkers.\n\n\nCode\nggplot(tcaff, aes(x = caff, y = rt)) +\n  geom_boxplot()+\n  labs(x=\"drinks caffeine\",y=\"reaction time (ms)\")"
  },
  {
    "objectID": "03b_inference2.html#paired-sample-t-test",
    "href": "03b_inference2.html#paired-sample-t-test",
    "title": "3B: Practical Inference",
    "section": "Paired sample t-test",
    "text": "Paired sample t-test\n\nPurpose\nThe paired sample t-test is used to test whether the mean difference between two sets of paired observations is different from 0.\n\nExamples:\n\nIs the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?\n\nAre scores on test 1 different on average from scores on test 2 (with participants completing both tests).\n\n\nAssumptions:\n\nThe data are continuous (not discrete)\nThe differences are independent (i.e. the value of a the difference for one pair does not depend on the values of another pair in any way)\nThe differences are normally distributed OR the sample size is large enough (rule-of-thumb n = 30) and the data are not strongly skewed\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Is the mean cognitive score of participants at age 60 different from when they are re-tested at age 70?\n\nAddenbrooke’s Cognitive Examination-III (ACE-III) is a brief cognitive test that assesses five cognitive domains: attention, memory, verbal fluency, language and visuospatial abilities. The total score is 100 with higher scores indicating better cognitive functioning. A research project is examining changes in cognitive functioning with age, and administers the ACE-III to a set of participants at age 60, then again at age 70. The data is accessible at https://uoepsy.github.io/data/usmr_tcaff.csv.\n\nacedata &lt;- read_csv(\"https://uoepsy.github.io/data/acedata.csv\")\nhead(acedata)\n\n# A tibble: 6 × 3\n  participant ace_60 ace_70\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 sub1            93     85\n2 sub2            95     92\n3 sub3            93     90\n4 sub4            93     95\n5 sub5            96     88\n6 sub6            91     85\n\n\n\n\n\n\n\n\n\n\n\nThe paired t test is the one sample t test in disguise\n\n\n\n\n\nWe can either perform this with the data exactly as it is:\n\nt.test(x = acedata$ace_60, y = acedata$ace_70, \n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  acedata$ace_60 and acedata$ace_70\nt = 2.2542, df = 24, p-value = 0.03359\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.2093364 4.7506636\nsample estimates:\nmean difference \n           2.48 \n\n\nOr we can compute the differences, and perform a one sample test on the mean of those differences being different from 0.\nIt’s just the same result:\n\nacedata &lt;- acedata |&gt;\n  mutate(diff_score = ace_60 - ace_70)\n\nt.test(acedata$diff_score, mu = 0)\n\n\n    One Sample t-test\n\ndata:  acedata$diff_score\nt = 2.2542, df = 24, p-value = 0.03359\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.2093364 4.7506636\nsample estimates:\nmean of x \n     2.48"
  },
  {
    "objectID": "03b_inference2.html#footnotes",
    "href": "03b_inference2.html#footnotes",
    "title": "3B: Practical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that confidence intervals provide a range of plausible values for the population mean. In this case, zero is a plausible value.↩︎\nThis is because with smaller samples we have less certainty in the estimate of the population standard deviation, and our estimates of mean and standard deviation are more dependent on one another. The bottom part of \\(\\frac{\\bar x - \\mu}{SE}\\) has a greater chance of being smaller than the top part, meaning that our resulting our test statistics will tend to be slightly bigger. To better represent this greater chance of seeing bigger test statistics from small samples, our \\(t\\)-distributions have heavier tails.↩︎\nThis is because, practically speaking, what we really need in order to make useful, defensible conclusions, is not that the population itself is normally distributed, but that the sampling distribution of the statistic is close enough to the \\(t\\)-distribution. This can often be the case when we have a large sample without much skew.↩︎\nThe formula here is for the Welch test.\nFor a standard two sample t-test that assumes equal variances, we first calculate the “pooled standard deviation” - \\(s_p = \\sqrt\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\\).\nWe then use this to calculate the standard error - \\(SE_{(\\bar{x}_1 - \\bar{x}_2)} = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)↩︎\nIf you really want it, the formula is: \\(\\text{df}=\\frac{\\left(\\dfrac{s_1^2}{n_1}+\\dfrac{s_2^2}{n_2}\\right)^2}{\\dfrac{\\left(\\dfrac{s_1^2}{n_1}\\right)^2}{n_1-1}+\\dfrac{\\left(\\dfrac{s_2^2}{n_2}\\right)^2}{n_2-1}}\\)↩︎"
  },
  {
    "objectID": "04_ex.html",
    "href": "04_ex.html",
    "title": "Exercises: Chi-Square Tests",
    "section": "",
    "text": "Research Question: Are students more likely to be born in certain months than others?\n\n\nData: Past Surveys\nIn the last few years, we have asked students of the statistics courses in the Psychology department to fill out a little survey.\nAnonymised data are available at https://uoepsy.github.io/data/surveydata_historical.csv.\nNote: this does not contain the responses from this year.\n\nsurveydata &lt;- \n  read_csv(\"https://uoepsy.github.io/data/surveydata_historical.csv\")\n\n\n\nQuestion 1\n\n\nWhat is your intuition about the distribution of all students’ birth-months?\nDo you think they will be spread uniformly across all months of the year (like a fair 12-sided dice), or do you think people are more likely to be born in certain months more than others?\nPlot the distribution and get an initial idea of how things are looking.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can do this quickly with barplot() and table(), or you could create try using ggplot() and looking into geom_bar().\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe quick and dirty way to plot:\n\nbarplot(table(surveydata$birthmonth))\n\n\n\n\n\n\n\n\nA ggplot option:\n\nggplot(data = surveydata, aes(x = birthmonth)) +\n    geom_bar() +\n    labs(x = \"- Birth Month -\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nWe’re going to perform a statistical test to assess the extent to which our data conforms to the hypothesis that people are no more likely to be born on one month than another.\nUnder this hypothesis, what would be the proportional breakdown of observed births in each of the months?\n\n\n\n\n\nSolution\n\n\n\nIf people are no more likely to be born in one month than another, then we would expect the same proportion of observed births in each month.\nThere are 12 months, so we would expect \\(\\frac{1}{12}\\) observations in each month.\nWe can write these as: \\[\n\\begin{align}\n& p_{jan} = 1/12 \\\\\n& p_{feb} = 1/12 \\\\\n& ... \\\\\n& p_{dec} = 1/12 \\\\\n\\end{align}\n\\]\n\n\n\n\nQuestion 3\n\n\nHow many observations in our sample would we expect to find with a birthday in January? And in February? … and so on?\n\n\n\n\n\n\nHints\n\n\n\n\n\nHow many responses (i.e. not missing values) do we have for this question?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are 470 people who have non-NA values (sum(!is.na(surveydata$birthmonth))).\nUnder the null hypothesis, we would expect \\(\\frac{1}{12} \\times\\) 470 = 39.17 observations born in each month.\n\n\n\n\nQuestion 4\n\n\nThe code below creates counts for each month. Before doing that, it removes the rows which have an NA in them for birthmonth:\n\nsurveydata |&gt;\n  filter(!is.na(birthmonth)) |&gt;\n  group_by(birthmonth) |&gt;\n  summarise(\n      observed = n()\n  )\n\n(A shortcut for this would be surveydata |&gt; filter(!is.na(birthmonth)) |&gt; count(birthmonth))\nAdd to the code above to create columns showing:\n\nthe expected counts \\(E_i\\)\nobserved-expected (\\(O_i - E_i\\))\nthe squared differences \\((O_i - E_i)^2\\)\nthe standardised square differences \\(\\frac{(O_i - E_i)^2}{E_i}\\)\n\nThen calculate the \\(\\chi^2\\) statistic (the sum of the standardised squared differences).\nIf your observed counts matched the expected counts perfectly, what would the \\(\\chi^2\\) statistic be?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis was all done in the step-by-step example of a \\(\\chi^2\\) test in 4A #chi2-goodness-of-fit-test\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nchi_table &lt;- \n    surveydata |&gt;\n    filter(!is.na(birthmonth)) |&gt;\n    group_by(birthmonth) |&gt;\n    summarise(\n        observed = n(),\n        expected = sum(!is.na(surveydata$birthmonth))/12,\n        diff = observed-expected,\n        sq_diff = diff^2,\n        std_sq_diff = sq_diff / expected\n    )\nchi_table\n\n# A tibble: 12 × 6\n   birthmonth observed expected   diff sq_diff std_sq_diff\n   &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 apr              31     39.2 -8.17   66.7        1.70  \n 2 aug              38     39.2 -1.17    1.36       0.0348\n 3 dec              37     39.2 -2.17    4.69       0.120 \n 4 feb              31     39.2 -8.17   66.7        1.70  \n 5 jan              40     39.2  0.833   0.694      0.0177\n 6 jul              46     39.2  6.83   46.7        1.19  \n 7 jun              41     39.2  1.83    3.36       0.0858\n 8 mar              37     39.2 -2.17    4.69       0.120 \n 9 may              42     39.2  2.83    8.03       0.205 \n10 nov              43     39.2  3.83   14.7        0.375 \n11 oct              43     39.2  3.83   14.7        0.375 \n12 sep              41     39.2  1.83    3.36       0.0858\n\n\nAnd we can calculate our \\(\\chi^2\\) test statistic by simply summing the values in the last column we created:\n\nsum(chi_table$std_sq_diff)\n\n[1] 6.017021\n\n\nIf all our observed counts are equal to our expected counts, then the diff column above will be all \\(0\\), and \\(0^2=0\\), and \\(\\frac{0}{E_i}\\) will be \\(0\\). So \\(\\chi^2\\) will be \\(0\\).\n\n\n\n\nQuestion 5\n\n\nYou can see the distribution of \\(\\chi^2\\) statistics with different degrees of freedom below.\n\n\n\n\n\nFigure 1: Chi-Square Distributions\n\n\n\n\nWe can find out the proportion of the distribution which falls to either side of a given value of \\(\\chi^2\\) using pchisq(). We need to give it our calculated \\(\\chi^2\\) statistic, our degrees of freedom (df), which is equal to the number of categories minus 1. We also need to specify whether we want the proportion to the left (lower.tail=TRUE) or to the right (lower.tail=FALSE).\n\nUsing pchisq(), calculate the probability of observing a \\(\\chi^2\\) statistic as least as extreme as the one we have calculated.\n\nCheck that these results match with those provided by R’s built-in function: chisq.test(table(surveydata$birthmonth)) (the table function will ignore NAs by default, so we don’t need to do anything extra for this).\n\n\n\n\n\n\nSolution\n\n\n\n\nsum(chi_table$std_sq_diff)\n\n[1] 6.017021\n\npchisq(sum(chi_table$std_sq_diff), df = 11, lower.tail = FALSE)\n\n[1] 0.8722261\n\n\n\nchisq.test(table(surveydata$birthmonth))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(surveydata$birthmonth)\nX-squared = 6.017, df = 11, p-value = 0.8722\n\n\n\n\n\n\nQuestion 6\n\n\nWhich months of year had the highest contributions to the chi-square test statistic?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about your standardised squared deviations.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nStandardized squared deviations\nOne possible way to answer this question is to look at the individual contribution of each category to the \\(\\chi^2\\) statistic. We computed these values in an earlier question.\n\nchi_table |&gt;\n  select(birthmonth, std_sq_diff)\n\n# A tibble: 12 × 2\n   birthmonth std_sq_diff\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 apr             1.70  \n 2 aug             0.0348\n 3 dec             0.120 \n 4 feb             1.70  \n 5 jan             0.0177\n 6 jul             1.19  \n 7 jun             0.0858\n 8 mar             0.120 \n 9 may             0.205 \n10 nov             0.375 \n11 oct             0.375 \n12 sep             0.0858\n\n\nFrom the barplot we created earlier on, we can see which months make up higher/lower proportions than expected:\n\nggplot(chi_table, aes(x = birthmonth, y = observed/nrow(surveydata))) +\n  geom_col(fill = 'lightblue') +\n  geom_hline(yintercept = 1/12, color = 'red') +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nPearson residuals\nEquivalently, you could answer by looking at Pearson residuals:\n\nchisq.test(table(surveydata$birthmonth))$residuals\n\n\n       apr        aug        dec        feb        jan        jul        jun \n-1.3049279 -0.1864183 -0.3462054 -1.3049279  0.1331559  1.0918785  0.2929430 \n       mar        may        nov        oct        sep \n-0.3462054  0.4527301  0.6125172  0.6125172  0.2929430 \n\n\nThe greatest absolute values are for apr and feb, showing that for these months the deviations from expected to observed were the greatest.\n\n\n\n\n\n\n\nResearch Question: Do childrens’ favourite colours correspond to the those suggested by the internet?\n\n\nAccording to one part of the internet, 30% of children have red as their favourite colour, 20% have blue, 15% yellow, 11% purple, 9% green, and 15% prefer some other colour.\nWe collected data from 50 children aged between 2 and 5, and got them to choose one of a set of objects that were identical apart from colour. You can see the data in Table 1\n\n\n\n\n\n\nTable 1:  Colour preferences of 50 children aged between 2 and 5 \n  \n    \n    \n      colour\n      Freq\n    \n  \n  \n    blue\n10\n    green\n6\n    other\n3\n    purple\n8\n    red\n8\n    yellow\n15\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 7\n\n\nPerform a \\(\\chi^2\\) goodness of fit test to assess the extent to which our sample of children conform to this theorised distribution of colour preferences.\nNo need to do this manually - once is enough. Just go straight to using the chisq.test() function.\nHowever, we will need to get the numbers into R somehow..\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can make a table from scratch using, for example: as.table(c(1,2,3,4,5)).\nFor the test, try using chisq.test(..., p = c(?,?,?,...) ).\nWe saw the use of chisq.test() in the example goodness of fit test, 4A #chi2-goodness-of-fit-test\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLet’s get the data in:\n\nchildcols &lt;- as.table(c(10,6,3,8,8,15))\nnames(childcols) &lt;- c(\"blue\",\"green\",\"other\",\"purple\",\"red\",\"yellow\")\nchildcols\n\n  blue  green  other purple    red yellow \n    10      6      3      8      8     15 \n\n\nOur theoretical probabilities of different colours must match the order in the table which we give chisq.test(). They must also always sum to 1.\n\nchisq.test(childcols, p = c(.20,.09,.15,.11,.30,.15))\n\nWarning in chisq.test(childcols, p = c(0.2, 0.09, 0.15, 0.11, 0.3, 0.15)):\nChi-squared approximation may be incorrect\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  childcols\nX-squared = 15.103, df = 5, p-value = 0.009931\n\n\nNote, we get a warning here of “Chi-squared approximation may be incorrect”. This is because some of the expected cell counts are &lt;5.\n\nchisq.test(childcols, \n           p = c(.20,.09,.15,.11,.30,.15))$expected\n\n  blue  green  other purple    red yellow \n  10.0    4.5    7.5    5.5   15.0    7.5 \n\n\nThere are a couple of options here, but the easiest is to use the functionality of chisq.test() that allows us to compute the p-value by using a simulation (similar to the idea we saw in 2B#sampling-&-sampling-distributions), rather than by comparing it to a theoretical \\(\\chi^2\\) distribution. We can do this by using:\n\nchisq.test(childcols, p = c(.20,.09,.15,.11,.30,.15),\n           simulate.p.value = TRUE)\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 2000 replicates)\n\ndata:  childcols\nX-squared = 15.103, df = NA, p-value = 0.01249\n\n\n\n\n\n\nQuestion 8\n\n\nWhat are the observed proportions of our sample with each eyecolour?\n\n\n\n\n\n\nHints\n\n\n\n\n\nLook up the prop.table() function?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nFrom the help documentation (?prop.table()), we see that we can pass prop.table() the argument x, which needs to be a table.\n\nprop.table(childcols)*100\n\n  blue  green  other purple    red yellow \n    20     12      6     16     16     30 \n\n\n\nbarplot(prop.table(childcols)*100)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: TipJokes\n\nResearch Question: Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?\n\nA study published in the Journal of Applied Social Psychology1 investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. The waiter randomly assigned coffee-ordering customers to one of three groups. When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all.\nThe data are available at https://uoepsy.github.io/data/TipJoke.csv.\nThe dataset contains the variables:\n\nCard: None, Joke, Ad.\nTip: 1 = The customer left a tip, 0 = The customer did not leave tip.\n\n\n\nQuestion 9\n\n\nProduce a plot and a table to display the relationship between whether or not the customer left a tip, and what (if any) card they received alongside the bill.\nDon’t worry about making it all pretty. Mosaic plots in R are a bit difficult.\n\n\n\n\n\n\nHints\n\n\n\n\n\nplot(table(...)) will give you something. You can see one in the example \\(\\chi^2\\) test of independence,4A #chi2-test-of-independence.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ntipjoke &lt;- read_csv('https://uoepsy.github.io/data/TipJoke.csv')\n\ntable(tipjoke$Card, tipjoke$Tip)\n\n      \n        0  1\n  Ad   60 14\n  Joke 42 30\n  None 49 16\n\nplot(table(tipjoke$Card, tipjoke$Tip))\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nWhat would you expect the cell counts to look like if there were no relationship between what the waiter left and whether or not the customer tipped?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about what proportion of customers tipped. Then work out how many customers got each type of card. If there were no relationship, then the proportions would be the same in each group.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIn total, 60 customers tipped (14+30+16), and 151 did not. So overall, 0.28 (\\(\\frac{60}{(60+151)}\\)) of customers tip.\n74 customers got an Ad card, 72 customers got a Joke, and 65 got None. If this were independent of whether or not they left a tip, we would expect equal proportions of tippers in each group.\nSo we would expect 0.28 of each group to leave a tip.\n\n\n\n\n\n\nsome calculations\n\n\n\n\n\nYou can think about observed vs expected by looking at the two-way table along with the marginal row and column totals given:\n\n\n\n\n\n\n0\n1\n\n\n\n\n\nAd\n\n\n74\n\n\nJoke\n\n\n72\n\n\nNone\n\n\n65\n\n\n\n151\n60\n211\n\n\n\n\n\n\n\nFor a given cell of the table we can calculate the expected count as \\(\\text{row total} \\times \\frac{\\text{column total}}{\\text{samplesize}}\\):\nExpected:\n\n\n\n\n\n\n0\n1\n\n\n\n\n\nAd\n52.96\n21.04\n74\n\n\nJoke\n51.53\n20.47\n72\n\n\nNone\n46.52\n18.48\n65\n\n\n\n151.00\n60.00\n211\n\n\n\n\n\n\n\nIf you’re wondering how we do this in R.. here’s our table:\n\nt &lt;- tipjoke |&gt;\n  select(Card, Tip) |&gt; table()\nt\n\n      Tip\nCard    0  1\n  Ad   60 14\n  Joke 42 30\n  None 49 16\n\n\nHere are the row totals:\n\nrowSums(t)\n\n  Ad Joke None \n  74   72   65 \n\n\nand column totals divided by total:\n\ncolSums(t) / sum(t)\n\n        0         1 \n0.7156398 0.2843602 \n\n\nthere’s a complicated bit of code using %o% which could do this for us. You don’t need to remember %o%, it’s very rarely used):\n\ne &lt;- rowSums(t) %o% colSums(t) / sum(t)\ne\n\n            0        1\nAd   52.95735 21.04265\nJoke 51.52607 20.47393\nNone 46.51659 18.48341\n\n\nOr, alternatively, do it one by one:\n\nrowSums(t) * (colSums(t) / sum(t))[1]\n\n      Ad     Joke     None \n52.95735 51.52607 46.51659 \n\nrowSums(t) * (colSums(t) / sum(t))[2]\n\n      Ad     Joke     None \n21.04265 20.47393 18.48341 \n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\nJust like we gave the chisq.test() function a table of observed frequencies when we conducted a goodness of fit test in earlier exercises, we can give it a two-way table of observed frequencies to conduct a test of independence.\nTry it now.\n\n\n\n\n\nSolution\n\n\n\n\nchisq.test(table(tipjoke$Card, tipjoke$Tip))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(tipjoke$Card, tipjoke$Tip)\nX-squared = 9.9533, df = 2, p-value = 0.006897"
  },
  {
    "objectID": "04_ex.html#birth-months",
    "href": "04_ex.html#birth-months",
    "title": "Exercises: Chi-Square Tests",
    "section": "",
    "text": "Research Question: Are students more likely to be born in certain months than others?\n\n\nData: Past Surveys\nIn the last few years, we have asked students of the statistics courses in the Psychology department to fill out a little survey.\nAnonymised data are available at https://uoepsy.github.io/data/surveydata_historical.csv.\nNote: this does not contain the responses from this year.\n\nsurveydata &lt;- \n  read_csv(\"https://uoepsy.github.io/data/surveydata_historical.csv\")\n\n\n\nQuestion 1\n\n\nWhat is your intuition about the distribution of all students’ birth-months?\nDo you think they will be spread uniformly across all months of the year (like a fair 12-sided dice), or do you think people are more likely to be born in certain months more than others?\nPlot the distribution and get an initial idea of how things are looking.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can do this quickly with barplot() and table(), or you could create try using ggplot() and looking into geom_bar().\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe quick and dirty way to plot:\n\nbarplot(table(surveydata$birthmonth))\n\n\n\n\n\n\n\n\nA ggplot option:\n\nggplot(data = surveydata, aes(x = birthmonth)) +\n    geom_bar() +\n    labs(x = \"- Birth Month -\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nWe’re going to perform a statistical test to assess the extent to which our data conforms to the hypothesis that people are no more likely to be born on one month than another.\nUnder this hypothesis, what would be the proportional breakdown of observed births in each of the months?\n\n\n\n\n\nSolution\n\n\n\nIf people are no more likely to be born in one month than another, then we would expect the same proportion of observed births in each month.\nThere are 12 months, so we would expect \\(\\frac{1}{12}\\) observations in each month.\nWe can write these as: \\[\n\\begin{align}\n& p_{jan} = 1/12 \\\\\n& p_{feb} = 1/12 \\\\\n& ... \\\\\n& p_{dec} = 1/12 \\\\\n\\end{align}\n\\]\n\n\n\n\nQuestion 3\n\n\nHow many observations in our sample would we expect to find with a birthday in January? And in February? … and so on?\n\n\n\n\n\n\nHints\n\n\n\n\n\nHow many responses (i.e. not missing values) do we have for this question?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are 470 people who have non-NA values (sum(!is.na(surveydata$birthmonth))).\nUnder the null hypothesis, we would expect \\(\\frac{1}{12} \\times\\) 470 = 39.17 observations born in each month.\n\n\n\n\nQuestion 4\n\n\nThe code below creates counts for each month. Before doing that, it removes the rows which have an NA in them for birthmonth:\n\nsurveydata |&gt;\n  filter(!is.na(birthmonth)) |&gt;\n  group_by(birthmonth) |&gt;\n  summarise(\n      observed = n()\n  )\n\n(A shortcut for this would be surveydata |&gt; filter(!is.na(birthmonth)) |&gt; count(birthmonth))\nAdd to the code above to create columns showing:\n\nthe expected counts \\(E_i\\)\nobserved-expected (\\(O_i - E_i\\))\nthe squared differences \\((O_i - E_i)^2\\)\nthe standardised square differences \\(\\frac{(O_i - E_i)^2}{E_i}\\)\n\nThen calculate the \\(\\chi^2\\) statistic (the sum of the standardised squared differences).\nIf your observed counts matched the expected counts perfectly, what would the \\(\\chi^2\\) statistic be?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis was all done in the step-by-step example of a \\(\\chi^2\\) test in 4A #chi2-goodness-of-fit-test\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nchi_table &lt;- \n    surveydata |&gt;\n    filter(!is.na(birthmonth)) |&gt;\n    group_by(birthmonth) |&gt;\n    summarise(\n        observed = n(),\n        expected = sum(!is.na(surveydata$birthmonth))/12,\n        diff = observed-expected,\n        sq_diff = diff^2,\n        std_sq_diff = sq_diff / expected\n    )\nchi_table\n\n# A tibble: 12 × 6\n   birthmonth observed expected   diff sq_diff std_sq_diff\n   &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 apr              31     39.2 -8.17   66.7        1.70  \n 2 aug              38     39.2 -1.17    1.36       0.0348\n 3 dec              37     39.2 -2.17    4.69       0.120 \n 4 feb              31     39.2 -8.17   66.7        1.70  \n 5 jan              40     39.2  0.833   0.694      0.0177\n 6 jul              46     39.2  6.83   46.7        1.19  \n 7 jun              41     39.2  1.83    3.36       0.0858\n 8 mar              37     39.2 -2.17    4.69       0.120 \n 9 may              42     39.2  2.83    8.03       0.205 \n10 nov              43     39.2  3.83   14.7        0.375 \n11 oct              43     39.2  3.83   14.7        0.375 \n12 sep              41     39.2  1.83    3.36       0.0858\n\n\nAnd we can calculate our \\(\\chi^2\\) test statistic by simply summing the values in the last column we created:\n\nsum(chi_table$std_sq_diff)\n\n[1] 6.017021\n\n\nIf all our observed counts are equal to our expected counts, then the diff column above will be all \\(0\\), and \\(0^2=0\\), and \\(\\frac{0}{E_i}\\) will be \\(0\\). So \\(\\chi^2\\) will be \\(0\\).\n\n\n\n\nQuestion 5\n\n\nYou can see the distribution of \\(\\chi^2\\) statistics with different degrees of freedom below.\n\n\n\n\n\nFigure 1: Chi-Square Distributions\n\n\n\n\nWe can find out the proportion of the distribution which falls to either side of a given value of \\(\\chi^2\\) using pchisq(). We need to give it our calculated \\(\\chi^2\\) statistic, our degrees of freedom (df), which is equal to the number of categories minus 1. We also need to specify whether we want the proportion to the left (lower.tail=TRUE) or to the right (lower.tail=FALSE).\n\nUsing pchisq(), calculate the probability of observing a \\(\\chi^2\\) statistic as least as extreme as the one we have calculated.\n\nCheck that these results match with those provided by R’s built-in function: chisq.test(table(surveydata$birthmonth)) (the table function will ignore NAs by default, so we don’t need to do anything extra for this).\n\n\n\n\n\n\nSolution\n\n\n\n\nsum(chi_table$std_sq_diff)\n\n[1] 6.017021\n\npchisq(sum(chi_table$std_sq_diff), df = 11, lower.tail = FALSE)\n\n[1] 0.8722261\n\n\n\nchisq.test(table(surveydata$birthmonth))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(surveydata$birthmonth)\nX-squared = 6.017, df = 11, p-value = 0.8722\n\n\n\n\n\n\nQuestion 6\n\n\nWhich months of year had the highest contributions to the chi-square test statistic?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about your standardised squared deviations.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nStandardized squared deviations\nOne possible way to answer this question is to look at the individual contribution of each category to the \\(\\chi^2\\) statistic. We computed these values in an earlier question.\n\nchi_table |&gt;\n  select(birthmonth, std_sq_diff)\n\n# A tibble: 12 × 2\n   birthmonth std_sq_diff\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 apr             1.70  \n 2 aug             0.0348\n 3 dec             0.120 \n 4 feb             1.70  \n 5 jan             0.0177\n 6 jul             1.19  \n 7 jun             0.0858\n 8 mar             0.120 \n 9 may             0.205 \n10 nov             0.375 \n11 oct             0.375 \n12 sep             0.0858\n\n\nFrom the barplot we created earlier on, we can see which months make up higher/lower proportions than expected:\n\nggplot(chi_table, aes(x = birthmonth, y = observed/nrow(surveydata))) +\n  geom_col(fill = 'lightblue') +\n  geom_hline(yintercept = 1/12, color = 'red') +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nPearson residuals\nEquivalently, you could answer by looking at Pearson residuals:\n\nchisq.test(table(surveydata$birthmonth))$residuals\n\n\n       apr        aug        dec        feb        jan        jul        jun \n-1.3049279 -0.1864183 -0.3462054 -1.3049279  0.1331559  1.0918785  0.2929430 \n       mar        may        nov        oct        sep \n-0.3462054  0.4527301  0.6125172  0.6125172  0.2929430 \n\n\nThe greatest absolute values are for apr and feb, showing that for these months the deviations from expected to observed were the greatest."
  },
  {
    "objectID": "04_ex.html#childrens-favourite-colours",
    "href": "04_ex.html#childrens-favourite-colours",
    "title": "Exercises: Chi-Square Tests",
    "section": "",
    "text": "Research Question: Do childrens’ favourite colours correspond to the those suggested by the internet?\n\n\nAccording to one part of the internet, 30% of children have red as their favourite colour, 20% have blue, 15% yellow, 11% purple, 9% green, and 15% prefer some other colour.\nWe collected data from 50 children aged between 2 and 5, and got them to choose one of a set of objects that were identical apart from colour. You can see the data in Table 1\n\n\n\n\n\n\nTable 1:  Colour preferences of 50 children aged between 2 and 5 \n  \n    \n    \n      colour\n      Freq\n    \n  \n  \n    blue\n10\n    green\n6\n    other\n3\n    purple\n8\n    red\n8\n    yellow\n15\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 7\n\n\nPerform a \\(\\chi^2\\) goodness of fit test to assess the extent to which our sample of children conform to this theorised distribution of colour preferences.\nNo need to do this manually - once is enough. Just go straight to using the chisq.test() function.\nHowever, we will need to get the numbers into R somehow..\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can make a table from scratch using, for example: as.table(c(1,2,3,4,5)).\nFor the test, try using chisq.test(..., p = c(?,?,?,...) ).\nWe saw the use of chisq.test() in the example goodness of fit test, 4A #chi2-goodness-of-fit-test\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLet’s get the data in:\n\nchildcols &lt;- as.table(c(10,6,3,8,8,15))\nnames(childcols) &lt;- c(\"blue\",\"green\",\"other\",\"purple\",\"red\",\"yellow\")\nchildcols\n\n  blue  green  other purple    red yellow \n    10      6      3      8      8     15 \n\n\nOur theoretical probabilities of different colours must match the order in the table which we give chisq.test(). They must also always sum to 1.\n\nchisq.test(childcols, p = c(.20,.09,.15,.11,.30,.15))\n\nWarning in chisq.test(childcols, p = c(0.2, 0.09, 0.15, 0.11, 0.3, 0.15)):\nChi-squared approximation may be incorrect\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  childcols\nX-squared = 15.103, df = 5, p-value = 0.009931\n\n\nNote, we get a warning here of “Chi-squared approximation may be incorrect”. This is because some of the expected cell counts are &lt;5.\n\nchisq.test(childcols, \n           p = c(.20,.09,.15,.11,.30,.15))$expected\n\n  blue  green  other purple    red yellow \n  10.0    4.5    7.5    5.5   15.0    7.5 \n\n\nThere are a couple of options here, but the easiest is to use the functionality of chisq.test() that allows us to compute the p-value by using a simulation (similar to the idea we saw in 2B#sampling-&-sampling-distributions), rather than by comparing it to a theoretical \\(\\chi^2\\) distribution. We can do this by using:\n\nchisq.test(childcols, p = c(.20,.09,.15,.11,.30,.15),\n           simulate.p.value = TRUE)\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 2000 replicates)\n\ndata:  childcols\nX-squared = 15.103, df = NA, p-value = 0.01249\n\n\n\n\n\n\nQuestion 8\n\n\nWhat are the observed proportions of our sample with each eyecolour?\n\n\n\n\n\n\nHints\n\n\n\n\n\nLook up the prop.table() function?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nFrom the help documentation (?prop.table()), we see that we can pass prop.table() the argument x, which needs to be a table.\n\nprop.table(childcols)*100\n\n  blue  green  other purple    red yellow \n    20     12      6     16     16     30 \n\n\n\nbarplot(prop.table(childcols)*100)"
  },
  {
    "objectID": "04_ex.html#jokes-and-tips",
    "href": "04_ex.html#jokes-and-tips",
    "title": "Exercises: Chi-Square Tests",
    "section": "",
    "text": "Data: TipJokes\n\nResearch Question: Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?\n\nA study published in the Journal of Applied Social Psychology1 investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. The waiter randomly assigned coffee-ordering customers to one of three groups. When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all.\nThe data are available at https://uoepsy.github.io/data/TipJoke.csv.\nThe dataset contains the variables:\n\nCard: None, Joke, Ad.\nTip: 1 = The customer left a tip, 0 = The customer did not leave tip.\n\n\n\nQuestion 9\n\n\nProduce a plot and a table to display the relationship between whether or not the customer left a tip, and what (if any) card they received alongside the bill.\nDon’t worry about making it all pretty. Mosaic plots in R are a bit difficult.\n\n\n\n\n\n\nHints\n\n\n\n\n\nplot(table(...)) will give you something. You can see one in the example \\(\\chi^2\\) test of independence,4A #chi2-test-of-independence.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ntipjoke &lt;- read_csv('https://uoepsy.github.io/data/TipJoke.csv')\n\ntable(tipjoke$Card, tipjoke$Tip)\n\n      \n        0  1\n  Ad   60 14\n  Joke 42 30\n  None 49 16\n\nplot(table(tipjoke$Card, tipjoke$Tip))\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nWhat would you expect the cell counts to look like if there were no relationship between what the waiter left and whether or not the customer tipped?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about what proportion of customers tipped. Then work out how many customers got each type of card. If there were no relationship, then the proportions would be the same in each group.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIn total, 60 customers tipped (14+30+16), and 151 did not. So overall, 0.28 (\\(\\frac{60}{(60+151)}\\)) of customers tip.\n74 customers got an Ad card, 72 customers got a Joke, and 65 got None. If this were independent of whether or not they left a tip, we would expect equal proportions of tippers in each group.\nSo we would expect 0.28 of each group to leave a tip.\n\n\n\n\n\n\nsome calculations\n\n\n\n\n\nYou can think about observed vs expected by looking at the two-way table along with the marginal row and column totals given:\n\n\n\n\n\n\n0\n1\n\n\n\n\n\nAd\n\n\n74\n\n\nJoke\n\n\n72\n\n\nNone\n\n\n65\n\n\n\n151\n60\n211\n\n\n\n\n\n\n\nFor a given cell of the table we can calculate the expected count as \\(\\text{row total} \\times \\frac{\\text{column total}}{\\text{samplesize}}\\):\nExpected:\n\n\n\n\n\n\n0\n1\n\n\n\n\n\nAd\n52.96\n21.04\n74\n\n\nJoke\n51.53\n20.47\n72\n\n\nNone\n46.52\n18.48\n65\n\n\n\n151.00\n60.00\n211\n\n\n\n\n\n\n\nIf you’re wondering how we do this in R.. here’s our table:\n\nt &lt;- tipjoke |&gt;\n  select(Card, Tip) |&gt; table()\nt\n\n      Tip\nCard    0  1\n  Ad   60 14\n  Joke 42 30\n  None 49 16\n\n\nHere are the row totals:\n\nrowSums(t)\n\n  Ad Joke None \n  74   72   65 \n\n\nand column totals divided by total:\n\ncolSums(t) / sum(t)\n\n        0         1 \n0.7156398 0.2843602 \n\n\nthere’s a complicated bit of code using %o% which could do this for us. You don’t need to remember %o%, it’s very rarely used):\n\ne &lt;- rowSums(t) %o% colSums(t) / sum(t)\ne\n\n            0        1\nAd   52.95735 21.04265\nJoke 51.52607 20.47393\nNone 46.51659 18.48341\n\n\nOr, alternatively, do it one by one:\n\nrowSums(t) * (colSums(t) / sum(t))[1]\n\n      Ad     Joke     None \n52.95735 51.52607 46.51659 \n\nrowSums(t) * (colSums(t) / sum(t))[2]\n\n      Ad     Joke     None \n21.04265 20.47393 18.48341 \n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\nJust like we gave the chisq.test() function a table of observed frequencies when we conducted a goodness of fit test in earlier exercises, we can give it a two-way table of observed frequencies to conduct a test of independence.\nTry it now.\n\n\n\n\n\nSolution\n\n\n\n\nchisq.test(table(tipjoke$Card, tipjoke$Tip))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(tipjoke$Card, tipjoke$Tip)\nX-squared = 9.9533, df = 2, p-value = 0.006897"
  },
  {
    "objectID": "04_ex.html#footnotes",
    "href": "04_ex.html#footnotes",
    "title": "Exercises: Chi-Square Tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. Journal of Applied Social Psychology, 32(9), 1955-1963.↩︎"
  },
  {
    "objectID": "04a_chisq.html",
    "href": "04a_chisq.html",
    "title": "4A: Chi-Square Tests",
    "section": "",
    "text": "This reading:\n\nWhat are the basic hypothesis tests that we can conduct when we are interested in variables that have categories instead of numbers?\n\nTests of the distribution of a single categorical variable\nTests of the relationship between two categorical variables\n\n\n\nJust like we did with the various types of \\(t\\)-test, we’re going to continue with some more brief explainers of different basic statistical tests. The past few weeks have focused on tests for numeric outcome variables, where we have been concerned with the mean of that variable (e.g. whether that mean is different from some specific value, or whether it is different between two groups). We now turn to investigate tests for categorical outcome variables.\nWhen studying categorical variables, we tend to be interested in counts (or “frequencies”). These can be presented in tables. With 1 variable, we have a 1 dimensional table.\nFor example, how many people in the data are left-handed, right-handed, or ambidextrous:\n\ntimehands &lt;- read_csv(\"https://uoepsy.github.io/data/timehands.csv\")\ntable(timehands$handed)\n\n\n ambi  left right \n   30   102   870 \n\n\nand with 2 variables, we have a 2-dimensional table (also referred to as a ‘contingency table’). For instance, splitting up those left/right/ambidextrous groups across whether they prefer the morning or night:\n\ntable(timehands$handed, timehands$ampm)\n\n       \n        morning night\n  ambi        6    24\n  left       81    21\n  right     435   435\n\n\nWe can perform tests to examine things such as:\n\nhow likely we are to see our sample frequencies in a single categorical variable, if some some hypothesised null distribution were true (e.g. how likely are we to see the numbers of left/right/ambi people in our sample if in bigger population we expect a 1/3 chance of each?)\n\nhow likely we are to see our sample frequencies across two categorical variables, if these variables are independent in the population. (e.g. how likely are we to see the counts in the 2x2 table above, if being left/right/ambi has nothing to do with whether you are a morning or night person).\n\nThe test-statistics for these tests (denoted \\(\\chi^2\\), spelled chi-square, pronounced “kai-square”) are obtained by adding up the standardized squared deviations in each cell of a table of frequencies:\n\\[\n\\chi^2 = \\sum_{all\\ cells} \\frac{(\\text{Observed} - \\text{Expected})^2}{\\text{Expected}}\n\\] where:\n\n\\(\\text{Observed}\\) = observed count for a cell\n\\(\\text{Expected}\\) = expected count for a cell\n\nJust like the \\(t\\)-statistics we calculate follow \\(t\\)-distributions, the \\(\\chi^2\\)-statistics follow \\(\\chi^2\\) distributions! If you look carefully at the formula above, it can never be negative (because the value on top of the fraction is squared, and so is always positive). For a given cell of the table, if we observe exactly what we expect, then \\(\\text{(Observed - Expected)}^2\\) becomes zero. The further away the observed count is from the expected count, the larger it becomes.\nThis means that under the null hypothesis, larger values are less likely. And the shape of our \\(\\chi^2\\) distributions follow this logic. They have higher probability for small values, getting progressively less likely for large values. \\(\\chi^2\\)-distributions also have a degrees of freedom, because with more cells in a table, there are more chances for random deviations between “observed and expected” to come in, meaning we are more likely to see higher test statistics when we have more cells in the table (and therefore more degrees of freedom). You can see the distribution of \\(\\chi^2\\) statistics with different degrees of freedom in Figure 1 below.\n\n\n\n\n\nFigure 1: Chi-Square Distributions\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) Goodness of Fit Test\n\nPurpose\nThe \\(\\chi^2\\) Goodness of Fit Test is typically used to investigate whether observed sample proportions are consistent with an hypothesis about the proportional breakdown of the various categories in the population.\n\nExamples:\n\nDo 20% of the adult population suffer from some form of depression?\nAre people equally likely to be born on any of the seven days of the week?\n\nAre 25% of Smarties brown?\n\nAre 2/3 of people ‘dog people’ and 1/3 of people ‘cat people’?\n\n\nAssumptions\n\nData should be randomly sampled from the population.\nData should be at the categorical or nominal level - goodness-of-fit test is not appropriate for continuous level data.\nExpected counts should be at least 5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Have proportions of adults suffering no/mild/moderate/severe depression changed from 2019?\n\nIn 2019, it was reported that 80% of adults (18+) experienced no symptoms of depression, 12% experienced mild symptoms, 4% experienced moderate symptoms, and 4% experienced severe symptoms.\nThe dataset is accessible at https://uoepsy.github.io/data/usmr_chisqdep.csv contains data from 1000 people to whom the PHQ-9 depression scale was administered in 2022.\n\ndepdata &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_chisqdep.csv\") \nhead(depdata)\n\n# A tibble: 6 × 3\n  id    dep    fam_hist\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   \n1 ID1   severe n       \n2 ID2   mild   n       \n3 ID3   no     n       \n4 ID4   no     n       \n5 ID5   no     n       \n6 ID6   no     n       \n\n\nWe can see our table of observed counts with the table() function:\n\ntable(depdata$dep)\n\n\n    mild moderate       no   severe \n     143       34      771       52 \n\n\n\n\n\n\n\n\n\n\n\nQuick and easy chisq.test()\n\n\n\n\n\nWe can perform the \\(\\chi^2\\) test very easily, by simply passing the table to the chisq.test() function, and passing it the hypothesised proportions. If we don’t give it any, it will assume they are equal.\n\nNote: the proportions must be in the correct order as the entries in the table!\n\nThis will give us the test statistic, degrees of freedom, and the p-value:\n\n# note the order of the table is mild, moderate, no, severe. \n# so we put the proportions in that order\nchisq.test(table(depdata$dep), p = c(.12, .04, .8, .04))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(depdata$dep)\nX-squared = 9.9596, df = 3, p-value = 0.01891\n\n\nIf the distribution of no/mild/moderate/severe depression were as suggested (80%/12%/4%/4%), then the probability that we would obtain a test statistic this large (or larger) by random chance alone is .019. With an \\(\\alpha = 0.05\\), we reject the null hypothesis that the proportion of people suffering from different levels of depression are the same as those indicated previously in 2019.\n\n\\(\\chi^2\\) goodness of fit test indicated that the observed proportions of people suffering from no/mild/moderate/severe depression were significantly different (\\(\\chi^2(3)=9.96, p = .019\\)) from those expected under the distribution suggested from a 2019 study (80%/12%/4%/4%).\n\nWe can examine where the biggest deviations from the hypothesised distribution are by examining the ‘residuals’:\n\nchisq.test(table(depdata$dep), p = c(.12, .04, .8, .04))$residuals\n\n\n      mild   moderate         no     severe \n 2.0996031 -0.9486833 -1.0253048  1.8973666 \n\n\nThis matches with what we see when we look at the table of counts. With \\(n=1000\\), under our 2019 distribution, we would expect 800 to have no depression, 120 mild, 40 moderate, and 40 severe.\n\ntable(depdata$dep)\n\n\n    mild moderate       no   severe \n     143       34      771       52 \n\n\nThe difference in the moderate “observed - expected” is 6, and the difference in the “no depression” is 29. But these are not comparable, because really the 6 is a much bigger amount of the expected for that category than 29 is for the no depression category. The residuals are a way of standardising these.\nThey are calculated as: \\[\n\\text{residual} = \\frac{\\text{observed} - \\text{expected}}{\\sqrt{expected}}\n\\]\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nFirst we calculate the observed counts:\n\ndepdata |&gt; \n  count(dep)\n\n# A tibble: 4 × 2\n  dep          n\n  &lt;chr&gt;    &lt;int&gt;\n1 mild       143\n2 moderate    34\n3 no         771\n4 severe      52\n\n\nLet’s add to this the expected counts:\n\ndepdata |&gt; \n  count(dep) |&gt;\n  mutate(\n    expected = c(.12, .04, .8, .04)*1000\n  )\n\n# A tibble: 4 × 3\n  dep          n expected\n  &lt;chr&gt;    &lt;int&gt;    &lt;dbl&gt;\n1 mild       143      120\n2 moderate    34       40\n3 no         771      800\n4 severe      52       40\n\n\nHow do we measure how far the observed counts are from the expected counts under the null? If we simply subtracted the expected counts from the observed counts and then add them up, you will get 0. Instead, we will square the differences between the observed and expected counts, and then add them up.\nOne issue, however, remains to be solved. A squared difference between observed and expected counts of 100 has a different weight in these two scenarios:\nScenario 1: \\(O = 30\\) and \\(E = 20\\) leads to a squared difference \\((O - E)^2 = 10^2 = 100\\).\nScenario 2: \\(O = 3000\\) and \\(E = 2990\\) leads to a squared difference \\((O - E)^2 = 10^2 = 100\\)\nHowever, it is clear that a squared difference of 100 in Scenario 1 is much more substantial than a squared difference of 100 in Scenario 2. It is for this reason that we divide the squared differences by the the expected counts to “standardize” the squared deviation.\n\\[\n\\chi^2 = \\sum_{i} \\frac{(\\text{Observed}_i - \\text{Expected}_i)^2}{\\text{Expected}_i}\n\\]\nWe can calculate each part of the equation:\n\ndepdata |&gt; \n  count(dep) |&gt;\n  mutate(\n    expected = c(.12, .04, .8, .04)*1000,\n    sq_diff = (n - expected)^2,\n    std_sq_diff = sq_diff/expected\n  )\n\n# A tibble: 4 × 5\n  dep          n expected sq_diff std_sq_diff\n  &lt;chr&gt;    &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 mild       143      120     529        4.41\n2 moderate    34       40      36        0.9 \n3 no         771      800     841        1.05\n4 severe      52       40     144        3.6 \n\n\nThe test-statistic \\(\\chi^2\\) is obtained by adding up all the standardized squared deviations:\n\ndepdata |&gt; \n  count(dep) |&gt;\n  mutate(\n    expected = c(.12, .04, .8, .04)*1000,\n    sq_diff = (n - expected)^2,\n    std_sq_diff = sq_diff/expected\n  ) |&gt; \n  summarise(\n    chi = sum(std_sq_diff)\n  )\n\n# A tibble: 1 × 1\n    chi\n  &lt;dbl&gt;\n1  9.96\n\n\nThe p-value for a \\(\\chi^2\\) Goodness of Fit Test is computed using a \\(\\chi^2\\) distribution with \\(df = \\text{nr categories} - 1\\).\nWe calculate our p-value by using pchisq() and we have 4 levels of depression, so \\(df = 4-1 = 3\\).\n\npchisq(9.959583, df=3, lower.tail=FALSE)\n\n[1] 0.01891284\n\n\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) Test of Independence\n\nPurpose\nThe \\(\\chi^2\\) Test of Independence is used to determine whether or not there is a significant association between two categorical variables. To examine the independence of two categorical variables, we have a contingency table:\n\n\n                   Family History of Depression\nDepression Severity   n   y\n           mild      93  50\n           moderate  23  11\n           no       532 239\n           severe    37  15\n\n\n\nExamples:\n\nIs depression severity associated with having a family history of depression?\n\nAre people with blue eyes more likely to be over 6 foot tall?\n\nAre people who carry the APOE-4 gene more likely to have mild cognitive impairment?\n\n\nAssumptions\n\nTwo or more categories (groups) for each variable.\nIndependence of observations\n\nthere is no relationship between the subjects in each group\n\nLarge enough sample size, such that:\n\nexpected frequencies for each cell are at least 1\nexpected frequencies should be at least 5 for the majority (80%) of cells\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Question & Data\n\n\n\n\n\n\nResearch Question: Is severity of depression associated with having a family history of depression?\n\nThe dataset accessible at https://uoepsy.github.io/data/usmr_chisqdep.csv contains data from 1000 people to whom the PHQ-9 depression scale was administered in 2022, and for which respondents were asked a brief family history questionnaire to establish whether they had a family history of depression.\n\ndepdata &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_chisqdep.csv\")\nhead(depdata)\n\n# A tibble: 6 × 3\n  id    dep    fam_hist\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   \n1 ID1   severe n       \n2 ID2   mild   n       \n3 ID3   no     n       \n4 ID4   no     n       \n5 ID5   no     n       \n6 ID6   no     n       \n\n\nWe can create our contingency table:\n\ntable(depdata$dep, depdata$fam_hist)\n\n          \n             n   y\n  mild      93  50\n  moderate  23  11\n  no       532 239\n  severe    37  15\n\n\nAnd even create a quick and dirty visualisation of this too:\n\nplot(table(depdata$dep, depdata$fam_hist))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick and easy chisq.test()\n\n\n\n\n\nAgain, we can perform this test very easily by passing the table to the chisq.test() function. We don’t need to give it any hypothesised proportions here - it will work them out based on the null hypothesis that the two variables are independent.\n\nchisq.test(table(depdata$dep, depdata$fam_hist))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(depdata$dep, depdata$fam_hist)\nX-squared = 1.0667, df = 3, p-value = 0.7851\n\n\nIf there was no association between depression severity and having a family history of depression, then the probability that we would obtain a test statistic this large (or larger) by random chance alone is 0.79. With an \\(\\alpha=.05\\), we fail to reject the null hypothesis that there is no association between depression severity and family history of depression.\n\nA \\(\\chi^2\\) test of independence indicated no significant association between severity and family history (\\(\\chi^2(3)=1.07, p=.785\\)), suggesting that a participants’ severity of depression was not dependent on whether or not they had a family history of depression.\n\nWe can see the expected and observed counts:\n\nchisq.test(table(depdata$dep, depdata$fam_hist))$expected\n\n          \n                 n       y\n  mild      97.955  45.045\n  moderate  23.290  10.710\n  no       528.135 242.865\n  severe    35.620  16.380\n\nchisq.test(table(depdata$dep, depdata$fam_hist))$observed\n\n          \n             n   y\n  mild      93  50\n  moderate  23  11\n  no       532 239\n  severe    37  15\n\n\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nWe have our observed table:\n\ntable(depdata$dep, depdata$fam_hist)\n\n          \n             n   y\n  mild      93  50\n  moderate  23  11\n  no       532 239\n  severe    37  15\n\n\nTo work out our expected counts, we have to do something a bit tricky. Let’s look at the variables independently:\n\ntable(depdata$fam_hist)\n\n\n  n   y \n685 315 \n\ntable(depdata$dep)\n\n\n    mild moderate       no   severe \n     143       34      771       52 \n\n\nWith \\(\\frac{315}{315+685} = 0.315\\) of the sample having a family history, then if depression severity is independent of family history, we would expect that 0.315 of each severity group to have a family history of depression. For example, for the mild depression, with 143 people, we would expect \\(143 \\times 0.315 = 45.045\\) people in that group to have a family history of depression.\nFor a given cell of the table we can calculate the expected count as \\(\\text{row total} \\times \\frac{\\text{column total}}{\\text{samplesize}}\\).\nOr, quickly in R:\n\nobs &lt;- table(depdata$dep, depdata$fam_hist)\nexp &lt;- rowSums(obs) %o% colSums(obs) / sum(obs)\nexp\n\n               n       y\nmild      97.955  45.045\nmoderate  23.290  10.710\nno       528.135 242.865\nsevere    35.620  16.380\n\n\nNow that we have our table of observed counts, and our table of expected counts, we can actually fit these into our formula to calculate the test statistic:\n\nsum ( (obs - exp)^2 / exp )\n\n[1] 1.066686\n\n\nThe p-value is computed using a \\(\\chi^2\\) distribution with \\(df = (\\text{nr rows} - 1) \\times (\\text{nr columns} - 1)\\).\nWhy is this? Well, remember that the degrees of freedom is the number of values that are free to vary as we estimate parameters. In a table such as the one below, where we have 4 rows and 2 columns, the degrees of freedom is the number of cells in the table that can vary before we can simply calculate the values of the other cells (where we’re constrained by the need to sum to our row/column totals).\nWe have 4 rows, and 2 columns, so \\(df = (4-1) \\times (2-1) = 3\\).\n\npchisq(1.066686, df = 3, lower.tail=FALSE)\n\n[1] 0.7851217"
  },
  {
    "objectID": "04b_revisitnhst.html",
    "href": "04b_revisitnhst.html",
    "title": "4B: Revisiting NHST",
    "section": "",
    "text": "This reading:\n\nWhy is “statistical significance” only one part of the picture?\nIn the last couple of weeks we have performed a number of different types of statistical hypothesis test, and it is worth revisiting the general concept in order to consolidate what we’ve been doing."
  },
  {
    "objectID": "04b_revisitnhst.html#footnotes",
    "href": "04b_revisitnhst.html#footnotes",
    "title": "4B: Revisiting NHST",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe could instead make the group variable a factor and specify the order of the levels↩︎\nFor those of you who are interested in what alternative definitions there are, do a google search for “frequentist vs bayesian”. Be prepared that this will open a big can of worms!↩︎"
  },
  {
    "objectID": "05_ex.html",
    "href": "05_ex.html",
    "title": "Exercises: Covariance, Correlation & Linear Regression",
    "section": "",
    "text": "Question 0\n\n\nGo to http://guessthecorrelation.com/ and play the “guess the correlation” game for a little while to get an idea of what different strengths and directions of \\(r\\) can look like.\n\n\n\n\n\n\nData: Sleep levels and daytime functioning\nA researcher is interested in the relationship between hours slept per night and self-rated effects of sleep on daytime functioning. She recruited 50 healthy adults, and collected data on the Total Sleep Time (TST) over the course of a seven day period via sleep-tracking devices.\nAt the end of the seven day period, participants completed a Daytime Functioning (DTF) questionnaire. This involved participants rating their agreement with ten statements (see Table 1). Agreement was measured on a scale from 1-5. An overall score of daytime functioning can be calculated by:\n\nreversing the scores for items 4,5 and 6 (because those items reflect agreement with positive statements, whereas the other ones are agreement with negative statement);\nsumming the scores on each item; and\nsubtracting the sum score from 50 (the max possible score). This will make higher scores reflect better perceived daytime functioning.\n\nThe data is available at https://uoepsy.github.io/data/sleepdtf.csv.\n\n\n\n\n\n\nTable 1:  Daytime Functioning Questionnaire \n  \n    \n    \n      Item\n      Statement\n    \n  \n  \n    Item_1\nI often felt an inability to concentrate\n    Item_2\nI frequently forgot things\n    Item_3\nI found thinking clearly required a lot of effort\n    Item_4\nI often felt happy\n    Item_5\nI had lots of energy\n    Item_6\nI worked efficiently\n    Item_7\nI often felt irritable\n    Item_8\nI often felt stressed\n    Item_9\nI often felt sleepy\n    Item_10\nI often felt fatigued\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 1\n\n\nLoad the required libraries (probably just tidyverse for now), and read in the data.\nCalculate the overall daytime functioning score, following the criteria outlined above, and make this a new column in your dataset.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo reverse items 4, 5 and 6, we we need to make all the scores of 1 become 5, scores of 2 become 4, and so on… What number satisfies all of these equations: ? - 5 = 1, ? - 4 = 2, ? - 3 = 3?\nTo quickly sum across rows, you might find the rowSums() function useful (you don’t have to use it though)\nIf my items were in columns between 4 to 15:\n\ndataframe$sumscore = rowSums(dataframe[, 4:15])\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nsleepdtf &lt;- read_csv(\"https://uoepsy.github.io/data/sleepdtf.csv\")\nsummary(sleepdtf)\n\n      TST             item_1         item_2         item_3         item_4    \n Min.   : 4.900   Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00  \n 1st Qu.: 7.225   1st Qu.:1.00   1st Qu.:2.00   1st Qu.:1.25   1st Qu.:1.00  \n Median : 7.900   Median :1.00   Median :2.00   Median :2.00   Median :1.00  \n Mean   : 8.004   Mean   :1.58   Mean   :2.46   Mean   :2.38   Mean   :1.26  \n 3rd Qu.: 9.025   3rd Qu.:2.00   3rd Qu.:3.00   3rd Qu.:3.00   3rd Qu.:1.00  \n Max.   :11.200   Max.   :3.00   Max.   :5.00   Max.   :5.00   Max.   :3.00  \n     item_5         item_6         item_7         item_8        item_9    \n Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.0   Min.   :1.00  \n 1st Qu.:2.00   1st Qu.:2.00   1st Qu.:1.00   1st Qu.:2.0   1st Qu.:2.00  \n Median :2.00   Median :3.00   Median :2.00   Median :2.5   Median :3.00  \n Mean   :2.36   Mean   :2.78   Mean   :2.04   Mean   :2.5   Mean   :2.96  \n 3rd Qu.:3.00   3rd Qu.:4.00   3rd Qu.:3.00   3rd Qu.:3.0   3rd Qu.:4.00  \n Max.   :4.00   Max.   :5.00   Max.   :4.00   Max.   :4.0   Max.   :5.00  \n    item_10    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :3.00  \n Mean   :2.54  \n 3rd Qu.:3.00  \n Max.   :5.00  \n\n\nTo reverse the items, we can simply do 6 minus the score:\n\nsleepdtf &lt;- \n  sleepdtf |&gt; mutate(\n    item_4=6-item_4,\n    item_5=6-item_5,\n    item_6=6-item_6\n  ) \n\nNow we can use rowSums(), and subtract the sum scores from from 50 (the max score):\n\nsleepdtf$dtf = 50-rowSums(sleepdtf[, 2:11])\n\nAn alternative way to do this would be:\n\nsleepdtf |&gt; \n  mutate(\n    dtf = 50 - (item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10)\n  )\n\n\n\n\n\nQuestion 2\n\n\nCalculate the correlation between the total sleep time (TST) and the overall daytime functioning score calculated in the previous question.\nConduct a test to establish the probability of observing a correlation this strong in a sample of this size assuming the true correlation to be 0.\nWrite a sentence or two summarising the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can do this all with one function, see 5A #correlation-test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncor.test(sleepdtf$TST, sleepdtf$dtf)\n\n\n    Pearson's product-moment correlation\n\ndata:  sleepdtf$TST and sleepdtf$dtf\nt = 6.244, df = 48, p-value = 1.062e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4807039 0.7989417\nsample estimates:\n      cor \n0.6694741 \n\n\n\nThere was a strong positive correlation between total sleep time and self-reported daytime functioning score (\\(r\\) = 0.67, \\(t(48)\\) = 6.24, \\(p &lt; .001\\)) in the current sample. As total sleep time increased, levels of self-reported daytime functioning increased.\n\n\n\n\n\nQuestion 3 (open-ended)\n\n\nThink about this relationship in terms of causation.\n Claim: Less sleep causes poorer daytime functioning.\n Why might it be inappropriate to make the claim above based on these data alone? Think about what sort of study could provide stronger evidence for such a claim.\n\n\n\n\n\n\nThings to think about:\n\n\n\n\n\n\ncomparison groups.\n\nrandom allocation.\n\nmeasures of daytime functioning.\n\nmeasures of sleep time.\n\nother (unmeasured) explanatory variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: Education SIMD Indicators\nThe Scottish Government regularly collates data across a wide range of societal, geographic, and health indicators for every “datazone” (small area) in Scotland.\nThe dataset at https://uoepsy.github.io/data/simd20_educ.csv contains some of the education indicators (see Table 2).\n\n\n\n\n\n\nTable 2:  Education indicators from the 2020 SIMD data \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    intermediate_zone\nAreas of scotland containing populations of between 2.5k-6k household residents\n    attendance\nAverage School pupil attendance\n    attainment\nAverage attainment score of School leavers (based on Scottish Credit and Qualifications Framework (SCQF))\n    university\nProportion of 17-21 year olds entering university\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 4\n\n\nConduct a test of whether there is a correlation between school attendance and school attainment in Scotland.\nPresent and write up the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe readings have not included an example write-up for you to follow. Try to follow the logic of those for t-tests and \\(\\chi^2\\)-tests.\n\ndescribe the relevant data\nexplain what test was conducted and why\npresent the relevant statistic, degrees of freedom (if applicable), statement on p-value, etc.\nstate the conclusion.\n\nBe careful figuring out how many observations your test is conducted on. cor.test() includes only the complete observations.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nsimd &lt;- read_csv(\"https://uoepsy.github.io/data/simd20_educ.csv\")\n\nHere are the means of the two variables. We should remember that these calculations will include some observations which have missing data on the other variable.\n\nsimd |&gt; \n  summarise(\n    m_attendance = mean(attendance, na.rm = TRUE),\n    m_attainment = mean(attainment, na.rm = TRUE)\n)\n\n# A tibble: 1 × 2\n  m_attendance m_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;\n1        0.811         5.53\n\n\nInstead, to match with out analysis, we might be inclined to filter our data to complete data:\n\nsimd_comp &lt;- simd |&gt; \n  filter(!is.na(attendance) & !is.na(attainment))\n\nsimd_comp |&gt;\n  summarise(\n    m_attendance = mean(attendance),\n    m_attainment = mean(attainment),\n    sd_attendance = sd(attendance),\n    sd_attainment = sd(attainment)\n)\n\n# A tibble: 1 × 4\n  m_attendance m_attainment sd_attendance sd_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.811         5.53        0.0696         0.386\n\n\n\ncor.test(simd_comp$attendance, simd_comp$attainment)\n\n\n    Pearson's product-moment correlation\n\ndata:  simd_comp$attendance and simd_comp$attainment\nt = 51.495, df = 1242, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8066637 0.8421951\nsample estimates:\n      cor \n0.8252442 \n\n\n\nA correlation test was conducted to assess whether there is a relationship between an area’s average school attendance, and its average school attainment level. A total of 1244 geographical areas were included in the analysis, with a mean school attendance of 0.81 (SD = 0.07) and a mean school attainment score of 5.53 (SD = 0.39).\nThere was a strong positive correlation between a geographical area’s level of school attendance and its school attainment (\\(r\\) = 0.83, \\(t(1242\\) = 51.5, \\(p &lt; 0.001\\)). We therefore reject the null hypothesis that there is no correlation between an area’s school attendance and attainment. Figure 1 provides a visualisation of the relationship.\n\n\nCode\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  labs(x = \"School attendance\",\n       y = \"School attainment\")\n\n\n\n\n\nFigure 1: Positive relationship between geographical areas’ level of school attendance and school attainment\n\n\n\n\n\n\n\n\n\n\n\nOptional: some extra plotting bits\n\n\n\n\n\nSometimes we may want to highlight certain parts of a plot. We can do that using the gghighlight package, and giving it a set of conditions (like we do for filter()) in order for it to decide which points to highlight.\nYou can see an example below.\nWe have also created the title by referring to the cor() function, and ’paste’ing it together to “r =”\n\nlibrary(gghighlight)\n\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  gghighlight( (attainment&gt;6 & attendance&lt;.75) | \n               attendance &gt; .95 | \n               (attendance &gt; .82 & attainment&lt;5),\n               label_key = intermediate_zone) + \n  labs(x = \"School attendance\",\n       y = \"School attainment\",\n       title = paste0(\"r = \",\n                       round(\n                         cor(simd_comp$attendance,\n                                  simd_comp$attainment),\n                         2)\n                       ))"
  },
  {
    "objectID": "05_ex.html#sleepy-time",
    "href": "05_ex.html#sleepy-time",
    "title": "Exercises: Covariance, Correlation & Linear Regression",
    "section": "",
    "text": "Data: Sleep levels and daytime functioning\nA researcher is interested in the relationship between hours slept per night and self-rated effects of sleep on daytime functioning. She recruited 50 healthy adults, and collected data on the Total Sleep Time (TST) over the course of a seven day period via sleep-tracking devices.\nAt the end of the seven day period, participants completed a Daytime Functioning (DTF) questionnaire. This involved participants rating their agreement with ten statements (see Table 1). Agreement was measured on a scale from 1-5. An overall score of daytime functioning can be calculated by:\n\nreversing the scores for items 4,5 and 6 (because those items reflect agreement with positive statements, whereas the other ones are agreement with negative statement);\nsumming the scores on each item; and\nsubtracting the sum score from 50 (the max possible score). This will make higher scores reflect better perceived daytime functioning.\n\nThe data is available at https://uoepsy.github.io/data/sleepdtf.csv.\n\n\n\n\n\n\nTable 1:  Daytime Functioning Questionnaire \n  \n    \n    \n      Item\n      Statement\n    \n  \n  \n    Item_1\nI often felt an inability to concentrate\n    Item_2\nI frequently forgot things\n    Item_3\nI found thinking clearly required a lot of effort\n    Item_4\nI often felt happy\n    Item_5\nI had lots of energy\n    Item_6\nI worked efficiently\n    Item_7\nI often felt irritable\n    Item_8\nI often felt stressed\n    Item_9\nI often felt sleepy\n    Item_10\nI often felt fatigued\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 1\n\n\nLoad the required libraries (probably just tidyverse for now), and read in the data.\nCalculate the overall daytime functioning score, following the criteria outlined above, and make this a new column in your dataset.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo reverse items 4, 5 and 6, we we need to make all the scores of 1 become 5, scores of 2 become 4, and so on… What number satisfies all of these equations: ? - 5 = 1, ? - 4 = 2, ? - 3 = 3?\nTo quickly sum across rows, you might find the rowSums() function useful (you don’t have to use it though)\nIf my items were in columns between 4 to 15:\n\ndataframe$sumscore = rowSums(dataframe[, 4:15])\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nsleepdtf &lt;- read_csv(\"https://uoepsy.github.io/data/sleepdtf.csv\")\nsummary(sleepdtf)\n\n      TST             item_1         item_2         item_3         item_4    \n Min.   : 4.900   Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00  \n 1st Qu.: 7.225   1st Qu.:1.00   1st Qu.:2.00   1st Qu.:1.25   1st Qu.:1.00  \n Median : 7.900   Median :1.00   Median :2.00   Median :2.00   Median :1.00  \n Mean   : 8.004   Mean   :1.58   Mean   :2.46   Mean   :2.38   Mean   :1.26  \n 3rd Qu.: 9.025   3rd Qu.:2.00   3rd Qu.:3.00   3rd Qu.:3.00   3rd Qu.:1.00  \n Max.   :11.200   Max.   :3.00   Max.   :5.00   Max.   :5.00   Max.   :3.00  \n     item_5         item_6         item_7         item_8        item_9    \n Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.0   Min.   :1.00  \n 1st Qu.:2.00   1st Qu.:2.00   1st Qu.:1.00   1st Qu.:2.0   1st Qu.:2.00  \n Median :2.00   Median :3.00   Median :2.00   Median :2.5   Median :3.00  \n Mean   :2.36   Mean   :2.78   Mean   :2.04   Mean   :2.5   Mean   :2.96  \n 3rd Qu.:3.00   3rd Qu.:4.00   3rd Qu.:3.00   3rd Qu.:3.0   3rd Qu.:4.00  \n Max.   :4.00   Max.   :5.00   Max.   :4.00   Max.   :4.0   Max.   :5.00  \n    item_10    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :3.00  \n Mean   :2.54  \n 3rd Qu.:3.00  \n Max.   :5.00  \n\n\nTo reverse the items, we can simply do 6 minus the score:\n\nsleepdtf &lt;- \n  sleepdtf |&gt; mutate(\n    item_4=6-item_4,\n    item_5=6-item_5,\n    item_6=6-item_6\n  ) \n\nNow we can use rowSums(), and subtract the sum scores from from 50 (the max score):\n\nsleepdtf$dtf = 50-rowSums(sleepdtf[, 2:11])\n\nAn alternative way to do this would be:\n\nsleepdtf |&gt; \n  mutate(\n    dtf = 50 - (item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10)\n  )\n\n\n\n\n\nQuestion 2\n\n\nCalculate the correlation between the total sleep time (TST) and the overall daytime functioning score calculated in the previous question.\nConduct a test to establish the probability of observing a correlation this strong in a sample of this size assuming the true correlation to be 0.\nWrite a sentence or two summarising the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can do this all with one function, see 5A #correlation-test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncor.test(sleepdtf$TST, sleepdtf$dtf)\n\n\n    Pearson's product-moment correlation\n\ndata:  sleepdtf$TST and sleepdtf$dtf\nt = 6.244, df = 48, p-value = 1.062e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4807039 0.7989417\nsample estimates:\n      cor \n0.6694741 \n\n\n\nThere was a strong positive correlation between total sleep time and self-reported daytime functioning score (\\(r\\) = 0.67, \\(t(48)\\) = 6.24, \\(p &lt; .001\\)) in the current sample. As total sleep time increased, levels of self-reported daytime functioning increased.\n\n\n\n\n\nQuestion 3 (open-ended)\n\n\nThink about this relationship in terms of causation.\n Claim: Less sleep causes poorer daytime functioning.\n Why might it be inappropriate to make the claim above based on these data alone? Think about what sort of study could provide stronger evidence for such a claim.\n\n\n\n\n\n\nThings to think about:\n\n\n\n\n\n\ncomparison groups.\n\nrandom allocation.\n\nmeasures of daytime functioning.\n\nmeasures of sleep time.\n\nother (unmeasured) explanatory variables."
  },
  {
    "objectID": "05_ex.html#attendance-and-attainment",
    "href": "05_ex.html#attendance-and-attainment",
    "title": "Exercises: Covariance, Correlation & Linear Regression",
    "section": "",
    "text": "Data: Education SIMD Indicators\nThe Scottish Government regularly collates data across a wide range of societal, geographic, and health indicators for every “datazone” (small area) in Scotland.\nThe dataset at https://uoepsy.github.io/data/simd20_educ.csv contains some of the education indicators (see Table 2).\n\n\n\n\n\n\nTable 2:  Education indicators from the 2020 SIMD data \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    intermediate_zone\nAreas of scotland containing populations of between 2.5k-6k household residents\n    attendance\nAverage School pupil attendance\n    attainment\nAverage attainment score of School leavers (based on Scottish Credit and Qualifications Framework (SCQF))\n    university\nProportion of 17-21 year olds entering university\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 4\n\n\nConduct a test of whether there is a correlation between school attendance and school attainment in Scotland.\nPresent and write up the results.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe readings have not included an example write-up for you to follow. Try to follow the logic of those for t-tests and \\(\\chi^2\\)-tests.\n\ndescribe the relevant data\nexplain what test was conducted and why\npresent the relevant statistic, degrees of freedom (if applicable), statement on p-value, etc.\nstate the conclusion.\n\nBe careful figuring out how many observations your test is conducted on. cor.test() includes only the complete observations.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nsimd &lt;- read_csv(\"https://uoepsy.github.io/data/simd20_educ.csv\")\n\nHere are the means of the two variables. We should remember that these calculations will include some observations which have missing data on the other variable.\n\nsimd |&gt; \n  summarise(\n    m_attendance = mean(attendance, na.rm = TRUE),\n    m_attainment = mean(attainment, na.rm = TRUE)\n)\n\n# A tibble: 1 × 2\n  m_attendance m_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;\n1        0.811         5.53\n\n\nInstead, to match with out analysis, we might be inclined to filter our data to complete data:\n\nsimd_comp &lt;- simd |&gt; \n  filter(!is.na(attendance) & !is.na(attainment))\n\nsimd_comp |&gt;\n  summarise(\n    m_attendance = mean(attendance),\n    m_attainment = mean(attainment),\n    sd_attendance = sd(attendance),\n    sd_attainment = sd(attainment)\n)\n\n# A tibble: 1 × 4\n  m_attendance m_attainment sd_attendance sd_attainment\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.811         5.53        0.0696         0.386\n\n\n\ncor.test(simd_comp$attendance, simd_comp$attainment)\n\n\n    Pearson's product-moment correlation\n\ndata:  simd_comp$attendance and simd_comp$attainment\nt = 51.495, df = 1242, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8066637 0.8421951\nsample estimates:\n      cor \n0.8252442 \n\n\n\nA correlation test was conducted to assess whether there is a relationship between an area’s average school attendance, and its average school attainment level. A total of 1244 geographical areas were included in the analysis, with a mean school attendance of 0.81 (SD = 0.07) and a mean school attainment score of 5.53 (SD = 0.39).\nThere was a strong positive correlation between a geographical area’s level of school attendance and its school attainment (\\(r\\) = 0.83, \\(t(1242\\) = 51.5, \\(p &lt; 0.001\\)). We therefore reject the null hypothesis that there is no correlation between an area’s school attendance and attainment. Figure 1 provides a visualisation of the relationship.\n\n\nCode\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  labs(x = \"School attendance\",\n       y = \"School attainment\")\n\n\n\n\n\nFigure 1: Positive relationship between geographical areas’ level of school attendance and school attainment\n\n\n\n\n\n\n\n\n\n\n\nOptional: some extra plotting bits\n\n\n\n\n\nSometimes we may want to highlight certain parts of a plot. We can do that using the gghighlight package, and giving it a set of conditions (like we do for filter()) in order for it to decide which points to highlight.\nYou can see an example below.\nWe have also created the title by referring to the cor() function, and ’paste’ing it together to “r =”\n\nlibrary(gghighlight)\n\nggplot(simd_comp, aes(x=attendance, y=attainment)) + \n  geom_point() + \n  gghighlight( (attainment&gt;6 & attendance&lt;.75) | \n               attendance &gt; .95 | \n               (attendance &gt; .82 & attainment&lt;5),\n               label_key = intermediate_zone) + \n  labs(x = \"School attendance\",\n       y = \"School attainment\",\n       title = paste0(\"r = \",\n                       round(\n                         cor(simd_comp$attendance,\n                                  simd_comp$attainment),\n                         2)\n                       ))"
  },
  {
    "objectID": "05_ex.html#monkey-exploration",
    "href": "05_ex.html#monkey-exploration",
    "title": "Exercises: Covariance, Correlation & Linear Regression",
    "section": "Monkey Exploration",
    "text": "Monkey Exploration\n\nData: monkeyexplorers.csv\nLiu, Hajnosz & Li (2023)1 have conducted a study on monkeys! They were interested in whether younger monkeys tend to be more inquisitive about new things than older monkeys do. They sampled 108 monkeys ranging from 1 to 24 years old. Each monkey was given a novel object, the researchers recorded the time (in minutes) that each monkey spent exploring the object.\nFor this week, we’re going to be investigating the research question:\n\nDo older monkeys spend more/less time exploring novel objects?\n\nThe data is available at https://uoepsy.github.io/data/monkeyexplorers.csv and contains the variables described in Table 3\n\n\n\n\n\n\nTable 3:  Data dictionary for monkeyexplorers.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    name\nMonkey Name\n    age\nAge of monkey in years\n    exploration_time\nTime (in minutes) spent exploring the object\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 5\n\n\nFor this week, we’re going to be investigating the following research question:\n\nDo older monkeys spend more/less time exploring novel objects?\n\nRead in the data to your R session, then visualise and describe the marginal distributions of those variables which are of interest to us. These are the distribution of each variable (time spent exploring, and monkey age) without reference to the values of the other variables.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nYou could use, for example, geom_density() for a density plot or geom_histogram() for a histogram.\nLook at the shape, center and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\nDo you notice any extreme observations?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmonkeyexp &lt;- read_csv(\"https://uoepsy.github.io/data/monkeyexplorers.csv\")\nhead(monkeyexp)\n\n# A tibble: 6 × 3\n  name            age exploration_time\n  &lt;chr&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 Ed Sheeran        8             13.1\n2 Vince Gill       17              6.8\n3 Jewel            13             12.8\n4 Jaden Smith      18              8.9\n5 Reba McEntire    16              1.1\n6 Metallica        13              5.7\n\n\nWe can plot the marginal distribution of these two continuous variables as density curves, and add a boxplot underneath to check for the presence of outliers. The width of the geom_boxplot() is always quite wide, so I want to make it narrower so that we can see it at the same time as the density plot. Deciding on the exact value for the width here is just trial and error:\n\nlibrary(patchwork)\n# the patchwork library allows us to combine plots together\nggplot(data = monkeyexp, aes(x = age)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Age (in years)\", \n       y = \"Probability density\") +\n\nggplot(data = monkeyexp, aes(x = exploration_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/175) +\n  labs(x = \"Time spent exploring a\\n novel object (in minutes)\", \n       y = \"Probability density\")\n\n\n\n\nFigure 2: Density plot and boxplot of monkey’s age and their time spent exploring novel objects\n\n\n\n\nThe plots suggests that the distributions of monkeys’ ages and the time they spend exploring novel objects are both unimodal. Most of the monkeys are between roughly 8 and 18 years old, and most of them spent between 7 and 12 minutes exploring the objects. The boxplots suggest an outlier in the distribution of exploration-times, with one monkeys spending more than \\(1.5 \\times IQR\\) beyond the 3rd quartile.\nTo further summarize a distribution, it is typical to compute and report numerical summary statistics such as the mean and standard deviation.\nAs we have seen, in earlier weeks, one way to compute these values is to use the summarise()/summarize() function from the tidyverse library:\n\nmonkeyexp |&gt; \n  summarize(\n    mean_age = mean(age), \n    sd_age = sd(age),\n    mean_exptime = mean(exploration_time),\n    sd_exptime = sd(exploration_time)\n    )\n\n# A tibble: 1 × 4\n  mean_age sd_age mean_exptime sd_exptime\n     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1     13.5   5.92         9.40       4.50\n\n\n\nThe marginal distribution of age is unimodal with a mean of 13.5 years, and a standard deviation of 5.9.\nThe marginal distribution of time-spent-exploring is unimodal with a mean of 9.4 years, and a standard deviation of 4.5.\n\n\n\n\n\nQuestion 6\n\n\nAfter we’ve looked at the marginal distributions of the variables of interest in the analysis, we typically move on to examining relationships between the variables.\nVisualise and describe the relationship between age and exploration-time among the monkeys in the sample.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about:\n\nDirection of association\nForm of association (can it be summarised well with a straight line?)\n\nStrength of association (how closely do points fall to a recognizable pattern such as a line?)\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nBecause we are investigating how time-spent-exploring varies with monkeys’ ages, the exploration-time here is the dependent variable (on the y-axis), and age is the independent variable (on the x-axis).\n\nggplot(data = monkeyexp, aes(x = age, y = exploration_time)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Age (in years)\", \n       y = \"Time spent exploring a\\n novel object (in minutes)\")\n\n\n\n\nFigure 3: The relationship between monkeys’ age and time-spent-exploring.\n\n\n\n\nThere appears to be a moderate negative linear relationship between age and exploration time in these monkeys. Older monkeys appear to spend less time, on average, exploring a novel object. The scatterplot does highlight that there is one one young monkey who is behaving a bit weirdly, and spent quite a long time exploring the object!\nTo comment numerically on the strength of the linear association we might compute the correlation coefficient that we were introduced to in 5A: Covariance & Correlation\n\nmonkeyexp |&gt;\n  select(age, exploration_time) |&gt;\n  cor()\n\n                        age exploration_time\nage               1.0000000       -0.2885495\nexploration_time -0.2885495        1.0000000\n\n\nthat is, \\(r_{\\text{age, exploration-time}} = -0.29\\)\n\n\n\n\nQuestion 7\n\n\nUsing the lm() function, fit a linear model to the sample data, in which time that monkeys spend exploring novel objects is explained by age. Assign it to a name to store it in your environment.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou can see how to fit linear models in R using lm() in 5B #fitting-linear-models-in-r\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAs the variables are in the monkeyexp dataframe, we would write:\n\nmodel1 &lt;- lm(exploration_time ~ 1 + age, data = monkeyexp)\n\n\n\n\n\nQuestion 8\n\n\nInterpret the estimated intercept and slope in the context of the question of interest.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe saw how to extract lots of information on our model using summary() (see 5B #model-summary), but there are lots of other functions too.\nIf we called our linear model object “model1” in the environment, then we can use:\n\ntype model1, i.e. simply invoke the name of the fitted model;\ntype model1$coefficients;\nuse the coef(model1) function;\nuse the coefficients(model1) function;\nuse the summary(model1)$coefficients to extract just that part of the summary.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncoef(model1)\n\n(Intercept)         age \n   12.36253    -0.21897 \n\n\nFrom this, we get that the fitted line is: \\[\n\\widehat{\\text{ExplorationTime}} = 12.36 - 0.22 \\cdot \\text{Age} \\\\\n\\]\nWe can interpret the estimated intercept as:\n\nThe estimated average time spent exploring novel objects associated with age of zero is 12.36 minutes.\n\nFor the estimated slope we get:\n\nThe estimated decrease in average time spent exploring associated with a one year increase in age is -0.22 minutes (or -13 seconds).\n\n\n\n\n\nQuestion 9\n\n\nTest the hypothesis that the population slope is zero — that is, that there is no linear association between exploration time and age in the population.\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou don’t need to do anything for this, you can find all the necessary information in summary() of your model.\nSee 5B #inference-for-regression-coefficients.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe information is already contained in the row corresponding to the variable “age” in the output of summary(), which reports the t-statistic under t value and the p-value under Pr(&gt;|t|):\n\nsummary(model1)\n\n\nCall:\nlm(formula = exploration_time ~ 1 + age, data = monkeyexp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3056  -2.3461   0.3841   2.1936  21.8323 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.36253    1.04263  11.857  &lt; 2e-16 ***\nage         -0.21897    0.07057  -3.103  0.00246 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.324 on 106 degrees of freedom\nMultiple R-squared:  0.08326,   Adjusted R-squared:  0.07461 \nF-statistic: 9.627 on 1 and 106 DF,  p-value: 0.002458\n\n\nRecall that very small p-values such as the one for the intercept 2e-16 in the Pr(&gt;|t|) column simply means \\(2 \\times 10^{-16}\\), or 0.0000000000000002. Conventions such as the APA guidelines give rules on how to report these numbers (see, e.g. APA’s number and stats guide). For the p-values in this summary (the one for the intercept and the one for the slope) we could report them as “&lt;.001” and “0.003” respectively.\n\nA significant association was found between age (in years) and time spent exploring novel objects, with exploration time decreasing by on average -0.22 minutes (or -13 seconds) for every additional year of age (\\(b = -0.22\\), \\(SE = 0.071\\), \\(t(106)=-3.103\\), \\(p=.003\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nCreate a visualisation of the estimated association between age and exploration time.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThere are lots of ways to do this. Check 5B #example, which shows an example of using the sjPlot package to create a plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(sjPlot)\nplot_model(model1, type=\"eff\", show.data = TRUE)\n\n$age\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: Question 11A\n\n\nConsider the following:\n\nIn fitting a linear regression model, we make the assumption that the errors around the line are normally distributed around zero (this is the \\(\\epsilon \\sim N(0, \\sigma)\\) bit.)\n\nAbout 95% of values from a normal distribution fall within two standard deviations of the centre.\n\nWe can obtain the estimated standard deviation of the errors (\\(\\hat \\sigma\\)) from the fitted model using sigma() and giving it the name of our model.\nWhat does this tell us?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee 5B #the-error.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe estimated standard deviation of the errors can be obtained by:\n\nsigma(model1)\n\n[1] 4.324399\n\n\n\nFor any particular age, the time monkeys spend exploring novel objects should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 4.32\\). Since \\(2 \\hat \\sigma = 2 (4.32) = 8.64\\), we expect most (about 95%) of the monkeys’ exploration times to be within about 8.6 minutes from the regression line.\n\n\n\n\n\nOptional: Question 11B\n\n\nCompute the model-predicted exploration-time for a monkey that is 1 year old.\n\n\n\n\n\n\nHints\n\n\n\n\n\nGiven that you know the intercept and slope, you can calculate this algebraically. However, try to also use the predict() function (see 5B #model-predictions).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nUsing predict(), we need to give it our model, plus some new data which contains a monkey that has 1 in the age column. First we make a new dataframe with an age variable, with one entry which has the value 1, and then we give that to predict():\n\nage_query &lt;- data.frame(age = c(1))\npredict(model1, newdata = age_query)\n\n       1 \n12.14356 \n\n\nGiven that our fitted model takes the form:\n\\[\n\\widehat{\\text{ExplorationTime}} = 12.36 - 0.22 \\cdot \\text{Age}\n\\]\nWe are asking what the predicted exploration time is for a monkey with 1 year of age. So we can substitute in “1” for the Age variable: \\[\n\\begin{align}\n\\text{ExplorationTime} &= 12.36 - 0.22 \\cdot \\text{Age} \\\\\n\\text{ExplorationTime} &= 12.36 - 0.22 \\cdot 1 \\\\\n\\text{ExplorationTime} &= 12.36 - 0.22\\\\\n\\text{ExplorationTime} &= 12.14\\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\nInfluential Monkeys\n\nQuestion 12\n\n\nTake a look at the assumption plots (see 5B #assumptions) for your model.\n\nThe trick to looking at assumption plots in linear regression is to look for “things that don’t look random”.\nAs well as looking for patterns, these plots can also higlight individual datapoints that might be skewing the results. Can you figure out if there are any unusual monkeys in our dataset? Can you re-fit the model without that monkey? When you do so, do your conclusions change?\n\n\n\n\n\n\nSolution\n\n\n\n\nplot(model1)\n\n\n\n\n\n\n\n\nFrom these plots, we can see that the 57th observation is looking a bit influential. It is the one datapoint that is looking weird in all of the plots.\nLet’s look at them:\n\nmonkeyexp[57, ]\n\n# A tibble: 1 × 3\n  name       age exploration_time\n  &lt;chr&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n1 Lil Fizz     5             33.1\n\n\nThis is a young monkey (5 years old) called “Lil Fizz”, who seems to have spent quite a long time exploring the toy. It’s important to remember that this monkey is a valuable datapoint, despite being a bit different from the general pattern.\nHowever, it would be nice to know how much Lil Fizz is affecting our conclusions, so let’s re-fit the model on everybody except that one monkey\n\nmodel1a &lt;- lm(exploration_time ~ age, data = monkeyexp[-57, ])\nsummary(model1a)\n\n\nCall:\nlm(formula = exploration_time ~ age, data = monkeyexp[-57, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.558  -2.321   0.427   2.386   9.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.46135    0.92155  12.437  &lt; 2e-16 ***\nage         -0.16781    0.06212  -2.701  0.00805 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.769 on 105 degrees of freedom\nMultiple R-squared:  0.06498,   Adjusted R-squared:  0.05608 \nF-statistic: 7.297 on 1 and 105 DF,  p-value: 0.008053\n\n\nOur conclusions haven’t changed - we still have a significant association.\nWhat we have just done is called a “sensitivity analysis” - we’ve asked if our analyses are sensitive to a specific decision we could make (whether or not we include/exclude this monkey).\n\n\nCode\nggplot(monkeyexp, aes(x=age,y=exploration_time))+\n  geom_point()+\n  geom_smooth(method=lm, fullrange=TRUE)+\n  ylim(0,34) + \n  labs(title=\"With monkey 57\") +\n\nggplot(monkeyexp[-57,], aes(x=age,y=exploration_time))+\n  geom_point()+\n  geom_smooth(method=lm, fullrange=TRUE)+\n  ylim(0,34) + \n  labs(title=\"Without monkey 57\")\n\n\n\n\n\n\n\n\n\nWe now have a decision to make. Do we continue with the monkey removed, or do we keep them in? There’s not really a right answer here, but it’s worth noting a practical issue - our assumption plots look considerably better for our model without this monkey.\nWhatever we do, when writing up the analysis we need to mention clearly if and why we exclude any observations, and how that decision has/hasn’t influenced our conclusions.\n\n\n\n\n\nMonkey Exploration in Adulthood\nLet’s suppose instead of having measured monkeys’ ages in years, researchers simply recorded whether each monkey was an adult or a juvenile (both Capuchins and Rhesus Macaques reach adulthood at 8 years old).\nThe code below creates a this new variable for us:\n\nmonkeyexp &lt;- monkeyexp |&gt; \n  mutate(\n    isAdult = ifelse(age &gt;= 8, \"yes\",\"no\")\n  )\n\n\nQuestion 13\n\n\nFit the following model, and interpret the coefficients.\n\\[\n\\text{ExplorationTime} = b_0 + b_1 \\cdot \\text{isAdult} + \\varepsilon\n\\]\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor help interpreting the coefficients, see 5B #binary-predictors.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel2 &lt;- lm(exploration_time ~ 1 + isAdult, data = monkeyexp)\nsummary(model2)\n\n\nCall:\nlm(formula = exploration_time ~ 1 + isAdult, data = monkeyexp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2556  -2.3694  -0.0444   2.5556  21.4444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   11.656      1.037  11.239   &lt;2e-16 ***\nisAdultyes    -2.711      1.136  -2.386   0.0188 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.4 on 106 degrees of freedom\nMultiple R-squared:  0.05099,   Adjusted R-squared:  0.04204 \nF-statistic: 5.695 on 1 and 106 DF,  p-value: 0.01878\n\n\n\n(Intercept) = the estimated exploration time of juvenile monkeys (11.7 minutes)\nisAdultyes = the estimated change in exploration time from juvenile monkeys to adult monkeys (-2.7 minutes)\n\n\nggplot(monkeyexp, aes(x = isAdult, y = exploration_time)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 14\n\n\nWe’ve actually already seen a way to analyse questions of this sort (“is the average exploration-time different between juveniles and adults?”)\nRun the following t-test, and consider the statistic, p value etc. How does it compare to the model in the previous question?\n\nt.test(exploration_time ~ isAdult, data = monkeyexp, var.equal = TRUE)\n\n\n\n\n\n\nSolution\n\n\n\n\nt.test(exploration_time ~ isAdult, data = monkeyexp, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  exploration_time by isAdult\nt = 2.3865, df = 106, p-value = 0.01878\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n 0.4588049 4.9634174\nsample estimates:\n mean in group no mean in group yes \n        11.655556          8.944444 \n\n\nIt is identical! the \\(t\\)-statistics are the same, the p-values are the same, the degrees of freedom. Everything!\nThe two sample t-test is actually just a special case of the linear model, where we have a numeric outcome variable and a binary predictor!\nAnd… the one-sample t-test is the linear model without any predictors, so just with an intercept.\n\nt.test(monkeyexp$exploration_time, mu = 0)\n\n\n    One Sample t-test\n\ndata:  monkeyexp$exploration_time\nt = 21.722, df = 107, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  8.538785 10.253807\nsample estimates:\nmean of x \n 9.396296 \n\ninterceptonly_model &lt;- lm(exploration_time ~ 1, data = monkeyexp)\n\nsummary(interceptonly_model)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 9.396296  0.4325657 21.72224 5.060938e-41"
  },
  {
    "objectID": "05_ex.html#footnotes",
    "href": "05_ex.html#footnotes",
    "title": "Exercises: Covariance, Correlation & Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot a real study!↩︎"
  },
  {
    "objectID": "05a_covcor.html",
    "href": "05a_covcor.html",
    "title": "5A: Covariance and Correlation",
    "section": "",
    "text": "This reading:\n\nHow can we describe the relationship between two continuous variables?\n\nHow can we test the relationship between two continuous variables?\n\n\nIn the last couple of weeks we have covered a range of the basic statistical tests that can be conducted when we have a single outcome variable, and sometimes also a single explanatory variable. Our outcome variables have been both continuous (\\(t\\)-tests) and categorical (\\(\\chi^2\\)-tests). We’re going to look at one more relationship now, which is that between two continuous variables. This will also provide us with our starting point for the second block of the course.\n\n\n\n\n\noutcome\nexplanatory\ntest\nexamines\n\n\n\n\ncontinuous\n\nt.test(y, mu = ?)\nis the mean of y different from [specified value]?\n\n\ncontinuous\nbinary\nt.test(y ~ x)\nis the mean of y different between the two groups?\n\n\ncategorical\n\nchisq.test(table(y), prob = c(?,...,?))\nis the distribution of categories of y different from [specified proportions]?\n\n\ncategorical\ncategorical\nchisq.test(table(y,x))\nis the distribution of categories of y dependent on the category of x?\n\n\ncontinuous\ncontinuous\n???\n???\n\n\n\n\n\n\n\n\n\nResearch Question: Is there a correlation between accuracy and self-perceived confidence of memory recall?\n\nOur data for this walkthrough is from a (hypothetical) study on memory. Twenty participants studied passages of text (c500 words long), and were tested a week later. The testing phase presented participants with 100 statements about the text. They had to answer whether each statement was true or false, as well as rate their confidence in each answer (on a sliding scale from 0 to 100). The dataset contains, for each participant, the percentage of items correctly answered, and the average confidence rating. Participants’ ages were also recorded.\nThe data are available at “https://uoepsy.github.io/data/recalldata.csv.\n\nLet’s take a look visually at the relationships between the percentage of items answered correctly (recall_accuracy) and participants’ average self-rating of confidence in their answers (recall_confidence). Let’s also look at the relationship between accuracy and age.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nrecalldata &lt;- read_csv('https://uoepsy.github.io/data/recalldata.csv')\n\nggplot(recalldata, aes(x=recall_confidence, recall_accuracy))+\n  geom_point() + \nggplot(recalldata, aes(x=age, recall_accuracy))+\n  geom_point()\n\n\n\n\n\n\n\n\nThese two relationships look quite different.\n\nFor participants who tended to be more confident in their answers, the percentage of items they correctly answered tends to be higher.\n\nThe older participants were, the lower the percentage of items they correctly answered tended to be.\n\nWhich relationship should we be more confident in and why?\nIdeally, we would have some means of quantifying the strength and direction of these sorts of relationship. This is where we come to the two summary statistics which we can use to talk about the association between two numeric variables: Covariance and Correlation.\n\nCovariance\nCovariance is the measure of how two variables vary together. It is the change in one variable associated with the change in another variable.\nFor samples, covariance is calculated using the following formula:\n\\[\\mathrm{cov}(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})\\]\nwhere:\n\n\\(x\\) and \\(y\\) are two variables; e.g., age and recall_accuracy;\n\\(i\\) denotes the observational unit, such that \\(x_i\\) is value that the \\(x\\) variable takes on the \\(i\\)th observational unit, and similarly for \\(y_i\\);\n\\(n\\) is the sample size.\n\n\nCovariance in R\nWe can calculate covariance in R using the cov() function.\ncov() can take two variables cov(x = , y = ).\n\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 118.0768\n\n\nIf necessary, we can choose to remove NAs using na.rm = TRUE inside the cov() function.\n\n\n\n\n\n\n\nCovariance explained visually\n\n\n\n\n\nConsider the following scatterplot:\n\n\n\n\n\n\n\n\n\n Now let’s superimpose a vertical dashed line at the mean of \\(x\\) (\\(\\bar{x}\\)) and a horizontal dashed line at the mean of \\(y\\) (\\(\\bar{y}\\)):\n\n\n\n\n\n\n\n\n\n Now let’s pick one of the points, call it \\(x_i\\), and show \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\).\n Notice that this makes a rectangle.\n As \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\) are both positive values, their product - \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) - is positive.\n\n\n\n\n\n\n\n\n\n In fact, for all these points in red, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is positive (remember that a negative multiplied by a negative gives a positive):\n\n\n\n\n\n\n\n\n\n And for these points in blue, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is negative:\n\n\n\n\n\n\n\n\n\n Now take another look at the formula for covariance:\n\\[\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}\\]\nIt is the sum of all these products divided by \\(n-1\\). It is the average of the products! Sort of like the average area of the rectangles!\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\n\nCreate 2 new columns in the memory recall data, one of which is the mean recall accuracy, and one which is the mean recall confidence.\n\n\nrecalldata &lt;-\n  recalldata |&gt; mutate(\n    maccuracy = mean(recall_accuracy),\n    mconfidence = mean(recall_confidence)\n  )\n\n\nNow create three new columns which are:\n\nrecall accuracy minus the mean recall accuracy - this is the \\((x_i - \\bar{x})\\) part.\n\nconfidence minus the mean confidence - and this is the \\((y_i - \\bar{y})\\) part.\n\nthe product of i. and ii. - this is calculating \\((x_i - \\bar{x})\\)\\((y_i - \\bar{y})\\).\n\n\n\nrecalldata &lt;- \n  recalldata |&gt; \n    mutate(\n      acc_minus_mean_acc = recall_accuracy - maccuracy,\n      conf_minus_mean_conf = recall_confidence - mconfidence,\n      prod_acc_conf = acc_minus_mean_acc * conf_minus_mean_conf\n    )\n\nrecalldata\n\n# A tibble: 20 × 9\n   ppt    recall_accuracy recall_confidence   age maccuracy mconfidence\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 ppt_1               72              66.6    72      69.2        55.4\n 2 ppt_2               66              47.1    35      69.2        55.4\n 3 ppt_3               47              43.8    48      69.2        55.4\n 4 ppt_4               84              58.9    52      69.2        55.4\n 5 ppt_5               84              75.1    46      69.2        55.4\n 6 ppt_6               58              53.5    41      69.2        55.4\n 7 ppt_7               52              48.5    86      69.2        55.4\n 8 ppt_8               76              67.1    58      69.2        55.4\n 9 ppt_9               41              40.4    59      69.2        55.4\n10 ppt_10              67              46.8    22      69.2        55.4\n11 ppt_11              60              50.6    62      69.2        55.4\n12 ppt_12              67              28.7    40      69.2        55.4\n13 ppt_13              76              69.0    47      69.2        55.4\n14 ppt_14              93              67.9    51      69.2        55.4\n15 ppt_15              71              54.5    34      69.2        55.4\n16 ppt_16              71              64.6    37      69.2        55.4\n17 ppt_17              99              66.3    37      69.2        55.4\n18 ppt_18              66              49.0    51      69.2        55.4\n19 ppt_19              77              58.5    41      69.2        55.4\n20 ppt_20              58              51.4    57      69.2        55.4\n# ℹ 3 more variables: acc_minus_mean_acc &lt;dbl&gt;, conf_minus_mean_conf &lt;dbl&gt;,\n#   prod_acc_conf &lt;dbl&gt;\n\n\n\nFinally, sum the products, and divide by \\(n-1\\)\n\n\nrecalldata |&gt;\n  summarise(\n    prod_sum = sum(prod_acc_conf),\n    n = n()\n  )\n\n# A tibble: 1 × 2\n  prod_sum     n\n     &lt;dbl&gt; &lt;int&gt;\n1    2243.    20\n\n2243.46 / (20-1)\n\n[1] 118.0768\n\n\nWhich is the same result as using cov():\n\ncov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 118.0768\n\n\n\n\n\n\n\nCorrelation\nYou can think of correlation as a standardised covariance. It has a scale from negative one to one, on which the distance from zero indicates the strength of the relationship.\nJust like covariance, positive/negative values reflect the nature of the relationship.\nThe correlation coefficient is a standardised number which quantifies the strength and direction of the linear relationship between two variables. In a population it is denoted by \\(\\rho\\), and in a sample it is denoted by \\(r\\).\nWe can calculate \\(r\\) using the following formula:\n\\[\nr_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\n\\]\nWe can actually rearrange this formula to show that the correlation is simply the covariance, but with the values \\((x_i - \\bar{x})\\) divided by the standard deviation (\\(s_x\\)), and the values \\((y_i - \\bar{y})\\) divided by \\(s_y\\): \\[\nr_{(x,y)}=\\frac{1}{n-1} \\sum_{i=1}^n \\left( \\frac{x_{i}-\\bar{x}}{s_x} \\right) \\left( \\frac{y_{i}-\\bar{y}}{s_y} \\right)\n\\]  The correlation is the simply the covariance of standardised variables (variables expressed as the distance in standard deviations from the mean).\n\nProperties of correlation coefficients\n\n\\(-1 \\leq r \\leq 1\\)\nThe sign indicates the direction of association\n\npositive association (\\(r &gt; 0\\)) means that values of one variable tend to be higher when values of the other variable are higher\nnegative association (\\(r &lt; 0\\)) means that values of one variable tend to be lower when values of the other variable are higher\nno linear association (\\(r \\approx 0\\)) means that higher/lower values of one variable do not tend to occur with higher/lower values of the other variable\n\nThe closer \\(r\\) is to \\(\\pm 1\\), the stronger the linear association\n\\(r\\) has no units and does not depend on the units of measurement\nThe correlation between \\(x\\) and \\(y\\) is the same as the correlation between \\(y\\) and \\(x\\)\n\n\n\nCorrelation in R\nJust like R has a cov() function for calculating covariance, there is a cor() function for calculating correlation:\n\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 0.6993654\n\n\nIf necessary, we can choose to remove NAs using na.rm = TRUE inside the cor() function.\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nWe calculated above that \\(\\text{cov}(\\text{recall-accuracy}, \\text{recall-confidence})\\) = 118.077.\nTo calculate the correlation, we can simply divide this by the standard deviations of the two variables \\(s_{\\text{recall-accuracy}} \\times s_{\\text{recall-confidence}}\\)\n\nrecalldata |&gt; summarise(\n  s_ra = sd(recall_accuracy),\n  s_rc = sd(recall_confidence)\n)\n\n# A tibble: 1 × 2\n   s_ra  s_rc\n  &lt;dbl&gt; &lt;dbl&gt;\n1  14.5  11.6\n\n118.08 / (14.527 * 11.622)\n\n[1] 0.6993902\n\n\nWhich is the same result as using cor():\n\ncor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)\n\n[1] 0.6993654\n\n\n\n\n\n\n\n\n\n\nCorrelation Tests\nNow that we’ve seen the formulae for covariance and correlation, as well as how to quickly calculate them in R using cov() and cor(), we can use a statistical test to establish the probability of finding an association this strong by chance alone.\n\nHypotheses:\nThe hypotheses of the correlation test are, as always, statements about the population parameter (in this case the correlation between the two variables in the population - i.e., \\(\\rho\\)).\nIf we are conducting a two tailed test, then\n\n\\(H_0: \\rho = 0\\). There is not a linear relationship between \\(x\\) and \\(y\\) in the population.\n\n\\(H_1: \\rho \\neq 0\\) There is a linear relationship between \\(x\\) and \\(y\\).\n\nIf we instead conduct a one-tailed test, then we are testing either\n\n\\(H_0: \\rho \\leq 0\\) There is a negative or no linear relationship between \\(x\\) and \\(y\\)\nvs\n\\(H_1: \\rho &gt; 0\\) There is a positive linear relationship between \\(x\\) and \\(y\\).\n\\(H_0: \\rho \\geq 0\\) There is a positive or no linear relationship between \\(x\\) and \\(y\\)\nvs\n\\(H_1: \\rho &lt; 0\\) There is a negative linear relationship between \\(x\\) and \\(y\\).\n\nTest Statistic\nThe test statistic for this test here is another \\(t\\) statistic, the formula for which depends on both the observed correlation (\\(r\\)) and the sample size (\\(n\\)):\n\\[t = r \\sqrt{\\frac{n-2}{1-r^2}}\\]\np-value\nWe calculate the p-value for our \\(t\\)-statistic as the long-run probability of a \\(t\\)-statistic with \\(n-2\\) degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed \\(t\\)-statistic.\nAssumptions\nFor a test of Pearson’s correlation coefficient \\(r\\), we need to make sure a few conditions are met:\n\nBoth variables are quantitative\nBoth variables should be drawn from normally distributed populations.\nThe relationship between the two variables should be linear.\n\n\n\n\n\n\n\n\nQuick and easy cor.test()\n\n\n\n\n\nWe can test the significance of the correlation coefficient really easily with the function cor.test():\n\ncor.test(recalldata$recall_accuracy, recalldata$recall_confidence)\n\n\n    Pearson's product-moment correlation\n\ndata:  recalldata$recall_accuracy and recalldata$recall_confidence\nt = 4.1512, df = 18, p-value = 0.0005998\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3719603 0.8720125\nsample estimates:\n      cor \n0.6993654 \n\n\nby default, cor.test() will include only observations that have no missing data on either variable.\ne.g., running cor.test() on x and y in the dataframe below will include only the yellow rows:\n\n\n\n\n\nx\ny\n\n\n\n\n1\nNA\n\n\n2\n6\n\n\nNA\n8\n\n\n4\n7\n\n\n5\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep-by-step calculations\n\n\n\n\n\nOr, if we want to calculate our test statistic manually:\n\n#calculate r\nr = cor(recalldata$recall_accuracy, recalldata$recall_confidence)\n\n#get n\nn = nrow(recalldata)\n\n#calculate t    \ntstat = r * sqrt((n - 2) / (1 - r^2))\n\n#calculate p-value for t, with df = n-2 \n2*(1-pt(tstat, df=n-2))\n\n[1] 0.0005998222\n\n\n\n\n\n\n\n\n\n\n\noptional: (completely optional!) why t?\n\n\n\n\n\nWhy exactly do we have a \\(t\\) statistic? We’re calculating \\(r\\), not \\(t\\)??\nRemember that in hypothesis testing, we need a distribution against which to compare a statistic. But \\(r\\) is bounded (it can only be between -1 and 1). This means the distributions of “\\(r\\)’s that we would expect if under repeated sampling” is not easily defined in a standard way. Consider how the shape changes when our sample size changes:\n\n\n\n\n\n\n\n\n\nSo what we do is convert the \\(r\\) statistic to a \\(t\\) statistic, and then we can compare that to a \\(t\\) distribution!\n\\(t\\) statistics are generally calculated by using \\(\\frac{estimate - 0}{standard\\, error}\\).\nThe standard error for a correlation \\(r\\) is quantifiable as \\(\\sqrt{\\frac{(1-r^2)}{(n-2)}}\\).\nWe can think of this as what variance gets left-over (\\(1-r^2\\)) in relation to how much data is free to vary (\\(n-2\\) because we have calculated 2 means in the process of getting \\(r\\)). This logic maps to how our standard error of the mean was calculated \\(\\frac{\\sigma}{\\sqrt{n}}\\), in that it is looking at \\(\\frac{\\text{leftover variation}}{\\text{free datapoints}}\\).\nWhat this means is we can convert \\(r\\) into a \\(t\\) that we can then test!\n\\[\nt = \\, \\, \\frac{r}{SE_r} \\,\\,=\\,\\, \\frac{r}{\\sqrt{\\frac{(1-r^2)}{(n-2)}}} \\,\\,=\\,\\, r \\sqrt{\\frac{n-2}{1-r^2}}\n\\]\n\n\n\n\n\n\n\n\nCautions!\nCorrelation is an invaluable tool for quantifying relationships between variables, but must be used with care.\nBelow are a few things to be aware of when we talk about correlation.\n\nCorrelation can be heavily affected by outliers. Always plot your data!\nThe two plots below only differ with respect to the inclusion of one observation. However, the correlation coefficient for the two sets of observations is markedly different.\n\n\n\n\n\n\n\n\n\n\n\nr = 0 means no linear association. The variables could still be otherwise associated. Always plot your data!\nThe correlation coefficient in Figure 1 below is negligible, suggesting no linear association. The word “linear” here is crucial - the data are very clearly related.\n\n\n\n\n\nFigure 1: Unrelated data?\n\n\n\n\nSimilarly, take look at all the sets of data in Figure 2 below. The summary statistics (means and standard deviations of each variable, and the correlation) are almost identical, but the visualisations suggest that the data are very different from one another.\n\n\n\n\n\nFigure 2: Datasaurus! From Matejka, J., & Fitzmaurice, G. (2017, May): Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing.\n\n\n\n\n\n\nCorrelation does not imply causation!\n\n\n\n\n\nFigure 3: https://twitter.com/quantitudepod/status/1309135514839248896\n\n\n\n\nYou will have likely heard the phrase “correlation does not imply causation”. There is even a whole wikipedia entry devoted to the topic.\nJust because you observe an association between x and y, we should not deduce that x causes y\nAn often cited paper which appears to fall foul of this error took a correlation between a country’s chocolate consumption and its number of nobel prize winners (see Figure 4) to suggest a causal relationship between the two (“chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates”):\n\n\n\n\n\nFigure 4: Chocolate consumption causes more Nobel Laureates?"
  },
  {
    "objectID": "05b_slr.html",
    "href": "05b_slr.html",
    "title": "5B: Simple Linear Regression",
    "section": "",
    "text": "This reading:\n\nModelling an outcome variable as a linear function of an explanatory variable (correlation as an intercept and a slope).\n\nTesting the parameters that define our model.\n\nSimple statistical tests as linear models."
  },
  {
    "objectID": "05b_slr.html#the-model",
    "href": "05b_slr.html#the-model",
    "title": "5B: Simple Linear Regression",
    "section": "The Model",
    "text": "The Model\nWhen we fit a simple regression model, the bit we refer to as the ‘model’ is the line that is defined by two numbers, an ‘intercept’ and a ‘slope’ (see Figure 2):\n\nthe intercept, denoted \\(b_0\\), is the point at which the line hits the y-axis (i.e. where \\(x=0\\))\nthe slope, denoted \\(b_1\\), is the angle of the line. It is the amount which the line increases for every 1 increase in \\(x\\).\n\n\n\n\n\n\nFigure 2: Simple linear regression model, with the systematic part of the model in blue\n\n\n\n\nThis line implies some predicted values for our observed \\(x\\) values. For instance, we can see that when \\(x=3\\), the model (the blue line) will predict that \\(y\\) is approximately 4. If we take each of our datapoints, and project them up/down to the line, then we get our fitted values (Figure 3). We often denote these as \\(\\hat y\\) (or “y hat”), with the hat indicating that they are the model-estimated values of \\(y\\).\n\\[\n\\begin{align}\n\\color{red}{Outcome}\\color{black} \\qquad=\\qquad & \\color{blue}{Model}\\color{black}{} & +\\qquad Error\\\\\n\\color{red}{y}\\color{black} \\qquad = \\qquad & \\color{blue}{\\hat y}\\color{black} & +\\qquad \\varepsilon \\quad \\\\\n\\color{red}{y}\\color{black} \\qquad = \\qquad & \\color{blue}{b_0 + b_1 \\cdot x}\\color{black} & +\\qquad \\varepsilon \\quad \\\\\n\\end{align}\n\\]\n\n\n\n\n\nFigure 3: Simple linear regression model, fitted values in blue\n\n\n\n\n\n\n\n\n\n\noptional: Regression Slope vs Covariance\n\n\n\n\n\nWith simple linear regression, the fitted line we are describing is actually a scaled version of our covariance.\nRemember that covariance is the average of the products of \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\), which is a bit like the average area of the rectangles in Figure 4. If we think about what the average width of these rectangles is, it is the average of \\((x_{i}-\\bar{x})\\), which is actually just the variance of \\(x\\)!\n\n\n\n\n\nFigure 4: Covariance\n\n\n\n\nWe can divide the area of the average rectangle (\\(cov(x, y)\\)) by its width (\\(var(x)\\)), thereby scaling it so that the width is 1. What we’re getting from our coefficient is the area of this new rectangle which has width = 1. Because width = 1, the area is also the height (\\(\\text{area} = \\text{width} \\times \\text{height} = 1 \\times \\text{height}\\)). So what we get is the amount that \\(y\\) increases (the height) as \\(x\\) increases by 1 (the width).\nWe can see this working:\n\ncov(my_data$x, my_data$y)\n\n[1] 0.6877738\n\nvar(my_data$x)\n\n[1] 0.8823097\n\n\nThis calculation gives us the same linear regression slope of 0.78 that we see when we fit the model using lm().\n\ncov(my_data$x, my_data$y)/var(my_data$x)\n\n[1] 0.7795152"
  },
  {
    "objectID": "05b_slr.html#the-error",
    "href": "05b_slr.html#the-error",
    "title": "5B: Simple Linear Regression",
    "section": "The Error",
    "text": "The Error\nOur model is not perfect. It is a model - i.e. it is a simplification of the world, and so is inherently going to be inaccurate for individuals. This inaccuracy can be seen in our plots so far - some points are higher than the model predicts, some lower. These deviations from the model (shown by the black dotted lines in Figure 5) from the model are the random error component \\(\\hat \\varepsilon\\), or “residuals”.\n\\[\n\\begin{align}\nError &= \\color{red}{Outcome}\\color{black}-\\color{blue}{Model} \\\\\n\\hat{\\varepsilon} &= \\color{red}{y}\\color{black}- \\color{blue}{\\hat{y}}\n\\end{align}\n\\]\n\n\n\n\n\nFigure 5: Simple linear regression model, with the systematic part of the model in blue, and residuals in red\n\n\n\n\nIn full, we should really write our linear regression model out as:\n\\[\n\\begin{align}\n& y = b_0 + b_1 \\cdot x + \\varepsilon \\quad \\\\\n& \\text{where} \\\\\n& \\varepsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n\\]\nThe new bit here: “\\(\\varepsilon \\sim N(0, \\sigma) \\text{ independently}\\)” means that the errors around the line have mean zero and constant spread as x varies (we’ll read more about what this means later on when we discuss the assumptions underlying regression). You can think of \\(\\sim N(0, \\sigma)\\) as meaning “normally distributed with a mean of zero and a standard deviation of \\(\\sigma\\)”.\nThe standard deviation of the errors, denoted by \\(\\sigma\\), is an important quantity that our model estimates. It measures how much individual data points tend to deviate above and below the regression line. A small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\n\\(\\sigma\\) is estimated by essentially averaging squared residuals (giving the variance) and taking the square-root:\n\\[\n\\begin{align}\n& \\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - 2}} \\\\\n\\qquad \\\\\n& \\text{where} \\\\\n& SS_{Residual} = \\textrm{Sum of Squared Residuals} = \\sum_{i=1}^n{(\\varepsilon_i)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "05b_slr.html#lm",
    "href": "05b_slr.html#lm",
    "title": "5B: Simple Linear Regression",
    "section": "lm()",
    "text": "lm()\nIn R it is very easy to fit linear models, we just need to use the lm() function.\nThe syntax of the lm() function is:\nmodel_name &lt;- lm(outcome ~ 1 + predictor, data = dataframe)\nWe don’t have to include the 1 + when we specify the model, as this will be included by default, so we can also simply write:\nmodel_name &lt;- lm(outcome ~ predictor, data = dataframe)\n\n\n\n\n\n\nWhat is the ~1 + doing?\n\n\n\n\n\nThe fitted model can be written as \\[\n\\hat y = \\hat b_0 + \\hat b_1 \\cdot x\n\\] The predicted values for the outcome are equal to our intercept, \\(\\hat b_0\\), plus our slope \\(\\hat b_1\\) multiplied by the value on our explanatory variable \\(x\\).\nThe intercept is a constant. That is, we could write it as multiplied by 1: \\[\n\\hat y = \\color{blue}{\\hat b_0}\\color{black}{}\\cdot\\color{orange}{1}\\color{blue}{ + \\hat b_1 }\\color{black}{}\\cdot\\color{orange}{x}\\color{black}{}\n\\]\nWhen we specify the linear model in R, we include after the tilde sign ~ all the things which appear to the right of each of the \\(\\hat b\\)s (the bits in green in the equartion above). That’s why the 1 is included. It is just saying “we want the intercept, \\(b_0\\), to be estimated”."
  },
  {
    "objectID": "05b_slr.html#model-summary",
    "href": "05b_slr.html#model-summary",
    "title": "5B: Simple Linear Regression",
    "section": "Model Summary",
    "text": "Model Summary\nWe can then view lots of information by giving our model to the summary() function:\n\nmy_model &lt;- lm(y ~ x, data = my_data)\nsummary(my_model)\n\n\n\n\n\n\nFigure 6: Output of lm() for a simple regression in R\n\n\n\n\nThe intercept \\(b_0\\) is the point at which the line hits the y-axis (i.e. where \\(x=0\\)), and the slope \\(b_1\\) is the amount which the line increases for every 1 increase in \\(x\\). We can see the estimated values of these in Figure 6, and these provide us with our fitted lin:\n\\[\n\\begin{align}\ny =& 1.54 + 0.78 \\cdot x + \\varepsilon \\\\\n\\end{align}\n\\] We also see that the standard deviation of the residuals, \\(\\sigma\\), is 0.93, which means we consider the actual observed values of Y to vary randomly around this line with a standard deviation of 0.93.\n\n\n\n\n\nFigure 7: Simple linear regression model, estimated intercept and slope included"
  },
  {
    "objectID": "05b_slr.html#model-predictions",
    "href": "05b_slr.html#model-predictions",
    "title": "5B: Simple Linear Regression",
    "section": "Model Predictions",
    "text": "Model Predictions\nWe can get out the model predicted values for \\(y\\), the “y hats” (\\(\\hat y\\)), using functions such as:\n\npredict(my_model)\nfitted(my_model)\nfitted.values(my_model)\nmy_model$fitted.values\n\nA nice package which will come in handy is the broom package. It allows us to use the function augment(), which gives us out lots of information, such as the model predicted values, the residuals, and many more:\n\nlibrary(broom)\naugment(my_model)\n\n# A tibble: 100 × 8\n       y     x .fitted .resid   .hat .sigma  .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  4.42  3.19    4.03  0.388 0.0102  0.935 0.000903      0.420\n 2  4.48  2.57    3.54  0.941 0.0130  0.931 0.00681       1.02 \n 3  2.72  3.91    4.59 -1.87  0.0180  0.916 0.0378       -2.03 \n 4  5.39  4.79    5.28  0.107 0.0438  0.935 0.000319      0.118\n 5  3.85  4.00    4.66 -0.809 0.0197  0.932 0.00776      -0.878\n 6  4.42  4.11    4.74 -0.327 0.0222  0.935 0.00143      -0.355\n 7  4.30  2.72    3.66  0.638 0.0114  0.933 0.00274       0.689\n 8  5.94  4.02    4.68  1.26  0.0202  0.927 0.0193        1.37 \n 9  1.70  3.05    3.92 -2.22  0.0100  0.908 0.0291       -2.40 \n10  4.79  4.58    5.11 -0.318 0.0358  0.935 0.00224      -0.348\n# ℹ 90 more rows\n\n\nWe can also compute model-predicted values for other (unobserved) data. For instance, what about for an observation where \\(x=10\\), or \\(20\\)?\n\n# make a dataframe with values for the predictor:\nsome_newdata &lt;- data.frame(x=c(10, 20))\n# model predicted values of y, for the values of x inside the 'some_newdata' object:\npredict(my_model, newdata = some_newdata)\n\n       1        2 \n 9.33792 17.13307 \n\n\nGiven that our fitted model takes the form below, we can work this out ourselves as well:\n\\[\n\\begin{align}\ny &= 1.54 + 0.78\\cdot x \\\\\ny &= 1.54 + 0.78\\cdot 10 \\\\\ny &= 1.54 + 7.80\\\\\ny &= 9.34 \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "05b_slr.html#footnotes",
    "href": "05b_slr.html#footnotes",
    "title": "5B: Simple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecall that a standard error gives a numerical answer to the question of how variable a statistic will be because of random sampling.↩︎\nWhy \\(n-2\\)? The most intuitive answer is that we have already used up 2 pieces of information in estimating the intercept and the slope. Once these things are fixed, \\(n-2\\) of the datapoints could be wherever they like around that line, but the remaining 2 must be placed in such a way that results in that line↩︎"
  },
  {
    "objectID": "06_wt.html",
    "href": "06_wt.html",
    "title": "WalkThrough: Advanced Data Wrangling",
    "section": "",
    "text": "In many projects (both in academic research & in other sectors), more time will be spent cleaning and organising data than will actually be spent conducting statistical analyses (a well designed study with a well-thought through data collection process can go a long way to remedy this!).\nFor this reason, we’re going to take a little detour away from statistics to get some more practice wrangling and cleaning data in R. Don’t worry about the trying to remember all of the new R functions introduced in this topic - there are a lot. Use them as a means of learning about some of the different ways of doing things in R."
  },
  {
    "objectID": "06_wt.html#build-a-model",
    "href": "06_wt.html#build-a-model",
    "title": "WalkThrough: Advanced Data Wrangling",
    "section": "Build a model!",
    "text": "Build a model!\nWe’re now finally getting to the analysis. As we said earlier, this can sometimes be very straightforward in comparison to the amount of effort involved in cleaning data.\nRecall that we’re interested in whether the perception of whether or not a speaker is lying about the location of some hidden treasure (as measured by the pattern of eye fixations towards the object not referred to by the speaker) is influenced by the number of times the speaker is seen to blink while producing the utterance.\n\n\n\n\nPlot the relationship between the two variables you think will be best used to answer this question.\n\n\n\n\n\nSolution\n\n\n\n\nggplot(blinks_full, aes(x=nr_blinks, y = distractor_fix))+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion B12\n\n\nFit the linear model specified below to the data using the lm() function and store the output in the environment as an object named blinks_mdl.\n\\[\n\\begin{align}\n& \\text{Fixation time to distractor} = b_0 + b_1 \\ \\text{Number of blinks} + \\epsilon \\\\\n\\quad \\\\\n& \\text{where} \\quad \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n\\]\n\n\n\n\n\nSolution\n\n\n\n\nblinks_mdl &lt;- lm(distractor_fix ~ 1 + nr_blinks, data=blinks_full)\n\nsummary(blinks_mdl)\n\n\nCall:\nlm(formula = distractor_fix ~ 1 + nr_blinks, data = blinks_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-944.47 -253.55    5.58  217.74 1033.81 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  517.296     36.076   14.34   &lt;2e-16 ***\nnr_blinks     64.128      5.808   11.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 355.1 on 447 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.2143,    Adjusted R-squared:  0.2126 \nF-statistic: 121.9 on 1 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nQuestion B13\n\n\nThe \\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\) bit of our model is an assumption we have to make. It concerns the errors (the deviations from observations to our line). Our model assumes that these are normally distributed and centered on 0. We can plot the distribution of residuals to check how well our assumption holds:\n\nhist(residuals(blinks_mdl))\n\n\n\n\n\n\n\n\nHowever, we also make the statement that the errors are independent - i.e. they are not related to one another.\nFor us, this is not the case, and so we should not be using this simple linear regression here.\nIn what way are we violating the assumption of independence?\n\n\n\n\n\nSolution\n\n\n\nThe dataset to which we are fitting our model does not contain independent observations. We have multiple observations from each participant. i.e. Subject 1 makes up 20 rows, and subject 2 makes up 20 rows.\nAs such, this means that our linear regression model is not appropriate. We will learn about how to deal with this sort of study design next semester, in the Multivariate Statistics & Methodoligy using R (MSMR) course.\n\n\n\n\nQuestion B14\n\n\nOne option open to us now is to simply “aggregate up”, so that we remove the dependence from our rows in our dataset. However, this means reducing the number of rows.\nThis is sub-optimal (and we’ll see why next semester!), but we can calculate the average distractor_fix for each level of nr_blinks, and then use those in our model.\nWe can do this the same way as we saw with our summary plots - with group_by() and summarise()!\n\n\n\n\n\nSolution\n\n\n\n\nblinks_agg &lt;- blinks_full |&gt;\n  group_by(nr_blinks) |&gt;\n  summarise(\n    meanDF = mean(distractor_fix, na.rm=TRUE)\n  )\n\nmod &lt;- lm(meanDF ~ nr_blinks, data = blinks_agg)\n\nsummary(mod)\n\n\nCall:\nlm(formula = meanDF ~ nr_blinks, data = blinks_agg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.780 -45.383  -8.233  35.998  78.532 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  514.501     36.354   14.15 6.04e-07 ***\nnr_blinks     64.584      5.859   11.02 4.08e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53.22 on 8 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9382,    Adjusted R-squared:  0.9305 \nF-statistic: 121.5 on 1 and 8 DF,  p-value: 4.083e-06\n\n\nLooking at the distractor increases by 64ms for every additional blink in the video!"
  },
  {
    "objectID": "07_ex.html",
    "href": "07_ex.html",
    "title": "Exercises: Multiple Regression",
    "section": "",
    "text": "Data: monkeytoys.csv\nAfter their recent study investigating how age is associated with inquisitiveness in monkeys (see Week 5 Exercises) our researchers have become interested in whether primates show preferences for certain types of object - are they more interested in toys with moving parts, or with soft plush toys?\nThey conduct another study (Liu, Hajnosz, Xu & Li, 20231) in which they gave 119 monkeys each a different toy, and recorded the amount of time each monkey spent exploring their toy. Toys were categorised as either being ‘mechanical’ or ‘soft’. Mechanical toys had several parts that could be manipulated, while soft toys did not. They also recorded the age of each monkey, and a few further attributes of each toy (its size and colour).\nThe aim of this study is to investigate the following question:\n\nDo monkeys have a preference between soft toys vs toys with moving parts?\n\nThe data is available at https://uoepsy.github.io/data/monkeytoys.csv and contains the variables described in Table 1\n\n\n\n\n\n\nTable 1:  Data dictionary for monkeytoys.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    name\nMonkey Name\n    age\nAge of monkey in years\n    obj_type\nType of novel object given (mechanical / soft)\n    obj_colour\nMain colour of object (red / green / blue)\n    obj_size\nSize of object in cm (length of largest dimension of the object)\n    exploration_time\nTime (in minutes) spent exploring the object\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 1\n\n\nFit a simple linear model examining whether exploration_time depends on the type of object given to monkeys (obj_type).\nMake a plot too if you want!\n\n\n\n\n\n\nHints\n\n\n\n\n\nThere’s nothing new here. It’s just lm(outcome ~ predictor).\nFor the plot, try a boxplot maybe? or even a violin plot if you’re feeling adventurous!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmonkeytoys &lt;-  read_csv(\"https://uoepsy.github.io/data/monkeytoys.csv\")\n\n\nmodel1 &lt;- lm(exploration_time ~ obj_type, data = monkeytoys)\nsummary(model1)\n\n\nCall:\nlm(formula = exploration_time ~ obj_type, data = monkeytoys)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5361 -2.8155  0.2639  2.7492 11.2639 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    9.2361     0.5470  16.884   &lt;2e-16 ***\nobj_typesoft  -1.2705     0.7835  -1.622    0.108    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.272 on 117 degrees of freedom\nMultiple R-squared:  0.02198,   Adjusted R-squared:  0.01362 \nF-statistic: 2.629 on 1 and 117 DF,  p-value: 0.1076\n\n\nFrom this, we would conclude that monkeys do not significantly differ in how much time they spend exploring one type of toy over another (mechanical or soft).\nLet’s go wild and put a boxplot on top of a violin plot!\n\nggplot(monkeytoys, \n       aes(x=obj_type, y=exploration_time, \n           col=obj_type)) +\n  geom_violin() + \n  geom_boxplot(alpha=.3, width=.4)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nIs the distribution of ages of the monkeys with soft toys similar to those with mechanical toys?\nIs there a way you could test this?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’re wanting to know if age (continuous) is different between two groups (monkeys seeing soft toys and monkeys seeing moving toys). Anyone for \\(t\\)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nt.test(age ~ obj_type, data = monkeytoys)\n\n\n    Welch Two Sample t-test\n\ndata:  age by obj_type\nt = 4.2966, df = 116.97, p-value = 3.605e-05\nalternative hypothesis: true difference in means between group mechanical and group soft is not equal to 0\n95 percent confidence interval:\n 2.604953 7.059829\nsample estimates:\nmean in group mechanical       mean in group soft \n                15.57377                 10.74138 \n\n\n\nThe average age of monkeys with the soft toys is 10.7 years (SD = 6), and the average of those with the mechanical toys is 15.6 (SD = 6.2). This difference is significant as indicated by a Welch two-sample \\(t\\)-test (\\(t(117)=4.3, \\, p&lt;0.001\\)).\n\n\n\n\n\nQuestion 3\n\n\nDiscuss: What does this mean for our model of exploration_time? Remember - the researchers already discovered last week that younger monkeys tend to be more inquisitive about new objects than older monkeys are.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIf older monkeys spend less time exploring novel objects\nAnd our group of monkeys with mechanical toys are older than the group with soft toys.\nThen…\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe have reason to believe that older monkeys spend less time exploring novel objects. We discovered this last week.\nBecause our group of monkeys with mechanical toys are of a different age than the group with soft toys, surely we can’t discern whether any difference in exploration_time between the two types of toy is because of these age differences or because of the type of toy?\nEmbed from Getty Images\n\n\n\n\n\n\nQuestion 4\n\n\nFit a model the association between exploration time and type of object while controlling for age.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWhen we add multiple predictors in to lm(), it can sometimes matter what order we put them in (e.g. if we want to use anova(model) to do a quick series of incremental model comparisons as in 7A #shortcuts-for-model-comparisons). Good practice is to put the thing you’re interested in (the ‘focal predictor’) at the end, e.g.: lm(outcome ~ covariates + predictor-of-interest)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel2 &lt;- lm(exploration_time ~ age + obj_type, data = monkeytoys)\n\nsummary(model2)\n\n\nCall:\nlm(formula = exploration_time ~ age + obj_type, data = monkeytoys)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3277 -2.2886  0.3889  2.3417  8.4889 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.98828    1.03058  13.573  &lt; 2e-16 ***\nage          -0.30514    0.05808  -5.253 6.86e-07 ***\nobj_typesoft -2.74512    0.76092  -3.608 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.856 on 116 degrees of freedom\nMultiple R-squared:  0.2099,    Adjusted R-squared:  0.1963 \nF-statistic: 15.41 on 2 and 116 DF,  p-value: 1.159e-06\n\n\n\n\n\n\nQuestion 5\n\n\nThe thing we’re interested in here is association between exploration_time and obj_type.\nHow does it differ between the models we’ve created so far, and why?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nTo quickly compare several models side by side, the tab_model() function from the sjPlot package can be quite useful, e.g. tab_model(model1, model2, ...).\n\nalternatively, just use summary() on each model.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(sjPlot)\ntab_model(model1, model2)\n\n\n\n\n \nexploration time\nexploration time\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n9.24\n8.15 – 10.32\n&lt;0.001\n13.99\n11.95 – 16.03\n&lt;0.001\n\n\nobj type [soft]\n-1.27\n-2.82 – 0.28\n0.108\n-2.75\n-4.25 – -1.24\n&lt;0.001\n\n\nage\n\n\n\n-0.31\n-0.42 – -0.19\n&lt;0.001\n\n\nObservations\n119\n119\n\n\nR2 / R2 adjusted\n0.022 / 0.014\n0.210 / 0.196\n\n\n\n\n\n\n\nThe coefficient for obj_type is much bigger when we include age in the model, and it is significant!\nIn the model without age, we’re just comparing the two groups. We can see this in the left hand panel of the plot below - it’s the difference between the two group means.\nWhen we include age in the model, the coefficient for obj_type represents the difference in the heights of the two lines in the right hand panel below.\n\n\nCode\nlibrary(patchwork)\np1 &lt;- ggplot(monkeytoys, aes(x=obj_type, y=exploration_time,\n             col=obj_type))+\n  geom_jitter(width=.05)+\n  stat_summary(geom=\"pointrange\", fun.data = mean_cl_normal, \n               position = position_nudge(x=.14)) +\n  labs(subtitle=\"exp_time~obj_type\")+\n  scale_y_continuous(breaks=seq(0,22,2))+\n  guides(col=\"none\")\n\np2 &lt;- broom::augment(model2, interval=\"confidence\") |&gt;\n  ggplot(aes(x=age, col=obj_type))+\n  geom_point(aes(y=exploration_time))+\n  geom_line(aes(y=.fitted))+\n  geom_ribbon(aes(ymin=.lower,ymax=.upper,\n                  fill=obj_type),col=NA,alpha=.2)+\n  scale_y_continuous(breaks=seq(0,22,2))+\n  labs(subtitle=\"exp_time~age+obj_type\")\n\np1 + p2 \n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nPlot the model estimated difference in exploration time for each object type.\nTo do this, you’ll need to create a little data frame for plotting, then give that to the augment() function from the broom package. This will then give us the model fitted value and the confidence interval, which we can plot!\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nAn example of this whole process is in 7A#model-visualisations.\n\nThe example has a continuous predictor, so we plotted a line and a ribbon. An alternative for a categorical predictor might be a geom_pointrange().\n\n\n\n\n\n\n\n\nWe’ve split this solution in to parts so that you can have a go at some bits without seeing it all at once.\n\n\nSolution Part 1 - make a dataframe\n\n\n\nHere’s our dataframe to add model estimated values to:\n\nplotdat &lt;- data.frame(\n  obj_type = c(\"soft\",\"mechanical\"),\n  age = mean(monkeytoys$age)\n)\n\n\n\n\n\n\nSolution Part 2 - use broom::augment\n\n\n\nThis gives us our estimates and intervals:\n\nlibrary(broom)\naugment(model2, newdata = plotdat, interval=\"confidence\")\n\n# A tibble: 2 × 5\n  obj_type     age .fitted .lower .upper\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 soft        13.2    7.21   6.17   8.25\n2 mechanical  13.2    9.95   8.94  11.0 \n\n\n\n\n\n\n\nSolution Part 3 - into ggplot!\n\n\n\n\naugment(model2, newdata = plotdat, interval=\"confidence\") |&gt;\n  ggplot(aes(x=obj_type,y=.fitted))+\n  geom_pointrange(aes(ymin=.lower,ymax=.upper))\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nAre other aspects of the toys (their size and colour) also associated with more/less exploration time?\nWe can phrase this as “do size and colour explain additional variance in exploration time?”. How might we test such a question?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWe basically just want to add these new predictors into our model.\n\nDon’t worry about interpreting the coefficients right now (we’ll talk more about categorical predictors next week), but we can still test whether the inclusion of size and colour improve our model! (see 7A#model-comparisons).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThis is our current model:\n\nmodel2 &lt;- lm(exploration_time ~ age + obj_type, data = monkeytoys)\n\nAnd we can add the two colour and size variables:\n\nmodel3 &lt;- lm(exploration_time ~ age + obj_type + obj_colour +\n               obj_size, data = monkeytoys)\n\nLet’s compare them:\n\nanova(model2, model3)\n\nAnalysis of Variance Table\n\nModel 1: exploration_time ~ age + obj_type\nModel 2: exploration_time ~ age + obj_type + obj_colour + obj_size\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    116 1725.2                                \n2    113 1546.7  3    178.47 4.3464 0.006148 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAfter accounting for differences due to age and type of object (mechanical vs soft), other features of objects - size (cm) and colour (red/green/blue) - were found to significantly explain variation in the time monkeys spent exploring those objects (\\(F(3,113)=4.35, \\, p=0.0061\\))."
  },
  {
    "objectID": "07_ex.html#footnotes",
    "href": "07_ex.html#footnotes",
    "title": "Exercises: Multiple Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnother fake study!↩︎\napparently coughing is a method of immediately lowering heart rate!↩︎"
  },
  {
    "objectID": "07a_mlr.html",
    "href": "07a_mlr.html",
    "title": "7A: Multiple Linear Regression",
    "section": "",
    "text": "This reading:\n\n“Variance explained” in a linear model\nMultiple regression: Building models where more than one thing explains variance\nComparing models\nAssociations in multiple regression\nWhen we introduced the linear regression model (see 5B: simple linear regression), we talked mainly about the intercept and the slope (collectively referred to as “the coefficients”). We saw that the model summary output (from summary(model)) gave us some tests next to each coefficient:\nslr_data &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_slr.csv\")\nsimplemod &lt;- lm(y ~ x, data = slr_data)\nsummary(simplemod)\n\n\nCall:\nlm(formula = y ~ x, data = slr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4383 -0.6593  0.1075  0.5945  2.1867 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.54277    0.32005   4.820 5.24e-06 ***\nx            0.77952    0.09959   7.827 5.92e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9308 on 98 degrees of freedom\nMultiple R-squared:  0.3847,    Adjusted R-squared:  0.3784 \nF-statistic: 61.26 on 1 and 98 DF,  p-value: 5.918e-12\nHowever, there is another way we can look at our regression model, and that is by considering how the model, as a whole, explains variation in our outcome model."
  },
  {
    "objectID": "07a_mlr.html#r2-proportion-of-variance-explained",
    "href": "07a_mlr.html#r2-proportion-of-variance-explained",
    "title": "7A: Multiple Linear Regression",
    "section": "\\(R^2\\): proportion of variance explained",
    "text": "\\(R^2\\): proportion of variance explained\nA useful statistic is the \\(R^2\\), which shows us the proportion of the total variability in the outcome (y) that is explained by the linear relationship with the predictor (x).\n\nThe \\(R^2\\) coefficient is defined as the proportion of the total variability in the outcome variable which is explained by our model:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\n\nWe can find the \\(R^2\\) easily in the summary() of the model!\nThe output of summary() displays the R-squared value in the following line:\nMultiple R-squared:  0.3847\n\n\nFor the moment, ignore “Adjusted R-squared”. We will come back to this in a little bit.\n\nApproximately 38% of the total variability in y is explained by the linear association with x.\n\n\n\n\n\n\n\noptional: manual calculation of R-Squared\n\n\n\n\n\n\nslr_data &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_slr.csv\")\nsimplemod &lt;- lm(y ~ x, data = slr_data)\n\nsimplemod_fitted &lt;- slr_data |&gt;\n  mutate(\n    y_hat = predict(simplemod),\n    resid = y - y_hat\n  )\nhead(simplemod_fitted)\n\n# A tibble: 6 × 4\n      x     y y_hat  resid\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  3.19  4.42  4.03  0.388\n2  2.57  4.48  3.54  0.941\n3  3.91  2.72  4.59 -1.87 \n4  4.79  5.39  5.28  0.107\n5  4.00  3.85  4.66 -0.809\n6  4.11  4.42  4.74 -0.327\n\nsimplemod_fitted |&gt;\n  summarise(\n    SSModel = sum( (y_hat - mean(y))^2 ),\n    SSTotal = sum( (y - mean(y))^2 )\n  ) |&gt;\n  summarise(\n    RSquared = SSModel / SSTotal\n  )\n\n# A tibble: 1 × 1\n  RSquared\n     &lt;dbl&gt;\n1    0.385"
  },
  {
    "objectID": "07a_mlr.html#the-f-statistic",
    "href": "07a_mlr.html#the-f-statistic",
    "title": "7A: Multiple Linear Regression",
    "section": "The \\(F\\) Statistic",
    "text": "The \\(F\\) Statistic\nWe can also perform a test to investigate if the model is ‘useful’ — that is, a test to see if our explanatory variable explains more variance in our outcome than we would expect by just some random chance variable.\nOur test is framed in terms of the following hypotheses:\n\\[\n\\begin{aligned}\nH_0 &: \\text{the model is ineffective, } b_1 = 0 \\\\\nH_1 &: \\text{the model is effective, } b_1 \\neq 0\n\\end{aligned}\n\\] The relevant test-statistic is the F-statistic, which uses “Mean Squares” (these are Sums of Squares divided by the relevant degrees of freedom). We then compare that against (you guessed it) an F-distribution! F-distributions vary according to two parameters, which are both degrees of freedom.\nWe won’t go into details of why we use \\(F\\), but the logic of the significance test is just the same as it was for \\(t\\) - we have a statistic, and we have a distribution of what we would expect that statistic to be if the null hypothesis is true. We can then ask how unlikely it is that we would observe the statistic (or more extreme) if the null hypothesis were true.\n\n\\(F\\)-statistic for simple linear regression\n\\[\n\\begin{split}\nF = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}\n\\end{split}\n\\]\nThis is a comparison between amount of variation in the outcome explained by the model and the amount of variation ‘explained by’ (or leftover in) the residuals.\nThe sample F-statistic is compared to an F-distribution with \\(df_{1} = 1\\) and \\(df_{2} = n - 2\\) degrees of freedom.1\n\nLike the R-squared, the summary() of our model prints out the F-statistic, degrees of freedom, and p-value. These are right at the bottom of the summary output, printed as:\n...\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.54277    0.32005   4.820 5.24e-06 ***\nx            0.77952    0.09959   7.827 5.92e-12 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.9308 on 98 degrees of freedom\nMultiple R-squared:  0.3847,    Adjusted R-squared:  0.3784 \nF-statistic: 61.26 on 1 and 98 DF,  p-value: 5.918e-12\n\nThe F-test of model utility was significant (\\(R^2=0.38, F(1,98) = 61.26,\\ p &lt;.001\\)), suggesting that predictor \\(x\\) is effective in explaining variance in the outcome.\n\nNote that the p-value here is exactly the same as the one for the coefficient. This is because in testing “the model is (in)effective”, the “model” is really only the relationship between the outcome and our one predictor. We’re about to start adding more explanatory variables into our model, which means that our hypotheses for the \\(F\\)-test will be about a set of \\(b\\)’s, and so the \\(p\\)’s won’t be the same. .\n\n\n\n\n\n\noptional: with only one predictor variable, the F-test is equivalent to the t-test of the slope\n\n\n\n\n\nIn simple linear regression only (where we have just one predictor), the F-statistic for overall model significance is equal to the square of the t-statistic for \\(H_0: b_1 = 0\\).\nYou can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:\n\nslr_data &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_slr.csv\")\nsimplemod &lt;- lm(y ~ x, data = slr_data)\n\nsummary(simplemod)$fstatistic['value']\n\n   value \n61.26497 \n\nsummary(simplemod)$coefficients['x','t value']\n\n[1] 7.827194\n\n\n\\[\n\\begin{align}\n& t^2 = F \\\\\n& 7.827194^2 = 61.26497\n\\end{align}\n\\]\nWe can also show the equivalence of the F-test for model effectiveness and t-test for the slope through their respecive formulae.\nRecall the formula of the sum of squares due to the model. We are going to re-express this in an equivalent form below: \\[\n\\begin{aligned}\nSS_{Model} &= \\sum_i (\\hat y_i - \\bar y)^2 \\\\\n&= \\sum_i (\\hat b_0 + \\hat b_1 x_i - \\bar y)^2 \\\\\n&= \\sum_i (\\bar y - \\hat b_1 \\bar x + \\hat b_1 x_i - \\bar y)^2 \\\\\n&= \\sum_i (\\hat b_1 (x_i - \\bar x))^2 \\\\\n&= \\hat b_1^2 \\sum_i (x_i - \\bar x)^2\n\\end{aligned}\n\\]\nThe F-statistic is given by: \\[\n\\begin{aligned}\nF = \\frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)}\n= \\frac{\\hat b_1^2 \\sum_i (x_i - \\bar x)^2}{\\hat \\sigma^2}\n= \\frac{\\hat b_1^2 }{\\hat \\sigma^2 / \\sum_i (x_i - \\bar x)^2}\n\\end{aligned}\n\\]\nNow recall the formula of the t-statistic, \\[\nt = \\frac{\\hat b_1}{SE(\\hat b_1)} = \\frac{\\hat b_1}{\\hat \\sigma / \\sqrt{\\sum_i (x_i - \\bar x)^2}}\n\\]\nIt is evident that the latter is obtained as the square root of the former.\n\n\n\n\n\n\n\n\n\noptional: expressing \\(F\\) in terms of \\(R^2\\)\n\n\n\n\n\nWith some algebra we can also show that: \\[\nF = \\frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \\frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }\n\\]\nProof:\n\\[\n\\begin{aligned}\nF = \\frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)}\n= \\frac{\\frac{SS_{Model}}{SS_{Total}}}{\\frac{SS_{Residual}}{SS_{Total}} \\cdot \\frac{1}{(n - 2)}}\n= \\frac{R^2 / 1}{(1 - R^2) / (n - 2)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07a_mlr.html#fitting-multiple-regression-models-in-r",
    "href": "07a_mlr.html#fitting-multiple-regression-models-in-r",
    "title": "7A: Multiple Linear Regression",
    "section": "Fitting Multiple Regression Models in R",
    "text": "Fitting Multiple Regression Models in R\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\nmodel_name &lt;- lm(y ~ 1 + x1 + x2 + ... + xp, data = dataframe)\nAnd we can use all the same functions that we have already seen such as summary(), predict(), fitted(), coef() etc.\n\nmlr_data &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmodel2 &lt;- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = mlr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.5201  -4.2912  -0.0268   3.3044  16.2154 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -2.39138    3.67735  -0.650  0.51867   \nx1           0.17570    0.06435   2.730  0.00888 **\nx2          -0.64756    0.19959  -3.244  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.069 on 47 degrees of freedom\nMultiple R-squared:  0.2643,    Adjusted R-squared:  0.233 \nF-statistic: 8.443 on 2 and 47 DF,  p-value: 0.0007369\n\n\nJust like we saw for the regression model with one predictor, the summary() output of a multiple regression model shows us all the same information: residuals, coefficients, \\(R^2\\) and and \\(F\\)-test. We’ll get to the coefficients a little later on, but first we’re going to take a look at the overall model in terms of how much variance is now explained."
  },
  {
    "objectID": "07a_mlr.html#the-adjusted-r2",
    "href": "07a_mlr.html#the-adjusted-r2",
    "title": "7A: Multiple Linear Regression",
    "section": "The Adjusted \\(R^2\\)",
    "text": "The Adjusted \\(R^2\\)\nWe know from above that in simple linear regression the R-squared can be obtained as: \\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\nIn multiple regression, the “multiple \\(R^2\\)” uses this exact same formula. However, when we add more and more predictors into a multiple regression model, \\(SS_{Residual}\\) cannot increase. In fact, it will always decrease, regardless of how useful our new predictors are. This means that \\(R^2\\) will always increase (because \\(SS_{Total}\\) is constant, so \\(1-\\frac{SS_{Residual}}{SS_{Total}}\\) will increase as \\(SS_{Residual}\\) decreases).\nIf we added randomly generated 1000 new predictors (completely random, so they have nothing to do with the outcome), then by chance alone they will explain some variance in the outcome \\(y\\), and the multiple \\(R^2\\) will always increase.\nAn alternative, the “Adjusted-\\(R^2\\)”, does not necessarily increase with the addition of more explanatory variables, by the inclusion of a penalty according to the number of explanatory variables in the model. The number by itself isn’t directly meaningful, but can be useful in determining the amount of additional variance explained by adding predictor(s) into a model.\n\nThe Adjusted \\(R^2\\) is a measure of the proportion of variability in the outcome that is explained by our model, adjusted for the number of predictors in the model.\n\\[\n\\begin{align}\n& Adjusted{-}R^2=1-\\frac{(1-R^2)(n-1)}{n-k-1} \\\\\n& \\quad \\\\\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\nIn R, we can view both the mutiple and adjusted \\(R^2\\) at the bottom of the output of summary(&lt;modelname&gt;):\n\nmodel2 &lt;- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)"
  },
  {
    "objectID": "07a_mlr.html#the-f-statistic-a-joint-test",
    "href": "07a_mlr.html#the-f-statistic-a-joint-test",
    "title": "7A: Multiple Linear Regression",
    "section": "The \\(F\\)-statistic: a joint test",
    "text": "The \\(F\\)-statistic: a joint test\nWe saw just above that with one predictor, the F-statistic is used to test the null hypothesis that the regression slope for that predictor is zero. In multiple regression, the logic is the same, but we are now testing against the null hypothesis that all regression slopes are zero (now that we have multiple predictors, “all” is more than 1).\n\\[\n\\begin{aligned}\nH_0: & \\text{the model is ineffective, } \\\\\n& b_1, ..., b_k = 0 \\\\\nH_1: &\\text{the model is effective, } \\\\\n& \\text{any of }b_1, ..., b_k \\neq 0\n\\end{aligned}\n\\]\nThe \\(F\\)-statistic is sometimes called the \\(F\\)-ratio because it is the ratio of the how much of the variation is explained by the model (per parameter) versus how much of the variation is unexplained (per remaining degrees of freedom). We can generalise the formula for the \\(F\\)-statistic in simple regression that we saw above, to encompass situations where there are more predictors:\n\n\\(F\\)-ratio\n\\[\n\\begin{align}\n& F_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n& \\quad \\\\\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\nIn R, at the bottom of the output of summary(&lt;modelname&gt;), you can view the F statistic, along with the hypothesis test against the null hypothesis that all the coefficients are 0:3\n\nmodel2 &lt;- lm(y ~ x1 + x2, data = mlr_data)\nsummary(model2)\n\n\n\n\n\n\n\n\n\n\n\nthe linear model with \\(x1\\) and \\(x2\\) explained a significant amount of variance in \\(y\\) beyond what we would expect by chance (\\(R^2=0.26, F(2, 47) = 8.44,\\ p &lt;.001\\))."
  },
  {
    "objectID": "07a_mlr.html#footnotes",
    "href": "07a_mlr.html#footnotes",
    "title": "7A: Multiple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n \\(SS_{Total}\\) has \\(n - 1\\) degrees of freedom as one degree of freedom is lost in estimating the population mean with the sample mean \\(\\bar{y}\\). \\(SS_{Residual}\\) has \\(n - 2\\) degrees of freedom. There are \\(n\\) residuals, but two degrees of freedom are lost in estimating the intercept and slope of the line used to obtain the \\(\\hat y_i\\)s. Hence, by difference, \\(SS_{Model}\\) has \\(n - 1 - (n - 2) = 1\\) degree of freedom.↩︎\nOne could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!↩︎\nunder the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1↩︎\nand vice versa for the coefficient of \\(x_1\\).↩︎\nthese intervals are using the standard errors multiplied by the appropriate value of \\(t\\). We did this more explicitly in previous weeks, but the augment() function will do it for us here↩︎"
  },
  {
    "objectID": "07b_assumptdiag.html",
    "href": "07b_assumptdiag.html",
    "title": "7B: Assumptions, Diagnostics, and Troubleshooting",
    "section": "",
    "text": "This reading:\n\nWhat assumptions do we need to make when we use a linear model to generalise beyond our sample data (i.e. to make statements about the world)\nHow can we investigate how individual observations influence our model?\nWhat can we do when it looks like our assumptions are not satisfied?\nWhen we introduced simple linear regression, we talked about the assumptions that these models rely on (see 5B #a-first-look-at-linear-model-assumptions).\nWe talked about the very high-level view: \\[\n\\color{red}{outcome} \\color{black}{= }\\color{blue}{model}\\color{black}{ + error}\n\\] The idea here is that our error, which is represented by the residuals from our model (the deviations from observed values to model predicted values) should look more or less like randomness. If they don’t, then it indicates that something might be wrong.\nTo examine our assumptions, we can either rely on plotting, or on conducting statistical tests, or both."
  },
  {
    "objectID": "07b_assumptdiag.html#testing-normality",
    "href": "07b_assumptdiag.html#testing-normality",
    "title": "7B: Assumptions, Diagnostics, and Troubleshooting",
    "section": "testing normality",
    "text": "testing normality\nThe Shapiro-Wilk test (we saw this very briefly with our \\(t\\)-tests in 3B#assumptions) is a test against the alternative hypothesis that the residuals were not sampled from a normally distributed population. \\(p &gt;.05\\) indicates that we do not have evidence that the assumption has been violated. We can perform this test quickly in R using shapiro.test(residuals(modelname)).\n\nshapiro.test(residuals(mymodel))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(mymodel)\nW = 0.98161, p-value = 0.6215\n\nplot(mymodel, which = 2)\n\n\n\n\n\n\n\n\n\nModel residuals showed slight deviation from normality (see QQplot), but this was not deemed problematic as the Shapiro-Wilk test failed to reject the null hypothesis that the residuals were drawn from a normally distributed population (\\(W = 0.98\\), \\(p = 0.62\\))"
  },
  {
    "objectID": "07b_assumptdiag.html#testing-constant-variance",
    "href": "07b_assumptdiag.html#testing-constant-variance",
    "title": "7B: Assumptions, Diagnostics, and Troubleshooting",
    "section": "testing constant variance",
    "text": "testing constant variance\nThe ncvTest() function (from the car package) performs a test against the alternative hypothesis that the error variance changes with the level of the fitted values (also known as the “Breusch-Pagan test”). \\(p &gt;.05\\) indicates that we do not have evidence that the assumption has been violated.\n\nplot(mymodel, which = 3)\n\n\n\n\n\n\n\nlibrary(car)\nncvTest(mymodel)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 2.50283, Df = 1, p = 0.11364\n\n\n\nVisual inspection of suggested little sign of non-constant variance, with the Breusch-Pagan test failing to reject the null that error variance does not change across the fitted values (\\(\\chi^2(1)=2.5\\), \\(p = 0.11\\))"
  },
  {
    "objectID": "07b_assumptdiag.html#transformations",
    "href": "07b_assumptdiag.html#transformations",
    "title": "7B: Assumptions, Diagnostics, and Troubleshooting",
    "section": "Transformations",
    "text": "Transformations\nOne option that may be useful is to consider transforming your outcome variable. Common variables such as neuropsychological tests and scales have a lot of people bunched up near the bottom or top of the scale (referred to as the “ceiling” and “floor”). Transformations can spread out the bunched-up scores.\nFor an example, consider a scale where most people score fairly low:\n\nhist(agetest$score, breaks=40)\n\n\n\n\n\n\n\n\nIf we are interested in how scores are predicted by age, it may well be the case that the amount scores increase/decrease with age is different depending on how far up the scores we are thinking about. It may be that modelling the raw scores predicted by age results in some problematic assumption plots:\n\nmod &lt;- lm(score ~ age, data = agetest)\nplot(mod, which = 2)\n\n\n\n\n\n\n\n\nWe can transform the scores using something such as log()3. This basically stretches out the differences at the lower end of the scale. So scoring 2 vs scoring 3 is considered to be a bigger difference than scoring 20 vs 21:\n\ntibble(\n  raw = c(2,3,20,21),\n  log = log(raw)\n)\n\n# A tibble: 4 × 2\n    raw   log\n  &lt;dbl&gt; &lt;dbl&gt;\n1     2 0.693\n2     3 1.10 \n3    20 3.00 \n4    21 3.04 \n\n\nThe resulting distribution of transformed scores is closer to normally distributed, and the residuals of modelling lm(log(score) ~ age) looking less problematic:\n\nhist(log(agetest$score), breaks=40)\nmodlog &lt;- lm(log(score) ~ age, data = agetest)\nplot(modlog, which = 2)\n\n\n\n\n\n\n\n\nThe big downside of transforming an outcome variable is that you are no longer modelling the outcome variable on its original scale. Coefficients now represent “a 1 unit change in x is associated with a \\(b\\) change in log y”. Which makes it a lot harder to interpret.\nIt’s best to only transform your outcome variable if you have theoretical justification to do so."
  },
  {
    "objectID": "07b_assumptdiag.html#back-to-the-bootstrap",
    "href": "07b_assumptdiag.html#back-to-the-bootstrap",
    "title": "7B: Assumptions, Diagnostics, and Troubleshooting",
    "section": "Back to the Bootstrap?",
    "text": "Back to the Bootstrap?\nBootstrapping provides a different technique for drawing inferences. We actually learned the logic of bootstrapping back in 2B#standard-error-in-practice. It all comes down to the construction of the standard error. In the output of summary(model), we are given, alongside each coefficient \\(b\\), a standard error, a \\(t\\)-value and a \\(p\\)-value. These all rely on using a formula to calculate the standard error, and it is this which requires making assumptions about the distribution of residuals.\nBootstrapping is an approach that constructs a distribution of possible values for coefficient estimates \\(b\\), from which we can calculate a confidence interval. It does so by repeatedly resampling with replacement from the original sample (do check back to 2B#standard-error-in-practice for an example).\nRemember that conducting significance tests requires thinking about the variability that we would expect with repeated sampling. To do this, we use the standard error of a statistic (the standard deviation of doing this lots of times). Bootstrapping is a way to estimate the standard error without having to rely on a formula - bootstrapping acts out the process of repeated sampling, which give us a distribution of statistics we can compute the standard error from. Bootstrapping means we don’t have to rely on assumptions of normality and constant variance of the residuals. Because bootstrapping involves fitting 1000s of models to 1000s of resampled datasets, our residuals are different for each one.\nIn practice, this can all be done very easily in R by using the Boot() function from the car package. We can ask it to resample 1000 times (getting a distribution of 1000 values for the coefficients):\n\nmydata &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmymodel &lt;- lm(y ~ x1 + x2, data = mydata)\n\nlibrary(car)\nbootmymodel &lt;- Boot(mymodel, R = 1000)\n\nThe bootmymodel object actually contains lots of stuff (it contains all 1000 sets of coefficient estimates!). We can get some confidence intervals easily:\nNote: the actual estimates are those from our original model, it is just the bounds of the interval that bootstrapping is providing us with)\n\nConfint(bootmymodel)\n\nBootstrap bca confidence intervals\n\n              Estimate       2.5 %     97.5 %\n(Intercept) -2.3913797 -6.81733162  3.4833958\nx1           0.1757046  0.06226639  0.3014810\nx2          -0.6475619 -1.02709666 -0.2969874\n\n\n\nBootstrapping is not a panacea!!\nThe bootstrap may provide us with an alternative way of conducting inference, but our model may still be misspecified. Furthermore, studies show that bootstrap doesn’t perform well in small samples, and actually results increased Type 1 errors(3A#making-mistakes).\nIt’s very important to remember that we can’t get something from nothing. Bootstrapping is entirely reliant on utilising our original sample to pretend that it is a population (and mimick sampling from that population). If our original sample is not representative of the population that we’re interested in, bootstrapping doesn’t help us at all."
  },
  {
    "objectID": "07b_assumptdiag.html#footnotes",
    "href": "07b_assumptdiag.html#footnotes",
    "title": "7B: Assumptions, Diagnostics, and Troubleshooting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQQplots plot the values against the associated percentiles of the normal distribution. So if we had ten values, it would order them lowest to highest, then plot them on the y against the 10th, 20th, 30th.. and so on percentiles of the standard normal distribution (mean 0, SD 1)↩︎\nFor an example, consider two variables: foot length (in cm) and UK shoe size. These will be very strongly correlated. If we used them both to predict some outcome like height, then trying to imagine “holding shoe size constant, having feet 1cm longer is associated with \\(b\\) increase in height” doesn’t make sense. Because having feet 1cm longer will very likely also mean wearing a bigger shoe size. In cases like this, the regression surface becomes unstable.↩︎\nanother useful transformation is BoxCox() from the forecast package↩︎"
  },
  {
    "objectID": "08_ex.html",
    "href": "08_ex.html",
    "title": "Exercises: Scaling | Categorical Predictors",
    "section": "",
    "text": "neuronews.csv\nThis dataset is from a study1 looking at the influence of presenting scientific news with different pictures on how believable readers interpret the news.\n120 participants took part in the study. Participation involved reading a short article about some research in neuroscience, and then rating how credible they found the research. Participants were randomly placed into one of three conditions, in which the article was presented a) in text alone, b) with a picture of a brain, or c) with a picture of a brain and a fancy looking (but unrelated to the research) graph. They rated credibility using a sliding scale from 0 to 100, with higher values indicating more credibility.\nThe data is available at https://uoepsy.github.io/data/usmr_neuronews.csv.\n\n\n\n\n\nusmr_neuronews.csv data dictionary\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    pid\nParticipant ID\n    name\nParticipant Name\n    condition\nCondition (text-only / text+brain / text+brain+graph)\n    credibility\nCredibility rating (0-100)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and take a look around (this is almost always the first thing to do!)\n\n\n\n\n\nSolution\n\n\n\n\nnndat &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_neuronews.csv\")\n\nhist(nndat$credibility)\n\n\n\n\n\n\n\n# geom jitter is a way of randomly 'jittering' points so that the don't overlap\n# i want them to be the right height, so no jitter in height, but i'll give them a little width jitter\nggplot(nndat, aes(x=condition,y=credibility)) +\n   geom_jitter(height = 0, width = .2)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nFit a model examining whether credibility of the research article differs between conditions.\n\n\n\n\n\nSolution\n\n\n\n\nmod1 &lt;- lm(credibility ~ condition, data = nndat)\n\n\n\n\n\nQuestion 3\n\n\nDo conditions differ in credibility ratings?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is an overall question, not asking about differences between specific levels. You can find a way to test this question either at the bottom of the summary() output, or by comparing it with a model without condition differences in it (see 8B #testing-group-differences).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe can either do this as a model comparison:\n\nmod0 &lt;- lm(credibility ~ 1, data = nndat)\nanova(mod0, mod1)\n\nAnalysis of Variance Table\n\nModel 1: credibility ~ 1\nModel 2: credibility ~ condition\n  Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    119 12820                              \n2    117 12146  2    674.03 3.2464 0.04245 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nand this info is also at the bottom of the summary, because we just have one predictor in the model\n\nsummary(mod1)\n\nF-statistic: 3.246 on 2 and 117 DF,  p-value: 0.04245\n\nConditions significantly differed in credibility ratings \\(F(2,117) = 3.25, p = .0425\\)\n\n\n\n\n\nQuestion 4\n\n\nHow do groups differ?\n\n\n\n\n\n\nHints\n\n\n\n\n\nNote that this is a subtly different question to the previous one. It will require us to look at something that tests between specific groups (8B #testing-differences-between-specific-groups).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nsummary(mod1)\n\n\nCall:\nlm(formula = credibility ~ condition, data = nndat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.4476  -6.6086  -0.3778   6.5657  26.8589 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 59.663      1.611  37.035   &lt;2e-16 ***\nconditiontext+brain          4.909      2.278   2.155   0.0332 *  \nconditiontext+brain+graph    5.138      2.278   2.255   0.0260 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.19 on 117 degrees of freedom\nMultiple R-squared:  0.05258,   Adjusted R-squared:  0.03638 \nF-statistic: 3.246 on 2 and 117 DF,  p-value: 0.04245\n\n\n\nCompared to when presented in text-only, conditions where the article was presented alongside a picture of a brain, or alongside both a brain-picture and a graph, resulted in higher average credibility ratings. Including a picture of a brain was associated with a 4.91 increase in credibility over the text-only article (\\(b = 4.91\\), \\(t(117)=2.15\\), \\(p=0.033\\)), and including both a brain-picture and a graph was associated with a 5.14 higher average credibility rating (\\(b = 5.14\\), \\(t(117)=2.26\\), \\(p=0.026\\)).\n\n\n\n\n\nQuestion 5\n\n\nLet’s prove something to ourselves.\nBecause we have no other predictors in the model, it should be possible to see how the coefficients from our model map exactly to the group means.\nCalculate the mean credibility for each condition, and compare with your model coefficients.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo calculate group means, we can use group_by() and summarise()!\n\n\n\n\n\n\n\n\nSolution Part 1 - calculate group means\n\n\n\n\nnndat |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    meancred = mean(credibility)\n  )\n\n# A tibble: 3 × 2\n  condition        meancred\n  &lt;chr&gt;               &lt;dbl&gt;\n1 text+brain           64.6\n2 text+brain+graph     64.8\n3 text-only            59.7\n\n\n\n\n\n\n\nSolution Part 2 - compare to the coefficients\n\n\n\nHere are our model coefficients:\n\ncoef(mod1)\n\n              (Intercept)       conditiontext+brain conditiontext+brain+graph \n                59.662662                  4.909058                  5.138197 \n\n\nAnd here are our means:\n\nnndat |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    meancred = mean(credibility)\n  )\n\n# A tibble: 3 × 2\n  condition        meancred\n  &lt;chr&gt;               &lt;dbl&gt;\n1 text+brain           64.6\n2 text+brain+graph     64.8\n3 text-only            59.7\n\n\nWe can see that the intercept is the mean of the text-only group.\nThe next coefficient - named “conditiontext+brain” is the difference from the text-only group to the text+brain group.\nThe final coefficient - named “conditiontext+brain+graph” is the difference from the text-only group to the text+brain+graph group.\nSo this means we can get to our group means by using:\n\ncoef(mod1)[c(1,2)] |&gt; sum() # the mean of text+brain\n\n[1] 64.57172\n\ncoef(mod1)[c(1,3)] |&gt; sum() # the mean of text+brain+graph\n\n[1] 64.80086"
  },
  {
    "objectID": "08_ex.html#footnotes",
    "href": "08_ex.html#footnotes",
    "title": "Exercises: Scaling | Categorical Predictors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnot a real one, but inspired very loosely by this one↩︎\napparently coughing is a method of immediately lowering heart rate!↩︎"
  },
  {
    "objectID": "08a_scaling.html",
    "href": "08a_scaling.html",
    "title": "8A: Centering and Scaling",
    "section": "",
    "text": "For this section we’re going to play with some random data looking at whether peoples’ resting heart rates depend on how much sleep they get.\nOur data contains 70 people, for which we have a variety of measures. The only ones we are going to concern ourselves with are heart rate (HR) and hours slept (hrs_sleep), but there are plenty of other ones for you to play around with if you like\nhrdat &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\nhead(hrdat)\n\n# A tibble: 6 × 8\n  name        age height shoe_size hrs_sleep ampm  smoke    HR\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 Biancha      42   151.        39      14   pm    n        46\n2 Werner       71   160.        39       6.3 pm    n        73\n3 Martin       NA   182         43       9.5 pm    n        62\n4 Danika       60   181.        35       9.1 am    n        66\n5 Thuy Minh    25   178         42      11.3 am    n        44\n6 Jena         22   169.        44      11.5 pm    n        53"
  },
  {
    "objectID": "08a_scaling.html#transformations-prepost",
    "href": "08a_scaling.html#transformations-prepost",
    "title": "8A: Centering and Scaling",
    "section": "Transformations, pre/post",
    "text": "Transformations, pre/post\nWhen we apply these transformations we can do so either during the process of fitting the model (e.g. by using scale() inside the lm() function as we have just seen). We can also do this prior to fitting the model, by creating a new variable using code like hrs_sleepZ = scale(hrs_sleep), and then using that variable in the model.\nIt is also possible to work out the slope for transformed variables after we’ve just fitted our original model. This is because when we scale a predictor, all that happens to our coefficient is that it gets scaled accordingly. Consider the example in Figure 2, where we change from using hours to minutes. To do this we can just multiply our hrs_sleep variable by 60 (so that, e.g., 1.5 hours becomes 90 minutes).\nThe coefficient from the model changes from\n“change in HR for every 1 hour of sleep”\nto\n“change in HR for every 1 minute of sleep”\nbut we can actually do that calculation ourselves, because 1 minute is 1/60th of an hour. So we already know that \\(\\frac{\\text{change in HR for every 1 hour of sleep}}{60}\\) is the same thing as “change in HR for every 1 minute of sleep”.\n\n\n\n\n\nFigure 2: Changing from hours_slept to minutes_slept will make our coefficient 1/60th of the size\n\n\n\n\nThe same applies when we standardise a predictor. Since standardising involves dividing a variable by its standard deviation, the coefficient for the standardised variable will be the original coefficient multiplied by the variable’s standard deviation.\n\n\nOriginal coefficients\nlm(HR ~ hrs_sleep, data = hrdat)\n\ncoef(mod_orig)\n\n(Intercept)   hrs_sleep \n  95.055220   -3.503526 \n\n\n\n\n\nStandardised coefficients\nlm(HR ~ scale(hrs_sleep), data = hrdat)\n\ncoef(mod_z)\n\n     (Intercept) scale(hrs_sleep) \n       64.314286        -8.363635 \n\n\n\n\nThe standard deviation of hrs_sleep we can calculate:\n\nsd(hrdat$hrs_sleep) \n\n[1] 2.387205\n\n\nAnd we can move from the original coefficient to the standardised one!\n\n# original coefficient multiplied by sd of hrs sleep\ncoef(mod_orig)[2] * sd(hrdat$hrs_sleep)\n\nhrs_sleep \n-8.363635"
  },
  {
    "objectID": "08a_scaling.html#apples-and-oranges",
    "href": "08a_scaling.html#apples-and-oranges",
    "title": "8A: Centering and Scaling",
    "section": "apples and oranges",
    "text": "apples and oranges\nVery often, people will consider a standardised coefficient to be a unitless measure of “size of effect size” that they can go and happily compare with other results (be it from another coefficient from the model, or a coefficient from a different study altogether, perhaps even on a different population!).\nHowever, a very important thing to remember is that standardised coefficients are dependent upon the sample standard deviations.\nThis means that any comarisons between standardised coefficients comparisons could be due to an actual difference in magnitude of the underlying relationship, but it could just as easily be due to differences in the standard deviations of the variables.\nAs a toy example, I have two datasets, each of 100 people. The first has people aged 18 to 40. The second has people aged 18 to 80. Both datasets have been taken from a population where the underlying linear relationship between age and vocabulary is \\(vocab_i = 10 + 1 \\cdot age_i\\). So the “association between age and vocabulary” should be more or less the same for both datasets (as seen in Figure 4).\n\n\n\n\n\nFigure 4: Two hypothetical studies, with different age ranges of participants\n\n\n\n\nBut the standardised coefficients for the second dataset will always be bigger, because the variance in the age variable is bigger.\nThis is plainly clear from when we remember that the standardised coefficient is simply the original raw age coefficient multiplied by the standard deviation of age (either \\(b \\cdot s_{age}\\) or \\(b \\cdot \\frac{s_{age}}{s_{vocab}}\\)). In the 2nd study, \\(s_{age}\\) is bigger, but (as clearly evident in the plot above) we don’t want to be saying that age has a bigger effect on vocabulary in study 2..\n\n\n# A tibble: 4 × 3\n  estimate                  `study 1, ages 18-40` `study 2, ages 18-80`\n  &lt;chr&gt;                                     &lt;dbl&gt;                 &lt;dbl&gt;\n1 sd(age)                                   6.03                 16.5  \n2 vocab ~ age                               1.08                  1.02 \n3 vocab ~ scale(age)                        6.50                 16.8  \n4 scale(vocab) ~ scale(age)                 0.911                 0.987\n\n\nThe take-home of this is that sometimes we just can’t compare apples and oranges. And sometimes we can’t even compare apples to other apples! Choosing whether or not to standardise coefficients is going to depend on many things, and sometimes the easiest thing is simply to report both raw and standardised coefficients."
  },
  {
    "objectID": "08a_scaling.html#footnotes",
    "href": "08a_scaling.html#footnotes",
    "title": "8A: Centering and Scaling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsome people use \\(\\beta\\) for a normal coefficient, and \\(\\beta^*\\) for a standardised coefficient↩︎"
  },
  {
    "objectID": "08b_catpred.html",
    "href": "08b_catpred.html",
    "title": "8B: Categorical Predictors",
    "section": "",
    "text": "Since we’ve started working with linear regression, we’ve seen a few examples of categorical variables as predictors in linear models, and most (if not all) of these have been binary predictors - i.e. having just two categories (yes/no, dog/cat, phonics/word). We talked initially (5B#binary-predictors) about how these get entered in the model as 0s and 1s.\nLet’s consider a dataset which includes the brain mass and body mass of different primates. Below, we can see the species variable has 3 levels indicating whether the observation is a Human, a Rhesus monkey, or a Potar monkey. We can easily create a binary variable such as the isMonkey variable (created below), which will allow us to compare monkeys (of any type) vs not-monkeys (or “Humans”, as we are better known!).\n\nlibrary(tidyverse)\nbraindata &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_braindata.csv\")\n\nbraindata &lt;- braindata %&gt;% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\")\n)\nhead(braindata)\n\n# A tibble: 6 × 4\n  species       mass_brain   age isMonkey\n  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 Rhesus monkey      0.449     5 YES     \n2 Human              0.577     2 NO      \n3 Potar monkey       0.349    30 YES     \n4 Human              0.626    27 NO      \n5 Potar monkey       0.316    31 YES     \n6 Rhesus monkey      0.398    11 YES     \n\n\nTo recap what we’ve seen already - when we use a binary variable like isMonkey in a model as a predictor, it gets inputted into the model as a series of 0s and 1s. Because the coefficients of regression models are always interpreted as “the change in \\(y\\) associated with a 1 unit change in \\(x\\)”, the use of 0s and 1s for different levels of a categorical variable allows us to make “a 1 unit change in \\(x\\)” represent moving from one level to another. Internally, our model is relying on a variable like the isMonkeyYES variable below:\n\n\n# A tibble: 6 × 5\n  species       mass_brain   age isMonkey isMonkeyYES\n  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Rhesus monkey      0.449     5 YES                1\n2 Human              0.577     2 NO                 0\n3 Potar monkey       0.349    30 YES                1\n4 Human              0.626    27 NO                 0\n5 Potar monkey       0.316    31 YES                1\n6 Rhesus monkey      0.398    11 YES                1\n\n\nThis means that we can get out the model below an estimate of the difference in brain mass from the group isMonkey == \"NO\" to the group isMonkey == \"YES\", because this is moving from 0 to 1. We can see this in the model and visualisation below.\n\n\n\n\nmonkmod &lt;- lm(mass_brain~isMonkey, data = braindata)\nsummary(monkmod)\n\n\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.60271    0.03475  17.346  &lt; 2e-16 ***\nisMonkeyYES -0.25986    0.04486  -5.793 1.78e-06 ***\n\n\n\n(Intercept): the estimated brain mass of Humans (when the isMonkeyYES variable is zero)\nisMonkeyYES: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1).\n\n\n\n\n\n\n\n\n\nFigure 1: A binary categorical predictor\n\n\n\n\n\n\nWhen used as predictors in multiple regression models, binary variables behave much the same way. The coefficient will give us the estimated change in \\(y\\) when moving from one level to the other1, while holding other predictors constant.\n\nmonkmod2 &lt;- lm(mass_brain~age + isMonkey, data = braindata)\nsummary(monkmod2)\n\n\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.668308   0.050478  13.240 1.57e-14 ***\nage         -0.004081   0.002337  -1.746   0.0904 .  \nisMonkeyYES -0.246933   0.044152  -5.593 3.54e-06 ***\n\n\n\n(Intercept): the estimated brain mass of new-born Humans (when both age is zero and isMonkeyYES is zero)\nage: the estimated change in brain mass for every 1 year increase in age, holding isMonkey constant.\nisMonkeyYES: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1), holding age constant.\n\n\n\n\n\n\n\noptional: a visual\n\n\n\n\n\nIf we want a visual intuition to how a binary predictor works in multiple regression - it’s actually just like any other predictor, in that it’s just another dimension to the model. The only difference is that it is on a discrete scale - observations fall on either 0 or 1, not on the continuum in between.\nSo for the model mass_brain ~ age + isMonkey, rather than a 3D surface, we might think of our model as two edges of a surface, as shown in the left-hand panel of Figure 2. And if ‘monkey-ness’ was some sort of measurable continuum (“I’m 70% monkey?”) then this just joins those edges up back to a surface (right panel).\n\n\n\n\n\nFigure 2: a binary predictor is just another dimension, but data can only exist at either 0 or 1. If a variable were continuous, then observations can take any value along a line"
  },
  {
    "objectID": "08b_catpred.html#tests",
    "href": "08b_catpred.html#tests",
    "title": "8B: Categorical Predictors",
    "section": "Tests",
    "text": "Tests\nWe have just seen how entering a categorical predictor with \\(k\\) levels in as a predictor to a regression model is essentially equivalent to adding in \\(k-1\\) different binary variables. We get out \\(k-1\\) coefficients relating to each of those variables.\nIn our primate-brains example, our model lm(mass_brain ~ species) might look like it only has one predictor (species), but because it has 3 levels, we end up with 2 coefficients - we are essentially making a multiple regression model. So what can we test when it comes to species? We can test comparisons between specific levels, but we can also ask an overall question of “do species differ?”\n\nTesting differences between specific groups\nTthe coefficients from our regression model represent specific comparisons between levels of a categorical predictor. The default behaviour in R is (as we have seen) to compare each level to the reference level.\nThe tests (Std. Error, t value, df, p value) associated of these coefficient estimates therefore provide us with a means of testing whether the difference in \\(y\\) between two levels is significantly different from zero.\nFor example, from our model output below, we can say that..\n\n… both Potar monkeys and Rhesus monkeys had significantly lower brain mass than Humans (\\(\\beta=-0.357,t(32)=-8.627, p&lt;.001\\) and \\(\\beta=-0.153,t(32)=-3.585, p=.0011\\) respectively).\n\n\n\n\nCall:\nlm(formula = mass_brain ~ species, data = braindata)\n...\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.60271    0.02748  21.936  &lt; 2e-16 ***\nspeciesPotar monkey  -0.35735    0.04142  -8.627 7.38e-10 ***\nspeciesRhesus monkey -0.15261    0.04257  -3.585   0.0011 ** \n\n\n\n\nTesting ‘group differences’\nIt will hopefully come as little surprise that the way to test an overall question of “are there differences between groups” can be assessed by a model comparison between models with and without the predictor.\nWhy? because this allows us to use the F test as a “joint test” (see 7A #model-comparisons) of whether including all the information about groups explains a significant amount of additional variance in our outcome (i.e. if their inclusion provides a significant reduction in residual sums of squares).\nSo from our model comparison below, we can say something like…\n\n… species differences (Human/Potar monkey/Rhesus monkey) explained a significant amount of variance in brain mass over the null model (\\(F(2, 32) = 37.22,\\ p &lt;.001\\)).\n\n\nm0 &lt;- lm(mass_brain ~ 1, data = braindata)\nm1 &lt;- lm(mass_brain ~ species, data = braindata)\nanova(m0, m1)\n\nAnalysis of Variance Table\n\nModel 1: mass_brain ~ 1\nModel 2: mass_brain ~ species\n  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     34 1.12500                                 \n2     32 0.33822  2   0.78678 37.22 4.453e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "08b_catpred.html#treatment-contrasts-the-default",
    "href": "08b_catpred.html#treatment-contrasts-the-default",
    "title": "8B: Categorical Predictors",
    "section": "Treatment Contrasts (the default)",
    "text": "Treatment Contrasts (the default)\n“Treatment contrasts” are the default that R uses. These are the ones we’ve already discussed above. It compares each level to a reference level. A common example is to compare people taking different ‘treatments’ (drug A, drug B and drug C) to a placebo group (the reference level).\nWhen we use this approach:\n\nthe intercept is the estimated y when all predictors are zero. Because the reference level is kind of like “0” in our contrast matrix, this is part of the intercept estimate.\n\nwe get out a coefficient for each subsequent level, which are the estimated differences from each level to the reference group."
  },
  {
    "objectID": "08b_catpred.html#sum-contrasts",
    "href": "08b_catpred.html#sum-contrasts",
    "title": "8B: Categorical Predictors",
    "section": "Sum Contrasts",
    "text": "Sum Contrasts\n“sum contrasts” (sometimes called “deviation contrasts” and “effects coding”) are the next most commonly used in psychological research. These are a way of comparing each level to the overall mean.\nThis involves a bit of trickery that uses -1s and 1s rather than 0s and 1s, in order to make “0” be mid-way between all the levels - the average of the levels.\nWe can adjust the coding scheme that we use like so:\n\ncontrasts(braindata$isMonkey) &lt;- \"contr.sum\"\ncontrasts(braindata$isMonkey)\n\n    [,1]\nNO     1\nYES   -1\n\n\n\nnote that the column of the contrast matrix no longer has a name! It’s just got a [,1]. This means that the coefficient we get out is not going to have a name either!!!\n\n\nmonkmod_sum &lt;- lm(mass_brain~isMonkey, braindata)\nsummary(monkmod_sum)\n\n\n\n\nCall:\nlm(formula = mass_brain ~ isMonkey, data = braindata)\n...\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.47279    0.02243  21.079  &lt; 2e-16 ***\nisMonkey1    0.12993    0.02243   5.793 1.78e-06 ***\n\n\nThe intercept from this model is the estimated average brain mass averaged across monkeys and non-monkeys. i.e. the estimated ‘grand mean’ brain mass.\nThe coefficient represents moving from the overall mean brain mass to the isMonkey==\"NO\" mean brain mass.3 This is visualised in Figure 4.\n\n\n\n\n\nFigure 4: A binary categorical predictor with sum contrasts\n\n\n\n\n\n\n\n\n\n\noptional: -1/1 vs -.5/.5\n\n\n\n\n\nFor sum contrasts, sometimes people prefer to use -.5 and .5 instead of -1 and 1. This is because it keeps the intercept as the “grand mean”, but makes the coefficient represent the difference between the two groups (which might be more useful as a number to talk about).\nThis works because with just these two groups, the distance from isMonkey==YES to isMonkey==NO is twice as far as the distance from the grand mean to the isMonkey==NO group (because the grand mean is the mid-point).\nBy halving the contrast, it doubles our coefficient (because ‘a change in 1’ is now twice as far).\n\ncontrasts(braindata$isMonkey) &lt;- c(.5, -.5)\ncontrasts(braindata$isMonkey)\n\n    [,1]\nNO   0.5\nYES -0.5\n\n\n\nmonkmod_sum &lt;- lm(mass_brain~isMonkey, braindata)\nsummary(monkmod_sum)\n\n\n\n\nCall:\nlm(formula = mass_brain ~ isMonkey, data = braindata)\n...\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.47279    0.02243  21.079  &lt; 2e-16 ***\nisMonkey1    0.25986    0.04486   5.793 1.78e-06 ***\n\n\n\n\n\n\n\nFigure 5: A binary categorical predictor with sum contrasts using -.5 and .5\n\n\n\n\n\n\n\nWhen we move to using variables with more than 2 levels, sum contrasts can look a lot more confusing, but the logic of how we interpret coefficients stays very much the same.\nFor instance, using sum contrasts with the species variable:\n\ncontrasts(braindata$species) &lt;- \"contr.sum\"\ncontrasts(braindata$species)\n\n              [,1] [,2]\nHuman            1    0\nPotar monkey     0    1\nRhesus monkey   -1   -1\n\n\n\nspecmod_sum &lt;- lm(mass_brain ~ species, braindata)\nsummary(specmod_sum)\n\n\n\n\nCall:\nlm(formula = mass_brain ~ species, data = braindata)\n...\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.43273    0.01755  24.654  &lt; 2e-16 ***\nspecies1     0.16999    0.02366   7.185 3.70e-08 ***\nspecies2    -0.18736    0.02507  -7.474 1.65e-08 ***\n\n\n\nOur intercept is the ‘grand mean’ (the mean of each species’ estimated mean brain mass).\nOur first coefficient is the difference from the grand mean to the mean of humans.\nOur second coefficient is the difference from the grand mean to the mean of Potar monkeys.\n\n\n\n\n\n\n\noptional: where have my rhesus monkeys gone?\n\n\n\n\n\nIt feels a bit odd, but we no longer have an estimate for our Rhesus monkeys.\nThis felt okay when we knew they were just being collapsed into our intercept, but where are they now? Our intercept is the grand mean. Where are my Rhesus Monkeys??\nAs stated above, our intercept is the mean of each species’ estimated mean brain mass (the ‘grand mean’). We can write this as:\n\\[\n\\begin{align}\n\\text{(Intercept)} &: \\frac{\\bar{R}+\\bar{P}+\\bar{H}}{3} = 0.43\\\\\n\\text{where:}&\\\\\n&\\bar{R}, \\,\\bar{P}, \\, \\bar{H} \\text{ are the mean brain mass for }\\\\\n&\\text{Rhesus monkeys, Potar monkeys, and Humans respectively}\n\\end{align}\n\\]\nThe second coefficient of our model represents the difference from this value to the mean brain mass of humans, and the third represents the difference from this value to the mean brain mass of Potar monkeys:\n\\[\n\\begin{align}\n\\text{species1} &: \\bar{H} - 0.43 = 0.17 \\\\\n\\text{species2} &: \\bar{P} - 0.43 = -0.19 \\\\\n\\end{align}\n\\] We can rewrite these to find those means for Humans and Potars as:\n$$ \\[\\begin{align}\n\\bar{H} &= \\underbrace{0.43}_{\\text{intercept}} &+ \\underbrace{0.17}_{\\text{species1 coefficient}}  &= &0.6\\\\\n\\, \\\\\n\\bar{P} &= \\underbrace{0.43}_{\\text{intercept}} &+ \\underbrace{-0.19}_{\\text{species2 coefficient}}  &= &0.24\\\\\n\n\\end{align}\\] $$ Our Rhesus monkeys are actually still there in our intercept! They’re just only represented as a third of the intercept (the other two thirds being the humans and potar monkeys). If we substitute in our \\(\\bar H\\) and \\(\\bar P\\) values to our intercept:\n\\[\n\\begin{align}\n\\text{(Intercept)} :&\\, \\frac{\\bar{R}+\\bar{P}+\\bar{H}}{3} = 0.43\\\\\n\\, \\\\\n& \\frac{\\bar{R}+0.24+0.6}{3} = 0.43\\\\\n\\, \\\\\n& \\bar{R}+0.24+0.6 = 3 \\times 0.43\\\\\n& \\bar{R}+0.24+0.6 = 1.29\\\\\n& \\bar{R} = 1.29 - 0.24 - 0.6\\\\\n& \\bar{R} = 0.45\\\\\n\\end{align}\n\\] And there we have our Rhesus monkeys! Because there are no other predictors in our model, this should match exactly (with rounding error in the above calculations!) with what the mean brain mass of Rhesus monkeys is in our data:\n\nmean(braindata$mass_brain[braindata$species==\"Rhesus monkey\"])\n\n[1] 0.4501"
  },
  {
    "objectID": "08b_catpred.html#optional-and-many-more..",
    "href": "08b_catpred.html#optional-and-many-more..",
    "title": "8B: Categorical Predictors",
    "section": "Optional: and many more..",
    "text": "Optional: and many more..\nThere are a whole load of other types of contrasts we can use, and we can even set custom ones of our own. The choices are many, and confusing, and it really depends on what exactly we want to get out of our model, which is going to depend on our research.\nSome useful resources for your future research:\n\nA page showing many many different contrast coding schemes (with R code and interpretation): https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/\nThe emmeans package (“estimated marginal means”) can come in handy for lots and lots of ways to compare groups. The package ‘vignette’ is at https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html"
  },
  {
    "objectID": "08b_catpred.html#footnotes",
    "href": "08b_catpred.html#footnotes",
    "title": "8B: Categorical Predictors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nand the intercept will be the estimated \\(y\\) when all predictors are zero↩︎\nThis is all assuming that we have things set to their defaults in R. We’ll see below that we can change things up using something called ‘contrasts’.↩︎\nwe know it is to this group because a 1 increase in the column of our contrast matrix takes us to this group↩︎"
  },
  {
    "objectID": "09_ex.html",
    "href": "09_ex.html",
    "title": "Exercises: Interactions!",
    "section": "",
    "text": "Processing a hangover\n\nDataset: hangover_speed.csv\nHow is hours of sleep associated with processing speed? Is this dependent upon whether or not alcohol was consumed the previous night? 107 participants completed the Symbol Digit Modalities Task (SDMT), a measure of processing speed. Participants also recorded how many hours they had slept the previous night (to the nearest 15 mins), and whether or not they had consumed alcohol.\nThe dataset is available at https://uoepsy.github.io/data/hangover_speed.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    hrs_sleep\nhours slept the previous night (to the nearest 15 mins)\n    alc\nwas alcohol consumed the previous evening? ('y'=yes, 'n'=no)\n    sdmt\nscore on the Symbol Digit Modalities Task (SDMT), a measure of processing speed (range 0 to 100)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and provide some simple descriptives.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWhat is the mean score on the SDMT, what is the variability in scores?\n\nHow many people had alcohol the previous night?\n\nHow many hours did people sleep on average? Did this vary between the drinkers and the non-drinkers?\n\n\n\n\n\n\n\n\nHere’s our data. Everything looks within plausible ranges for our two continuous variables hrs_sleep and sdmt.\n\nhodat &lt;- read_csv(\"https://uoepsy.github.io/data/hangover_speed.csv\")\n\nWe can get the means and SDs for hours slept and the SDMT:\n\nlibrary(psych)\ndescribe(hodat |&gt; select(-alc))\n\n          vars   n  mean    sd median trimmed   mad   min  max range skew\nhrs_sleep    1 107  7.20  1.20   7.25    7.18  1.11  4.25 10.5  6.25 0.20\nsdmt         2 107 54.59 12.13  53.00   54.23 10.38 22.00 86.0 64.00 0.28\n          kurtosis   se\nhrs_sleep     0.07 0.12\nsdmt          0.08 1.17\n\n\nWe can see that 57% of participants didn’t drink, and 43% of them did:\n\ntable(hodat$alc)\n\n\n n  y \n61 46 \n\n\nAnd it doesn’t look like they differed very much in their sleep times:\n\nhodat |&gt; group_by(alc) |&gt;\n  summarise(\n    slept = mean(hrs_sleep)\n  )\n\n# A tibble: 2 × 2\n  alc   slept\n  &lt;chr&gt; &lt;dbl&gt;\n1 n      7.12\n2 y      7.30\n\n\n\n\n\n\nQuestion 2\n\n\nMake a plot of SDMT predicted by hours slept, and colour the points by whether or not the participants had drank alcohol.\nCan you plot a separate lm line on the graph using geom_smooth for each group (alcohol v no alcohol)?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nto make geom_smooth() fit a linear model (lm), remember to use geom_smooth(method=lm).\n\nif you have the grouping in the aes(), then when you add geom_smooth() it should make a different one for each group!\n\n\n\n\n\n\n\n\n\nggplot(hodat, aes(x = hrs_sleep, y = sdmt, col = alc)) +\n  geom_point() +\n  geom_smooth(method=lm)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nAdding a different geom_smooth(method=lm) for each group is just fitting a different model to each groups’ data - i.e. a slope of sdmt~hrs_sleep for the drinkers and a slope for the non-drinkers.\nBut we actually want to test if the two slopes are different, and for that we need to create a model which includes the appropriate interaction term.\nFit the model to examine whether the association between hrs_sleep and sdmt is different depending upon alcohol consumption.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is the same logic as the air-pollution & APOE-4 example in 9A #it-depends.\n\n\n\n\n\n\n\n\nmod_int &lt;- lm(sdmt ~ hrs_sleep * alc, data = hodat)\nsummary(mod_int)\n\n\nCall:\nlm(formula = sdmt ~ hrs_sleep * alc, data = hodat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3384  -5.2919  -0.2911   6.6458  19.7389 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.1496     6.9631   0.452   0.6520    \nhrs_sleep        7.6499     0.9643   7.933 2.71e-12 ***\nalcy            17.7151    10.6874   1.658   0.1004    \nhrs_sleep:alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.914 on 103 degrees of freedom\nMultiple R-squared:  0.4753,    Adjusted R-squared:   0.46 \nF-statistic:  31.1 on 3 and 103 DF,  p-value: 2.136e-14\n\n\n\n\n\n\nQuestion 4\n\n\nInterpret each coefficient from your model.\n\n\n\n\n\n\nHints\n\n\n\n\n\nOur interaction involves a continuous variable (hrs_sleep) and a binary variable (alc). An interpretation of a similar example is in 9A #interpretation.\n\n\n\n\n\n\n\n\nsummary(mod_int)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.1496     6.9631   0.452   0.6520    \nhrs_sleep        7.6499     0.9643   7.933 2.71e-12 ***\nalcy            17.7151    10.6874   1.658   0.1004    \nhrs_sleep:alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\n\n\n\n\n\n\n  \n    \n    \n      coefficient\n      estimate\n      interpretation\n    \n  \n  \n    (Intercept)\n3.15\nA non-drinker who slept 0 hours is estimated to have an SDMT score of 3.1 - note this is not significantly different from zero\n    hrs_sleep\n7.65\nFor every additional hour slept, non-drinkers scores on SDMT are estimated to increase by 7.6\n    alcy\n17.72\ndrinkers who slept zero hours are estimated to have 17.7 higher scores on SDMT than non-drinkers who slept zero hours - note this is not significantly different from zero\n    hrs_sleep:alcy\n-3.59\nFor every additional hour's sleep, drinkers scores on SDMT  increase by 3.6 *less* than they do for non-drinkers\n  \n  \n  \n\n\n\n\n\n\n\n\nQuestion 5\n\n\nConstruct a plot of the model estimated associations between hours-slept and SDMT for drinkers and non-drinkers.\nBecause we have nothing else in our model, this should end up looking exactly the same as our initial plot in Question 2!\n\n\n\n\n\n\nHints\n\n\n\n\n\nIt all follows the same logic as we have used before:\n\nmake a dataframe of the values of the predictors that we wish to plot across\nusing augment(), add to that the predicted values of the model, and the associated confidence intervals\nshove it all in ggplot!\n\nBecause we are wanting to plot across multiple predictors (i.e. we want to plot across a range of hrs_slept and both values of alc), try using expand_grid().\nPlay around with this to see what it does:\n\n\n# A tibble: 15 × 2\n   continuous binary\n        &lt;int&gt; &lt;chr&gt; \n 1          1 dog   \n 2          1 cat   \n 3          1 parrot\n 4          2 dog   \n 5          2 cat   \n 6          2 parrot\n 7          3 dog   \n 8          3 cat   \n 9          3 parrot\n10          4 dog   \n11          4 cat   \n12          4 parrot\n13          5 dog   \n14          5 cat   \n15          5 parrot\n\n\nIf you get stuck, a very similar example is in 9A #visualisation.\n\n\n\n\n\n\n\n\n# plot data\nplotdat &lt;- expand_grid(\n  hrs_sleep = 0:14,\n  alc = c(\"n\",\"y\")\n)\n# plot\nbroom::augment(mod_int, newdata = plotdat, interval=\"confidence\") |&gt;\n  ggplot(aes(x= hrs_sleep, y = .fitted, \n             col = alc, fill = alc)) + \n  geom_line() +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nNo one in our dataset has slept zero hours, and i’m probably not interested in differences between drinkers/non-drinkers who never sleep.\nRefit the model to adjust the intercept to a more meaningful value. Everyone always goes on about 8 hours of sleep being the minimum?\nHow has the interpretation of your coefficient(s) changed?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee 9A #mean-centering for an example of mean-centering a predictor in the interaction. Remember that there are multiple ways to do this - you could make a new variable first, or you could do it all inside the model.\n\n\n\n\n\n\n\n\nSolution Part 1 - recentering and refitting\n\n\n\nIf we want to recenter our hrs_sleep variable on 8 hours, we could either create a new variable which is hrs_sleep - 8 (which would therefore make everyone who slept 8 hours be given a value of 0 in the variable), or we could do it in the model:\n\n\nre-centering first\n\nhodat &lt;- \n  hodat |&gt;\n    mutate(\n      hrs_8 = hrs_sleep - 8\n    )\n\nmod_int2 &lt;- lm(sdmt ~ hrs_8 * alc, data = hodat)\n\n\n\ndoing it all in the model\n\nmod_int2 &lt;- lm(sdmt ~ I(hrs_sleep-8) * alc, data = hodat)\n\n\n\n\n\n\n\n\nSolution Part 2 - interpreting coefficients\n\n\n\n\nsummary(mod_int2)\n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            64.3487     1.4206  45.297  &lt; 2e-16 ***\nI(hrs_sleep - 8)        7.6499     0.9643   7.933 2.71e-12 ***\nalcy                  -10.9786     2.0800  -5.278 7.27e-07 ***\nI(hrs_sleep - 8):alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\n\n\n\n\n\n\n  \n    \n    \n      coefficient\n      estimate\n      interpretation\n    \n  \n  \n    (Intercept)\n64.35\nA non-drinker who slept 8 hours is estimated to have an SDMT score of 64.3\n    I(hrs_sleep - 8)\n7.65\nFor every additional hour slept, non-drinkers scores on SDMT are estimated to increase by 7.6\n    alcy\n-10.98\ndrinkers who slept 8 hours are estimated to have 11 lower scores on SDMT than non-drinkers who slept 8 hours\n    I(hrs_sleep - 8):alcy\n-3.59\nFor every additional hour's sleep, drinkers scores on SDMT  increase by 3.6 *less* than they do for non-drinkers\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\nThe monkeys are back!\n\nData: ctmtoys.csv\nSo far, we have analysed the data for two studies (not real!) of the inquisitive nature of monkeys. Initially (week 5 exercises), Liu, Hajnosz & Li (2023) investigated age differences in exploration of novel objects, and found that older monkeys spend on average less time playing with a novel object than their younger counterparts (we looked at this with both with the linear effect of age in years, and by comparing adults to juveniles). Following this Liu, Hajnosz, Xu & Li (2023) wanted to see if monkeys showed a preference for different types of object (i.e. ones with moving parts vs ones that are soft). They found that, after accounting for differences due to age, monkeys showed a significant preference for toys with moving parts in comparison to soft toys.\nXu, Li, Liu & Hajnosz (2023c) are again asking for our help, and this time with a bigger study, of 216 monkeys. They are interested in whether the preference for mechanical toys over soft toys is different for different species of monkey. Both the previous studies were conducted on Rhesus Macaques (a species that have adapted very well to human dominated landscapes), so this study has re-run the same experiment on 69 Capuchin monkeys, 71 Tamarin monkeys and 76 Macaques.\nThe aim of this study is to investigate the following question:\n\nAre preferences between soft toys vs mechanical toys different for different species of monkey?\n\nThe data is available at https://uoepsy.github.io/data/ctmtoys.csv and contains the variables described in Table 1\n\n\n\n\n\n\nTable 1:  Data dictionary for ctmtoys.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    name\nMonkey Name\n    age\nAge of monkey in years\n    species\nSpecies (capuchin, macaque, tamarin)\n    obj_type\nType of novel object given (mechanical / soft)\n    obj_colour\nMain colour of object (red / green / blue)\n    obj_size\nSize of object in cm (length of largest dimension of the object)\n    exptime\nTime (in minutes) spent exploring the object\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 7\n\n\nAs always, begin by reading in your data and making some exploratory plots to get an idea of the distributions we’re dealing with.\n\n\n\n\nEverything looks okay in terms of our variable ranges here:\n\nctmtoys &lt;- read_csv(\"https://uoepsy.github.io/data/ctmtoys.csv\")\nsummary(ctmtoys)\n\n     name                age          species            obj_type        \n Length:216         Min.   : 1.00   Length:216         Length:216        \n Class :character   1st Qu.: 8.00   Class :character   Class :character  \n Mode  :character   Median :13.00   Mode  :character   Mode  :character  \n                    Mean   :12.92                                        \n                    3rd Qu.:18.00                                        \n                    Max.   :25.00                                        \n  obj_colour           obj_size        exptime     \n Length:216         Min.   : 5.00   Min.   : 4.20  \n Class :character   1st Qu.:36.00   1st Qu.:11.00  \n Mode  :character   Median :49.00   Median :13.45  \n                    Mean   :49.06   Mean   :13.66  \n                    3rd Qu.:62.25   3rd Qu.:16.75  \n                    Max.   :94.00   Max.   :25.10  \n\n\nLet’s shove it in pairs.panels:\n\nctmtoys |&gt; \n  select(age, obj_size, exptime) |&gt;\n  psych::pairs.panels()\n\n\n\n\n\n\n\n\nAnd let’s tabulate obj_type and species:\n\nctmtoys |&gt;\n  select(obj_type, species) |&gt;\n  table()\n\n            species\nobj_type     capuchin macaque tamarin\n  mechanical       33      43      32\n  soft             36      33      39\n\n\n\n\n\n\nQuestion 8\n\n\nTry making some initial exploratory plots of the relationships in the data that are relevant to the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’re wanting to plot exploration_time and obj_type here, but we’re also wanting to show it for each species. This means we’ll need things like colours, facets, etc.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nFrom initial exploration - it looks like capuchins are generally spending more time playing with the objects (of both types).\nThe slight preference for mechanical toys seems to be present Capuchins and Macaques, but it’s less clear in the Tamarins.\n\nggplot(ctmtoys, aes(x = obj_type, y = exptime, \n                    col = obj_type)) +\n  geom_violin() + \n  geom_jitter(width=.2, alpha=.4) + \n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nAll of this is just initial speculation, however - we haven’t considered lots of things such as the distributions of ages between the species, or whether they all saw objects of similar sizes and colours. These things could make these plots appear to show relationships that are actually driven by other things. They could also make these plots hide relationships that are present once these things are controlled for.\n\n\n\n\nQuestion 9\n\n\nFit an appropriate model to address the research question.\nThink about what we already know from previous studies - we’ll likely want to control for age and for other aspects of objects like size and colour.\nThen think about the specific research question and what is needed to test it.\n\n\n\n\n\nmodelmonkey &lt;- lm(exptime ~ age + obj_size + obj_colour +\n                obj_type * species, data = ctmtoys)\n\n\n\n\n\nQuestion 10\n\n\nDo species differ in their preference for different types of object?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about how the question is worded - there’s no “how”/“what” etc, it’s just “are there differences?” (this is just the same as we did last week, and in 8B #testing-group-differences - try a model comparison?).\n\n\n\n\n\n\n\nLet’s compare a reduced model without the interaction to our model with the interaction:\n\nmodelmonkey0 &lt;- lm(exptime ~ age + obj_size + obj_colour +\n                obj_type + species, data = ctmtoys)\n\nanova(modelmonkey0, modelmonkey)\n\nAnalysis of Variance Table\n\nModel 1: exptime ~ age + obj_size + obj_colour + obj_type + species\nModel 2: exptime ~ age + obj_size + obj_colour + obj_type * species\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    208 2339.2                              \n2    206 2257.0  2    82.136 3.7483 0.02518 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSpecies significantly differed in the extent to which exploration time varied between different types of object (\\(F(2, 206)=3.75, p=0.0252\\)).\n\n\n\n\n\nQuestion 11\n\n\nNow, we want to know how species differ in their preferences for different types of object?\nTake a look at the model coefficients.\n\n\n\n\n\nSolution\n\n\n\n\nsummary(modelmonkey)\n\n\nCall:\nlm(formula = exptime ~ age + obj_size + obj_colour + obj_type * \n    species, data = ctmtoys)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.959 -2.348 -0.045  2.015  9.745 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 19.33098    1.03529  18.672  &lt; 2e-16 ***\nage                         -0.16464    0.03961  -4.157 4.73e-05 ***\nobj_size                     0.01122    0.01252   0.896   0.3712    \nobj_colourgreen             -0.14963    0.55815  -0.268   0.7889    \nobj_colourred               -0.19467    0.55887  -0.348   0.7279    \nobj_typesoft                -1.96193    0.80839  -2.427   0.0161 *  \nspeciesmacaque              -3.90720    0.77012  -5.073 8.70e-07 ***\nspeciestamarin              -5.08712    0.82839  -6.141 4.16e-09 ***\nobj_typesoft:speciesmacaque -1.51124    1.11855  -1.351   0.1782    \nobj_typesoft:speciestamarin  1.53986    1.15128   1.338   0.1825    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.31 on 206 degrees of freedom\nMultiple R-squared:  0.392, Adjusted R-squared:  0.3655 \nF-statistic: 14.76 on 9 and 206 DF,  p-value: &lt; 2.2e-16\n\n\nIn our model coefficients, both interaction terms are non-significant.\nBut we just said before that in the previous question that there is an interaction (we got our F test for it).\nThe coefficients above are telling us that, compared to the reference species (capuchins), neither macaques nor tamarins show significant differences in their preference for mechanical toys vs soft toys.\nBut while macaques and tamarins may not be different from capuchins, it is entirely possible that macaques and tamarins could be different from each other?\n\n\n\n\nQuestion 12\n\n\nModel coefficients are always relative to some reference point (i.e. capuchins with mechanical toys).\n\nChange the reference point to Macaques with soft toys.\n\nRefit the model.\nInterpret the coefficients.\n\n\n\n\n\n\nSolution\n\n\n\nWe always get to choose which level we want as a reference. Sometimes, this will be easy because on level represents the norm/status quo. In this context, Macaques is possibly a more useful reference, given that the study description earlier mentioned the previous studies were conducted on Macaques.\nRe-levelling obj_type to have “soft” as the reference won’t make much difference - it will just swap the signs of the coefficients (comparing mechanical to soft, as opposed to comparing soft to mechanical).\n\nctmtoys &lt;- \n  ctmtoys |&gt;\n  mutate(\n    species = fct_relevel(species, \"macaque\"),\n    obj_type = fct_relevel(obj_type, \"soft\")\n  )\n\nlm(exptime ~ age + obj_size + obj_colour +\n                obj_type * species, data = ctmtoys) |&gt;\n  summary()\n\n\nCall:\nlm(formula = exptime ~ age + obj_size + obj_colour + obj_type * \n    species, data = ctmtoys)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.959 -2.348 -0.045  2.015  9.745 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        11.95061    1.01699  11.751  &lt; 2e-16 ***\nage                                -0.16464    0.03961  -4.157 4.73e-05 ***\nobj_size                            0.01122    0.01252   0.896  0.37122    \nobj_colourgreen                    -0.14963    0.55815  -0.268  0.78890    \nobj_colourred                      -0.19467    0.55887  -0.348  0.72794    \nobj_typemechanical                  3.47317    0.76957   4.513 1.07e-05 ***\nspeciescapuchin                     5.41843    0.80728   6.712 1.83e-10 ***\nspeciestamarin                      1.87117    0.80555   2.323  0.02116 *  \nobj_typemechanical:speciescapuchin -1.51124    1.11855  -1.351  0.17816    \nobj_typemechanical:speciestamarin  -3.05110    1.11480  -2.737  0.00674 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.31 on 206 degrees of freedom\nMultiple R-squared:  0.392, Adjusted R-squared:  0.3655 \nF-statistic: 14.76 on 9 and 206 DF,  p-value: &lt; 2.2e-16\n\n\nThese coefficients show us that Macaques tend to spend 3.5 more minutes playing with mechanical toys than soft toys, and that Capuchins do not significantly differ in this respect, but Tamarins do. Relative to Macaques, Tamarins spend 3.05 less additional time with mechanical toys compared to soft toys.\n\n\n\n\nQuestion 13\n\n\nThe interpretation of interaction coefficients tends to be quite confusing, and invariably it helps to tie these to a visualisation. We’re going to do it manually here, because it’s a very useful learning exercise.\nBelow are our coefficients of interest from the model, when the reference level for obj_type is “soft” and for species is “macaque”.\n...\nobj_typemechanical                  3.47317    0.76957   4.513 1.07e-05 ***\nspeciescapuchin                     5.41843    0.80728   6.712 1.83e-10 ***\nspeciestamarin                      1.87117    0.80555   2.323  0.02116 *  \nobj_typemechanical:speciescapuchin -1.51124    1.11855  -1.351  0.17816    \nobj_typemechanical:speciestamarin  -3.05110    1.11480  -2.737  0.00674 ** \nGrab a piece of paper, and draw the points for each species & obj_type combination, relative to the reference point.\nStart with the plot below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 1 - coefficient: obj_typemechanical\n\n\n\nThe coefficient “obj_typemechanical” tells us the difference between soft and mechanical toys when species is zero - i.e., when it is at the reference level (macaques).\nSo we know that macaques spend 3.5 more minutes with mechanical toys compared to soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 2 - coefficient: speciescapuchin\n\n\n\nThe coefficient “speciescapuchin” tells us the difference between capuchins and macaques, when obj_type is zero - i.e., when it is at the reference level (soft).\nSo we know that capuchins spend 5.4 more minutes with soft toys compared to macaques with soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 3 - coefficient: speciestamarin\n\n\n\nThe coefficient “speciestamarin” tells us the difference between tamarins and macaques, when obj_type is zero - i.e., when it is at the reference level (soft).\nSo we know that tamarins spend 1.9 more minutes with soft toys compared to macaques with soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 4 - coefficient: obj_typemechanical:speciescapuchin\n\n\n\nThe coefficient “obj_typemechanical:speciescapuchin” tells us the difference between capuchins and macaques’ soft-vs-mechanical differences.\ni.e. when moving from macaques to capuchins, the soft-to-mechanical difference changes by this much.\nSo we know that whereas macaques spend 3.5 more minutes with mechanical toys compared to soft toys, for capuchins this is 1.9 minutes less.\nSo rather than going up 3.5, we go up \\(3.5-1.9=1.6\\).\nHowever, this is non-significant. So the difference could easily just be 0 - i.e. they could increase by the same 3.5 minutes as Macaques do. So we know that the uncertainty here should also capture as if Capuchins and Macaques have the same increases from soft to mechanical.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 5 - coefficient: obj_typemechanical:speciestamarin\n\n\n\nThe coefficient “obj_typemechanical:speciestamarin” tells us the difference between tamarins and macaques’ soft-vs-mechanical differences.\ni.e. when moving from macaques to tamarins, the soft-to-mechanical difference changes by this much.\nSo we know that whereas macaques spend 3.5 more minutes with mechanical toys compared to soft toys, for tamarins this is 3.05 minutes less.\nSo rather than going up 3.5, we go up \\(3.5-3.05=0.45\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 14\n\n\nOkay, now let’s make a plot in R.\nTry running this code in pieces to see what each bit does, and then running it all at once to get the plot.\nDoes it match with what you sketched in the previous question?\n\nlibrary(effects)\neffect(\"obj_type*species\",modelmonkey) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    obj_type = fct_relevel(factor(obj_type), \"soft\")\n  ) |&gt;\n  ggplot(aes(x=obj_type, y=fit, col=species)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper))\n\n\n\n\n\n\n\nWhy use the effects package?\n\n\n\n\n\nUp to now, when we’ve been plotting our associations of interest we’ve been choosing to construct our plots at the mean of our other predictors.\nHowever, in our current monkey model, we’ve also got a categorical covariate (obj_colour) in our model. What should we do with that?\n\nplotdat &lt;- expand_grid(\n  age = mean(ctmtoys$age),\n  obj_size = mean(ctmtoys$obj_size),\n  obj_colour = ???\n  obj_type = c(\"soft\",\"mechanical\"), # of interest\n  species = c(\"macaque\",\"capuchin\",\"tamarin\") # of interest\n)\n\nWe could:\n\nchoose just one colour to plot it at\nmake separate plots to each colour\nplot the association of interest holding the colours at their proportions\n\nTo achieve a) or b), we can use the strategy we have been using already (make a little dataframe, use augment etc).\nHowever, to achieve c), it is easiest to use something like the effect() function from the effects package. This will also come in handy next semester, as we will use it for plotting effects from more complex models.\n\n\n\n\n\n\n\n\nlibrary(effects)\neffect(\"obj_type*species\",modelmonkey) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    obj_type = fct_relevel(factor(obj_type), \"soft\")\n  ) |&gt;\n  ggplot(aes(x=obj_type, y=fit, col=species)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeamwork and Communication\n\nDataset: teamprod.csv\nA company has recently decided to move towards having more structured team-based projects for its employees, rather than giving individual projects. They want better understanding what makes teams work well together. Specifically, they want to know whether the amount of communication in a team is associated with better quality work, and if this is different depending upon the teams’ ‘collective efficacy’ (their belief in their ability to do the work)?\nThey have collected data on 80 teams. Prior to starting the projects, each team completed a questionnaire measuring ‘collective efficacy’ (the teams’ belief in their ability to succeed at their project), and ‘collective experience’ (a measure of how much relevant experience the team has). At the end of the projects, each team’s work was rated across various measures (timeliness, quality, relevance etc) to provide a metric of ‘work quality’. In addition, information was gathered on the volume of each teams’ communication (via the company’s workspace chat platform).\nThe data is available at https://uoepsy.github.io/data/teamqual.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    team_id\nTeam ID\n    commvol\nVolume of communication (avg messages per day)\n    exp\nPrior experience (Z-scored)\n    colleff\nCollective Efficacy measure of team's belief in ability to succeed (range 0 - 70)\n    work_qual\nWork Quality measure (metric based on timeliness, quality, relevance etc). Ranges 0 to Infinity\n  \n  \n  \n\n\n\n\n\n\nQuestion 15\n\n\nBelow, we have provided a regression table, a plot, and a written paragraph.\nThere are lots of mistakes in the writing (both mismatching numbers and errors in interpretation). Note down as many errors as you can find.\nFeel free to read in the data yourself to play around.\n\n\nTable\n\n\n\n\n\n \nwork qual\n\n\nPredictors\nEstimates\nstd. Error\nStatistic\np\n\n\n(Intercept)\n-25.29\n15.21\n-1.66\n0.100\n\n\nexp\n0.87\n2.05\n0.42\n0.674\n\n\ncommvol\n2.69\n0.45\n6.04\n&lt;0.001\n\n\ncolleff\n1.60\n0.44\n3.63\n0.001\n\n\ncommvol × colleff\n-0.04\n0.01\n-2.63\n0.010\n\n\nObservations\n80\n\n\nR2 / R2 adjusted\n0.603 / 0.582\n\n\n\n\n\n\n\n\n\nPlot\n\n\n\n\n\n\n\n\n\n\n\nWriting\nWork quality was modelled using multiple regression. Team experience (Z-scored), Communication volume (messages per day) and Collective efficacy (Z-scored) were included as predictors, along with the interaction between communication and collective efficacy. The model explained 80% of the variance in work-quality scores. More experienced teams were found to produce significantly better quality work (\\(\\beta=0.82, t(74)=0.4, p&gt;.05\\)). Volume of communication was significantly associated with work quality (\\(\\beta=2.68, t(75)=6.17, p&lt;.001\\)), suggesting that teams that communicate more produced better quality work. Collective efficacy was also significantly associated with work quality (\\(\\beta=1.59, t(75)=3.64, p&lt;.001\\)), indicating that better quality work will be produced by a team that has collective efficacy (compared to those that do not). A significant interaction (\\(\\beta=-0.03, t=2.68, p &lt; .09\\)) was found between volume of communication and collective efficacy, suggesting that these two predictors are related. Overall these results suggest that for teams that have more collective efficacy, communication is more important in producing quality work."
  },
  {
    "objectID": "09_exSITAR.html",
    "href": "09_exSITAR.html",
    "title": "Exercises: Interactions!",
    "section": "",
    "text": "Processing a hangover\n\nDataset: hangover_speed.csv\nHow is hours of sleep associated with processing speed? Is this dependent upon whether or not alcohol was consumed the previous night? 107 participants completed the Symbol Digit Modalities Task (SDMT), a measure of processing speed. Participants also recorded how many hours they had slept the previous night (to the nearest 15 mins), and whether or not they had consumed alcohol.\nThe dataset is available at https://uoepsy.github.io/data/hangover_speed.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    hrs_sleep\nhours slept the previous night (to the nearest 15 mins)\n    alc\nwas alcohol consumed the previous evening? ('y'=yes, 'n'=no)\n    sdmt\nscore on the Symbol Digit Modalities Task (SDMT), a measure of processing speed (range 0 to 100)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and provide some simple descriptives.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWhat is the mean score on the SDMT, what is the variability in scores?\n\nHow many people had alcohol the previous night?\n\nHow many hours did people sleep on average? Did this vary between the drinkers and the non-drinkers?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere’s our data. Everything looks within plausible ranges for our two continuous variables hrs_sleep and sdmt.\n\nhodat &lt;- read_csv(\"https://uoepsy.github.io/data/hangover_speed.csv\")\n\nWe can get the means and SDs for hours slept and the SDMT:\n\nlibrary(psych)\ndescribe(hodat |&gt; select(-alc))\n\n          vars   n  mean    sd median trimmed   mad   min  max range skew\nhrs_sleep    1 107  7.20  1.20   7.25    7.18  1.11  4.25 10.5  6.25 0.20\nsdmt         2 107 54.59 12.13  53.00   54.23 10.38 22.00 86.0 64.00 0.28\n          kurtosis   se\nhrs_sleep     0.07 0.12\nsdmt          0.08 1.17\n\n\nWe can see that 57% of participants didn’t drink, and 43% of them did:\n\ntable(hodat$alc)\n\n\n n  y \n61 46 \n\n\nAnd it doesn’t look like they differed very much in their sleep times:\n\nhodat |&gt; group_by(alc) |&gt;\n  summarise(\n    slept = mean(hrs_sleep)\n  )\n\n# A tibble: 2 × 2\n  alc   slept\n  &lt;chr&gt; &lt;dbl&gt;\n1 n      7.12\n2 y      7.30\n\n\n\n\n\n\nQuestion 2\n\n\nMake a plot of SDMT predicted by hours slept, and colour the points by whether or not the participants had drank alcohol.\nCan you plot a separate lm line on the graph using geom_smooth for each group (alcohol v no alcohol)?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nto make geom_smooth() fit a linear model (lm), remember to use geom_smooth(method=lm).\n\nif you have the grouping in the aes(), then when you add geom_smooth() it should make a different one for each group!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nggplot(hodat, aes(x = hrs_sleep, y = sdmt, col = alc)) +\n  geom_point() +\n  geom_smooth(method=lm)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nAdding a different geom_smooth(method=lm) for each group is just fitting a different model to each groups’ data - i.e. a slope of sdmt~hrs_sleep for the drinkers and a slope for the non-drinkers.\nBut we actually want to test if the two slopes are different, and for that we need to create a model which includes the appropriate interaction term.\nFit the model to examine whether the association between hrs_sleep and sdmt is different depending upon alcohol consumption.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is the same logic as the air-pollution & APOE-4 example in 9A #it-depends.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_int &lt;- lm(sdmt ~ hrs_sleep * alc, data = hodat)\nsummary(mod_int)\n\n\nCall:\nlm(formula = sdmt ~ hrs_sleep * alc, data = hodat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3384  -5.2919  -0.2911   6.6458  19.7389 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.1496     6.9631   0.452   0.6520    \nhrs_sleep        7.6499     0.9643   7.933 2.71e-12 ***\nalcy            17.7151    10.6874   1.658   0.1004    \nhrs_sleep:alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.914 on 103 degrees of freedom\nMultiple R-squared:  0.4753,    Adjusted R-squared:   0.46 \nF-statistic:  31.1 on 3 and 103 DF,  p-value: 2.136e-14\n\n\n\n\n\n\nQuestion 4\n\n\nInterpret each coefficient from your model.\n\n\n\n\n\n\nHints\n\n\n\n\n\nOur interaction involves a continuous variable (hrs_sleep) and a binary variable (alc). An interpretation of a similar example is in 9A #interpretation.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nsummary(mod_int)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.1496     6.9631   0.452   0.6520    \nhrs_sleep        7.6499     0.9643   7.933 2.71e-12 ***\nalcy            17.7151    10.6874   1.658   0.1004    \nhrs_sleep:alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\n\n\n\n\n\n\n  \n    \n    \n      coefficient\n      estimate\n      interpretation\n    \n  \n  \n    (Intercept)\n3.15\nA non-drinker who slept 0 hours is estimated to have an SDMT score of 3.1 - note this is not significantly different from zero\n    hrs_sleep\n7.65\nFor every additional hour slept, non-drinkers scores on SDMT are estimated to increase by 7.6\n    alcy\n17.72\ndrinkers who slept zero hours are estimated to have 17.7 higher scores on SDMT than non-drinkers who slept zero hours - note this is not significantly different from zero\n    hrs_sleep:alcy\n-3.59\nFor every additional hour's sleep, drinkers scores on SDMT  increase by 3.6 *less* than they do for non-drinkers\n  \n  \n  \n\n\n\n\n\n\n\n\nQuestion 5\n\n\nConstruct a plot of the model estimated associations between hours-slept and SDMT for drinkers and non-drinkers.\nBecause we have nothing else in our model, this should end up looking exactly the same as our initial plot in Question 2!\n\n\n\n\n\n\nHints\n\n\n\n\n\nIt all follows the same logic as we have used before:\n\nmake a dataframe of the values of the predictors that we wish to plot across\nusing augment(), add to that the predicted values of the model, and the associated confidence intervals\nshove it all in ggplot!\n\nBecause we are wanting to plot across multiple predictors (i.e. we want to plot across a range of hrs_slept and both values of alc), try using expand_grid().\nPlay around with this to see what it does:\n\n\n# A tibble: 15 × 2\n   continuous binary\n        &lt;int&gt; &lt;chr&gt; \n 1          1 dog   \n 2          1 cat   \n 3          1 parrot\n 4          2 dog   \n 5          2 cat   \n 6          2 parrot\n 7          3 dog   \n 8          3 cat   \n 9          3 parrot\n10          4 dog   \n11          4 cat   \n12          4 parrot\n13          5 dog   \n14          5 cat   \n15          5 parrot\n\n\nIf you get stuck, a very similar example is in 9A #visualisation.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n# plot data\nplotdat &lt;- expand_grid(\n  hrs_sleep = 0:14,\n  alc = c(\"n\",\"y\")\n)\n# plot\nbroom::augment(mod_int, newdata = plotdat, interval=\"confidence\") |&gt;\n  ggplot(aes(x= hrs_sleep, y = .fitted, \n             col = alc, fill = alc)) + \n  geom_line() +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper), alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nNo one in our dataset has slept zero hours, and i’m probably not interested in differences between drinkers/non-drinkers who never sleep.\nRefit the model to adjust the intercept to a more meaningful value. Everyone always goes on about 8 hours of sleep being the minimum?\nHow has the interpretation of your coefficient(s) changed?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee 9A #mean-centering for an example of mean-centering a predictor in the interaction. Remember that there are multiple ways to do this - you could make a new variable first, or you could do it all inside the model.\n\n\n\n\n\n\n\n\nSolution Part 1 - recentering and refitting\n\n\n\nIf we want to recenter our hrs_sleep variable on 8 hours, we could either create a new variable which is hrs_sleep - 8 (which would therefore make everyone who slept 8 hours be given a value of 0 in the variable), or we could do it in the model:\n\n\nre-centering first\n\nhodat &lt;- \n  hodat |&gt;\n    mutate(\n      hrs_8 = hrs_sleep - 8\n    )\n\nmod_int2 &lt;- lm(sdmt ~ hrs_8 * alc, data = hodat)\n\n\n\ndoing it all in the model\n\nmod_int2 &lt;- lm(sdmt ~ I(hrs_sleep-8) * alc, data = hodat)\n\n\n\n\n\n\n\n\nSolution Part 2 - interpreting coefficients\n\n\n\n\nsummary(mod_int2)\n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            64.3487     1.4206  45.297  &lt; 2e-16 ***\nI(hrs_sleep - 8)        7.6499     0.9643   7.933 2.71e-12 ***\nalcy                  -10.9786     2.0800  -5.278 7.27e-07 ***\nI(hrs_sleep - 8):alcy  -3.5867     1.4593  -2.458   0.0156 *  \n---\n\n\n\n\n\n\n  \n    \n    \n      coefficient\n      estimate\n      interpretation\n    \n  \n  \n    (Intercept)\n64.35\nA non-drinker who slept 8 hours is estimated to have an SDMT score of 3.1\n    I(hrs_sleep - 8)\n7.65\nFor every additional hour slept, non-drinkers scores on SDMT are estimated to increase by 7.6\n    alcy\n-10.98\ndrinkers who slept 8 hours are estimated to have 17.7 lower scores on SDMT than non-drinkers who slept 8 hours\n    I(hrs_sleep - 8):alcy\n-3.59\nFor every additional hour's sleep, drinkers scores on SDMT  increase by 3.6 *less* than they do for non-drinkers\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\nThe monkeys are back!\n\nData: ctmtoys.csv\nSo far, we have analysed the data for two studies (not real!) of the inquisitive nature of monkeys. Initially (week 5 exercises), Liu, Hajnosz & Li (2023) investigated age differences in exploration of novel objects, and found that older monkeys spend on average less time playing with a novel object than their younger counterparts (we looked at this with both with the linear effect of age in years, and by comparing adults to juveniles). Following this Liu, Hajnosz, Xu & Li (2023) wanted to see if monkeys showed a preference for different types of object (i.e. ones with moving parts vs ones that are soft). They found that, after accounting for differences due to age, monkeys showed a significant preference for toys with moving parts in comparison to soft toys.\nXu, Li, Liu & Hajnosz (2023c) are again asking for our help, and this time with a bigger study, of 216 monkeys. They are interested in whether the preference for mechanical toys over soft toys is different for different species of monkey. Both the previous studies were conducted on Rhesus Macaques (a species that have adapted very well to human dominated landscapes), so this study has re-run the same experiment on 69 Capuchin monkeys, 71 Tamarin monkeys and 76 Macaques.\nThe aim of this study is to investigate the following question:\n\nAre preferences between soft toys vs mechanical toys different for different species of monkey?\n\nThe data is available at https://uoepsy.github.io/data/ctmtoys.csv and contains the variables described in Table 1\n\n\n\n\n\n\nTable 1:  Data dictionary for ctmtoys.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    name\nMonkey Name\n    age\nAge of monkey in years\n    species\nSpecies (capuchin, macaque, tamarin)\n    obj_type\nType of novel object given (mechanical / soft)\n    obj_colour\nMain colour of object (red / green / blue)\n    obj_size\nSize of object in cm (length of largest dimension of the object)\n    exptime\nTime (in minutes) spent exploring the object\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 7\n\n\nAs always, begin by reading in your data and making some exploratory plots to get an idea of the distributions we’re dealing with.\n\n\n\n\n\nSolution\n\n\n\nEverything looks okay in terms of our variable ranges here:\n\nctmtoys &lt;- read_csv(\"https://uoepsy.github.io/data/ctmtoys.csv\")\nsummary(ctmtoys)\n\n     name                age          species            obj_type        \n Length:216         Min.   : 1.00   Length:216         Length:216        \n Class :character   1st Qu.: 8.00   Class :character   Class :character  \n Mode  :character   Median :13.00   Mode  :character   Mode  :character  \n                    Mean   :12.92                                        \n                    3rd Qu.:18.00                                        \n                    Max.   :25.00                                        \n  obj_colour           obj_size        exptime     \n Length:216         Min.   : 5.00   Min.   : 4.20  \n Class :character   1st Qu.:36.00   1st Qu.:11.00  \n Mode  :character   Median :49.00   Median :13.45  \n                    Mean   :49.06   Mean   :13.66  \n                    3rd Qu.:62.25   3rd Qu.:16.75  \n                    Max.   :94.00   Max.   :25.10  \n\n\nLet’s shove it in pairs.panels:\n\nctmtoys |&gt; \n  select(age, obj_size, exptime) |&gt;\n  psych::pairs.panels()\n\n\n\n\n\n\n\n\nAnd let’s tabulate obj_type and species:\n\nctmtoys |&gt;\n  select(obj_type, species) |&gt;\n  table()\n\n            species\nobj_type     capuchin macaque tamarin\n  mechanical       33      43      32\n  soft             36      33      39\n\n\n\n\n\n\nQuestion 8\n\n\nTry making some initial exploratory plots of the relationships in the data that are relevant to the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’re wanting to plot exploration_time and obj_type here, but we’re also wanting to show it for each species. This means we’ll need things like colours, facets, etc.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nFrom initial exploration - it looks like capuchins are generally spending more time playing with the objects (of both types).\nThe slight preference for mechanical toys seems to be present Capuchins and Macaques, but it’s less clear in the Tamarins.\n\nggplot(ctmtoys, aes(x = obj_type, y = exptime, \n                    col = obj_type)) +\n  geom_violin() + \n  geom_jitter(width=.2, alpha=.4) + \n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nAll of this is just initial speculation, however - we haven’t considered lots of things such as the distributions of ages between the species, or whether they all saw objects of similar sizes and colours. These things could make these plots appear to show relationships that are actually driven by other things. They could also make these plots hide relationships that are present once these things are controlled for.\n\n\n\n\nQuestion 9\n\n\nFit an appropriate model to address the research question.\nThink about what we already know from previous studies - we’ll likely want to control for age and for other aspects of objects like size and colour.\nThen think about the specific research question and what is needed to test it.\n\n\n\n\n\nSolution\n\n\n\n\nmodelmonkey &lt;- lm(exptime ~ age + obj_size + obj_colour +\n                obj_type * species, data = ctmtoys)\n\n\n\n\n\nQuestion 10\n\n\nDo species differ in their preference for different types of object?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about how the question is worded - there’s no “how”/“what” etc, it’s just “are there differences?” (this is just the same as we did last week, and in 8B #testing-group-differences - try a model comparison?).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLet’s compare a reduced model without the interaction to our model with the interaction:\n\nmodelmonkey0 &lt;- lm(exptime ~ age + obj_size + obj_colour +\n                obj_type + species, data = ctmtoys)\n\nanova(modelmonkey0, modelmonkey)\n\nAnalysis of Variance Table\n\nModel 1: exptime ~ age + obj_size + obj_colour + obj_type + species\nModel 2: exptime ~ age + obj_size + obj_colour + obj_type * species\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    208 2339.2                              \n2    206 2257.0  2    82.136 3.7483 0.02518 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSpecies significantly differed in the extent to which exploration time varied between different types of object (\\(F(2, 206)=3.75, p=0.0252\\)).\n\n\n\n\n\nQuestion 11\n\n\nNow, we want to know how species differ in their preferences for different types of object?\nTake a look at the model coefficients.\n\n\n\n\n\nSolution\n\n\n\n\nsummary(modelmonkey)\n\n\nCall:\nlm(formula = exptime ~ age + obj_size + obj_colour + obj_type * \n    species, data = ctmtoys)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.959 -2.348 -0.045  2.015  9.745 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 19.33098    1.03529  18.672  &lt; 2e-16 ***\nage                         -0.16464    0.03961  -4.157 4.73e-05 ***\nobj_size                     0.01122    0.01252   0.896   0.3712    \nobj_colourgreen             -0.14963    0.55815  -0.268   0.7889    \nobj_colourred               -0.19467    0.55887  -0.348   0.7279    \nobj_typesoft                -1.96193    0.80839  -2.427   0.0161 *  \nspeciesmacaque              -3.90720    0.77012  -5.073 8.70e-07 ***\nspeciestamarin              -5.08712    0.82839  -6.141 4.16e-09 ***\nobj_typesoft:speciesmacaque -1.51124    1.11855  -1.351   0.1782    \nobj_typesoft:speciestamarin  1.53986    1.15128   1.338   0.1825    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.31 on 206 degrees of freedom\nMultiple R-squared:  0.392, Adjusted R-squared:  0.3655 \nF-statistic: 14.76 on 9 and 206 DF,  p-value: &lt; 2.2e-16\n\n\nIn our model coefficients, both interaction terms are non-significant.\nBut we just said before that in the previous question that there is an interaction (we got our F test for it).\nThe coefficients above are telling us that, compared to the reference species (capuchins), neither macaques nor tamarins show significant differences in their preference for mechanical toys vs soft toys.\nBut while macaques and tamarins may not be different from capuchins, it is entirely possible that macaques and tamarins could be different from each other?\n\n\n\n\nQuestion 12\n\n\nModel coefficients are always relative to some reference point (i.e. capuchins with mechanical toys).\n\nChange the reference point to Macaques with soft toys.\n\nRefit the model.\nInterpret the coefficients.\n\n\n\n\n\n\nSolution\n\n\n\nWe always get to choose which level we want as a reference. Sometimes, this will be easy because on level represents the norm/status quo. In this context, Macaques is possibly a more useful reference, given that the study description earlier mentioned the previous studies were conducted on Macaques.\nRe-levelling obj_type to have “soft” as the reference won’t make much difference - it will just swap the signs of the coefficients (comparing mechanical to soft, as opposed to comparing soft to mechanical).\n\nctmtoys &lt;- \n  ctmtoys |&gt;\n  mutate(\n    species = fct_relevel(species, \"macaque\"),\n    obj_type = fct_relevel(obj_type, \"soft\")\n  )\n\nlm(exptime ~ age + obj_size + obj_colour +\n                obj_type * species, data = ctmtoys) |&gt;\n  summary()\n\n\nCall:\nlm(formula = exptime ~ age + obj_size + obj_colour + obj_type * \n    species, data = ctmtoys)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.959 -2.348 -0.045  2.015  9.745 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        11.95061    1.01699  11.751  &lt; 2e-16 ***\nage                                -0.16464    0.03961  -4.157 4.73e-05 ***\nobj_size                            0.01122    0.01252   0.896  0.37122    \nobj_colourgreen                    -0.14963    0.55815  -0.268  0.78890    \nobj_colourred                      -0.19467    0.55887  -0.348  0.72794    \nobj_typemechanical                  3.47317    0.76957   4.513 1.07e-05 ***\nspeciescapuchin                     5.41843    0.80728   6.712 1.83e-10 ***\nspeciestamarin                      1.87117    0.80555   2.323  0.02116 *  \nobj_typemechanical:speciescapuchin -1.51124    1.11855  -1.351  0.17816    \nobj_typemechanical:speciestamarin  -3.05110    1.11480  -2.737  0.00674 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.31 on 206 degrees of freedom\nMultiple R-squared:  0.392, Adjusted R-squared:  0.3655 \nF-statistic: 14.76 on 9 and 206 DF,  p-value: &lt; 2.2e-16\n\n\nThese coefficients show us that Macaques tend to spend 3.5 more minutes playing with mechanical toys than soft toys, and that Capuchins do not significantly differ in this respect, but Tamarins do. Relative to Macaques, Tamarins spend 3.05 less additional time with mechanical toys compared to soft toys.\n\n\n\n\nQuestion 13\n\n\nThe interpretation of interaction coefficients tends to be quite confusing, and invariably it helps to tie these to a visualisation. We’re going to do it manually here, because it’s a very useful learning exercise.\nBelow are our coefficients of interest from the model, when the reference level for obj_type is “soft” and for species is “macaque”.\n...\nobj_typemechanical                  3.47317    0.76957   4.513 1.07e-05 ***\nspeciescapuchin                     5.41843    0.80728   6.712 1.83e-10 ***\nspeciestamarin                      1.87117    0.80555   2.323  0.02116 *  \nobj_typemechanical:speciescapuchin -1.51124    1.11855  -1.351  0.17816    \nobj_typemechanical:speciestamarin  -3.05110    1.11480  -2.737  0.00674 ** \nGrab a piece of paper, and draw the points for each species & obj_type combination, relative to the reference point.\nStart with the plot below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 1 - coefficient: obj_typemechanical\n\n\n\nThe coefficient “obj_typemechanical” tells us the difference between soft and mechanical toys when species is zero - i.e., when it is at the reference level (macaques).\nSo we know that macaques spend 3.5 more minutes with mechanical toys compared to soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 2 - coefficient: speciescapuchin\n\n\n\nThe coefficient “speciescapuchin” tells us the difference between capuchins and macaques, when obj_type is zero - i.e., when it is at the reference level (soft).\nSo we know that capuchins spend 5.4 more minutes with soft toys compared to macaques with soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 3 - coefficient: speciestamarin\n\n\n\nThe coefficient “speciestamarin” tells us the difference between capuchins and macaques, when obj_type is zero - i.e., when it is at the reference level (soft).\nSo we know that capuchins spend 1.9 more minutes with soft toys compared to macaques with soft toys:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 4 - coefficient: obj_typemechanical:speciescapuchin\n\n\n\nThe coefficient “obj_typemechanical:speciescapuchin” tells us the difference between capuchins and macaques’ soft-vs-mechanical differences.\ni.e. when moving from macaques to capuchins, the soft-to-mechanical difference changes by this much.\nSo we know that whereas macaques spend 3.5 more minutes with mechanical toys compared to soft toys, for capuchins this is 1.9 minutes less.\nSo rather than going up 3.5, we go up \\(3.5-1.9=1.6\\).\nHowever, this is non-significant. So the difference could easily just be 0 - i.e. they could increase by the same 3.5 minutes as Macaques do. So we know that the uncertainty here should also capture as if Capuchins and Macaques have the same increases from soft to mechanical.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Part 5 - coefficient: obj_typemechanical:speciestamarin\n\n\n\nThe coefficient “obj_typemechanical:speciestamarin” tells us the difference between tamarins and macaques’ soft-vs-mechanical differences.\ni.e. when moving from macaques to tamarins, the soft-to-mechanical difference changes by this much.\nSo we know that whereas macaques spend 3.5 more minutes with mechanical toys compared to soft toys, for tamarins this is 3.05 minutes less.\nSo rather than going up 3.5, we go up \\(3.5-3.05=0.45\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 14\n\n\nOkay, now let’s make a plot in R.\nTry running this code in pieces to see what each bit does, and then running it all at once to get the plot.\nDoes it match with what you sketched in the previous question?\n\nlibrary(effects)\neffect(\"obj_type*species\",modelmonkey) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    obj_type = fct_relevel(factor(obj_type), \"soft\")\n  ) |&gt;\n  ggplot(aes(x=obj_type, y=fit, col=species)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper))\n\n\n\n\n\n\n\nWhy use the effects package?\n\n\n\n\n\nUp to now, when we’ve been plotting our associations of interest we’ve been choosing to construct our plots at the mean of our other predictors.\nHowever, in our current monkey model, we’ve also got a categorical covariate (obj_colour) in our model. What should we do with that?\n\nplotdat &lt;- expand_grid(\n  age = mean(ctmtoys$age),\n  obj_size = mean(ctmtoys$obj_size),\n  obj_colour = ???\n  obj_type = c(\"soft\",\"mechanical\"), # of interest\n  species = c(\"macaque\",\"capuchin\",\"tamarin\") # of interest\n)\n\nWe could:\n\nchoose just one colour to plot it at\nmake separate plots to each colour\nplot the association of interest holding the colours at their proportions\n\nTo achieve a) or b), we can use the strategy we have been using already (make a little dataframe, use augment etc).\nHowever, to achieve c), it is easiest to use something like the effect() function from the effects package. This will also come in handy next semester, as we will use it for plotting effects from more complex models.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(effects)\neffect(\"obj_type*species\",modelmonkey) |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    obj_type = fct_relevel(factor(obj_type), \"soft\")\n  ) |&gt;\n  ggplot(aes(x=obj_type, y=fit, col=species)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeamwork and Communication\n\nDataset: teamprod.csv\nA company has recently decided to move towards having more structured team-based projects for its employees, rather than giving individual projects. They want better understanding what makes teams work well together. Specifically, they want to know whether the amount of communication in a team is associated with better quality work, and if this is different depending upon the teams’ ‘collective efficacy’ (their belief in their ability to do the work)?\nThey have collected data on 80 teams. Prior to starting the projects, each team completed a questionnaire measuring ‘collective efficacy’ (the teams’ belief in their ability to succeed at their project), and ‘collective experience’ (a measure of how much relevant experience the team has). At the end of the projects, each team’s work was rated across various measures (timeliness, quality, relevance etc) to provide a metric of ‘work quality’. In addition, information was gathered on the volume of each teams’ communication (via the company’s workspace chat platform).\nThe data is available at https://uoepsy.github.io/data/teamqual.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    team_id\nTeam ID\n    commvol\nVolume of communication (avg messages per day)\n    exp\nPrior experience (Z-scored)\n    colleff\nCollective Efficacy measure of team's belief in ability to succeed (range 0 - 70)\n    work_qual\nWork Quality measure (metric based on timeliness, quality, relevance etc). Ranges 0 to Infinity\n  \n  \n  \n\n\n\n\n\n\nQuestion 15\n\n\nBelow, we have provided a regression table, a plot, and a written paragraph.\nThere are lots of mistakes in the writing (both mismatching numbers and errors in interpretation). Note down as many errors as you can find.\nFeel free to read in the data yourself to play around.\n\n\nTable\n\n\n\n\n\n \nwork qual\n\n\nPredictors\nEstimates\nstd. Error\nStatistic\np\n\n\n(Intercept)\n-25.29\n15.21\n-1.66\n0.100\n\n\nexp\n0.87\n2.05\n0.42\n0.674\n\n\ncommvol\n2.69\n0.45\n6.04\n&lt;0.001\n\n\ncolleff\n1.60\n0.44\n3.63\n0.001\n\n\ncommvol × colleff\n-0.04\n0.01\n-2.63\n0.010\n\n\nObservations\n80\n\n\nR2 / R2 adjusted\n0.603 / 0.582\n\n\n\n\n\n\n\n\n\nPlot\n\n\n\n\n\n\n\n\n\n\n\nWriting\nWork quality was modelled using multiple regression. Team experience (Z-scored), Communication volume (messages per day) and Collective efficacy (Z-scored) were included as predictors, along with the interaction between communication and collective efficacy. The model explained 80% of the variance in work-quality scores. More experienced teams were found to produce significantly better quality work (\\(\\beta=0.82, t(74)=0.4, p&gt;.05\\)). Volume of communication was significantly associated with work quality (\\(\\beta=2.68, t(75)=6.17, p&lt;.001\\)), suggesting that teams that communicate more produced better quality work. Collective efficacy was also significantly associated with work quality (\\(\\beta=1.59, t(75)=3.64, p&lt;.001\\)), indicating that better quality work will be produced by a team that has collective efficacy (compared to those that do not). A significant interaction (\\(\\beta=-0.03, t=2.68, p &lt; .09\\)) was found between volume of communication and collective efficacy, suggesting that these two predictors are related. Overall these results suggest that for teams that have more collective efficacy, communication is more important in producing quality work.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "09a_interactions.html",
    "href": "09a_interactions.html",
    "title": "9A: Interactions",
    "section": "",
    "text": "So far, when we’ve been using multiple linear regression models, all our coefficients have been being interpreted as the “change in \\(y\\) for a 1-unit change in \\(x\\) while holding constant the other predictors in the model”.\nConsider a model with the following structure:\n\\[\ny = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + \\epsilon\n\\]\nWhen we fit this model structure to some data, we get out our estimated values for the coefficients \\(b_0\\), \\(b_1\\), and \\(b_2\\), e.g.,:\n\nmydata &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_mlr.csv\")\nmymodel &lt;- lm(y ~ x1 + x2, data = mydata)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -2.39138    3.67735  -0.650  0.51867   \nx1           0.17570    0.06435   2.730  0.00888 **\nx2          -0.64756    0.19959  -3.244  0.00217 **\nThe coefficient we get out for \\(x_1\\) tells us that the model estimates that \\(y\\) will increase by 0.17 for every increase of 1 unit in \\(x_1\\), provided that we hold \\(x_2\\) constant.\nSo for any value of \\(x_2\\), a one unit increase in \\(x_1\\) is associated with a \\(b_1\\) change in \\(y\\), provided that \\(x_2\\) value stays as it is.\nLet’s use our primate brains data for a more realistic example:\n\nbraindata &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_braindata.csv\")\nbraindata &lt;- braindata %&gt;% mutate(\n  isMonkey = ifelse(species != \"Human\", \"YES\", \"NO\")\n)\n\nmonkeymod &lt;- lm(mass_brain ~ age + isMonkey, data = braindata)\nsummary(monkeymod)\n\n...\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.668308   0.050478  13.240 1.57e-14 ***\nage         -0.004081   0.002337  -1.746   0.0904 .  \nisMonkeyYES -0.246933   0.044152  -5.593 3.54e-06 ***\n---\nOur coefficient for isMonkeyYES coefficient is the estimated difference in brain mass between a human and a monkey of the same age.\nSimilarly, our coefficient for age is the estimated change in brain mass for every additional year of age, provided we are not comparing a monkey to a human.\nThe idea of this can be seen on the plot in Figure 1. Imagine 4 different participants in our data. The coefficient for isMonkeyYES is the difference between A and C, or the difference between B and D. It doesn’t matter where on the x-axis we go, it is the vertical distance between the two lines.\nThe age coefficient is the comparison between A and B, or between C and D.\n\n\n\n\n\nFigure 1: multiple regression model of brain mass predicted by age and isMonkey. The age coefficient is the slope of the lines, and the isMonkey coefficient is the difference in their heights\n\n\n\n\n\n\n\n\n\n\nanother example (if you want one)\n\n\n\n\n\nWe’re interested in estimating the association between BMI and cognition, after controlling for differences due to age.\nWe want to use this model:\n\\[\n\\text{score on test of cognition} = b_0 + b_1 \\cdot \\text{age} + b_2 \\cdot \\text{BMI} + \\epsilon\n\\] which we can fit using lm():\n\nlm(score ~ 1 + age + BMI, data = ourdata)\n\nand we might get some coefficients (estimates for the \\(b_?\\) values) such as those below:\n(I’ve just made up some nice round numbers to make it easier to think about)\nCoefficients:\n            Estimate    ...   ...\n(Intercept)  160.000    ...   ...\nage         -1.50000    ...   ...\nBMI         -2.50000    ...   ...\nThe coefficient of interest, the one for BMI, is telling us that “a 1 unit increase in BMI is associated with a -2.5 decrease in Scores on the cognitive test, holding age constant”.\nConsider 3 people:\n\n\n\n\n\nFigure 2: Three theoretical people\n\n\n\n\nThe coefficient for BMI represents the difference in cognitive scores we would expect between Person A and Person B.\nThink about why this is.\nFor some person \\(i\\), their model predicted score is:\n\\[\n\\hat{score_i} = b_0 + b_1 \\cdot age_i + b_2 \\cdot BMI_i\n\\] Which from our model estimates is:\n\\[\n\\hat{score_i} = 160 - 1.5 \\cdot age_i - 2.5 \\cdot BMI_i\n\\]\n\nPerson A’s score = \\(160 - (1.5*50) - (2.5*22) = 30\\)\nPerson B’s score = \\(160 - (1.5*50) - (2.5*23) = 27.5\\)\nPerson C’s score = \\(160 - (1.5*60) - (2.5*23) = 12.5\\)\n\nThe difference in model estimated Score between Person A and Person B is the coefficient of BMI, because those two people only differ on BMI. Person A and Person C also differ on age. This is how the coefficient of BMI is interpreted as “holding age constant” - it is a comparison between two hypothetical people who differ on BMI but are identical with respect to the other predictors in the model."
  },
  {
    "objectID": "09a_interactions.html#mean-centering",
    "href": "09a_interactions.html#mean-centering",
    "title": "9A: Interactions",
    "section": "Mean Centering",
    "text": "Mean Centering\nBy mean centering a continuous predictor, we change what “0” means. Normally, when we don’t have an interaction, this simply changes the intercept value (see 8A #centering-and-scaling-predictors). If we have the interaction y ~ x1 + x2 + x1:x2, then mean centering x1 will make the coefficient for x2 now represent “the association between \\(x_2\\) and \\(y\\) for someone at the average of \\(x_1\\)”.\nUsing one of our examples from throughout this reading, we might mean-center the air-pollution so that we can consider the difference in MMSE scores between APOE4 positive and negative people at the average air-pollution level\n\n# data\nairpol &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_airpol.csv\")\n# model with original variable\neg2mod &lt;- lm(mmse ~ aqpi * apoe4, data = airpol)\n# model with mean centered predictor\neg2mod_cent &lt;- lm(mmse ~ scale(aqpi,scale=FALSE) * apoe4, data = airpol)\n\n\n# coefficients:\ncoef(eg2mod)\n\n  (Intercept)          aqpi      apoe4pos aqpi:apoe4pos \n  28.12892936   -0.01647447   -3.29790816   -0.01463533 \n\ncoef(eg2mod_cent)\n\n                        (Intercept)          scale(aqpi, scale = FALSE) \n                        25.16379896                         -0.01647447 \n                           apoe4pos scale(aqpi, scale = FALSE):apoe4pos \n                        -5.93202402                         -0.01463533 \n\n\nThis is because the coefficient for APOE4 compares the heights of the two lines when the other predictor is zero. So if we change what “zero” represents, we can change what that estimates. In the model plots below, we can see that the model doesn’t change, it is just extracting different information (it is the distance to move from the blue dot to the red dot):"
  },
  {
    "objectID": "09a_interactions.html#relevelling-factors",
    "href": "09a_interactions.html#relevelling-factors",
    "title": "9A: Interactions",
    "section": "Relevelling Factors",
    "text": "Relevelling Factors\nAnother thing that can be useful (especially when working with categorical variables with lots of levels) is to make sure your variables are factors in R, and to set a useful reference level. Typically, the reference level is what we think of as “normal”, e.g. if we have 3 groups: Placebo, Drug A, and Drug B, then we might compare each drug to the placebo condition, because that’s comparable to most people (i.e. who aren’t taking the drug).\nFor example, when we have two categorical variables:\n\nanonymity = “anonymous” vs “identifiable”\nasgroup = “alone” vs “group”\n\nThen the default is to take the alphabetical ordering. We can change the ordering using functions that “relevel” a factor. Note, this only works if the variable is already a factor in R (see 2A#categorical for a reminder of ‘factors’).\n\n# data \ncandy &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_candy.csv\")\n\n# make both predictors 'factors'\ncandy &lt;- \n  candy %&gt;% \n  mutate(\n    anonymity = factor(anonymity),\n    asgroup = factor(asgroup)\n  )\n\nOnce they are factors, we can see the default levels:\n\n\n\nlevels(candy$anonymity)\n\n[1] \"anonymous\"    \"identifiable\"\n\n\n\n\n\n\nlevels(candy$asgroup)\n\n[1] \"alone\" \"group\"\n\n\n\n\nThis is the original model, with the default levels:\n\neg3mod &lt;- lm(candybars ~ anonymity * asgroup, data = candy)\ncoef(eg3mod)\n\n                       (Intercept)              anonymityidentifiable \n                         10.055556                          -2.055556 \n                      asgroupgroup anonymityidentifiable:asgroupgroup \n                          3.844444                          -4.480808 \n\n\nLet’s relevel anonymity to have “identifiable” as the first level, and then refit the model:\n\ncandy &lt;- \n  candy %&gt;% \n  mutate(\n    anonymity = fct_relevel(anonymity, \"identifiable\")\n  )\n\neg3mod_rel &lt;- lm(candybars ~ anonymity * asgroup, data = candy)\n\ncoef(eg3mod_rel)\n\n                    (Intercept)              anonymityanonymous \n                      8.0000000                       2.0555556 \n                   asgroupgroup anonymityanonymous:asgroupgroup \n                     -0.6363636                       4.4808081 \n\n\nAgain, the model doesn’t change, we are simply extracting different bits from it:\n\n\nreference level = “anonymous”\n\n\n\n\n\n\n\n\n\n\n\nreference level = “identifiable”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: Interactions and Sum Contrasts\n\n\n\n\n\nWhen we have categorical predictors, our choice of contrasts coding changes the bits that we’re getting our of our model.\nSuppose we have a 2x2 design (condition A and B, in groups 1 and 2):\n\n\n\n\n\nFigure 7: Categorical x Categorical Interaction plot\n\n\n\n\nWhen we are using the default contrasts coding (treatment - see 8B #contrasts) in R, then our coefficients for the individual predictors represent moving between the dots in Figure 7.\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            1.9098     0.1759  10.855  &lt; 2e-16 ***\nconditionB             1.1841     0.2488   4.759 5.65e-06 ***\ngrouping2             -1.6508     0.2488  -6.635 1.09e-09 ***\nconditionB:grouping2  -2.1627     0.3519  -6.146 1.15e-08 ***\n---\n\nThe intercept is the red circle in Figure 7.\n\nThe coefficient for condition is the difference between the red circle and the red triangle in Figure 7.\n\nThe coefficient for grouping is the difference between the red circle and the blue circle in Figure 7.\n\nThe interaction coefficient is the difference from the slope of the red line to the slope of the blue line.\n\nHowever, when we change to using sum contrasts, we’re switching where zero is in our model. So if we change to sum contrasts (here we’ve changed both predictors to using sum contrasts), then we end up estimating the effect of each predictor averaged across the other.\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           1.13577    0.08796  12.912  &lt; 2e-16 ***\nconditionB            0.05141    0.08796   0.584     0.56    \ngrouping2            -1.36607    0.08796 -15.530  &lt; 2e-16 ***\nconditionB:grouping2 -0.54066    0.08796  -6.146 1.15e-08 ***\n---\n\nThe intercept is the grey X in Figure 8.\n\nThe coefficient for condition is the difference between the grey X and the grey triangle in Figure 8.\n\nThe coefficient for grouping is the difference between the grey X and the blue line in Figure 8.\n\nThe interaction coefficient is the difference from the slope of the grey line to slope of the blue line.\n\n\n\n\n\n\nFigure 8: Visualisation of sum-contrasts for categorical x categorical interaction plot\n\n\n\n\nIt can get quite confusing when we start switching up the contrasts, but it’s all just because we’re changing what “zero” means, and what “moving 1” means:\n\n\nTreatment contrasts\n\n\n\n\n\n\n\n\n\n\n\nSum contrasts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional Extra: Getting non-linear\n\n\n\n\n\nYou might be noticing that when we start talking about our regression surface “twisting” (e.g. in Figure 4), we’re starting to see curves in our model.\nWe can model curves in a “linear” model!?\nAn interaction does in a sense introduce non-linearity to our thinking, because there we no longer think of a linear effect of \\(x_1\\) (it “depends on” \\(x_2\\)). This a little bit of a trick, because ultimately our model is still linear - we are estimating our outcome \\(y\\) as the linear combination of a set of predictors. In the model \\(y = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2\\), the adjustment that we make \\(b_3\\) to each of the coefficients \\(b_1\\) and \\(b_2\\) is a constant.\nWe can even exploit this to model more clearly “non-linear” associations, such as the age and height example below.\n\n\n\n\n\nFigure 9: Two linear models, one with a quadratic term (right)\n\n\n\n\nWe will cover this a lot more in the multivariate stats course, so don’t worry too much about it right now.\nHowever, it’s useful as a means of seeing how we can extend linear models to fit these sort of relationships. In the model on the right of Figure 9, the model returns three coefficients:\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.09831    2.88606   9.736 4.99e-16 ***\nage         18.04500    0.88602  20.366  &lt; 2e-16 ***\nI(age^2)    -0.71034    0.05299 -13.406  &lt; 2e-16 ***\n\n\nIt is estimating a persons’ height as:\n\nan intercept of 28.1 (the estimated height of a newborn baby of age 0)\nplus 18.04 cm for every year of age they have\nplus -0.71 cm for every year of age they have, squared.\n\nSo for a 4 year old, the estimated height is:\n\\[\nheight = 28.1 + (18.04 \\times 4) + (-0.71 \\times 4^2) = 88.9\n\\] and for a 10 year old, it is:\n\\[\nheight = 28.1 + (18.04 \\times 10) + (-0.71 \\times 10^2) = 138.1\n\\] We can see how the quadratic “\\(\\text{age}^2\\) term has a larger (negative) effect as age increases (for the 4 year old it makes the estimated height \\(-0.71 \\times 4^2 = -11.4\\) lower, and for the 10 year old it makes the estimated height \\(-0.71 \\times 10^2 = -71\\) lower). This captures the plateauing of heights as children get older."
  },
  {
    "objectID": "09a_interactions.html#footnotes",
    "href": "09a_interactions.html#footnotes",
    "title": "9A: Interactions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis example comes from our lovely tutor Ian!↩︎\nAnd vice versa! It is also the adjustment we make to the slope of wellbeing with “relationship quality”, as we move 1 up in “time spent with partner”↩︎\nand this example comes from Otto!↩︎\nThis example was suggested by Tia↩︎\ndotted is a good way to indicate that there is no data across that line - it is linking two categories. Martin disagrees and thinks you should never have lines↩︎"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "Notes for Wizards\n\n\n\n\n\nhere’s a note!\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\n\n\n\n\nlearning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n\nSolution\n\n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Univariate Statistics and Methodology in R",
    "section": "",
    "text": "Univariate Statistics and Methodology in R (USMR) is a semester long crash-course aimed at providing Masters students in psychology with a competence in standard statistical methodologies and data analysis using R. Typically the analyses taught in this course are relevant for when there is just one source of variation - i.e. when we are interested in a single outcome measured across a set of independent observations. The first half of the course covers the fundamentals of statistical inference using a simulation-based approach, and introduces students to working with R & RStudio. The latter half of the course focuses on the general linear model, emphasising the fact that many statistical methods are simply special cases of this approach. This course introduces students to statistical modelling and empowers them with tools to analyse richer data and answer a broader set of research questions."
  },
  {
    "objectID": "10a_glm.html",
    "href": "10a_glm.html",
    "title": "10A: The generalised linear model",
    "section": "",
    "text": "Things have moved pretty quickly in the last couple of readings, so it’s worth taking a little bit of time on recaps to see how it all fits together.\n\n\n\n\n\n\nSimple linear regression\n\n\n\n\n\nIn the simple linear regression, we were modelling a continuous outcome variable \\(y\\) as a function of some predictor \\(x\\).\nOur model of \\(y\\) was defined by a number of components:\n\n\\(b_0\\): the “Intercept”. This is the predicted value of \\(y\\) where \\(x = 0\\). In the plot below, it is where the regression line cuts the y-axis.\n\n\\(b_1\\): the “Coefficient for \\(x\\)”. This is the predicted increase in \\(y\\) for every unit increase of \\(x\\). In the plot below, this is the amount the regression line goes up for every one unit we move across.\n\n\\(\\varepsilon\\): the “Residuals” or “Error term”. These are the distances from each of our observed values to the model predicted values. In the plot below, these are the vertical distances from each red dot to the black line. The standard deviation of all these gets denoted \\(\\sigma_\\varepsilon\\).\n\nModel:\n\\[\n\\begin{align}\ny &= b_0 + b_1 \\cdot x_1 + \\varepsilon\\\\\n\\qquad \\\\\n&\\varepsilon \\sim N(0, \\sigma_\\varepsilon)\n\\end{align}\n\\]\nIn R:\n\nlm(y ~ x1, data = mydata)\n\n\n\n\n\n\nFigure 1: Simple linear regression model. one continuous predictor.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple linear regression\n\n\n\n\n\nWhen we extended this to multiple regression, we essentially just added more explanatory variables (more ‘predictors’ of the outcome \\(y\\)).\nIn these models, the values of \\(y\\) are now modelled as a set of relationship with more than just one \\(x\\) variable. So our models are defined by more coefficients.\n\n\\(b_0\\): the “Intercept”. This is the predicted value of \\(y\\) when all predictors are zero.\n\n\\(b_1\\): the “Coefficient for \\(x_1\\)”. This is the predicted increase in \\(y\\) for every unit increase of \\(x_1\\) holding constant all other predictors.\n\n…\n…\n\\(b_p\\): the “Coefficient for \\(x_p\\)”. This is the predicted increase in \\(y\\) for every unit increase of \\(x_p\\) holding constant all other predictors.\n\n\\(\\varepsilon\\): the “Residuals” or “Error term”. These are the distances from each of our observed values to the model predicted values. The standard deviation of all these gets denoted \\(\\sigma_\\varepsilon\\).\n\nModel:\n\\[\n\\begin{align}\ny &= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\,\\, ... \\,\\, + b_p \\cdot x_p + \\varepsilon\\\\\n\\qquad \\\\\n&\\varepsilon \\sim N(0, \\sigma_\\varepsilon)\n\\end{align}\n\\]\nIn R:\n\n\nlm(y ~ x1 + x2 ... + xp, data = mydata)\n\nIn the plot below with two predictors, we would have a coefficient for each one, showing the slope of the regression surface along each dimension. The residuals continue to be the distances from the observed values to our model predicted values, so in this they are the vertical distances from each red dot to the surface.\n\n\n\n\n\nFigure 2: multiple linear regression model with two continuous predictors.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions\n\n\n\n\n\nWe then saw how we can let the relationships between the outcome \\(y\\) and an explanatory variable \\(x1\\) vary as a function of some other explanatory variable \\(x2\\).\nIn the plot below, if someone asks you “what is the relationship between \\(x_1\\) and \\(y\\), your reply should (hopefully) be”it depends on what the value of \\(x_2\\) is”\nAs we can see from the plot, for low values of \\(x_2\\), there is a steep downward slope between \\(y\\) and \\(x_1\\) (the red line in the plot). For high values of \\(x_2\\), the slope between \\(y\\) and \\(x_1\\) is much flatter (the blue line).\n\n\n\n\n\nFigure 3: multiple linear regression model with two continuous predictors that interact\n\n\n\n\nWe can capture this by using an additional coefficient in our linear model. To express an interaction, we use the product of the two predictors (\\(x_1 \\cdot x_2\\)).\nModel:\n\\[\n\\begin{align}\ny &= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2 + \\,\\, ... \\,\\, + b_p \\cdot x_p + \\varepsilon\\\\\n\\qquad \\\\\n&\\varepsilon \\sim N(0, \\sigma_\\varepsilon)\n\\end{align}\n\\]\nIn R:\n\n\nlm(y ~ x1 + x2 + x1:x2 + ..., data = mydata)  \n  OR  \nlm(y ~ x1*x2 + ..., data = mydata)  \n\nThe resulting coefficients (\\(b_1\\), \\(b_2\\) and \\(b_3\\) in the equation above) provide us with:\n\nthe association between \\(x_1\\) and \\(y\\) when \\(x_2 = 0\\)\n\nthe association between \\(x2\\) and \\(y\\) when \\(x_1 = 0\\)\n\nthe amount to which 1. and 2. are adjusted when there is an increase in the other variable\n\n\n\n\n\n\n\n\n\n\nCategorical explanatory variables\n\n\n\n\n\nThroughout the models we’ve been fitting we have also seen how they behave with categorical predictors.\nWhen we enter a categorical variable in as a predictor in a linear model, it gets entered as a set of “dummy variables” (typically zeroes or ones, but we it is possible to do more clever things). These represent differences between groups.\nIf we had a categorical variable which contained \\(k\\) levels (or “groups”, or “categories”), then R will put \\(k-1\\) variables into the model for us, and we will get out \\(k-1\\) coefficients. The default behaviour in R is that, depending on what we set as our reference level, the coefficients represent the difference from this level to each of the other levels.\nIn the plots below, if “Group1” is the reference, then the coefficient groupGroup2 would represent the vertical distance from the red line to the green line, and the coefficient groupGroup3 would be the vertical distance from the red line to the blue line.\n\n\n\n\n\nFigure 4: A categorical predictor in a regression model in R will by default set one level as a reference (e.g. Group1 here), and compare each level to that reference\n\n\n\n\nNote - because categorical predictors is all looking at the difference between groups - this is all still intrinsically “linear” - the only possible way to get from the red line to the green line is via a straight line.\nWe also saw how this same logic applies when categorical explanatory variables are involved in an interaction - e.g., a binary (two-level) predictor interacting with a continuous predictor allows two lines to be different slopes (as in Figure 5).\n\n\n\n\n\nFigure 5: Continuous*Binary interaction"
  },
  {
    "objectID": "10a_glm.html#interpretation-of-coefficients",
    "href": "10a_glm.html#interpretation-of-coefficients",
    "title": "10A: The generalised linear model",
    "section": "Interpretation of Coefficients",
    "text": "Interpretation of Coefficients\nAn important result of our model capturing the linear associations between the predictors and the log-odds of outcome Y, is that our coefficients are all in these units.\n\nsummary(fishmod)\n\n\n\n\nCall:\nglm(formula = caught_any ~ hours, family = binomial(link = \"logit\"), \n...\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n\n\n(Intercept): the log-odds of having caught some fish when spending zero hours fishing are -2.36.\n\nhours coefficient: for every hour someone has spent fishing, the log-odds of them having caught some fish increase by 0.67.\n\nIf you’re anything like me, these are not the most useful interpretations. I have no idea what increasing 0.67 log-odds means in real terms. What we can do instead is translate these back into odds, which might make things easier. The opposite of the natural logarithm is the exponential (see here if you’re interested). We can turn \\(log(x)\\) back into \\(x\\) by raising \\(e\\) to the power of it: \\(e^{\\log(x)} = x\\). In R, this is just the two functions log() and exp(): exp(log(2)) == 2.\nThis means we can turn our log-odds back into odds by using exp() on our coefficients! A crucial thing to note is that addition on the log-odds scale is the same as multiplication on the odds scale. A coefficient on the log-odds scale is interpreted the same way as we interpret normal linear regression coefficients: “if you go up 1 on X you add b on log-odds(Y)”. When we convert these coefficients to odds using exp(), the interpretation becomes “if you go up 1 on X, your odds are multiplied by exp(b)”.\nWe can see our exponentiated coefficients here:\n\nexp(coef(fishmod))\n\n(Intercept)       hours \n 0.09473306  1.95504622 \n\n\nThe intercept is now the odds of having caught a fish when spending zero hours fishing: 0.09.\nBecause our intercept is at a single point (it’s not an association), we can actually convert this to a probability. Remember that \\(odds = \\frac{p}{1-p}\\), which means that \\(p = \\frac{odds}{1 + odds}\\). So the probability of catching a fish if I spend zero hours fishing is \\(\\frac{0.09}{1 + 0.09} = 0.08\\).\nUnfortunately, we can’t do the same for any of the slope coefficients. This is because changes in odds are not the same for different levels of probability.\n\n\nquick demonstration\n\n\n\nConsider how when we multiply odds by 2, the increase in probability is not constant:\n\n\n\nOdds\nProbability\n\n\n\n\n0.5\n\\(\\frac{1}{1+0.5} = 0.33\\)\n\n\n1\n\\(\\frac{1}{1+1} = 0.5\\)\n\n\n2\n\\(\\frac{2}{1+2} = 0.66\\)\n\n\n4\n\\(\\frac{4}{1+4} = 0.8\\)\n\n\n8\n\\(\\frac{8}{1+8} = 0.88\\)\n\n\n\n\n\n\nOur best interpretation of our coefficient for hours is simply that for every hour someone has spent fishing, the odds of them having caught some fish are multiplied by 1.96.\n\nOdds Ratios\nWhen we exponentiate coefficients from a model fitted to the log-odds, the resulting association is referred to as an “odds ratio” (OR).\nFor every 1 unit increase in \\(x\\):\n\n“the odds of \\(y\\) change by a ratio of exp(b)”\n\n“the odds of \\(y\\) are multiplied by exp(b)”\n\n“there are exp(b) increased/decreased odds of \\(y\\)”\n\nInstead of thinking of a coefficient of 0 as indicating “no association”, in odds ratios this when the OR = 1.\n\nOR = 1 : equal odds (\\(1 \\times odds = odds \\text{ don't change}\\)).\nOR &lt; 1 : decreased odds (\\(0.5 \\times odds = odds \\text{ are halved}\\))\nOR &gt; 1 : increased odds (\\(2 \\times odds = odds \\text{ are doubled}\\))\n\n  Often you will hear people interpreting OR as “\\(y\\) is exp(b) times as likely to occur”.\nAlthough it is true that increased odds is increased likelihood of \\(y\\) occurring. Double the odds does not mean you will see twice as many occurrences of \\(y\\) (see the “I still don’t really get odds” box below). \n\n\n\nI still don’t really get odds..\n\n\n\nEven talking in odds and multiplying odds can be frustratingly confusing, so here’s a little more step-by-step explanation to try and help:\n\n\n\n\n\n\n  \n    \n    \n      term\n      b\n      exp(b)\n    \n  \n  \n    (Intercept)\n-2.36\n0.09\n    hours\n0.67\n1.96\n  \n  \n  \n\n\n\n\n\nFor people spending just 1 hour fishing, the log-odds of them having caught fish are -2.36 + 0.67 = -1.69.\n\nTranslating this to odds, we exponentiate it, so the odds of them having caught fish are \\(e^{(-2.36 + 0.67)} = e^{-1.69} = 0.19\\).\n(This is the same3 as \\(e^{-2.36} \\times e^{0.67}\\))\n\nThese odds of 0.19 equate to approximately \\(\\frac{1}{5}\\), which means that if we take 6 people who have each spent 1 hour fishing, we expect 1 of them to have caught fish, and 5 of them to have not caught anything.\n\nIf we consider how the odds increase for every hour spent fishing (i.e. for those spending 2 hours fishing as opposed to just 1):\n\nthe log-odds of catching fish increase by 0.67.\n\nthe odds of catching fish are multiplied by 1.96.\n\nso for 2 hours of fishing, the odds are \\(0.19 \\times 1.96 = 0.36\\).\n(And we can also calculate this as \\(e^{-1.69 + 0.67}\\))\n\n\nSo these new odds of 0.36 for 2 hours fished are 1.96 times greater than the odds of 0.19 for 1 hour fished. For ease, let’s just round 1.96 to 2 and say that the odds have doubled.\nWhat does this mean? It means we’ve gone from odds of \\(\\frac{1}{5}\\) to \\(\\frac{2}{5}\\). This doesn’t mean we’ll now see 2 out of 6 people catching fish. It means we’ll see 2 out of 7 (Figure 16).\n\n\n\n\n\n\nFigure 16: Increasing from odds of 1/5 to 2/5"
  },
  {
    "objectID": "10a_glm.html#inference",
    "href": "10a_glm.html#inference",
    "title": "10A: The generalised linear model",
    "section": "Inference",
    "text": "Inference\n\nCoefficient Tests\nYou might notice that the summary output of our logistic regression model has \\(z\\) statistics instead of \\(t\\) statistics. The logic of the tests remains the same as it was for the linear model, where it is a test against the null hypothesis that the coefficient is zero. Or, less formally, we can think of the corresponding p-value as “in the null universe where there is no relationship between \\(x\\) and the occurrence of event \\(y\\), there is a p-value chance of seeing a sample of this size with an association as strong as the one we have got”\n\nsummary(mallowmod)\n\n\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n\n\nFor every hour more someone spent fishing, the odds of them catching a fish increased by 0.95 (\\(z = -4.49\\), \\(p &lt; .001\\)).\n\n\n\n\n\n\n\noptional: why z not t?\n\n\n\n\n\nIn logistic regression, the variance and the mean are connected - we don’t have to estimate it.\nFor logistic regression, the distribution of the estimated coefficients gets close and closer to a Normal distribution as the sample size gets larger. All of the tests are based on the hypothesis that the sample size is large enough4 to justify a normal approximation.\n\n\n\n\n\nConfidence Intervals\nAs previously, we can obtain confidence intervals using the confint() function. Because confidence intervals are just lower and upper bounds for a range of plausible values for our estimates, we can turn these into the confidence intervals around our odds ratios!\n\nexp(confint(mallowmod))\n\n                2.5 %      97.5 %\n(Intercept) 5.6784519 151.2520735\nage         0.9310014   0.9717072\n\n\n\nFor every hour more someone spent fishing, the odds of them catching a fish increased by 0.95 (95% CI [0.93, 0.97]).\n\n\n\nVisualising\nWhile interpreting coefficients leaves us in speaking in the realm of odds-ratios, we can do more with visualisations! This is because we can simply display the predicted probability across values of some predictor. We don’t have to try and rely solely on a numerical description of a relationship - a picture speaks a thousand words (and it’s a lot easier!)\n\nlibrary(sjPlot)\nplot_model(mallowmod, type = \"eff\")\n\n$age\n\n\n\n\n\n\n\n\n\n\n\nOptional: making plots manually\n\n\n\nto make plots manually, we need to give our model some new data to predict values for, and then plot those predictions!\nFor these models, we can actually predict on various different scales (e.g. do we want to predict log-odds values (predict(mod, type=\"link\")) or probabilities (predict(mod, type=\"response\"))?\n\nplotdata &lt;- tibble(\n  # make hours a sequence of length 50, from 0 to 10\n  age = 36:120\n)\n\n# add the predcted probabilities\nplotdata &lt;- \n  plotdata %&gt;% mutate(\n    prob = predict(mallowmod, newdata = plotdata, type = \"response\"),\n  )\n\n# plot the predicted probabilities\nggplot(plotdata, aes(x = age, y = prob)) +\n  geom_line()\n\n\n\n\n\n\n\n# Getting the intervals is much trickier, but essentially requires computing the upper and lower bounds on the log-odds scale and then converting them back:\nplotdata &lt;- \n  plotdata %&gt;% \n  mutate(\n    logodd = predict(mallowmod, newdata = plotdata, type = \"link\"),\n    se = predict(mallowmod, newdata = plotdata, type = \"link\", se.fit = TRUE)$se,\n    logoddlower = logodd - (1.96*se),\n    logoddupper = logodd + (1.96*se),\n    prob = mallowmod$family$linkinv(logodd),\n    lower = mallowmod$family$linkinv(logoddlower),\n    upper = mallowmod$family$linkinv(logoddupper)\n  )\n\nggplot(plotdata, aes(x = age, y = prob)) +\n  geom_line() + \n  geom_ribbon(aes(ymin=lower,ymax=upper), alpha=.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssumptions\nBecause these models don’t have the same expected error distribution (we don’t expect residuals to be normally distributed around the mean, with constant variance), checking the assumptions of logistic regression is a little different.\nTypically, we look at the “deviance residuals”. But we don’t examine plots for patterns, we simply examine them for potentially outlying observations. If we use a standardised residual, it makes it easier to explore extreme values as we expect most residuals to be within -2, 2 or -3, 3 (depending on how strict we feel).\n\nDeviance Residuals\nThere are three ways we can get out deviance residuals, each scaled differently\n\n\\(i\\)th residual = measure of deviance contributed from the \\(i\\)th observation\n\\(i\\)th standardized residual = residual / SD(residual)\n\\(i\\)th studentized residual = residual / SD(residual from model fitted without observation \\(i\\))\n\nWe get these in R using:\n\n# deviance residuals\nresiduals(mallowmod, type = 'deviance')\n# studentised residuals\nrstudent(mallowmod, type = 'deviance')\n# standardised residuals\nrstandard(mallowmod, type = 'deviance')\n\nWe can check whether any residuals are larger than 2 or 3 in absolute value:\n\nplot(rstudent(mallowmod, type = 'deviance'), \n     ylab = 'Studentized Deviance Residuals')\n\n\n\n\n\n\n\n\nWarning: Don’t inspect this plot for patterns!!!\nThere appears to be 1 residual with a value slightly larger than 2 in absolute value. We will keep these in mind and check later if they are also influential points (using cooks.distance)\nSometimes a binned plot5 can be more informative, but not always! It works by combining together all responses for people having the same covariate \\(x_i\\) value, and taking the average studentized Pearson residual for those.\nBefore using this function, make sure you have installed the arm package!\n\narm::binnedplot(fitted(mallowmod), rstudent(mallowmod, type = 'deviance'), \n                xlab = 'Prob. of Catching fish', ylab = 'Studentized Deviance Residuals')\n\n\n\n\n\n\n\n\nThere doesn’t appear to be any extreme residuals."
  },
  {
    "objectID": "10a_glm.html#footnotes",
    "href": "10a_glm.html#footnotes",
    "title": "10A: The generalised linear model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwe have seen this type of variable already when looking at categorical predictors in our models, but we haven’t seen how we use it as the outcome.↩︎\nThis means we will end up with a lot of multiplying (computationally expensive), and a tiny tiny number. So instead we typically talk about the log-likelihood, and use summation instead of multiplication. The log-likelihood of the two data points highlighted in Figure 14, given the line is log(0.98)+log(0.84).↩︎\n\\(e^{a+b} = e^a \\times e^b\\). For example: \\(2^2 \\times 2^3 = 4 \\times 8 = 32 = 2^5 = 2^{2+3}\\)↩︎\nwhatever ‘large enough’ is!↩︎\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511790942↩︎"
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin flip. For a fair coin, the chance of getting a heads/tails for any given flip is 0.5.\nWe can simulate the number of “heads” in a single fair coin flip with the following code (because it is a single flip, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 1\n\n\nWe can simulate the number of “heads” in 8 fair coin flips with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 3\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin flips? Answer: 4! Doing another 8 flips:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin flips, but to do 1000 sets of 8 flips:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  3  30 111 244 236 219 120  32   5"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nSo what is the probability of observing \\(k\\) heads in \\(n\\) flips of a fair coin?\nAs coin flips are independent, we can calculate probability using the product rule (\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent).\nSo the probability of observing 2 heads in 2 flips is \\(0.5 \\cdot 0.5 = 0.25\\)\nWe can get to this probability using dbinom():\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 flips, those two heads could occur in various ways:\n\n\n\n\n\n\n  \n    \n    \n      Ways to get 2 heads in 8 flips\n    \n  \n  \n    HHTTTTTT\n    TTTTTHHT\n    TTHTHTTT\n    HTTTTTHT\n    TTTHTTTH\n    TTTHHTTT\n    TTTTTHTH\n    ...\n  \n  \n  \n\n\n\n\nAs it happens, there are 28 different ways this could happen.2\nThe probability of getting 2 heads in 8 flips of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin flips (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given flip)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin flips is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nSo how does likelihood differ?\nFor likelihood, we are interested in hypotheses about or models of our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?). Do we think it is biased to land on heads 60% of the time? or 30% of the time? All of these are different ‘models’.\nTo consider these hypotheses, we need to observe some data - we need to have a given number of flips, and the resulting number of heads.\nWhereas when discussing probability, we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given flip, for the likelihood we are fixing the number of heads observed, and can make statements about different possible parameters that might govern the coin’s behaviour.\nFor example, let’s suppose we did observe 2 heads in 8 flips, what is the probability of seeing this data given various parameters?\nHere, our parameter (the probability that we think the coin lands on heads) can take any real number between from 0 to 1, but let’s do it for a selection:\n\npossible_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, possible_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin flips, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). The idea that the coin is fair (0.5 probability) is more likely. The most likely parameter is 0.25 (because \\(\\frac{2}{8}=0.25\\)).\nYou can visualise this below:"
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nE.g. above we were talking about \\(P(\\text{2 heads in 8 flips}\\vert \\text{fair coin})\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\).\nGiven that we know that if we have a specific value for \\(\\theta\\), then \\(P(d \\vert \\theta)\\) will give us the probability of observing \\(d\\), we can ask “what value of \\(\\theta)\\) will maximise the probability of observing \\(d\\)?”.\nThis will sometimes get written as \\(\\mathcal{L}(\\theta \\vert d)\\) as the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. There are other ways to do statistics (not covered in this course) - e.g., in Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎\nIf you really want to see them all, try running combn(8, 2) in your console.↩︎"
  }
]