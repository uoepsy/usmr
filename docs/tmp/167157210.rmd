---
title: "USMR 2021-2022 Coursework"
author: "`r params$B191349`"
date: "23/12/2021"
output: html_document
params:
  examnumber: B191349
---

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)
# load any other packages that you require here:
library(tidyverse)
library(patchwork)
library(gridExtra)
library(knitr)
library(kableExtra)
library(sjPlot)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```
<br>

> **Research Question** <br>
Researchers are interested in the psychological factors that make people continue on the programme, and in the effects of taking the programme on health and wellbeing.<br/>

<br>

#### Question 0: Data Cleaning and Describing
<!-- If you have run the R code above this point (you can do this now by pressing Ctrl-Shift-Alt-P, or running the chunk above) then your data will be in a dataframe called `couchto5k`. -->
Data has been collected from 131 participants in Edinburgh and Glasglow, all of whom participated NHS-sponsored "Couch to 5k" 9-Week programme across a year in different season.
All participants completed questionnaire measuring the psychometric factors of accountability and self-motivation  at Week 0, with each item contains 5 questions, scoring 1-7.
Upon completing or dropping out the programme, all participants completed another questionnaire measuring self-reported happiness and health based on physiological tests. The scale is from 0-100. Details of the data collected can be found in Table 1<br>
<br>

```{r}
tibble(
  variables = names(couchto5k),
  description = c("Participant ID","Age (in Years)","psychometric measure of accountability ","psychometric measure of self-motivation","multi-test health measure","simple happiness scale", "season of the year participants were interviewed in", "city participant was recruited in", "week of programme participant stopped in (week 9 = completed the programme)")
) %>% knitr::kable() %>% kableExtra::kable_styling(.,full_width=T)
```


<center> ***Table 1. Details of the couch to 5k data.*** </center>
<br>

**Problem of the data** 

As we look at the data, we found that there are some impossible values in some categories.
Therefore, the researcher took some steps to fix the impossible values in the data.<br>

- For age, neither can be <0 nor >100.
  However, we know that the age variable has no entries < 0
  (because of the histogram above).
- For accountability and selfmot, neither can be <0 nor >35.
- For health and happiness, neither can be <0 nor >100.
- For week_stopped, neither can be <0 nor >100.
<br/><br/>

```{r warning = FALSE, fig.align = 'center'}
age_hist <- ggplot(data = couchto5k, aes(x = age)) +
  geom_histogram(bins = 30)

accountability_hist <- ggplot(data = couchto5k, aes(x = accountability)) +
  geom_histogram(bins = 30)

selfmot_hist <- ggplot(data = couchto5k, aes(x = selfmot)) +
  geom_histogram(bins = 30)

health_hist <-ggplot(data = couchto5k, aes(x = health)) +
  geom_histogram(bins = 30)

happiness_hist <- ggplot(data = couchto5k, aes(x = happiness)) +
  geom_histogram(bins = 30)

week_stopped_hist <- ggplot(data = couchto5k, aes(x = week_stopped)) +
  geom_histogram(bins = 30)


grid.arrange(age_hist, accountability_hist, selfmot_hist, health_hist, happiness_hist, week_stopped_hist, ncol = 2)
```

<center> ***Figure 1. Visualization of the couch to 5k data.*** </center>

<br/><br/>
```{r cleaning, include = FALSE}
# Neither output nor code from this chunk will be shown in the compiled document. 

couchto5k <-
  couchto5k %>%
  mutate(
    age = ifelse(age>100, NA, age),
    accountability = ifelse (accountability >35 | accountability <0, NA, accountability),
    selfmot = ifelse (selfmot >35 | selfmot <0, NA, selfmot),
    health = ifelse (health >100 | health < 0, NA, health),
    happiness = ifelse (happiness >100 | happiness < 0, NA, happiness),
    season = ifelse (season == "autunm", "autumn", season),
    week_stopped = ifelse (week_stopped > 9, NA, week_stopped)
  )
couchto5k <- couchto5k[-111, ]
#couchto5k <- couchto5k[-109, ]
```

**Visualizing the processed data**<br>

After preprocessing the data, the researcher visualises marginal distribution of the data and describes them properly. Note that even though there are some outliers from the below graphs, we decide not to delete them since their value is reasonable and we want to make sure our data is large enough for analysis.

```{r descriptives , warning=FALSE, fig.align = 'center'}
# Code will not be shown from this chunk (because we set echo = FALSE in the very first chunk)
# the output from this code will be shown. 
age_plot <- 
  ggplot(data = couchto5k, aes(x = age)) +
  geom_density() +
  geom_boxplot(width = 1/250) +
  labs(x = "age of participants", y = "Probability\ndensity")

accountability_plot <- 
  ggplot(data = couchto5k, aes(x = accountability)) +
  geom_density() +
  geom_boxplot(width = 1/75) +
  labs(x = "psychometric measure of accountability\n(range 1-35)", y = "Probability\ndensity")

selfmot_plot <- 
  ggplot(data = couchto5k, aes(x = selfmot)) +
  geom_density() +
  geom_boxplot(width = 1/55) +
  labs(x = "psychometric measure of self-motivation\n(range 1-35)", y = "Probability\ndensity")

health_plot <- 
  ggplot(data = couchto5k, aes(x = health)) +
  geom_density() +
  geom_boxplot(width = 1/150) +
  labs(x = "multi-test health measure\n(range 1-100)", y = "Probability\ndensity")

happiness_plot <- 
  ggplot(data = couchto5k, aes(x = happiness)) +
  geom_density() +
  geom_boxplot(width = 1/600) +
  labs(x = "simple happiness scale\n(range 1-100)", y = "Probability\ndensity")

week_stopped_plot <- 
  ggplot(data = couchto5k, aes(x = week_stopped)) +
  geom_density() +
  geom_boxplot(width = 1/30) +
  labs(x = "week of programme participant stopped in\n (week 9 = completed the programme)", y = "Probability\ndensity") +
  xlim (0,9)


#age_plot/accountability_plot/selfmot_plot
grid.arrange(age_plot, accountability_plot, selfmot_plot, health_plot, happiness_plot, week_stopped_plot, ncol = 2)
```

<center>***Figure 2. Marginal distribution plots of age, accountability, self-motivation, health, happiness, and week-stopped.***</center>
<br></br>


- The marginal distribution of age of participants is unimodel with a mean of approximately 41. There is variation in ages (SD = 12.4). <br>
- The marginal distribution of accountability's score is unimodel with a mean of approximately 20. There is variation in accountability (SD = 4.54). <br>
- The marginal distribution of self-motivation's score is unimodel with a mean of approximately 15. There is variation in self-motivation (SD = 2.8). <br>
- The marginal distribution of health's score is unimodel with a mean of approximately 57. There is variation in health (SD = 10). <br>
- The marginal distribution of happiness is unimodel with a mean of approximately 45. There is variation in health (SD = 31.7). <br>
- The marginal distribution of week participants stopped in looks like bimodel with a mean of approximately 6.51 weeks. There is variation in health (SD = 3.08). <br>

<br/><br/>


#### Question 1: General Check
##### Question 1a

> <font size="3"> In an earlier nationwide survey, researchers found that 45% of participants abandoned the programme before the halfway point in week 5, and a further 10% gave up before the end of the programme. Is the data in the sample I have been given in line with data from the earlier survey?</font> <br/>

To investigate the question, we performed a χ2 goodness of fit test to assess the extent to which our sample of data conform to the theorised distribution of the time people abandoned the programme. Our hypothesis is as followed.
<br/>


- ***H<sub>0</sub>** : The proportion of participants abandoned the programme in stopped before week 5, stopped after week 5, completed, is 45%, 10%, 45% respectively.*<br/>
- ***H<sub>1</sub>** :  At least one of the proportions in the null hypothesis is false.*<br/>
- ***$\alpha$** = 0.05*
<br/>
```{r q1a, include = FALSE}
week_categories <-cut(couchto5k$week_stopped, 
         breaks = c(1, 5, 8, 9),
         include.lowest=TRUE,
         labels = c("Weeks<5", "5<Weeks<9", "Weeks=9")
)
table(week_categories)

```

```{r}
chisq.test(table(week_categories), p=c(0.45, 0.1, 0.45))
```
> <font size="2"> **Results:** <br/>
Since p-value is larger than the significance level(alpha = 0.05), we fail to reject the null hypothesis that observed data are consistent with the specified distribution. Therefore, we conclude that our observed value  is in line with the data from the earlier survey.</font>

<br/>

##### Question 1b
> <font size="3">Using the same three categories (stopped before week 5, stopped after week 5, completed), examine whether the patterns of attrition rates differ by city.</font>

In this question, we first perform a χ2 test of independence to assess whether the patterns of attrition rates differ by city. However, a warning “Chi-squared approximation may be incorrect” appear since the contingency table (Table 1.)shown below confirms that there is at least one cell below 5. Therefore, we use the Fisher’s exact test instead of the Chi-square test because our sample is small (in this case the p-value is exact and is not an approximation.)
```{r, include=FALSE}
chisq.test(table(week_categories, couchto5k$city))
```

```{r q1b, warning=FALSE}
dt <- as.data.frame.matrix(chisq.test(table(week_categories, couchto5k$city))$expected)
dt %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "helvetica")
```
<br/>
<center>***Table 1. The expected value for week catagories and city.***</center>
<br/>

```{r}
fisher.test(table(week_categories, couchto5k$city))
```
> <font size="2"> **Results:** <br/>
After applying Fisher’s exact test, the p-value is 0.4, which is still larger than the significance level(alpha = 0.05). Therefore, we cannot say the patterns of attrition rates differ by city.
<br/></font>

<br/>

##### Question 1c

><font size="3">Do the average ages of participants who commenced the programme differ by city? </font>

```{r q1c}
t.test(couchto5k$age[couchto5k$city=='Edinburgh'], couchto5k$age[couchto5k$city=='Glasgow'])
```

> <font size="2"> **Results:** <br/>
In this question, we perform a t-test to assess whether the average ages of participants who commenced the programme differ by city. By evaluating the p-value, we fail to reject our null hypothesis that there is no relationship between the average ages of participants and the city they were recruited in. In other words, the result are due to chance and are not significant in terms of supporting the idea that average ages differ from city.</font>

<br/><br/>

#### Question 2: Happiness

##### Question 2a

> <font size="3">Are participants’ happiness ratings affected by the season they were interviewed in? Describe the way in which season influences happiness outcomes.</font>
<br/>

We first plotted a boxplot to roughly observe the happiness ratings in different season, as shown in Figure 3. We noticed that the mean of happiness is slightly higher in spring and winter, which might indicate that the participants lift their mood when they were recruited in those season. On top of that, the interquartile range of winter is comparatively shorter than the rest, suggesting that participants have score their happiness rating higher in winter than in other seasons.<br/>

Instead of using correlation to assess whether happiness and season are related, we applied One-way ANOVA to see their relationship since season (independent variable) is a categorical variable and happiness (dependent variable) is continuous variable. We assumed that the observations are independent of each other. If any group differs significantly from the overall group mean, then the ANOVA will report a statistically significant result.<br/>

- ***H<sub>0</sub>** : There is no difference among group means.*<br/>
- ***H<sub>1</sub>** : At least one pair of means is different.*
- ***$\alpha$** = 0.05*

```{r q2a, fig.align = 'center'}
ggplot(data=couchto5k, aes(x=season, y=happiness, fill = season))+
    geom_boxplot() +
    scale_fill_manual(values = c("#FFCC66", "#E39B4E", "#900C3F", "#581845"))
```
<br/><center>***Figure 3. The distribution of happiness ratings in different season.***</center><br/>

```{r}
model1 <- aov(couchto5k$happiness~couchto5k$season)
#model1 <- lm(happiness ~ season, data= couchto5k)
summary(model1)
```

> <font size="2">**Results:** <br/>
From the result, we see that F-value is 1.2 and it has a p-value of 0.314 which is not smaller than $\alpha$. Therefore, we fail to reject the null hypothesis, meaning that all means are the same; the differences in season are due to random variation. Happiness does not affected by the season at all.</font>

<br/><br/>

##### Question 2b

> <font size="3"> Accounting for any effects you discovered in (2a), is happiness affected by age?</font>

Figure 4. illustrates the relationship between the happiness ratings and participants’ age, which looks quite low since the dots scatter over the graph without showing any tendency. Next, we use correlation to further check the association between these two numeric variables.
<br/><br/>

```{r q2b, warning=FALSE, fig.align = 'center'}
ggplot(couchto5k, aes(x=age, happiness))+
  geom_point()+
  scale_x_continuous(limits = c(10, 70))
```
<br/><center>***Figure 4. The relationship between the happiness ratings and participants’ age.***</center><br/>

```{r, echo=TRUE}
cor(x = couchto5k$happiness, y = couchto5k$age, use = "complete.obs")
```

```{r, include=FALSE}
#model2 <- lm(happiness ~ age, data= couchto5k)
#summary(model2)
#plot(model2)
```

><font size="2">**Results:** <br/>
As we can see from the Figure 4. and the value of correlation, there is no evidence that the happiness is affected by participants' age. Since r ≈ 0, it means that there is no linear association between the two variables. The higher/lower values of the happiness do not tend to occur with higher/lower values of the ages.</font>

<br/><br/>

##### Question 2c

><font size="3"> Pick a specific baseline model and justify why you are using this.</font>

Since both season and age do not significantly affect the happiness ratings, we investigated whether there was other variables that might associate with the happiness in order to build a baseline model.<br/>
After examining other variables such as *accountability*, *selfmot*, and *city*, we decided to **choose *selfmot* as a baseline model** since it is the only variable that shows some relationship with the happiness rating. In this case, the slope, 3.64, has a standard error of 0.96. Then, we perform a test against the null hypothesis that there is no linear association between self-motivation and the happiness rating.
As we can see from the below result, the p-value is 0.00023, which is a small value, meaning that we have strong evidence against the null hypothesis.
<br/><br/>

```{r q2c}
couchto5k2 <- na.omit(couchto5k) # omit NA value
model3 <- lm(happiness ~ selfmot, data= couchto5k2)
summary(model3)$coefficients
```
**Check the residual plots to ensure trustworthy regression results**

After fitting a regression model, we check the residual plots (Figure 5) to see whether we violate the assumptions.<br/>

><font size="2">**Results:** <br/>
The model doesn't look bad.<br/>
- The top left plot (residuals vs fitted) shows a reasonably straight red line, indicating that the mean of the residuals is close to zero across the fitted values.<br/>
-  The top right plot (QQ plot of residuals) shows that the residuals are fairly close to the dotted line with only  a few residuals stray from the line quite a bit near the head and tails. Overall, it indicates they follow close to a normal distribution.<br/>
-  The bottom left plot (scale location plot) allows us to examine the extent to which the variance of the residuals changes accross the fitted values. The straight red line indicates reasonably constant variance.<br/>
-  The bottom right plot (residuals vs leverage plot) allows us to identify influential observations in a regression model. In this case, there are no influential cases.</font>

```{r, fig.align = 'center'}
par(mfrow=c(2,2))
plot(model3)
```
<br/><center>***Figure 5. The residual plot for the regression model of happiness and self-motivation.***</center><br/>
<br/><br/>

#### Question 3:Happiness and Health<br/>
##### Question 3a 

><font size="3">Building on your baseline model, are participants’ happiness ratings affected by whether or not they completed the programme? Describe the way in which programme completion influences happiness outcomes.</font>

To evaluate this hypothesis, we are going to fit the following model:
<br/>

<center>$Happiness = b_0 + b_1⋅Selfmot + b_2⋅Complete +\epsilon$</center>
<br/>
We fit the linear model specified by the formula above using lm( ). In this situation, <br/>

```{r q3a}
complete <- ifelse (couchto5k2$week_stopped < 9, 0, 1)
couchto5k2 <- cbind(couchto5k2, complete)
model4 <- lm(happiness ~ selfmot + complete, data= couchto5k2)
summary(model4)$coefficients
```

- $\hat{\beta}_0 = -14.56$, the estimated average happiness score associated with zero week of completion and zero self-motivation score.<br/>
- $\hat{\beta}_1 = 4.38$, the estimated increase in happiness score associated with self-motivation score, holding the completion week.<br/>
- $\hat{\beta}_2 = -11.34$, the estimated increase in happiness score associated with one week increase in completed the programme, holding the number of self-motivation score.<br/>
<br/>

**Check the residual plots to ensure trustworthy regression results**

Again, after fitting a regression model, we check the residual plots (Figure 6) to see whether we violate the assumptions.<br/>
According to the below four plots, our model does not look bad. The residuals vs fitted plot shows a reasonably straight red line, indicating that the model’s predictions are correct on average. QQ plot also indicates that residuals follow close to a normal distribution. The horizontal line with randomly spread points in the scale-location plot also suggest that residuls do not violate the assumptions of equal variance.

```{r, fig.align = 'center'}
par(mfrow=c(2,2))
plot(model4)
```
<br/><center>***Figure 6. The residual plot for the regression model $Happiness = b_0 + b_1⋅Selfmot + b_2⋅Complete +\epsilon$***</center><br/>
<br/>

Next, we used partial F-test to compare whether our linear regression model(with completion variable) provides a better fit to the data than a model that contains a subset of the predictor variables in the overall regression model. In other words, we compared the following two models:
<br/><br/>
<center>$Happiness = b_0 + b_1⋅Selfmot + b_2⋅Complete +\epsilon$</center><br/>
<center>$Happiness = b_0 + b_1⋅Selfmot + \epsilon$</center><br/>
This test uses the following null and alternative hypotheses:<br/>

- ***H<sub>0</sub>** : All coefficients removed from the full model are zero.*<br/>
- ***H<sub>1</sub>** : At least one of the coefficients removed from the full model is non-zero.*<br/>
- ***$\alpha$** = 0.05*

```{r}
#confint(model4, level = 0.95)
#summary(model3)
anova(model4, model3)
```
><font size="2">**Results:** <br/>
As we can see from the output, the F test-statistic from the ANOVA is 3.92 and the corresponding p-value is 0.05. 
Since this p-value is less than or equal to 0.05, we will reject the null hypothesis, meaning that we have sufficient evidence to say that the predictor variables complete is statistically significant. In other words, adding completion to the regression model indeed improve the fit of the model. This argument can also be supported by looking at the adjusted r square in the two model. Adjusted r square increases when adding completion as an independent variable, the value is `r summary(model4)$adj.r.sq`. For baseline model, the value of adjusted r square is only `r summary(model3)$adj.r.sq`. </font>

<br/><br/>

##### Question 3b <br/>
><font size="3">Building on the analysis in (3a), is happiness additionally affected by the “health metric”?</font>

In this question, we compare the following model: 
<br/><br/>
<center>$Happiness = b_0 + b_1⋅Selfmot + b_2⋅Complete + b_3⋅Health + \epsilon$</center><br/>
<center>$Happiness = b_0 + b_1⋅Selfmot +  + b_2⋅Complete + \epsilon$</center><br/>

First, we build another multiple linear model that includes health explanatory variable. And since we are analyzing whether health is another factor that affects happiness comparing to not adding it, we use partial F-test to make the comparison.

```{r q3b}
model5 <- lm(happiness ~ complete + selfmot + health, data= couchto5k2)
summary(model5)$coefficient
anova(model5, model4)
```
><font size="2">**Results:** <br/>
We can approach the problem in two different angle. First, the summary of the model $Happiness = b_0 + b_1⋅Selfmot + b_2⋅Complete + b_3⋅Health + \epsilon$  shows the p-value for health coefficients is 0.208, which is greater than 0.05. This indicates that there is no association between the changes in the health score and the shifts in the happiness. In other words, there is insufficient evidence to conclude that there is an effect at the population level. Second, the p-value in the result of partial F-test is also larger than 0.05, which supports the argument that the association between health and happiness is not significant.</font>

<br/><br/>

##### Question 3c <br/>
><font size="3">It’s been hypothesised that the effects of good health are amplified by the feeling of acting healthily, such that the happiness of participants who got further along the programme might be more affected by the health metric than that of those who stopped earlier. Building on the model in (3b), can you test this hypothesis?</font>


To address the question, we are going to fit the following model:
<br/><br/>
<center>$Happiness = b_0 + b_1⋅Weekstopped + b_2⋅Selfmot + b_3⋅(Health \times Weekstopped )+ \epsilon$</center><br/>
<center>$where \space \epsilon \sim \text{N}(0, \sigma) \space independently$</center><br/>

That means we are estimating the extent to which the effect of health on happiness is different across the values of week stopped.
```{r, fig.align = 'center'}
model6 <- lm(happiness ~ week_stopped + selfmot + health + health*week_stopped, data= couchto5k2)
summary(model6)$coefficients
plot_model(model6, type = 'int')
```
<br/><center>***Figure 7. Multiple regression model: happiness ~ week_stopped + selfmot + health + health*week_stopped. The interaction between health and week_stopped affected the score of happiness. ***</center><br/>

><font size="2">**Results:** <br/>
The p-values(=0.000196) in the output above tell us that the interaction effect (Week_stopped/Health) is statistically significant. Consequently, we know that the happiness we derive from the week stopped depends on the health score.
<br/>
Figure 7. shows the relationship between week stopped and happiness changes direction based on the health. The crossed lines on the graph suggest that there is an interaction effect, which the significant p-value for the Week_stopped/Health term confirms. For high health score, there is a positive between week stopped and happiness while for low health score it is a negative relationship.</font>

<br/><br/>

##### Question 3d <br/>
><font size="3">What can we conclude about the various causes of happiness in our data? Write a brief description of the effects in the model, such as you might find in an academic paper.</font>


```{r, message=FALSE}
tab_model(model6)
#summary(model6)
```
<br/><center>***Table 2. Summary of our multiple regression model: happiness ~ week_stopped + selfmot + health + health*week_stopped. ***</center><br/>

><font size="2">**Results:** <br/>
Results showed a significant conditional association between self-motivation score and happiness score ($\beta$=4.276, SE = 0.955, p <.01), suggesting that among those with the same number of week participation, those who have one additional self-motivation score tend to, on average, score 4.276 higher on the happiness score. Crucially, the association between how many weeks the participants engaged in the programme and the happiness score was found to be dependent upon the level of health score, with a positive association between the two for those with high levels of health scores ($\beta$=0.35, SE = 0.09, p <.01). This interaction is visually presented in Figure X. <br/>
The results presented here indicate that the association between how many weeks the participants engaged in the programme and the happiness score is depend upon with participants' health, which according to the assessment scheme showing on NHS website may be weight loss, improving fitness, or improvements in sleep quality etc. The higher the the health score, the more chances the participants continues the programme and gain happiness. On the other hand, if the score of health is low, the participant have tendency to drop out the programme and receive less happiness.</font>

<br/><br/>

#### Question 4

```{r}
# The subset of the data, including only those participants who completed the programme
ediw9 <- rbind(subset(couchto5k, week_stopped =="9" & city == "Edinburgh" & season == "spring"), subset(couchto5k, week_stopped =="9" & city == "Edinburgh" & season == "summer"), subset(couchto5k, week_stopped =="9" & city == "Edinburgh" & season == "autumn"), subset(couchto5k, week_stopped =="9" & city == "Edinburgh" & season == "winter"))

glasw9 <-rbind(subset(couchto5k, week_stopped =="9" & city == "Glasgow" & season == "spring"), subset(couchto5k, week_stopped =="9" & city == "Glasgow" & season == "summer"), subset(couchto5k, week_stopped =="9" & city == "Glasgow" & season == "autumn"), subset(couchto5k, week_stopped =="9" & city == "Glasgow" & season == "winter"))

subsetw9 <- rbind(ediw9, glasw9)

# Show subsetw9 
kable(head(subsetw9, 5), booktabs = TRUE) %>%
  kable_styling(font_size = 12)
```


```{css, echo = FALSE}
#{css, echo = FALSE}
caption {
      font-size: 16pt;
      color: black;
      font-weight: bold;
      text-align: center
}
```
<center> ***Table 3. The first 5 rows of participants who complete the programme.*** </center>
<br/><br/>

```{r q4, warning = FALSE, fig.align = 'center'}
# The Average happiness ratings for people in Edinburgh who completes the program, sorted by season
ediw9_hrs <- tibble(season = c("spring", "summer", "autumn", "winter"), 
             happiness_rating = c(mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Edinburgh" & couchto5k$season == "spring",]$happiness),
                      mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Edinburgh" & couchto5k$season == "summer",]$happiness),
                      mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Edinburgh" & couchto5k$season == "autumn",]$happiness),
                      mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Edinburgh" & couchto5k$season == "winter",]$happiness)),
             city = "Edinburgh")


# The Average happiness ratings for people in Glasgow who completes the program, sorted by season
glasw9_hrs <- tibble(season = c("spring", "summer", "autumn", "winter"), 
             happiness_rating = c(mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Glasgow" & couchto5k$season == "spring",]$happiness),
                      mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Glasgow" & couchto5k$season == "summer",]$happiness),
                      mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Glasgow" & couchto5k$season == "autumn",]$happiness),
                      mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Glasgow" & couchto5k$season == "winter",]$happiness)),
             city = "Glasgow")

# Combine the above two dataframes: the average happiness ratings grouped by season and city
com_prog <- rbind(ediw9_hrs, glasw9_hrs)

# Modify the variable so that R knows it’s a factor
com_prog$season <- factor(com_prog$season, ordered = TRUE, levels = c("spring","summer","autumn", "winter"))
com_prog$city <- factor(com_prog$city, ordered = TRUE, levels = c("Edinburgh", "Glasgow"))

# Create a plot of the average happiness ratings grouped by season and city
ggplot(com_prog, aes(x = season, y = happiness_rating, fill = city)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("#FFCC66", "#900C3F"))+
  labs(x = "Season", y = "Average Happiness Ratings", fill = "City")+
  geom_text(aes(label=paste(format(happiness_rating, nsmall = 1))), size = 3, vjust = -0.2, position = position_dodge(.9))+
#    ggtitle("The average happiness ratings grouped by season and city")+
#    theme(plot.title = element_text(
#    size = rel(1.1), lineheight = .9,
#    family = "Helvetica", face = "bold", hjust = 0.5,
#    margin = margin(t = 0, r = 20, b = 30, l = 0)
#  )) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 40, b = 0, l = 0)))+
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 20, b = 0, l = 0))
  )
```

<center> ***Figure 8. The average happiness ratings grouped by season and city.*** </center>
<br/><br/>

><font size="2">**Results:** <br/>
Figure 2 shows the average happiness ratings for participants completing the programme. The data was grouped by season and city(Edinburgh/Glasgow). We can see that participants who were interviewed in spring had the highest happiness ratings in both Edinburgh and Glasgow, with `r mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Edinburgh" & couchto5k$season == "spring",]$happiness)` and `r mean(couchto5k[couchto5k$week_stopped =="9" & couchto5k$city == "Glasgow" & couchto5k$season == "spring",]$happiness)` respectively. Participants in Edinburgh also had higher happiness ratings in winter compared to those taking part in summer and autumn, with 10 units apart.
Note that we don't have the data about participants in Glasgow who engaged the programme in winter.</font> 

<br/><br/>

#### Question 5: Predictors of Drop-out
##### Question 5a/5b
><font size="3">Build a model that predicts the likelihood of dropping out (at all). And briefly describe the effects in your model as you would in an academic paper.</font>

We built a model for the prediction of ydropping out the programme based on several variables, including self-motivation, age, and health. The result is shown below. Since the coefficients of the model is interpret in log-odds, which is not intuitive, we translated them back into odds by applying exponential (shown at the second chunk below). 

```{r, echo=FALSE, include=FALSE}
complete2 <- ifelse (couchto5k2$week_stopped < 9, 1, 0)
couchto5k2 <- cbind(couchto5k2, complete2)
fit <- glm(complete2 ~ selfmot +accountability+age+health+happiness+season+city, data = couchto5k2, family="binomial")
stepwise1 <- step(fit,direction="both")
formula(stepwise1)
```

```{r}
model7 <- glm(complete2 ~ selfmot + age + health, data = couchto5k2, family="binomial")
summary(model7)
exp(coef(model7))
```
<br/>

```{r, fig.align = 'center'}
df3<- data.frame(selfmot = c(9,14,18), age = c(40, 40, 40), health = c(57, 57, 57))
df3 <- 
  df3 %>% mutate(
    logodds = predict(model7, newdata = df3, type = "link"),
    odds = exp(logodds),
    probs = predict(model7, newdata = df3, type = "response"),
  )
df3 %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "helvetica")
```
<br/>
<center> ***Table 4. Probability of dropping out the programme. The three individuals have different self-motivation score but with the same age and health score.*** </center>
<br/>

><font size="2">**Results:** <br/>
In question 5a, we build a model to predict the likelihood of dropping out. The choice of predictive variables is carried out by stepwise regression method. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some criterion (Investopedia). Results showed that holding age and health constant, with one unit increase in self-motivation, chances of the participants' dropping out the programme might decrease ($\beta$=-0.305, SE = 0.08, p <.01). For example, we take three participants who have the same age and health score but different self-motivation score. The probability of dropping out the programme
decreases if they have higher self-motivation score, with the person who scores himself/herself 9 in self-motivation have 0.825 probability of dropping out; whereas the person who has 18 scores in self-motivation only have 0.232 probability of dropping out the programme.</font>

<br/><br/>


##### Question 5c

><font size="3">Draw a graph representing the probability of quitting as a function of how self motivated participants were.</font>

```{r, warning=FALSE, fig.align = 'center'}
model9 <- glm(complete2 ~ selfmot, data = couchto5k2, family="binomial")
df <- data.frame(selfmot = couchto5k$selfmot)
a <- predict(model9, newdata = df, type = "response")
ggplot(data = df, aes(x = selfmot, y = a)) +
  geom_line()+
  labs(y="predicted probability of dropping out")
```
<br/><center>***Figure 6. The probability of quitting based on self-motivation scores.***</center><br/>

><font size="2">**Interpretation:** <br/>
As we can see from Figure 6, participants with high self-motivation score are more likely to engage in the whole programme. On the other hand, those who have low self-motivation score tend to drop out the programme.</font>
