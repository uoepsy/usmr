---
title: "USMR 2021-2022 Coursework"
author: "`r params$examnumber`"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: default
params:
  examnumber: B190910
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)
# load any other packages that you require here:
library(tidyverse)
library(broom)
library(pander)
library(patchwork)
library(psych)
library(sjPlot)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0 - Cleaning and Describing

```{r cleaning, include = FALSE}
# instantiate wrongdata column
couchto5k$wrongdata <- NA

# mark users we are taking out
couchto5k$wrongdata[couchto5k$age>120] <- "impossible age"
couchto5k$wrongdata[couchto5k$selfmot<5] <- "impossible selfmot"

# take out pointless week-stopped values and replace with 9
couchto5k <- couchto5k %>% mutate(week_stopped = ifelse(week_stopped>9, 9, week_stopped))
# correct misspelling of autumn
autumn_spell <- sum(couchto5k$season=='autunm')
couchto5k <- couchto5k %>% mutate(season = ifelse(season=='autunm', 'autumn', season))

# put all error values into a table to be presented
errortab <- table(couchto5k$wrongdata)

# factorize the three categorical variables so R knows they are categories
couchto5k$city <- factor(couchto5k$city) 
couchto5k$season <- factor(couchto5k$season)
couchto5k$week_stopped <- factor(couchto5k$week_stopped)

#remove all the users that were causing issues
couchto5k <- couchto5k %>% filter(is.na(wrongdata))
```

## Cleaning Data

A sensible first step of every statistical experiment is to remove any impossible or clearly incorrect values, so one can look at the data accurately and begin to make intuitions. There was several instances of incorrect or impossible data, listed below in Table 1. 4 users were removed in total due to flawed data. There was several 'autunm' misspellings (`r autumn_spell` in total) that were corrected and one instance of 'week_stopped' being an irrelevant value which was corrected to 9.

```{r cleaning table, results="asis"}
# produce table for results
errortab %>% pander(caption="Table 1: Summary of incorrect data values", style = "simple", split.table = Inf)
```

## Describing Data

The data consists of 123 different participants after the invalid users have been removed. There are five continuous variables and three categorical variables in the data-set of interest. The categorical variables describe:

* the season the participants were interviewed in
* the city the participant was recruited from
* the week the participant stopped following the programme

Bar-plots can be seen for the three categorical variables below in Figure 1, that show significantly more participants were interviewed in Spring and Summer than Autumn or Winter. They also show that significantly more people were recruited from Edinburgh than from Glasgow, and that the majority of participants did make it to the end of the programme in week 9.

```{r describing categorical, fig.align= "center", fig.cap= "Figure 1 - Bar-plots showing distributions for (clockwork) season, city and week_stopped", warning=FALSE}
month_positions <- c("spring" , "summer" , "autumn", "winter")
season_p <- ggplot(data = couchto5k, aes(x=season)) + geom_bar() + scale_x_discrete(limits=month_positions) + theme_bw()
city_p <- ggplot(data = couchto5k, aes(x=city)) + geom_bar() + theme_bw()
weekstop_p <- ggplot(data = couchto5k, aes(x=week_stopped)) + geom_bar() + theme_bw()
season_p + city_p / weekstop_p
```

The six continuous variables of interest are best summarized in a table highlighting the relevant numbers for each variable. This can be seen in Table 2 below.

```{r describing continuous, results="asis", warning=FALSE}
# summarise the continuous data using describe, and put it into a table
c25k_described <- describe(couchto5k)
contin_summary <- c25k_described[c(2, 3, 4, 5, 6), c('n', 'mean', 'sd', 'min', 'max', 'range')]
contin_summary%>% pander(caption="Table 2: Summary of continuous data values", style = "simple")
```

The last thing useful to check is the 'normality' of the data visually: a density plot for each continuous variable was created and can be seen below in Fig. 2. The data does look as normal as one would expect with real data: 'age' doesn't quite have a normal distribution, which is mentioned later in the report. The 'happiness' variable also has a non-standard distribution: this is caused by participants giving 0's or 100's in their response.

```{r plotting continuous, fig.align= "center", fig.cap= "Figure 2 - Density plots for the continuous variables"}
# plot density plots for all of our continuous variables, and add them together into one plot
age_dense <- ggplot(data = couchto5k, aes(x=age)) + geom_density() + theme_bw()
account_dense <- ggplot(data = couchto5k, aes(x=accountability)) + geom_density() + theme_bw()
selfmot_dense <- ggplot(data = couchto5k, aes(x=selfmot)) + geom_density() + theme_bw()
selfmot_health <- ggplot(data = couchto5k, aes(x=health)) + geom_density() + theme_bw()
selfmot_happiness <- ggplot(data = couchto5k, aes(x=happiness)) + geom_density() + theme_bw()
(age_dense + account_dense) / (selfmot_dense + selfmot_health) / selfmot_happiness
```

# Question 1 - General Checks

## Question 1a

```{r q1a}
# map a variable that tracks when the person dropped out, and factorize it
couchto5k$drop_out_status <- 'completed'
couchto5k$drop_out_status <- ifelse(couchto5k$week_stopped== "1" |  couchto5k$week_stopped== "2" | couchto5k$week_stopped== "3" | couchto5k$week_stopped== "4", "week 1-4", couchto5k$drop_out_status)
couchto5k$drop_out_status <- ifelse(couchto5k$week_stopped=="5" | couchto5k$week_stopped== "6" | couchto5k$week_stopped== "7" | couchto5k$week_stopped== "8", 'week 5-8', couchto5k$drop_out_status)
couchto5k$drop_out_status <- factor(couchto5k$drop_out_status, ordered = TRUE, levels = c('week 1-4', 'week 5-8', 'completed'))

# add additional variable for later modelling
couchto5k$completed <- ifelse(couchto5k$drop_out_status=='completed', 'completed', 'did not complete')
couchto5k$completed <- factor(couchto5k$completed)
# add number representation of this for later modelling
couchto5k$completed_num <- ifelse(couchto5k$drop_out_status=='completed', TRUE, FALSE)

# chi squared test using probs given
chi_1a <- chisq.test(table(couchto5k$drop_out_status), p = c(0.45, 0.10, 0.45))
```

The first question to be answered is to check if the data lines up with findings from a previous study:

1. 45% of participants abandoned the programme before the halfway point in week 5.
2. 55% of participants abandoned the programme before it was completed.

To formalize that statistically, in that studies sample there was a probability of 0.45 that the participants abandoned the programme before the halfway point, a probability of 0.10 that the participants made it past that point but still abandoned it before completion, and an implicit further probability of 0.45 that they reached the end. 

To test if this years studies results matched up, a chi-square goodness of fit test was carried out on our years study: it was discovered that there wasn't a statistically significant difference between the distributions of the data  $X^2$(`r chi_1a$parameter`, N = 127) = `r chi_1a$statistic`, p = `r chi_1a$p.value` with this year's p-value being > 0.05.

## Question 1b

The next thing was to examine whether patterns of attrition rates differ by city. The data was first plotted below in Fig. 3 to get a visual representation. From the graphs, it seems as if the ratio of people dropping out does seem to be similar between cities, but further tests are needed to confirm that.

```{r q1b table, fig.align="center", fig.cap="Figure 3 - Bar-plots showing drop-out time grouped by city", warning=FALSE}
ggplot(couchto5k, aes(x=drop_out_status, fill = city)) + geom_bar(position='dodge')
```

```{r q1b tests, warning=FALSE}
# filter the participants into two groups
gla_participants <- filter(couchto5k, city == 'Glasgow')
edi_participants <- filter(couchto5k, city == 'Edinburgh')

# carry out two chi-squares for each group to see if they differ
chi_gla <- chisq.test(table(gla_participants$drop_out_status), p = c(0.45, 0.10, 0.45))
chi_edi <- chisq.test(table(edi_participants$drop_out_status), p = c(0.45, 0.10, 0.45))
```

Two separate chi-squared tests were carried out for both the Edinburgh and Glasgow participants: neither were significantly different than the initial probabilities listed in Q1A (0.45, 0.10, 0.45), with the Glasgow values $X^2$(`r chi_gla$parameter`, N = 127) = `r chi_gla$statistic`, p = `r chi_gla$p.value` and the Edinburgh values $X^2$(`r chi_edi$parameter`, N = 127) = `r chi_edi$statistic`, p = `r chi_edi$p.value` both being above the p-value threshold (p < 0.05) to be statistically significant.

## Question 1c

```{r q1c test, results="asis"}

# add lines that these have been done but not actually normal
shapiro_edi <- shapiro.test(edi_participants$age)
shapiro_gla <- shapiro.test(gla_participants$age)

# t test gla edi age 
q1c_test <- t.test(gla_participants$age, edi_participants$age)
```

To examine how the average ages of participants differed by city, one would typically want normally distributed data. Both groups of participants were found to be non-normally distributed via Shapiro-Wilk Tests (Glasgow and Edinburgh both p < 0.02): due to the larger sizes of the datasets and the general robustness of the t-test to non-normality, it was assumed to be fine to continue.

The average ages of each group are what we are comparing: the distribution of the data can be seen in the boxplot Fig. 4 below, which at first glance seem relatively similar (which woud imply there isn't a statistically significant difference).

```{r q1c figure, fig.align='center', fig.cap="Figure 4 - A boxplot of both cities ages and how they differ", warning=FALSE}
ggplot(data = couchto5k, aes(x = age, y = city)) + theme_bw() + #spruce up with colours
  geom_boxplot()
```

A two-sample t-test revealed there was no statistically significant difference for age between the two groups  (t = `r q1c_test$statistic`, p = `r q1c_test$p.value`), despite Edinburgh participants (M = `r mean(edi_participants$age, na.rm = TRUE)`, SD = `r sd(edi_participants$age, na.rm = TRUE)`) having higher ages on average than the Glasgow participants (M = `r mean(gla_participants$age)`, SD = `r sd(gla_participants$age)`). 
# Question 2 - Happiness

## Question 2a

To examine first whether happiness ratings are affected by the season they were in, it's sensible to plot the data. Fig. 4 below plots the average happiness rating per season. From examining the bar-plot, it doesn't seem as if there's a huge difference between the seasons, but further testing is needed to confirm that.

```{r q2a plot, warning=FALSE, results="asis", fig.align="center", fig.cap="Figure 5 - Bar-plot showing the average happiness rating vs the season"}
ggplot(data=couchto5k, aes(x=season, y=happiness)) + geom_bar(stat = "summary", fun = "mean") + scale_x_discrete(limits=month_positions) +
  theme_bw()
```

```{r q2a build model, fig.align= "center", fig.cap= "Figure 6 - Bar-plots showing drop-out time grouped by city"}
# build the model with appropriate variables
basemod1 <- lm(happiness ~ 1 + season, data=couchto5k)
# use this to get our data variables in easy-to-pull-from tables
basemod1sum <- summary(basemod1)
basemod1stat <- glance(basemod1)
```

A linear model was created to test if the season significantly predicted happiness. It was found that the season a participant was recruited in was not statistically significant ($R^2$ = `r basemod1$r.squared`, F(`r basemod1$df.residual`) = `r basemod1stat$statistic`, p = `r basemod1stat$p.value`)) in predicting the happiness of the participant. This matches up with the Fig. 5 above, that didn't show a large difference from season to season.

## Question 2b

```{r q2b}
# build the model with appropriate variables
basemod2<- lm(happiness ~ 1 + season + age, data=couchto5k)
# use this to get our data variables in easy-to-pull-from tables
basemod2sum <- summary(basemod2)
basemod2stat <- glance(basemod2)
```

It was also of interest to determine if happiness was predicted by age. The model was further built on to include the effect of age. This new model including the predictor of age was also deemed to not be statistically significant ($R^2$ = `r basemod2$r.squared`, F(`r basemod2$df.residual`) = `r basemod2stat$statistic`, p = `r basemod2stat$p.value`)). The inclusion of the age variable hasn't improved the model, with the age variables high p-value implying very little influence on happiness.

## Question 2c

```{r q2c}
# build null model and complete anova test
null_model <- lm(happiness ~ 1, data = couchto5k)
anov1 <- anova(null_model, basemod1, basemod2)

# try out motivation model to see if this is actually useful
basemod3 <- lm(happiness ~ 1 + selfmot, data=couchto5k)
basemod3sum <- summary(basemod3)
basemod3stat <- glance(basemod3)
```

As the models are nested, the two baseline models built were compared using an incremental F-test against a null model (that just models happiness with no predictors): using season as a predictor did improve the F-score (F + 0.57), while the model including age as a predictor did not increase the F-score further (F + < 0.02). The results in full can be seen below in Table 2.

```{r, results="asis"}
anova(null_model, basemod1, basemod2) %>% pander(caption="Table 3: ANOVA results for comparing the models from 2(a) and 2(b) ", style = "simple", split.table = Inf)
```

Strangely, neither model in question 2(a) or 2(b) was effective in predicting the effect of happiness. Looking at other variables, it was decided to test the model out using the 'self-motivation' variable to see if that worked as a predictor of happiness: the model using self-motivation as a predictor of happiness ($R^2$ = `r basemod3$r.squared`, F(`r basemod3$df.residual`) = `r basemod3stat$statistic`, p = `r basemod3stat$p.value`)) was statistically significant, with p < 0.05. Therefore, this 'self-motivation' model was chosen to be the baseline going into Q3.

# Question 3 - Happiness and Health

## Question 3a

```{r q3a}
# build on baseline model including the completed variable and complete ANOVA
mod3a <- lm(happiness ~ 1 + selfmot + completed_num, data=couchto5k)
anov2 <- anova(basemod3, mod3a)
```

The effect of whether or not a participant completed the programme was not found to have a statistically significant difference on happiness (F(1, 113) = `r anov2[2, 'F']`, p = `r anov2[2, 'Pr(>F)']`) so it doesn't function as a predictor of happiness. The full results of the ANOVA test can be seen below in Table 4.

```{r q3a table}
anova(basemod3, mod3a) %>% pander(caption="Table 4: ANOVA results for comparing the 3(a) model to the baseline ", style = "simple", split.table = Inf)
```

## Question 3b

```{r q3b}
# build further on baseline model including the completed variable and complete ANOVA
mod3b <- lm(happiness ~ 1 + selfmot + completed + health, data=couchto5k)
anov3 <- anova(basemod3, mod3a,  mod3b)
```

The effect of the participant's health score was not found to have a statistically significant difference on happiness when comparing the models using an ANOVA test (F(1, 113) = `r anov3[3, 'F']`, p = `r anov3[3, 'Pr(>F)']`): however, the addition of health has brought the model closer to an accurate model of happiness. The full table for the ANOVA results are shown below in Table 5.

```{r q3b table}
anova(basemod3, mod3a,  mod3b) %>% pander(caption="Table 5: ANOVA results for comparing the 3(b) model to the 3(a) model and the baseline", style = "simple", split.table = Inf)
```

## Question 3c

```{r q3c}
# create two models, one with the interaction and one without
mod3c_interact <- lm(happiness ~ 1 + selfmot + completed + health * week_stopped, data=couchto5k)
mod3c_no_interact <- lm(happiness ~ 1 + selfmot + completed + health + week_stopped, data=couchto5k)
# carry out an ANOVA test
anov4 <- anova(basemod3, mod3c_no_interact, mod3c_interact)
```

The next thing to be examined was the potential interaction between health and the week the participant stopped the programme, implying a dependency between the two that doesn't quite fit the analysis that's been carried out so far.
To test this, it was decided to build on the existing model created in Q3(b) to create two different models: one would have the addition of the 'week_stopped' variable (without accounting for the interaction) and the other would account for it. An ANOVA test could then be carried out to see if accounting for the interaction has actually improved the model or not.
This ANOVA test was carried out, with the results below in Table 6.

```{r q3c table}
anova(basemod3, mod3c_no_interact,  mod3c_interact) %>% pander(caption="Table 6: ANOVA results for investigating the interaction between health and week_stopped ", style = "simple", split.table = Inf)
```

The data does show that there is a significant interaction between health and the week the participant stopped: once the interaction is being accounted for, the model improves to a statistically significant degree (F(1, 113) = `r anov4[3, 'F']`, p = `r anov4[3, 'Pr(>F)']`). It was decided to plot the relationship between health and happiness for the subsection of people who dropped out in each week to try and observe this visually: this can be seen below in Fig. 6. While some of the graphs are slightly odd-looking due to sparse data, the implication here can be seen that the participants who made it to week 9 have a more positive trend than the participants who dropped out straight away.

```{r, message=FALSE, results="asis", fig.align="center", fig.cap="Figure 6 - Relationship between week dropped out of the program vs health/happiness"}
# create figure showing pattern of data for each week dropped out 
ggplot(data = couchto5k, aes(x = health, y = happiness)) + geom_point() + facet_wrap(~week_stopped) + geom_smooth(method = "lm", se = FALSE)
```


## Question 3d

The potential factors that influence happiness have been thoroughly investigated. For this years data:

* the week the participant stopped does not have a significant impact on the happiness score
* the effect of health does not have a significant impact on the happiness score **unless** the conditional effect of the week the participant dropped out is also considered: if this is considered, the effect is then deemed statistically significant

# Question 4 - Participants who completed the challenge

```{r q4}
# filter participants to only those who completed the programme
completed_participants <- filter(couchto5k, drop_out_status=='completed')
# pick out some values to look at in-text
glasgow_autumn_part <- nrow(filter(completed_participants, city == 'Glasgow' & season == 'autumn'))
glasgow_winter_part <- nrow(filter(completed_participants, city == 'Glasgow' & season == 'winter'))
```

The below Fig. 7 shows the average happiness for each city's participants from each season. It is worth noting that there was only `r glasgow_autumn_part` participant who was in the 'Glasgow/Autumn' subset and `r glasgow_winter_part` participants who were in the 'Glasgow/Winter' subset, so their happiness scores may be being negatively skewed through the lack of data (though from personal experience, the Glasgow Autumn/Winter can be incredibly depressing so the low scores are not too surprising)!


```{r fancy table, fig.align= "center", fig.cap= "Figure 7 - Bar-plots showing average happiness ratings grouped by seasons and city"}
# plot the average of the data, separated by season and coloured by city
ggplot(completed_participants, aes(x=season, y = happiness, fill = city)) + geom_bar(stat='summary', position = 'dodge', fun = 'mean') + scale_x_discrete(limits=month_positions)
```


# Question 5 - Predictors of Drop-Out

## Question 5a

A generalized linear model was built to assess the likelihood of dropping out (at all). The first model built was with all variables included, and a chi-square ANOVA test was carried out to determine which variables were actually relevant. The results of this can be seen in Table 7 below.

```{r q5a}
# build model with all factors in it to see if they are useful
model_dropout <- glm(completed_num ~ selfmot + age + accountability + health + happiness + season, data = couchto5k, family="binomial")

# test to see which variables are actually relevant
anova_glm <- anova(model_dropout, test="Chisq")
anova(model_dropout, test="Chisq") %>% pander(caption="Table 7: Investigating the most relevant variables for use in the GLM", style = "simple", split.table = Inf)
```

The only statistically significant variables in the model were self-motivation (p = `r anova_glm['selfmot', 'Pr(>Chi)']`), health (p = `r anova_glm['health', 'Pr(>Chi)']`), and the effect of season (p = `r anova_glm['season', 'Pr(>Chi)']`). Therefore, these are the variables used to build the generalized model.

```{r GLM}
# build the relevant GLM with appropriate variables
model_dropout <- glm(completed_num ~ selfmot + health + season, data = couchto5k, family="binomial")

# accuracy
guess <- predict(model_dropout)
guess <- ifelse(guess>0.5,TRUE,FALSE)
hits <- sum(guess == couchto5k$completed_num)
accuracy <- hits/length(couchto5k$completed_num)
```

To check the model was functioning as expected, an accuracy test was set up to examine how good the created model is able to predict actual data. When guessing to determine if (given the rest of the data) the participant dropped out or not, the built system had a `r accuracy*100`% accuracy at guessing the correct outcome.

## Question 5b

Now the model has been created and tested, it's worth walking through the effects within. The coefficients of the model can be seen below in Table 8.


```{r q5b, results="asis"}
model_dropout_coef <- table(coef(model_dropout))

coef(model_dropout) %>% pander(caption = "Table 8: Coefficients of the final GLM model for drop-outs", style = "simple", split.table = Inf)

# calculate prob from exp
selfmot_log_odd <- coef(model_dropout)[2]
selfmot_odd <- exp(selfmot_log_odd)
selfmot_prob <- selfmot_odd / (1 + selfmot_odd)
```


These coefficients above each indicate how much the variable in question either decreases or increases the log-odds of dropping out before the end of the programme. For example, for each point on the self-motivation scale, the log-odd probability of making it to the end of the programme increases by `r selfmot_log_odd` log-odd. This can be converted to a standard odd (`r selfmot_odd`) and then into a probability value (`r selfmot_prob`).

The linear model can be summarized in the below equation:

$Dropout Likelihood = -6.816 +0.5586selfmot + 0.001312health -3.876(ifspring) + 1.771(ifsummer) + 1.683(ifwinter)$

## Question 5c

The final question asks to create a graph showing the probability of dropping out of the programme as a function of self-motivation. This can be seen below in Fig.8, and concludes this statistical report.

```{r q5c, fig.align= "center", fig.cap= "Figure 8 - Probability of drop-out vs self-motivation", message=FALSE}
couchto5k %>% ggplot(aes(x=selfmot,y=completed_num*1)) + geom_smooth(method="glm",method.args=list(family=binomial)) + geom_jitter(size=3,width=0,height=.2,alpha=.1) + scale_y_continuous(breaks=seq(0,1,by=.2)) + labs(y = "Logprob of completing programme")
```
