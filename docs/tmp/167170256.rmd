---
title: "USMR 2021-2022 Coursework"
author: "`r params$examnumber`"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
  html_document: default
params:
  examnumber: B196758
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)

# load any other packages that you require here:
library(tidyverse)
library(plotly)
library(dplyr)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0
<!-- If you have run the R code above this point (you can do this now by pressing Ctrl-Shift-Alt-P, or running the chunk above) then your data will be in a dataframe called `couchto5k`. -->


```{r cleaning, include = FALSE}
# Neither output nor code from this chunk will be shown in the compiled document. 
#avoid manipulating original data
ck5 <- couchto5k

#function judging values outside of 3 sd (return TRUE OR FALSE)
outliers <- function(obs, x = 3){
  abs(obs - mean(obs)) > (x * sd(obs))
}

#exclude values in terms of logic impossibility, age not included
ck5 <- 
  ck5 %>%
    mutate(
      accountability = ifelse(accountability<5 | accountability >35, NA, accountability),
      selfmot = ifelse(selfmot<5 | selfmot >35, NA, selfmot),
      health = ifelse(health<0 | health >100, NA, health),
      happiness = ifelse(happiness<0 | happiness >100, NA, happiness),
      week_stopped = ifelse(week_stopped<0 | week_stopped >9, NA, week_stopped)
          ) %>% 
  na.omit()

#Outliers excluded 
num1_outlier = nrow(couchto5k) - nrow(ck5)

#exclude values outside of 3 sd, age included this time
ck5 <- 
  ck5 %>%
    mutate(
      age = ifelse(outliers(age), NA, age),
      accountability = ifelse(outliers(accountability), NA, accountability),
      selfmot = ifelse(outliers(selfmot), NA, selfmot),
      health = ifelse(outliers(health), NA, health),
      happiness = ifelse(outliers(happiness), NA, happiness)
          ) %>% 
  na.omit()

#Outliers excluded in total
num2_outlier = nrow(couchto5k) - nrow(ck5)

ck5 <- 
  ck5 %>%
    mutate(
      season = ifelse(season == "autunm", "autumn", season) 
    )


```

I set two criteria to exclude the data:

1) Containing values logically impossible, i.e., out of range of the scores.

2) Values that are 3 standard deviations away from the mean.

I check the criterion 1 first, where age is not involved. For those impossible values, I would mark them as NA for later deletion. Tracking the NA value, I find out these two impossible values are from negative self-motivation values (both of them are -99 in original data) and one from the stop week. Then, participants with any NA value are excluded. 

The reason to tracking where the impossible values come from is to decide which factors are used to draw a outlier plot. Since the outliers are related to the range problem, I draw two histograms to show the outliers.

```{r plots_1, echo = FALSE}

hist(couchto5k$selfmot, main = "Histogram of outliers", xlab =  "Self-motivation score")

hist(couchto5k$week_stopped, main = "Histogram of outliers", xlab =  "Stop week")

```


After exclude `r num2_outlier` participants, I then check the criterion 2. This time the age is involved.

A function to help find out the outliers 3 standard deviation far away is adopted:
```{r function code, echo = TRUE}
#function judging values outside of 3 sd (return TRUE OR FALSE)
outliers <- function(obs, x = 3){
  abs(obs - mean(obs)) > (x * sd(obs))
}
```
Again, after setting the outliers as NA and tracking them down, I find out the outliers are from the age category. There are `r num2_outlier-num1_outlier` participants who seem to be inappropriate ages for this programme. So I consider the data may be wrong and exclude them.

The plots for the outliers in age category are displayed below.

```{r plots_2, echo = FALSE}

hist(couchto5k$age, main = "Histogram of outliers", xlab =  "Age")

ggplot(data = couchto5k, aes(y = age)) +
  geom_boxplot()+ylab("Age") +
  xlab("Frequency")+
  scale_x_continuous(limits = c(-2,2), breaks = NULL)
```

In all, there are `r num2_outlier` participants are excluded because some of their values seem to be impossible and unreasonable.

Finally, to test the abnormal strings, I use a two-dimension table of the data type to have a quick check, so the table's meaning is not important here.

```{r table, echo = FALSE}

table(couchto5k$city, couchto5k$season)

```
As in the table, there are some spelling mistakes in seasons. Efficiently, I merge the "autumn" and "autunm" to solve the problem. 


```{r descriptives}
# Code will not be shown from this chunk (because we set echo = FALSE in the very first chunk)
# the output from this code will be shown. 

```

# Question 1 

## Question 1a

```{r q1a, include = FALSE}
# use three ifelse to classify the three week stop categories into early_stop, late_stop, finish

ck5<- 
  ck5 %>%
   mutate(
    stp_week = week_stopped,
    stp_week = ifelse(week_stopped < 5, 'early_stop', stp_week),
    stp_week = ifelse(week_stopped >= 5 & week_stopped <9, 'late_stop', stp_week),
    stp_week = ifelse(week_stopped == 9, 'finish', stp_week))

as.factor(ck5$stp_week)

shapiro.test(ck5$age)

table(ck5$stp_week)

# The three categories are actually listed in the alphabet order: early_stop, finish, late_stop.
chisq.test(table(ck5$stp_week), p = c(0.45,0.45,0.10))

```
  That sample data seems to fit with the nationwide survey outcomes with a p value of 0.4 (>>0.05) in the chi-square test, which suggests that no significant difference between those two data sets. 

  Firstly, I divide the participants into three new groups in terms of their stopped week (before week 5, between week 5 to week 8 and week 9). Before adopting chi-square test, I use the Shapiro-Wilk to test if the distribution of the stop weeks fit with the normal distribution. The p value of 3e-04 (<<0.05) shows the it is significantly away from the normal distribution. 
  
```{r q1a test, include = TRUE}
shapiro.test(ck5$age)
```
  
  Since the sample is not subject to normal distribution and needs to see if fit with a larger sample, it becomes natural to think about the chi-square test and get the results.
  
```{r q1a chi-test, include = TRUE}
chisq.test(table(ck5$stp_week), p = c(0.45,0.45,0.10))
```


## Question 1b

```{r q1b, include = FALSE}
table(ck5$city, ck5$stp_week)

plot(table(ck5$city, ck5$stp_week), main = "A comparison of the distribution of two cities")

chisq.test(table(ck5$city, ck5$stp_week))
```
  To examine if the patterns of attrition rates differ by city is equal to examine whether the distribution pattern of the stopped weeks is different between the two cities. I use chi-square test to see if a significant difference can be find between the two cities.
  
```{r q1b chi test, include=TRUE}
chisq.test(table(ck5$city, ck5$stp_week))
```

  The p value is high as 0.8 which strongly rejects the significant difference of the two cities. And thus the impression by the plot get proved.

```{r q1b plot, echo = FALSE}
plot(table(ck5$city, ck5$stp_week), main = "A comparison of the distribution of two cities")
```


## Question 1c

```{r q1c, include = FALSE}
#Get people's age in terms of their cities
people_g <- ck5$age[ck5$city == "Glasgow"]
people_e <- ck5$age[ck5$city == "Edinburgh"]

shapiro.test(ck5$age)

hist(ck5$age)

wilcox.test(people_e, people_g, paired= 0)

t.test(people_e, people_g)

ggplot(data = ck5, aes(x = age, y = city)) +
  geom_boxplot()

```
  T test can be ideal for testing mean values, but it requires the fit to the normal distribution to ensure validity. To improve reliability, I adopt both the T test and Wilcoxon Signed-Rank Test to test if the difference of average age between the two cities is significant.  

  The results are similar, both get a tiny v values (0.001 by T test and 0.003 by Wilcoxon Test), which suggests significant difference of age for participants from different cities. A straightforward boxplot is drawn to visualise the difference of the mean ages.
  
```{r q1c plot, echo = FALSE}
ggplot(data = ck5, aes(x = age, y = city)) +
  geom_boxplot()
```

# Question 2

## Question 2a

```{r q2a, include = FALSE}
#draw a plot 
gd <- ck5 %>% group_by(season) %>%
  summarise(mean_se(happiness))
gd

q<- gd %>% 
  ggplot(aes(x=season, y=y, ymin=ymin, ymax=ymax)) +
  geom_point(size=2) + geom_bar(stat="identity") +
  geom_errorbar(width=.2) +
  ylab("happiness") +
  xlab("season")
q

#Linear model applied to multiple predictors
model1 <- lm(happiness ~ season, data = ck5)
summary(model1)

```
  The season is a categorical variable. To study how it relates to the a numeric vector: the happiness ratings, it seems plausible to exploit the linear model to do analysis.

  To have a glance of the relations among the seasons and the happiness scores, a barplot is drawn below. The top of each bar reaches the mean happiness scores in each season. A letter-I-like line is marked to suggest the distance of a standard deviation. The mean and the sd give a general impression but it could not tell more.

```{r q2a plot, echo = FALSE}
#draw a plot 
gd <- ck5 %>% group_by(season) %>%
  summarise(mean_se(happiness))
gd

q<- gd %>% 
  ggplot(aes(x=season, y=y, ymin=ymin, ymax=ymax)) +
  geom_point(size=2) + geom_bar(stat="identity") +
  geom_errorbar(width=.2) +
  ylab("happiness") +
  xlab("season")
q

```

  To dig out the closer relation among those factors, the linear relationships are demonstrated below. The intercept estimate represents season autumn, which is not directly demonstrated though. Other estimates in the coefficients area reflexes how they influence the happiness score. Only autumn and spring has a significance because their P value (Pr(>|t|)) is far more less than 0.05. So spring and autumn has an significant influence on the happiness scores. Moreover, spring seems to bring more happiness to people compared with autumn. The discussion about summer and winter would be much less meaningful due to their less significance in terms of the p value. In other words, they have less impact on the happiness scores.

```{r q2a lm, include = TRUE}
#Linear model applied to multiple predictors
model1 <- lm(happiness ~ season, data = ck5)
summary(model1)
```

## Question 2b

```{r 2b, include = FALSE}
#linear model
model1 <- lm(happiness ~ age, data = ck5)
summary(model1)


#plot
ck5 %>% ggplot(
  aes(x=age,y=happiness)) +
  xlab("age") +
  ylab("happiness") +
  geom_point(size=3) +
  geom_smooth(method="lm")

```
  Here we adopt the linear model again. Unlike the one in the previous question that suggests no linear relation, this time a linear correlation can be found. First we run the code for their linear model. 
  
```{r q2b lm, include = TRUE}
model1 <- lm(happiness ~ age, data = ck5)
summary(model1)
```

  
  The age does not seem to have a significance due to the p value of about 0.2, which is much higher than 0.05. Moreover, the p value in the F-statistic tells us that to suppose a correlation between age and happiness is nothing better than we don't make any assumptions about the correlation. The estimate for age is not obvious as well. Thus, we could claim that there is no evidence suggest that the happiness is obviously affected by age. If we draw a linear line, we would expect an almost parallel line to represent their correlation as below.

```{r qq2b, echo = FALSE}

ck5 %>% ggplot(
  aes(x=age,y=happiness)) +
  xlab("age") +
  ylab("happiness") +
  geom_point(size=3) +
  geom_smooth(method="lm")

```
  
  
  

  
## Question 2c

  Although age is a weaker predictor than seasons as we discuss before. However, in question 3, other involved factors like the health and completion need to be involved, and they seemed to correlate with age more than seasons. Moreover, including 4 seasons would make the model much more complex than the consideration of only age.

  Now we run the linear model for health ~ age and health ~ season again to justify our claim. From the p values of the F-statistic (0.134), we can tell that seasons suggest no significant correlation with health. If we look at the coefficients, only autumn appear to have significant correlation with health even though it is of less meaning (p value of F statistic is bigger than 0.05). For age, we can tell from these two tiny p values that it has a significance to correlate with health, even though the estimate seems to be small. The plot below suggest the correlation between health and age looks more obvious than what the small slope number suggests. 
  
  Now that we find out the health can at least affect one outcome variable of happiness, it is better to test in question 3 if health will affect happiness through affecting its correlation variables. And this may be the purpose of conducting a baseline model test.

```{r q2c, echo = FALSE}
# linear models concerning two factors: age and health
model1 <- lm(health ~ age, data = ck5)
summary(model1)

model2 <- lm(health ~ season, data = ck5)
summary(model2)

#plot
ck5 %>% ggplot(
  aes(x=age,y=health)) +
  xlab("age") +
  ylab("health") +
  geom_point(size=3) +
  geom_smooth(method="lm")
```

# Question 3

## Question 3a


```{r q3a, include = FALSE}

#Completion group.

ck5 <- 
  ck5 %>%
    mutate(
      is_finish = week_stopped,
      is_finish = ifelse(week_stopped == 9, "completed", "not completed")
    )

#linear model
model1 <- lm(happiness ~ age + is_finish, data = ck5)
summary(model1)

```
Firstly, participants are grouped in terms of whether they complete the programme (stop week = 9). The completion state becomes a two-category factor.Then we run the linear model for completion state and happiness.

```{r qq3a, included = TRUE}
model1 <- lm(happiness ~ age + is_finish, data = ck5)
summary(model1)
```

Base on the baseline model concerning age and happiness. Since the completion factor contains two contrastive levels, it is meaningless to check the p value for F-statistics. If we look at the p values of coefficients, not-finish level suggest no correlation with happiness while completion appears to be significant enough. Therefore, we can infer that completing the programme would bring more joy while stop the programme seems not to influence the happiness either positively or negatively. And the completion significantly relates with the happiness. 

## Question 3b

```{r q3b}

model1 <- lm(happiness ~ age + is_finish + health, data = ck5)

summary(model1)

```
After adding the health factor to out model, no significance appears even though p-value of the F-statistics slightly increases. As a result, happiness is not additionally affect by health. Age is tested to significantly correlate with health, but they do not make a significance together. Even the p value improves a bit, but it may be nothing different from adding any random factors. 

## Question 3c

```{r q3c1, include = FALSE}
model1 <- lm(happiness ~ age + is_finish + health + is_finish :  health, data = ck5)

summary(model1)
```
  To test the hypothesis, a interaction model is adopted as below where health combines with the completion. 
  
```{r intm, include = TRUE}
model1 <- lm(happiness ~ age + is_finish + health + is_finish :  health, data = ck5)
summary(model1)
```
  We assume there is a correlation between the stopped weeks and health that contribute to the happiness. It turns out non-completion and the health connects with each other tightly according to their significant low p value. Also, it leads to the p value of the F-statistics changing severely to 0.006 compared with before. So the results changed completely. Non-completion becomes quite significant to the happiness scores with a very low value of p, which contrasts with its results before. So we could claim that there is no strong connection between non-completion and health, which seems to support its opposite: completion and health are correlated and contribute to the happiness positively.
  
## Question 3d
Participants seem to be more happy when it is spring or autumn. The longer they spend time in the programme, they might be more happy. No evidence suggests that health and age significantly affects the happiness. Additionally, when one factor is not significant to happiness, it is not likely to improve its significance together with other variable that are not significant to the outcome of happiness as well.

# Question 4

```{r q4}

# selection of completed participants
sub_ck5 <-
  ck5 %>% filter(is_finish == "completed")

#calculate the group mean

happy_means <- 
  sub_ck5 %>% 
    group_by(season, city) %>% 
      summarise(happiness_mean = mean(happiness))

# making the mean plot
ggplot(happy_means, aes(x = season, y = happiness_mean, fill = city)) +
    geom_bar(stat = "identity", position = "dodge") + 
      scale_fill_manual("city", values = c("Edinburgh" = "maroon", "Glasgow" = "darkolivegreen2"))
        
        
```


# Question 5

## Question 5a

```{r q5a, included = FALSE}

#process y value

ck5 <- 
  ck5 %>%
    mutate(
      is_finish = ifelse(week_stopped == 9, 1, 0)
    )

#general linear model

model1 <- glm(is_finish ~ selfmot + happiness,
             family = binomial, data= ck5)
summary(model1)
```
I suppose the probability of dropping out is concerned with someone's self-motivation and happiness. Since there are only two values: either drop-out or completion, I decide to use the general linear model with binomial distribution to test the prediction.

```{r 51, include = TRUE}
model1 <- glm(is_finish ~ selfmot + happiness,
             family = binomial, data= ck5)
summary(model1)

```


## Question 5b


```{r q5b, include= TRUE}
summary(model1)

```
According to the results, self-motivation is a significant effect to predict dropping out, but happiness sis not significant enough to predict the dropping out probability.

## Question 5c

```{r q5c, echo = FALSE}
ck5 %>% 
    ggplot(aes(x = selfmot, y = is_finish)) +
      ylab("p(Drop out)") +
      geom_jitter(size=3,width=0,height=.2,alpha=.1) +
      geom_smooth(method="glm",method.args=list(family=binomial)) +
      scale_y_continuous(breaks=seq(0,1,by=.2))

```










