---
title: "USMR 2021-2022 Coursework"
author: "`r params$examnumber`"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
  pdf_document: default
params:
  examnumber: B203421
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)
# load any other packages that you require here:
library(tidyverse)
library(ggplot2)
library(psych)
library(effects)
library(sjPlot)
library(knitr)
library(gt)
library(pander)
library(car)
library(psycho)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0
Couch to 5k is an NHS-sponsored fitness programme, the data used in this report was obtained from https://uoepsy.github.io/data/usmr_2122_data.R: a dataset containing information on 126 participants. There were 8 observational units: age, accountability, self-motivation, multi-test health measure, simple happiness scale, season of the year participants were interviewed in, 	city participant was recruited in and week of programme participant stopped in.
First the function `describe` was used to look at each variable in the original dataframe and check if there is any impossible data (see Table 1). 
```{r fig.asp=.6, fig.cap="Figure 4: The ages of participants", fig.align="center"}
describe(couchto5k) %>% 
  kable(caption = "Table1: Summary of all variables")
```

There are obvious problems with the variables `season` and `week_stopped`, so here the `levels` function was used to check the levels of `season`.
```{r}
levels(as.factor(couchto5k$season))
```
Since a spelling error occured here, `autunm` was changed to `autumn`. For `week_stopped`, all values greater than 9 were converted to `NA`.

Next, box plot were used to see if there are extreme values (see Figure 1).
```{r fig.asp=.6, fig.cap="Figure 1: Box plots of all continuous variables", fig.align="center"}
boxplot(couchto5k[c(2,3,4,5,6)])
```

There are extreme and impossible values in the two variables `age` and `selfmot`, so first the values greater than 100 in `age` were change to `NA` and the values less than 0 in `selfmot` to `NA` as well. For `accountability`, although the minimum value `r min(couchto5k$accountability)` was also considered as extreme value in box plot, it was still possible that it is the true in the survey, so no change was made.

```{r cleaning, include = FALSE}
couchto5k$selfmot <- ifelse(couchto5k$selfmot < 0, NA, couchto5k$selfmot)
couchto5k$age <- ifelse(couchto5k$age > 100, NA, couchto5k$age)
couchto5k$week_stopped <- ifelse(couchto5k$week_stopped > 9, NA, couchto5k$week_stopped)
couchto5k$season = ifelse(couchto5k$season == "autunm", "autumn", couchto5k$season)

```

Table 2 showed all the values that were changed to `NA`. Next, all of these participants were removed from data analysis.

```{r}
couchto5k[!complete.cases(couchto5k),] %>% 
  kable(caption = "Table2: Summary of all missing values")
couchto5k <- na.omit(couchto5k)
```


After data cleaning, a total of 121 participants from Glasgow and Edinburgh took part in the programme in four different seasons and their ages ranged from 18-60 years. The distribution of the other variables can be seen in the table 3.

```{r descriptives}
kable(describe(couchto5k),caption = "Table 3: Summary of all variables after cleaning")

```

Figure 2 showed the correlations between all continuous variables. There were a moderate negative relationship between `age` and `health`; moderate positive relationships between `selfmot` and `happiness`, `selfmot` and `week_stopped`.
```{r fig.asp=.6, fig.cap="Figure 2: Correlations between all continuous variables", fig.align="center", warning=FALSE, message=FALSE}
library(GGally)
ggcorr(couchto5k,label = TRUE, label_size = 3)
```

# Question 1 

## Question 1a
To investigate whether the pattern of attrition rates of this programme differed from data from the earlier survey, a Chi-Square Goodness of Fit Test should be conducted.
First, a new variable `stoptime` was created through `mutate` function to indicate when the participant dropped out, i.e., all participants with a `week_stopped` value less than 5 will be categorized as `before`, while participants after five weeks and before the end will be categorized as `after`, and participants who finished will be categorized as `completed`.
Figure 3 show the proportions of participants in the three categories.

```{r q1a}
couchto5k <- couchto5k %>% 
  mutate(stoptime = ifelse(week_stopped<5, "before", ifelse(week_stopped<9, "after", "completed")))
table1 <- table(couchto5k$stoptime)

perc1 <- couchto5k%>%
  count(stoptime) %>% 
  mutate(proportion = prop.table(n))
```
```{r fig.asp=.6, fig.cap="Figure 3: Proportions of participants who abandoned the programme at different time", fig.align="center"}
ggplot(perc1, aes(stoptime, proportion))+
  geom_bar(stat= 'identity', position = 'dodge', fill="steelblue", alpha = 4/5)+
  theme_bw()+
  scale_x_discrete(breaks=c("after", "before", "completed"),
                      labels=c("before week 9", "before week 5", "completed"))+
  geom_text(aes(label=round(proportion, digits=2)), vjust=-0.3, size=3.5)
```
Results showed that the value of the chi squared was 1, and with a degree of freedom of 2 the *p*-value was 0.5, which was much greater than the α = 0.05, so the null hypothesis should be accepted, which means that the proportion of participants who gave up at different times in this programme was not significantly different from that in the previous programmes. 

```{r}
chisq.test(table1, p = c(0.1, 0.45, 0.45))
```




## Question 1b
To test for different patterns of attrition rates across cities, a Chi-Square Test of Independence should be conducted, where first the variable `stoptime` should be sorted by `city`. Figure 4 showed the distribution of attrition rates between cities.

```{r}
perc2 <- couchto5k %>%
  group_by(city) %>% 
  count(stoptime) %>% 
  mutate(proportion = prop.table(n))
```

```{r fig.asp=.6, fig.cap="Figure 4: Proportions in different cities", fig.align="center"}
ggplot(perc2, aes(city, proportion, fill = stoptime))+
  geom_bar(stat= 'identity', position = 'dodge', alpha = 2/3)+
  geom_text(aes(label=round(proportion, digits=2)), position = position_dodge(0.9), vjust=-0.3, size=3.5)+
  scale_fill_discrete(name = "stoptime", labels = c("before week 9", "before week 5", "completed"))
```

The test result showed that the value of the chi squared was 2, and with a degree of freedom of 2 the *p* was 0.5, which was  much greater than α = 0.05, so the null hypothesis should be accepted, which means that the patterns of attrition rates in Edinburgh and Glasgow did not differ from each other.

```{r warning=FALSE}
chisq.test(couchto5k$city, couchto5k$stoptime)
```

## Question 1c

To test whether there is a difference in the mean of the age between the two samples, a t-test should be conducted. First, the distribution of age from the two cities were shown in Figure 5.

```{r fig.asp=.6, fig.cap="Figure 5: The ages of participants", fig.align="center"}
ggplot(couchto5k, aes(city, age, color = city))+
  geom_boxplot()+
  theme_bw()+
  geom_point(shape=1, position=position_jitter(width=.05,height=.05))

```

The results of t-test indicated that the mean age of `r length(filter(couchto5k, city == "Edinburgh")$age)` participants in Edinburgh was `r mean(filter(couchto5k, city == "Edinburgh")$age)`, with a standard deviation of `r sd(filter(couchto5k, city == "Edinburgh")$age)`. The mean age of `r length(filter(couchto5k, city == "Glasgow")$age)` participants in Glasgow was `r mean(filter(couchto5k, city == "Glasgow")$age)`, with a standard deviation of `r sd(filter(couchto5k, city == "Glasgow")$age)`. The t value was 2, and with a degree of freedom of 62 the *p* was equal to 0.02, which means that the mean difference between the two groups was statistically significant. Therefore, the null hypothesis should be rejected and we could conclude that the average ages of participants who commenced the programme indeed differ by city. The average age of participants in Edinburgh was `r mean(filter(couchto5k, city == "Edinburgh")$age)-mean(filter(couchto5k, city == "Glasgow")$age)` higher than in Glasgow.


```{r}
t.test(filter(couchto5k, city == "Edinburgh")$age, filter(couchto5k, city == "Glasgow")$age, paired = F, alternative = "two.sided")
```

# Question 2
## Question 2a

To investigate the effect of `season` on `happiness` ratings, a regression model should be built, in which `season` as independent variable and `happiness` as dependent variable. Here I first rearranged the levels of the `season` and used `spring` as the baseline for the model. According to the results we can see that the happiness ratings in `winter` (*ß* = -21.92, *t* = -2.44, *p* < 0.05) differs from that in `spring`, which indicate that for participants who were interviewed in winter, the average happiness rating was `r mean(filter(couchto5k, season == "winter")$happiness)`, which was 21.92 lower than those who were interviewed in spring (*M* = `r mean(filter(couchto5k, season == "spring")$happiness)`) (*F*(3,117) = 2.39, *p* = 0.0722, adjusted *R^2^* = 0.0336, note that here the *p* value was not significant).  


```{r message=FALSE}
couchto5k$season <- factor(couchto5k$season, levels = c("spring", "summer","autumn","winter"))
lm1 <- lm(happiness ~ season, couchto5k)
tab_model(lm1, title = "Table 4: Results of Regression model of question 2a", show.stat = T, p.style = "numeric_stars")

```
  
  

However, if we take a closer look at the changes in `happiness` indices by season in Figure 6, we should wonder if the mean values of participants in spring and summer were significantly higher than the mean values in autumn and winter?



```{r fig.asp=.6, fig.cap="Figure 6: Happiness in different Seasons", fig.align="center"}
ggplot(couchto5k, aes(season, happiness, color = season))+
  geom_boxplot()+
  theme_bw()+
  geom_point(shape=1, position=position_jitter(width=.05,height=.05))
```
```{r}
couchto5k <- couchto5k %>% 
  mutate(new_season = ifelse(couchto5k$season == "spring", "spring and summer", ifelse(couchto5k$season == "summer", "spring and summer", "autumn and winter")))

```

  
  
So I created a new variable `new_season` that categorized spring and summer, and autumn and winter respectively. The regression analysis was then conducted with `new_season` as the independent variable and the `happiness` rating as the dependent variable. According to the results shown in Table 5 we can see that there was a significant difference (*ß* = 19.25, *t* = 2.64, *p* < 0.05), which indicate that for participants who were interviewed in autumn and winter, the average happiness rating was `r mean(filter(couchto5k, new_season == "spring and summer")$happiness)`, which was almost 19.25 higher than those who were interviewed in autumn and winter (*M* = `r mean(filter(couchto5k, new_season == "autumn and winter")$happiness)`) (*F*(1,119) = 6.97, *p* < 0.05, adjusted *R^2^* = 0.0474, note that here the *p* value was significant).

```{r message=FALSE}
lm_season <- lm(happiness ~ new_season, couchto5k)
tab_model(lm_season, title = "Table 5: Results of the Regression model using spring and summer, autumn and winter as predictors ", show.stat = T, p.style = "numeric_stars")

```

  
  
  
Figure 7 shows the `happiness` ratings in these two time periods.



```{r fig.asp=.6, fig.cap="Figure 7: Happiness in different Seasons", fig.align="center"}
ggplot(couchto5k, aes(new_season, happiness, color = new_season))+
  geom_boxplot()+
  theme_bw()+
  geom_point(shape=1, position=position_jitter(width=.05,height=.05))+
  xlab("season")
```


## Question 2b

A linear regression model was built to test the effect of `age` on `happiness` ratings, in which `age` as independent variable and `happiness` as dependent variable. Result showed that there was no significant effect could be observed (*ß* = 0.38, *t* = 1.65, *p* = 0.102), which means that we cannot use `age` to predict the changes in `happiness` (*F*(1,119) = 2.72, *p* = 0.102, adjusted *R^2^* = 0.0141).  


```{r message=FALSE}
lm2 <- lm(happiness ~ age, couchto5k)
tab_model(lm2, title = "Table 6: Results of the Regression model using age as predictor", show.stat = T,  p.style = "numeric_stars")
```

## Question 2c

Based on the previous analyses, I could assume that due to the significant effect of season on the changes in happiness ratings, it would be much better to choose the regression model, which use `new_season`as independent variable and `happiness` as dependent variable, as the baseline model. Since no significant effect of `age` on `happiness` can be observed, the baseline model will not include age as independent variable. Also, through anova tests between null model and model using `new_season`, model using `season` as predictor, respectively, we can see that model using `new_season` was significantly better than the null model (*F* = 6.97, *p* < 0.05), but the model using `season` as predictor was not significantly better than the null model (*F* = 2.39, *p* = 0.072).

```{r q2c}
anova(lm_season) %>% kable(caption = "Table 7: Result of the comparison betwenn null model and model using `new_season` as predictor")
anova(lm1) %>% kable(caption = "Table 8: Result of the comparison betwenn null model and model using `season` as predictor")

```

# Question 3

## Question 3a

To test the effect of completion on the changes in `happiness` ratings, I created a new variable first, which equals to `yes` when the value of `stoptime` created before is `completed`, and for other conditions `no`. Then this new variable `completion` was added to the baseline model. According to the results shown in table 9, there is no significant effect of `completion` can be observed in this model (*ß* = 0.02, *t* = 0.00, *p* = 0.997), which means that the `happiness` ratings are not affected by whether the participants completed the programme or not (*F*(2,118) = 3.46, *p* < 0.05, adjusted *R^2^* = 0.0393).

```{r message=FALSE}
couchto5k <- couchto5k %>% 
  mutate(completion = ifelse(stoptime == "completed", "yes", "no"))

lm4 <- lm(happiness ~ completion + new_season, couchto5k)
tab_model(lm4, title = "Table 9: Results of the Regression model using completion and season as predictors", show.stat = T, p.style = "numeric_stars")

```

## Question 3b

Based on the model created in the question 3a, `health` was added in to this model as independent variable. As shown in table 10, there was no significant effect of `health` can be observed in this model (*ß* = -0.39, *t* = -1.31, *p* = 0.191), which means that the `happiness` ratings are not additionally affected by the `health` of participants (*F*(3,117) = 2.89, *p* < 0.05, adjusted *R^2^* = 0.0452).  


```{r message=FALSE}
lm5 <-lm(happiness ~ completion + new_season + health, couchto5k)
tab_model(lm5, title = "Table 10: Results of the Regression model using completion, health and season as predictors", show.stat = T, p.style = "numeric_stars")
```

## Question 3c

Based on the model built in question 3b, the hypothesis is actually to test whether participants who completed the programme or not followed different patterns of the relationship between `health` and `happiness`. So the interaction effect between `health` and `completion` should be examined. The regression model therefore included the main effect of `new_season`, `health` and `completion`, and also the interaction `health*completion`. Result showed that all the three main effects,`new_season`(*ß* = 21.25, *t* = 2.89, *p* < 0.05), `health`(*ß* = -1.26, *t* = -3.34, *p* < 0.05) and `completion`(*ß* = -111.28, *t* = -3.37, *p* < 0.05), were significant. Also, the interaction between `health` and `completion` was significant (*ß* = 1.97, *t* =  3.47, *p* < 0.05). This regression model therefore indicated that the relationship between `health` and `happiness` did differ for participants who completed the programme or not (*F*(4,116) = 5.38, *p* < 0.05, adjusted *R^2^* = 0.127).  



```{r message=FALSE}
lm6 <-lm(happiness ~ new_season + health*completion, couchto5k)
tab_model(lm6, title = "Table 11: Results of the Regression model using completion, health and season as predictors, including the interaction between completion and health", show.stat = T, p.style = "numeric_stars")

```

  
  
  
However, when we check the basic assumptions of linear regression, the results showed that there were still some problems with this model. Shapiro-Wilk test indicated evidence against the null hypothesis that the residuals were not drawn from a normally distributed population: W=1, *p* < 0.05). Also, the results of Variance Inflation Factor (VIF) test showed that there could be some correlations between predictors.  


```{r}
shapiro.test(residuals(lm6))

vif(lm6) 
```

Therefore, to solve this problem, I first scaled the value of `health`, then used the scaled variable `health_z` to replace the `health` variable in the regression model. The result of the new model using `health_z` as predictor showed that the main effects of `new_season`(*ß* = 21.25, *t* = 2.89, *p* < 0.05) and `health_z`(*ß* = -12.69, *t* = -3.34, *p* < 0.05) were significant. Also, the interaction between `health` and `completion` was significant (*ß* = 19.83, *t* = 3.47, *p* < 0.05). But the main effect of `completion` was no longer significant (*ß* = 1.32, *t* = 0.22, *p* = 0.824). The new regression model also indicated that the relationship between `health` and `happiness` did differ for participants who completed the programme or not (*F*(4,116) = 5.38, *p* < 0.05, adjusted *R^2^* = 0.127).  


```{r message=FALSE, warning=FALSE}
couchto5k <- 
  couchto5k %>% 
    mutate(health_z = (health-mean(health))/sd(health))
lm7 <-lm(happiness ~ new_season + health_z*completion, couchto5k)
tab_model(lm7, title = "Table 12: Results of the Regression model using completion, health and season as predictors, including the interaction between completion and health", show.stat = T, p.style = "numeric_stars")
```
  
  
  
More importantly, now the results of Variance Inflation Factor (VIF) test no longer showed any problems. However, the residuals were still not drawn from a normally distributed population (w = 1, *p* < 0.05).   


```{r}
shapiro.test(residuals(lm7)) #normality
vif(lm7) # multicollinearity
```
  
  
For other assumptions check test, no problem was detected (Durbin-Watson test for autocorrelation of residuals: *DW* = 2.06, *p* =0.692, non-constant variance test: *χ^2^*(1)=0.15, p=0.7).  


```{r}
dwt(lm7) #independence
ncvTest(lm7)#homogeneity of variance
```

Therefore, it is necessary to discuss about the effect of residuals normality, although it is one of the necessary assumptions of linear regression, it has been pointed out that linear regression models are fairly robust to violations of the normality assumption, especially in large sample sizes (observations per variable >10) (Schmidt & Finan, 2018). In this model, where the number of observations is relatively high (121), we can assume that violating this assumption does not have noticeably impact results, especially when all other assumptions are not violated.

Figure 8 showed the relationship between `health` and `happiness` considering different completion conditions.

```{r  fig.asp=.6, fig.cap="Figure 8: Changes in Happiness as a Function of Health considering completion", fig.align="center", message=FALSE}
ggplot(couchto5k, aes(health_z, happiness, colour = completion))+
  geom_smooth(method = "lm", se = F)+
  geom_point()+
  xlab("health (Z-scored)")
  
```


## Question 3d
Based on the regression model built in question 3c, we can conclude some causes of happiness in the data of 121 participants. First, the form of the final model is:  
  Happiness = 28.11 + 21.25⋅New_season - 12.69⋅Health + 1.32⋅Completion + 19.83⋅Health⋅Completion + *ϵ*  
  
  
According to this form and the regression model, we can conclude that there was a significant conditional association between happiness and season (*p* < 0.05), suggesting that for those at the mean level of health, the scores of on the happiness ratings participants who were interviewed in spring and summer was higher by 21.25 compared to those who were interviewed in autumn and winter. Besides, a significant conditional association was evident between health (Z-scored) and happiness rating (*p* < 0.05), and this association was dependent upon whether participants completed the programme. For participants who completed the programme, scores on the happiness ratings increase by 7.14 for every 1 standard deviation increase in health scores. But for participants who did not completed the programme, scores on the happiness ratings decrease by -12.69 for every 1 standard deviation increase in health scores. This interaction is visually presented in Figure 8.  


The results indicate that the happiness rating first affected by the season (spring & summer or autumn & winter), and also can be predicted by health test, but this association depends largely on whether participants stopped earlier. 

# Question 4
Figure 9 and 10 showed the average happiness ratings of participants who completed the programme grouped by season and city. In Figure 9 the data was group by the original variable `season` (spring, summer, autumn, winter), in Figure 10 the data was group by the created variable `new_season` (spring & summer, autumn & winter).  


```{r fig.asp=.6, fig.cap="Figure 9: The average happiness for participants who completed the programme group by season (spring, summer, autumn, winter)", fig.align="center", message=FALSE}

filter(couchto5k, completion == "yes") %>% 
  group_by(season, city) %>%
  summarise(happiness = mean(happiness)) %>% 
  ggplot(aes(city, happiness, fill = season))+
  geom_bar(stat= 'identity', position = 'dodge', alpha = 2/3)+
  geom_text(aes(label=round(happiness, digits=2)), position = position_dodge(0.9), vjust=-0.3, size=3.5)

```

```{r fig.asp=.6, fig.cap="Figure 10: The average happiness for participants who completed the programme grouped by new_season (spring & summer, autumn & winter)", fig.align="center", message=FALSE}
couchto5k$new_season <- factor(couchto5k$new_season, levels = c("spring and summer","autumn and winter"))
filter(couchto5k, completion == "yes") %>% 
  group_by(new_season, city) %>%
  summarise(happiness = mean(happiness)) %>% 
  ggplot(aes(city, happiness, fill = new_season))+
  geom_bar(stat= 'identity', position = 'dodge', alpha = 2/3)+
  geom_text(aes(label=round(happiness, digits=2)), position = position_dodge(0.9), vjust=-0.3, size=3.5)+
  scale_fill_manual("season", values=c("orange","red"))
```

# Question 5

## Question 5a
To investigate which variable can predict the likelihood of dropping out, generalized linear models should be conducted. First, a new variable `complete` was created, for `completion` equaled to "yes", the value equaled to 0 and for `completion` equaled to "no", the value equaled to 1.  
Then 7 generalized linear models (`complete` as dependent variable) were built to examine the effect of all variable on the likelihood of dropping out.


```{r q5a}
couchto5k <- couchto5k %>% 
  mutate(complete = ifelse(completion == "yes", 0,1))
glm <- glm(complete ~ selfmot, data=couchto5k, family = "binomial")

glm2 <- glm(complete ~ new_season, data=couchto5k, family = "binomial")

glm3 <- glm(complete ~ accountability, data=couchto5k, family = "binomial")

glm4 <- glm(complete ~ age, data=couchto5k, family = "binomial")

glm5 <- glm(complete ~ happiness, data=couchto5k, family = "binomial")

glm6 <- glm(complete ~ health_z, data=couchto5k, family = "binomial")

glm7 <- glm(complete ~ city, data=couchto5k, family = "binomial")
```


Results in table 13 showed that only `selfmot` (*odds ratios* = -0.15, *z* = -2.13, *p* < 0.05) and `new_season` (*odds ratios* = -1.69, *z* = -3.11, *p* < 0.05) had significant effect on the likelihood of dropping out. 

```{r}
tab_model(glm, glm2, glm3, glm4, glm5, glm6, glm7, title = "Table 13: Results of the generalized linear models", show.stat = T, transform = NULL, p.style = "numeric_stars")

```
  
  
  
After, `selfmot` was scaled and used as predictor in a new generalized linear models to test the effect on the likelihood of dropping out. Results in table 14 showed that the effect of scaled `selfmot` (*odds ratios* = -0.41, *z* = -2.13, *p* < 0.05) was still significant. Also, this model was significantly better than null model (p < 0.05, see table 15).  


```{r}
couchto5k <- 
  couchto5k %>% 
    mutate(selfmot_z = (selfmot-mean(selfmot))/sd(selfmot))

glm1 <- glm(complete ~ selfmot_z, data=couchto5k, family = "binomial")
tab_model(glm1, title = "Table 14: Results of the generalized linear model using self-motivation (Z-scored) as predictor", show.stat = T, transform = NULL, p.style = "numeric_stars")
anova(glm1, test = "Chisq") %>% kable(caption = "Table 15: Comparision between generalized linear model using self-motivation (Z-scored) as predictor and null model")


```

The assumptions check indicated also no problem for this model (independence of errors: Durbin-Watson test for autocorrelation of residuals: *DW*=2.1, *p*>0.05; see Figure 11 for the residuals vs fitted values)
```{r fig.asp=.6, fig.cap="Figure 11: Residuals vs Fitted plot of generalized linear model using self-motivation as predictor", fig.align="center", message=FALSE}
dwt(glm)
plot(glm, which = 1)
```

For the generalized linear model using season (spring & summer, autumn & winter) as predictor, it was also better than the null model (*p* <0.05, see table 16). 
```{r}
glm2 <- glm(complete ~ new_season, data=couchto5k, family = "binomial")
anova(glm2, test = "Chisq") %>% kable(caption = "Table 16: Comparision between generalized linear model using season (spring & summer, autumn & winter) as predictor and null model")


```
Also there was no probelm with the assumptions check (independence of errors: Durbin-Watson test for autocorrelation of residuals: *DW*=2.1, *p*>0.05; see Figure 12 for the residuals vs fitted values)

```{r fig.asp=.6, fig.cap="Figure 12: Residuals vs Fitted plot of generalized linear model using season  (spring & summer, autumn & winter) as predictor", fig.align="center", message=FALSE}
dwt(glm2)
plot(glm2, which = 1)
```


## Question 5b

For the generalized linear model using season (spring & summer, autumn & winter) as predictor, this model indicated than for participants who were interviewed in spring and summer, the likelihood of dropping out was 37.9% higher than for those who were interviewed in autumn and winter (see Figure 13).



```{r fig.asp=.6, fig.cap="Figure 13: Probability of quitting in seasons (spring & summer, autumn & winter)", fig.align="center", message=FALSE}
plot(allEffects(glm2), xlevels=list(complete = 0:1), ylab="Probability of quitting", xlab = "season", main = "")
```



For the generalized linear model using self-motivation (Z-scored) as predictor, it indicated that the the likelihood of dropping out decrease by 10.1% for every 1 standard deviation increase in self-motivation (see Figure 14).

## Question 5c

```{r fig.asp=.6, fig.cap="Figure 14: Changes in probability of quitting as a function of self-motivation", fig.align="center", message=FALSE}

ggplot(couchto5k, aes(x=selfmot_z, y=complete)) + 
  geom_point(shape=1, position=position_jitter(width=.05,height=.05)) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F, colour="red") +
  labs( x = "Self-motivation (Z-scored)", y = "Probability of quitting")+
  theme_bw()
```



# References
Schmidt, A. F., & Finan, C. (2018). Linear regression and the normality assumption. *Journal of clinical epidemiology*, 98, 146-151. https://doi.org/10.1016/j.jclinepi.2017.12.006






