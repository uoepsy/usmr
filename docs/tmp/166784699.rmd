---
title: "USMR 2021-2022 Coursework"
author: "`r params$examnumber`"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
  pdf_document: default
params:
  examnumber: B195720
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)
# load any other packages that you require here:
library(tidyverse)
library(knitr)
library(sjPlot)
library(gridExtra)
library(pander)
library(kableExtra)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0
<!-- If you have run the R code above this point (you can do this now by pressing Ctrl-Shift-Alt-P, or running the chunk above) then your data will be in a dataframe called `couchto5k`. -->


```{r cleaning, include = FALSE}
total = length(couchto5k$pptID)

couchto5k <- couchto5k %>% 
  mutate (
    age = ifelse(age<0 | age>120, NA, age),
    accountability = ifelse(accountability < 5 | accountability >35, NA, accountability),
    selfmot = ifelse(selfmot < 5 | selfmot >35, NA, selfmot),
    health = ifelse(health < 0 | health >100, NA, health),
    happiness = ifelse(happiness<0 | health >100, NA, happiness),
    season = ifelse(match(season, c('autumn', 'winter', 'spring','summer')), season, NA),
    city = ifelse(city %in% c('Edinburgh', 'Glasgow'), city, NA),
    week_stopped = ifelse(week_stopped <1 | week_stopped>9, NA, week_stopped)
    ) %>%drop_na()

couchto5k <- couchto5k[((c('pptID','age','accountability','selfmot','health','happiness','week_stopped','season','city')))]

couchto5k$season <- as.factor(couchto5k$season)
couchto5k$city <- as.factor(couchto5k$city)

samples = length(couchto5k$pptID)
```

`r total - samples` examples were removed from the dataset due to impossible values. The below table details the mean values for the remaining samples which have numeric values.

```{r descriptive1, warning = FALSE}

couchto5k %>%summarise(
  across(.cols = c(2:7), mean)
  ) %>% kable(col.names = c('Age','Accountability (/35)','Self motivation (/35)','Health (/100)','Happiness (/100)', 'Week Stopped (/9)'), caption = 'Mean values for numerical variables', "simple")

couchto5k %>%summarise(
  across(.cols = c(2:7), sd)
  ) %>% kable(col.names = c('Age','Accountability (/35)','Self motivation (/35)','Health (/100)','Happiness (/100)', 'Week Stopped (/9)'), caption = 'Standard Deviation for numerical variables', "simple")

```

Additionally, the density of the numerical variables are also plotted below:

```{r descriptive2,fig.align='center', warning = FALSE }

par(mfrow = c(2,2))
fst<-ggplot(couchto5k, aes(x = accountability)) + geom_density()  + labs(x = 'Accountability', y= 'Density')
snd<-ggplot(couchto5k, aes(x = selfmot))  + geom_density() + labs(x ='Self-Motivation', y= 'Density')
trd<-ggplot(couchto5k, aes(x = age)) + geom_density() + labs(x='Age', y ='Density')
foth<-ggplot(couchto5k, aes(x = health)) + geom_density() + labs(x='Health', y ='Density')
fth<- ggplot(couchto5k, aes(x = happiness)) + geom_density() + labs(x='Happiness', y ='Density')
sth<- ggplot(couchto5k, aes(x = week_stopped)) + geom_density() + labs(x='Week Stopped', y ='Density')

grid.arrange(fst,snd,trd,foth, fth,sth)
```

As we can see from these graphs, Accountability, Self-Motivation and Health all appear to be normally distributed.

The categorical variables are distributed as demonstrated below:

```{r descriptive3, fig.align='center'}

fst<-ggplot(couchto5k, aes(x=season)) +geom_bar() +labs(x='Season',y='Count',title='Number of Participants by Season')
snd<-ggplot(couchto5k, aes(x=city)) +geom_bar() +labs(x='City',y='Count',title='Number of Participants by City')

grid.arrange(fst,snd,nrow =1)

```

# Question 1 

## Question 1a

A Chi-squared test will be used to determine whether our data reflects the results in the previous survey. The null hypothesis is that the distribution of our samples does match the distribution in the previous survey and the alternative hypothesis is that it does not.

```{r q1a, fig.align='center'}

couchto5k <- couchto5k %>% mutate(
  before_halfway = ifelse(week_stopped<5, T, F),
  before_end = ifelse(week_stopped<9 & week_stopped>4, T, F),
  completed = ifelse(week_stopped ==9, T, F)
)

chi_df <- couchto5k %>% summarise(across(.cols = c(10:12), sum))

chisq.test(chi_df, p=c(.45, .1,.45)) %>% pander()

```

Conducting a Chi-squared test with 2 degrees of freedom using probabilities (.45, .1, .45), a p-value of 0.5576 is achieved, which is higher than our threshold $\alpha$ p-value for significance of 0.05. This means we cannot reject the null hypothesis and therefore our data is likely distributed in the same manner as that in the previous survey.


## Question 1b

The table below details attrition rates for each city.

```{r q1b}

stopped_glasgow <- couchto5k[couchto5k$city == 'Glasgow',] %>% summarise(across(.cols =c(10:12), sum))
stopped_edinburgh <- couchto5k[couchto5k$city == 'Edinburgh',] %>% summarise(across(.cols =c(10:12), sum))

stopped_glasgow_per <- (stopped_glasgow/sum(stopped_glasgow))
stopped_edinburgh_per <- (stopped_edinburgh/sum(stopped_edinburgh))
both <- rbind(stopped_glasgow_per, stopped_edinburgh_per)
colnames(both) <- c('Stopped Before Halfway', 'Stopped Before The End', 'Completed')
rownames(both) <- c('Glasgow', 'Edinburgh')

both %>% pander()
```


A Chi-Squared test, with the participants that stopped in Edinburgh constituting the expected counts and the participants in Glasgow the observed counts, will help determine whether these counts are in significantly similar proportions (null hypothesis), or significantly dissimilar (alternative hypothesis)

```{r q1b2, warning=FALSE}
chisq.test(table(stopped_edinburgh, stopped_glasgow)) %>% pander()

```

The p-value from the Chi-squared test is 0.1991, which is larger than our threshold p-value for significance (0.05). Therefore we cannot reject the null hypothesis and we will conclude that the distributions are not significantly dissimilar.

## Question 1c

The below table details the mean ages for participants from Glasgow and Edinburgh.

```{r q1c}

glasgow_ages <- couchto5k[couchto5k$city == 'Glasgow',]$age
edinburgh_ages <- couchto5k[couchto5k$city == 'Edinburgh',]$age
glasgow_age <- mean(glasgow_ages)
edinburgh_age <- mean(edinburgh_ages)

age_both <- rbind(glasgow_age, edinburgh_age)
colnames(age_both) <- 'Age'
rownames(age_both) <- c('Glasgow', 'Edinburgh')
age_both %>% pander()
```



We can perform a t-test to determine whether these means are significantly different (alternative hypothesis).

```{r q1c2, warning=FALSE}
t.test(glasgow_ages, edinburgh_ages)%>%pander()
```

The p-value in our t-test is 0.9, which is much larger than our threshold p-value for significance (0.05). Therefore we cannot reject the null hypothesis and we will conclude that the mean ages for both cities are not significantly dissimilar.

# Question 2

## Question 2a

To measure whether happiness rating is affected by season, we will fit a linear regression model with the categorical variable season predicting happiness as per the below equation:

$Happiness = \beta_0 +  Se_{sp}\beta_1 + Se_{su}\beta_2 + Se_{wi}\beta_3$

Where 'SE' represents season and the subscript 'sp' stands for Spring, 'su' for Summer and 'wi' for Winter, with Autumn being merged into the intercept value. Each category is represented by a dummy variable which can be set to either 1 or 0, depending on whether that category is selected. 

The coefficients for our model are below:

#### Coefficients for Linear Model of Happiness Predicted by Season
```{r q2a, table.align='center'}

b<-lm(happiness ~ season, data = couchto5k)

summary(b) %>% pander()

```

In the above table we can see that the value of happiness for the baseline category (Autumn) is 45.82, and Spring and Summer have positive values (indicating people are happier than they are in Autumn) whilst winter has a negative value (indicating that people are less happy than they are in Autumn). 

The residual plots for this model are below:

```{r q2a2, fig.align ='center'}
par(mfrow = c(2,2))
plot(b, which=1:4)
mtext('Residual Plots for Linear Model of Happiness as Predicted by Season', side = 3,line = -2, outer = TRUE)
```

From these graphs, it appears that the residuals are likely normally distributed. The only metric which could cause us some worry are the ends of the QQ-plot, which do not align with the rest of the data. However, this is likely explained by the large number of people who put a value of either 0 or 100 in the happiness metric. This does highlight the difficulty with a simplistic self-reporting measure for happiness, where large numbers of people tend to select the extreme values in the whole range, rather than making a considered decision about their overall happiness levels. To determine whether season significantly improves our model's ability to predict happiness, we can run an Anova test, the results of which are below:

```{r q2a3}

anova(b) %>% pander()

```

Looking at the Anova results shows that adding season to our model does not produce significant results over a model with only zero valued coefficients as the p-value for our F statistic is 0.36 which is lower than the threshold p-value for significance (0.05). This does not allow us to reject the null hypothesis and so adding season does not significantly improve our model's ability to predict happiness.

## Question 2b

Adding age to our linear model changes the coefficients as shown in the table below:

#### Coefficients for Linear Model of Happiness Predicted by Season and Age
```{r q2b}

c<-lm(happiness ~ season + age, data = couchto5k)
summary(c) %>% pander()
```

Although the $R^2$ value (amount of variability explained by our model) has improved over the model which only used season as a predictor, $R^2$ will always improve when we add a new predictor to our model. To determine whether the improvement is significant, we will perform an anova test, the results from which are below:

```{r q2ba}
anova(c) %>% pander()
```

The p-value for age (0.1311) is less than the threshold p-value for significance, therefore we cannot reject the null hypothesis and we must conclude that adding age does not significantly improve our model's ability to predict happiness.

## Question 2c

As neither age, nor season, were able to significantly improve our model's ability to predict happiness, we will use a baseline model which models happiness with the self-motivation metric (selfmot) which, as will be demonstrated below, is able to significantly improve our models ability to predict happiness. The coefficients for this model are below:

#### Coefficients for Linear Model of Happiness Predicted by Self-Motivation
```{r q2c}

d <- lm(happiness ~ selfmot, data = couchto5k)

summary(d) %>% pander()
```

The interpretation of these coefficients would be that for a one unit increase in self-motivation score, we would expect a 2.75 unit increase in happiness from a baseline happiness score of 10.06.

As we can see from the $R^2$ value, this model is able to explain 8% of the variability of the data. This is much larger than the 2% explained by the model where we were predicting happiness using season. The residual plots below show that the residuals are also roughly normally distributed. 

```{r q2c2, fig.align = 'center'}
par(mfrow = c(2,2))
plot(d, which=1:4)
mtext('Residual Plots for Linear Model of Happiness as Predicted by Self-Motivation', side = 3,line = -2, outer = TRUE)
```

To determine whether self-motivation significantly improves our model's ability to predict happiness we can perform an Anova test, the results from which are below:

```{r q2c3}
anova(d)%>%pander()
```

The p-value of the F statistic in the Anova test for this model is 0.001425, which is lower than our threshold p-value for significance. Therefore we can reject the null hypothesis and say that selfmot significantly improves our model's ability to predict happiness, as compared to a model where all the regression coefficients are zero.

# Question 3

## Question 3a

We can model whether completion affects happiness outcomes by adding it as a term to our baseline model. The coefficients for this model are below:

#### Coefficients for Linear Model of Happiness Predicted by Self-Motivation and Completion
```{r q3a}

e <- lm(happiness ~ selfmot + completed, data = couchto5k)
summary(e) %>% pander()

```

The interpretation of these coefficients would be that for a one unit increase in self-motivation, the happiness score would increase by 2.96, from a baseline score of 8.47 where the participant had not completed the programme. Completing the programme lowers happiness score by 3.50, however the p-value for this coefficient is lower than our threshold for significance. The $R^2$ value does not change from our baseline model, so adding in completion does not allow our model to explain any additional variability.

When we carry out an anova test on this model we can see that adding in completion does not significantly improve our model's ability to predict happiness as the p-value for completed (0.5688) is higher than our threshold value for significance.

```{r q3a2}
anova(e)%>% pander()
```

For future models, we will remove the completed variable. It is not a significant predictor of happiness and, as we are using Type I sum of squares in the anova function, the order of predictors matters, so any extra terms will only serve to muddy calculations.


## Question 3b

As completion was not a significant predictor of happiness, we will model health's effect on happiness by adding it as a predictor to the baseline model. The coefficients for this model are below:

#### Coefficients for Linear Model of Happiness Predicted by Self-Motivation and Health
```{r q3b}

e <- lm(happiness ~ selfmot +  health, data = couchto5k)

summary(e) %>% pander()
```

The coefficient for health is not significant, with a p-value lower than our threshold for significance, and is also very small. We can confirm that health does not significantly improve our model's ability to predict happiness using an Anova test.

```{r q3b2}
anova(e) %>% pander()
```

The p-value for health (0.7526) is much higher than our threshold for significance, so we cannot reject the null hypothesis and therefore health does not significantly improve our model's ability to predict happiness.


## Question 3c

To determine whether there is any interaction between the health metric and progression in the programme, we will add an interaction predictor into our linear model. The coefficients for this model are below:

#### Coefficients for Linear Model of Happiness with Interaction Term
```{r q3c}
f <- lm(happiness ~ selfmot + health + health:week_stopped, data = couchto5k)
summary(f) %>% pander()
```

Adding in this interaction term does not affect our $R^2$ value, so the extra term does not explain any more of the variability in the system. We can run an anova test to determine whether the added term significantly improves our model's ability to predict happiness.

```{r q3c2}
anova(f) %>% pander()
```

The p-value for our interaction term is less than the threshold for significance, therefore we cannot reject the null hypothesis and the interaction term does not significantly improve our ability to predict happiness.

## Question 3d

A dataset of 130 participants were obtained from https://uoepsy.github.io/data/usmr_2122_data.R. `r total-samples` participants were removed due to impossible values in the dataset, leaving `r samples` participants for analysis. Each participant completed two quizzes which assesed their accountability and self-motivation through a series of 5 questions, where the final score was a sum of the scores from the 5 questions (which were themselves marked from 1-7), with a minimum total score of 5 and maximum score of 35. Additionally, participants were asked to rate their happiness and health on a scale from 1-100. Other information provided was the participant's age, the week they stopped the course (a number from 1-9 where 9 means the participant completed the course), the season that they were interviewed in and the city they were recruited in (Edinburgh or Glasgow).

To determine the various causes of happiness, a baseline linear model was fit defined by the below equation:

$Happiness = \beta_0 + S\beta_1$

Where S is the self-motivation metric. This was chosen as the baseline model as self-motivation significantly improved the models ability to predict happiness, as per the p-value of the selfmot coefficient in the anova test result below:

`r anova(d)%>%pander()`

This p-value of 0.001425 is lower than the threshold value for statistical significance (0.05), the  null hypothesis was rejected and it was concluded that adding self-motivation as a predictor does significantly improve the model's ability to predict happiness.

The residuals of this model are roughly normally distributed, as is demonstrated by the roughly flat residuals vs fitted and linear Q-Q plots:

```{r q3d1, fig.align='center'}
par(mfrow = c(1,2))
plot(d, which=1:2)
mtext('Residual Plots for Linear Model of Happiness as Predicted by Self-Motivation', side = 3,line = -2, outer = TRUE)
```

Each of the other predictors were then added to this baseline model and an Anova test was conducted on each new model to determine whether adding the predictor significantly improved the model's ability to predict happiness. Besides self-motivation, no other predictor was able to significantly improve the model's ability to predict happiness. Therefore we must conclude that self-motivation is the only driving factor in determining overall happiness levels from the predictors used in this experiment.

# Question 4

The plot for average happiness ratings by city for participants that completed the programme is below:

```{r q4, fig.align='center'}
sep_data <- couchto5k[couchto5k$completed == TRUE,]

glasgow_compl <- sep_data[sep_data$city == 'Glasgow',]
edinburgh_compl <- sep_data[sep_data$city == 'Edinburgh',]
colours <- c("#E69F00", "#56B4E9")
ggplot(data = sep_data, aes(x = season, y = happiness, fill = city)) + geom_bar(stat = 'summary', fun = 'mean', position = 'dodge') +scale_fill_manual(values=colours) + labs(x='Season', y='Happiness', title='Mean Happiness Ratings by Season and City', fill='City')
```


# Question 5

## Question 5a

The model chosen for predicting the likelihood of dropping out is a binomial logistic regression model. The predictors for the model were chosen by initially creating a model with all the predictors added to it, performing an anova test on the model and removing the predictors which did not significantly improve the model's ability to model likelihood. The anova table for the full model is presented below:


```{r q5a1}
logitic<-glm(completed ~ selfmot+season + age + accountability+health+happiness+city, data = couchto5k, family = "binomial")
anova(logitic, test="Chisq") %>% pander()
```

From the above values, we can see that only season and selfmot have p-values which are below our significance threshold value. Therefore we can reject the null hypothesis for these coefficients and conclude that they significantly improve our model's ability to model likelihood over a model with zero coefficients. 

The next step is to build a model which only uses these predictors to model likelihood of completion, the coefficients for which are below:

```{r q5a2}
logitic<-glm(completed ~ selfmot+season, data = couchto5k, family = "binomial")
summary(logitic) %>%pander()
test<-couchto5k[c('selfmot','season')]
guess<-predict(logitic, newdata =test, type = 'response')
guess<-ifelse(guess>0.5,TRUE,FALSE)
hits <- sum(guess == couchto5k$completed)
accuracy<-hits/samples
```

This model is able to achieve `r accuracy*100`% accuracy when predicting whether a participant from the original 119 participants will complete the programme, based on the season and self-motivation score.

## Question 5b

To final model was fitted to 119 participants and took the form:

$Completed = \beta_0 + S\beta_1 + Se_{sp}\beta_2 + Se_{su}\beta_3 + Se_{wi}\beta_4$

Where 'S' stands for the self-motivation metric and 'Se' represents the categorical variable of season, with the subscript 'sp' for Spring, 'su' for Summer and 'wi' for Winter, with autumn being represented by the intercept coefficient. The coefficients for this model are below:

#### Coefficients for Logistic Regression Model of Completion
```{r q5b}
summary(logitic) %>% pander()
```

For self-motvation (self-mot), significance indicates that the coefficient is significantly different from zero. For the categorical variables (season), significance indicates that the mean value for that category is significantly different from the baseline category (in this case Autumn).

The coefficient values represent logits, or log-odds ratios, so for positive valued coefficients if that metric increases the probability of completion increases and for negative valued coefficients if that metric increases the probability of completion decreases. The probability of completion can be directly calculated by exponentiating the coefficients and then converting them to probabilities. The exponentiated coefficients are below:

#### Exponentiated Coefficients of Logistic Regression Model of Completion
```{r q5b2}

exp(coefficients(logitic)) %>% pander() 
```

For a participant interviewed in the Autumn, with a self-motivation value of 30 the probability of completion could be determined by the below calculation:

$$Odds = 0.0002632+1.937*30 \\
=   58.11\\
ProbComplete = \frac{Odds}{1+Odds}\\
             = \frac{58.11}{1+58.11}\\
             = 0.98$$

So the probability of completion for this participant would be 0.98.

From these calculations, it is clear that self-motivation is the main driving factor as to whether a participant completes the programme or not. The odds ratios for each of the coefficients is plotted below:

```{r q5b3, fig.align='center'}
plot_model(logitic, title = "Odds Ratios for Logistic Regression Model of Completion")
```

## Question 5c

```{r q5c, fig.align='center'}
couchto5k<- couchto5k%>% mutate(
  quit = ifelse(completed ==TRUE, FALSE, TRUE)
  
)
logitic <- glm(quit ~ selfmot, data = couchto5k, family = 'binomial')
selfmotivation <- tibble(selfmot = 5:35)
selfmotivation<-
 selfmotivation %>%
  mutate (
    predprobs = predict(logitic, newdata = selfmotivation, type = "response")
  )
ggplot(data = selfmotivation, aes(x=selfmot, y =predprobs)) + geom_line() + labs(x = 'Self-Motivation', y='Probability', title = 'Probability of Quitting as a Function of Self-motivation')
```










