---
title: "USMR 2021-2022 Coursework"
author: "`r params$examnumber`"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
params:
  examnumber: B203423 
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)
# load any other packages that you require here:
library(tidyverse)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0
<!-- If you have run the R code above this point (you can do this now by pressing Ctrl-Shift-Alt-P, or running the chunk above) then your data will be in a dataframe called `couchto5k`. -->


```{r cleaning, include = FALSE}
# Neither output nor code from this chunk will be shown in the compiled document. 
plot.ts(couchto5k)
sub_couchto5k=subset(couchto5k,age!=144 & age!=119 & selfmot!=-99 & week_stopped!=12)
sub_couchto5k$season[sub_couchto5k$season=='autunm']='autumn'

```
1. Impossible values
In 'age', there are two impossible values which are greater than 100 years old.
In 'self-mot', there are also two impossible values which are negative values. Based on the description of the data, the self-mot values should be between 5 to 35. Because if one gets 1 for every question, the total score of the one should be 5; if one gets 7 for every question, the total score of the one will be 7 multiplies 5 equals to 35. 
Moreover, the spelling of 'autumn' in season is wrong as 'autunm', which causes a representation of five seasons. By correcting the spelling of 'autumn' in season, the values in season finally become between zero and four. 
Finally, in 'week', there is an impossible value which is greater than 9, however, the whole programme stops at week 9.
For these impossible values, my action is deleting them, only a few of impossible values may be better to just ignore. 

```{r descriptives}
# Code will not be shown from this chunk (because we set echo = FALSE in the very first chunk)
# the output from this code will be shown. 
sub_couchto5k
plot.ts(sub_couchto5k)

```
2. Data description
After deleting those impossible values, here's above the graph for couchto5K data right now. 
In the data, participants with random ID code participated in the research in four seasons in two cities (the values in the graph are either 1 or 2) from a start day in week 1 to week 9 (the values in the graph are mostly between 1 and 9).
Based on the instruction, the values for happiness and health should be between 0 to 100, values for 'happiness' are between between 0 and 100, and values for 'health' are between 30 and 85.
The values for 'self-mot' are now all between 5 to 35. The values for 'accountability' are also all between 5 to 35 based on the instructions. 
After cleaning the data, the age of the participants are now between 20 and 60. 
# Question 1 

## Question 1a

```{r q1a}
sum=count(sub_couchto5k)$n
stopped_before_five=count(sub_couchto5k[which(sub_couchto5k[["week_stopped"]]<=5),])$n
stopped_after_five=count(sub_couchto5k[which(sub_couchto5k[["week_stopped"]]>5&sub_couchto5k[["week_stopped"]]<=8),])$n
completed=sum-stopped_before_five-stopped_after_five
chisq.test(x=c(stopped_after_five,stopped_after_five,completed),p=c(0.45,0.1,0.45))
```
The null hypothesis here is 45% of participants abandoned the programme before the halfway point in week 5, and a further 10% gave up before the end of the programme. 
I created a chi-square test, the p-value is 1e-07 which is much smaller than 0.01, which means we have evident significance to reject the null hypothesis, therefore, in this research, it is not 45% of participants abandoned the programme before the halfway point in week 5, and it is not a further 10% gave up before the end of the programme In another word, the data in the sample is not in line with data from the earlier survey.

## Question 1b

```{r q1b}

count_sum=sum(count(sub_couchto5k))
glasgow_stopped=sub_couchto5k[which(sub_couchto5k[["city"]]=="Glasgow"),]$week_stopped
Glasgow5=sum(glasgow_stopped<=5)
Glasgow8=sum(glasgow_stopped>5&glasgow_stopped<9)
Glasgow9=sum(glasgow_stopped==9)
ed_stopped=sub_couchto5k[which(sub_couchto5k[["city"]]=="Edinburgh"),]$week_stopped
ed5=sum(ed_stopped<=5)
ed8=sum(ed_stopped>5&ed_stopped<9)
ed9=sum(ed_stopped==9)
glasgow_3_phases=c(Glasgow5,Glasgow8,Glasgow9)
ed_3_phases=c(ed5,ed8,ed9)
glasgow_n=dim(sub_couchto5k[which(sub_couchto5k[["city"]]=="Glasgow"),])[1]
ed_n=dim(sub_couchto5k[which(sub_couchto5k[["city"]]=="Edinburgh"),])[1]

table=matrix(c(glasgow_3_phases,ed_3_phases),nrow=3)
print(table)
fisher.test(table)
```
Fisher's Exact Test used to answer this question is more accurate in counting data of small sample. 
The null hypothesis of the test is, the patterns of attrition rates may not differ by city.
1. We can see in the table above, the first row is the attrition rate of Glasgow, the second row is the attrition rate of Edinburgh. We can see from the first line that the attrition rate before week 5 of Glasgow is smaller than Edinburgh. However, in the second line, which is between week 5 and 9, the attrition rate of the two cities equal with each other. In the final line, Glasgow has smaller attrition rate than Edinburgh again. It can be observed from the table that the attrition rate of the two cities differ from each other.
In addtion, we can also find out the conclusion from the p-value of the test. 
2. The p-value equals 0.05=0.05, thus, we may compare it with 0.1, 0.05 is smaller than 0.1, we may reject the null hypothesis, therefore, we may conclude that the attrition rates vary by city.

## Question 1c

```{r q1c}
t.test(sub_couchto5k$age~sub_couchto5k$city,var.equal=T)
```
Yes, the average ages of participants differ by city. The null hypothesis is the average ages of participants who commenced the programme are the same by city, but the t-test shows, the p-value is 0.001, which is smaller than 0.05, that means we have significant evidence to reject the null hypothesis, therefore, we should consider the average ages differ by city.

# Question 2

## Question 2a

```{r q2a}

sub_couchto5k$season=as.factor(sub_couchto5k$season)
model1=lm(happiness~season,data=sub_couchto5k)
summary(model1)
anova(model1)
```
In this question, happiness is a response variable, season is an explanatory variable, I firstly created a simple linear regression model, based on that, I used ANOVA to analyze whether the correlation between happiness and season is significant. 
The null hypothesis (H0) is when season coefficient equals to 0, which means season will not have an impact on happiness outcomes. However, as we can see from the table about coefficients for season above, none of them equal to zero.
Furthermore, according to variance table, the p-value in f-test (ANOVA)=0.067>0.05,which rejects H0. However, the question demanded me to explain the way that season affects happiness outcomes. Thus, I compared p-value to 0.1, the p-value is smaller than 0.1, which rejects H0. 
As a result, in this way, season has an impact on happiness ratings. 

## Question 2b

```{r q2b}
model2=lm(happiness~season+age,data=sub_couchto5k)
summary(model2)

```
Following up (2a), I created a linear model with coefficient for age to discuss whether happiness affects by age. The p-value for age is 0.237 which is significantly larger than 0.05, which means the coefficient for age is not significant, in another word, accepts H0: age coefficient==0. 
Therefore, happiness is not affected by age.

## Question 2c

```{r q2c}
anova(model1,model2)
AIC(model1,model2)
##choose model1
```
For baseline model, I will choose model 1.
Model 2 has one more variable ‘age’ than model 1, thus, using ANOVA to compare the difference between the two models is actually to compare whether the coefficient for age is zero. From the variance table, f-test yields p-value=0.24>0.05, which means it accepts H0: age coefficient==0, hence we may not need variable age in the model, therefore, model 1 without variable 'age' may be a better choice to go for. 
What's more, from another aspect, we may examine the two models with the help of AIC. AIC is an index of evaluation models, the smaller the value of AIC, the better the model. Therefore, in the second table, model 1 has a lower AIC value than model 2, which illustrates that model 1 is better than model 2.

# Question 3

## Question 3a

```{r q3a}
completion=ifelse(sub_couchto5k$week_stopped==9,1,0)
sub_couchto5k$completion=as.factor(completion)

model3=lm(happiness~season+completion,data=sub_couchto5k)
summary(model3)
plot(sub_couchto5k$completion,sub_couchto5k$happiness,xlab='completion',ylab='happiness',main='happiness grouped by completion')

```
Based on model 1 picked in the previous question, we now need to describe in what way programme completion influences participants' happiness outcomes.
The p-value for completion is 0.47 which is much higher than 0.05 or 0.1. We may suggest that it rejects H0: completion coefficient==0.It is suggested that if we only look at the p-value to test the hypothesis, we might not be able to answer the question or describe the way in which programme completion influences happiness outcomes. Therefore, I drew a graph and planned to use variance to test the hypothesis.
As we can see in the graph, when participants' completion is zero (participants quit the research at any time before completion), the variance of their happiness rates are larger than participants who completed the whole research. Although the average value of happiness does not show significant difference between completion and non-completion, we may suggest that after participants complete the research, the variation of their happiness is smaller which means happiness can be affected by completion.


## Question 3b

```{r q3b}
model4=lm(happiness~season+completion+health,data=sub_couchto5k)
summary(model4)
```
The p-value for health in t-test is 0.7>0.1, which means the coefficient for health is not significant, thus, it accepts H0. H0 suggests that happiness will not be affected by health. Therefore, health does not influence happiness.

## Question 3c

```{r q3c}
model5=lm(happiness~season+completion+health+completion:health,data=sub_couchto5k)
summary(model5)
anova(model5,model4)
```
To test the hypothesis, I firstly built a linear model, based on that, I built ANOVA.
By including the interaction item 'health' into the test, it can be easily observed from the table above that the coefficient for interaction item between completion and health is significant under t-test, since p value=0.00021<0.05, there is evident significance to reject H0, which means the hypothesis is true. Therefore, the influence of completion to happiness is affected by health metric.  

## Question 3d
From the last model, the conclusion made in 3a and 3b are overthrown, we may conclude that completion and health metric both have significant influence to participants' happiness (p-value=0.00021<0.05). The interaction effect in 3c between health and completion is also significant. Therefore, we may conclude that health and completion does not affect happiness separately, they influence happiness significantly when there is an interaction item between health and completion is considered. To sump up, the causes of happiness may be affected by the impact of health and completion.

# Question 4

```{r q4}
completed_record=subset(sub_couchto5k,completion==1)

boxplot(completed_record$happiness~completed_record$season,xlab='season',ylab='happiness',main='average happiness grouped by season')
boxplot(completed_record$happiness~completed_record$city,xlab='city',ylab='happiness',main='average happiness grouped by city')

season_city=rep('0',dim(completed_record)[1])
for(i in 1:dim(completed_record)[1]){
  if(completed_record$city[i]=='Edinburgh'&completed_record$season[i]=='spring'){
    season_city[i]="spring in Edinburgh"
  }else if(completed_record$city[i]=='Edinburgh'&completed_record$season[i]=='summer'){
    season_city[i]="summer in Edinburgh"
  }else if(completed_record$city[i]=='Edinburgh'&completed_record$season[i]=='autumn'){
    season_city[i]="autumn in Edinburgh"
  }else if(completed_record$city[i]=='Edinburgh'&completed_record$season[i]=='winter'){
    season_city[i]="winter in Edinburgh"
  }else if(completed_record$city[i]=='Glasgow'&completed_record$season[i]=='spring'){
    season_city[i]="spring in Glasgow"
  }else if(completed_record$city[i]=='Glasgow'&completed_record$season[i]=='summer'){
    season_city[i]="summer in Glasgow"
  }else if(completed_record$city[i]=='Glasgow'&completed_record$season[i]=='autumn'){
    season_city[i]="autumn in Glasgow"
  }else if(completed_record$city[i]=='Glasgow'&completed_record$season[i]=='winter'){
    season_city[i]="winter in Glasgow"
  }
}

completed_record$season_city=as.factor(season_city)

a=group_by(completed_record,season_city)
b=summarise(a,mean(happiness))
plot(as.factor(b$season_city),b$`mean(happiness)`,xlab='season and city',ylab='happiness',main='average happiness grouped by season and city')

```
Here are the graphs we can use for presenting the average happiness ratings grouped by season and city. From graph 1, we may see the average value of happiness varies by seasons; in the second graph, different cities where participants joined and did the programme also have different average value for happiness. We may see clearer from the last graph that the average happiness varies in different cities and seasons. 

# Question 5

## Question 5a

```{r q5a}
sub_couchto5k$dropout=1-completion
model6=glm(dropout~selfmot,family = 'binomial',data=sub_couchto5k)
summary(model6)
```
I built a generalized linear model to predict the likelihood of dropping out, strictly, it is a logistic regression model. The explanatory variable used in this model is self motivation in sub_couchto5k dataset, and the response variable refers to the drop out column in the original dataset. The estimated  value of coeffcients for interpret and selfmot are 2.08 and -0.13 respectively.

## Question 5b

```{r q5b}
log_odds_ratio_selfmot=coefficients(model6)[2]
odds_ratio=exp(log_odds_ratio_selfmot)
confint=exp(confint.default(model6)[2,])
odds_ratio_summary=c(confint[1],odds_ratio,confint[2])
names(odds_ratio_summary)=c('2.5% lower bound','estimation for odds ratio ralated to 1 unit increase in self motivation ratings','97.5% upper bound')
print(odds_ratio_summary)
```
As we may see above, 95% (97.5%-2.5%=95%) confidence interval (CI) of the odds ratio is below 1 (0.772<1; 0.986<1), therefore, the increase of self motivation has a significant negative effect on the probability of dropping out, under the significant level of 5% (100%-95%=5%).
In another word, the higher self motivation a participant gets, the more likely this participant can complete the research/ programme.

## Question 5c

```{r q5c}
newdata=data.frame(selfmot=seq(0,35,by=0.5))

log_odds=predict(model6,newdata = newdata)
odds=exp(log_odds)
predicted_p=odds/(1+odds)

plot(newdata[,1],predicted_p,type='l',col='red',xlab='self motivation ratings',ylab='drop out probability',main='quitting probability vs self motivation ratings')

```
Here is the a graph representing the probability of quitting as a function of how self motivated participants were. As we can see from the graph, with the increase of self-motivation ratings, the drop out probability decreases. The ratio of self-motivation and the probability of drop out in the research showed a negative growth trend.









