---
title: "USMR 2021-2022 Coursework"
author: "`r params$examnumber`"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
  html_document: default
params:
  examnumber: B201824
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)
# load any other packages that you require here:
library(tidyverse)
library(tidyr)
library(pander)
library(broom)
library(dplyr)
library(ggmosaic)
library(sjPlot)
library(corrplot)
library(car)
library(patchwork)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0
<!-- If you have run the R code above this point (you can do this now by pressing Ctrl-Shift-Alt-P, or running the chunk above) then your data will be in a dataframe called `couchto5k`. -->

Data was obtained from https://uoepsy.github.io/data/usmr_2122_data.R: a dataset originally containing information on 130 participants across 9 variables. In Week 0, prior to participation in the Couch to 5k program, participants completed a single questionnaire giving: (1) a psychometric measure of accountability; and (2) a psychometric measure of self-motivation. For both these measures, only total scores are available. Each measure consisted of five questions, each scored 1-7, giving minimum and maximum scores of 5 and 35 respectively. Upon completing the programme (Week 9) or dropping out before Week 9, participants completed a second questionnaire in which included: (1) their self-reported happiness; and (2) a health measure number derived from a series of physiological tests. Of both these post-participation measures, only total scores are available and both had a minimum score of 0 and a maximum score of 100. 

All participant data was complete (no missing values). However, 6 season entries were initially misentered as the incorrectly spelled "autunm". These were recoded as the correctly spelled "autumn". Further data inspection resulted in the following issues being noted: implausible age values, impossible self motivation values and impossible week stopped values. Initially, I decided to retain, rather than completely delete, the observational units to which these values were associated by recoding them as 'NA' to retain the other valid values for the given observational unit. However, since the total number of problematic observational units was small (11/130) I decided to delete the entire observational unit to ensure an equal number of values for each variable. Therefore, 125 observations were retained for analysis. Table 1 gives a summary of the problematic values.


```{r cleaning, include = FALSE}
# Neither output nor code from this chunk will be shown in the compiled document. 

#tabulating the missing values to be able to report their removal
couchto5k$problem <- NA
couchto5k$problem[couchto5k$age>122] <- "implausible age"
couchto5k$problem[couchto5k$selfmot<0] <- "impossible self_mot value"
couchto5k$problem[couchto5k$week_stopped>9] <- "impossible week_stopped value"
couchto5k$problem[couchto5k$season=="autunm"] <- "misspelled season"
problem <- couchto5k$problem

#table of all the data that is problematic
problem_tab <- table(problem)
problem_tab

#recoding misspelled season
couchto5k$season[couchto5k$season=="autunm"] <- "autumn"

#recoding self_mot out of range
couchto5k$selfmot[couchto5k$selfmot<0] <- "NA"

#recoding implausible age
couchto5k$age[couchto5k$age>120] <- "NA"

#recoding week_stopped out of range
couchto5k$week_stopped[couchto5k$week_stopped>9] <- "NA"

#remove NA rows from week stopped, age and self_mot
couchto5k <- couchto5k[couchto5k$age != "NA", ]
couchto5k <- couchto5k[couchto5k$selfmot != "NA", ]
couchto5k <- couchto5k[couchto5k$week_stopped != "NA", ]

#remove problem column for rest of analysis
couchto5k$problem <- NULL 

#telling R which variables are factors and which are numeric
couchto5k$season <- as.factor(couchto5k$season)
couchto5k$city <- as.factor(couchto5k$city)
couchto5k$age <- as.numeric(couchto5k$age)
couchto5k$selfmot <- as.numeric(couchto5k$selfmot)
couchto5k$week_stopped <- as.numeric(couchto5k$week_stopped)

#renaming variables
ppt1d <- couchto5k$ppt1D
age <- couchto5k$age
accountability <- couchto5k$accountability
selfmot <- couchto5k$selfmot
health <- couchto5k$health
happiness <- couchto5k$happiness
season <- couchto5k$season
city <- couchto5k$city
weekstopped <- couchto5k$week_stopped

#reordering levels of season
season <- factor(season, levels = c("spring", "summer", "autumn", "winter"))

```

```{r}
#for visualising the problematic data
problem_tab %>% pander(width = 5000, split.cells = c(1,1,1,1), caption="Summary of problematic values: misspelled season was recoded with correct spelling; impossible and implausible values were recoded as 'NA' before being removed.")
```

The research question had two goals: 1) explore the relationships between psychological factors and programme participation/completion; and 2) the effects of the programme on health and wellbeing. To these ends, there were 8 variables that required consideration and visual description before analysis. These are summarised in Table 2. These data were modelled as continuous variables, with the exception of season and city which were modelled as categorical.

```{r}

#for overall summary of marginal distributions of data
summary <- summary(couchto5k)
summary %>% pander(width = 5000, split.cells = c(1,1,1,1, 1, 1, 1, 1, 1), caption="Summary of marginal distributions of updated dataframe.")

```


```{r descriptives, results = FALSE}
# Code will not be shown from this chunk (because we set echo = FALSE in the very first chunk)
# the output from this code will be shown. 


#assigning names to plots to be able to display them together
#marginal distributions of each variable without reference to the other variables.

accountability_plot <- 
  ggplot(data = couchto5k, aes(x = accountability)) +
  geom_density() +
  geom_boxplot(width = 1/250) +
  labs(x = "Accountability (range 5-35)", y = "Probability\ndensity") +
  ggtitle('Figure 1: Accountability')

selfmot_plot <- 
  ggplot(data = couchto5k, aes(x = selfmot)) +
  geom_density() +
  geom_boxplot(width = 1/250) +
  labs(x = "Self-motivation (range 5-35)", y = "Probability\ndensity") +
  ggtitle('Figure 2: Self-motivation')

happiness_plot <- 
  ggplot(data = couchto5k, aes(x = happiness)) +
  geom_density() +
  geom_boxplot(width = 1/250) +
  labs(x = "Happiness (range 0-100)", y = "Probability\ndensity") +
  ggtitle('Figure 3: Happiness')

health_plot <- 
  ggplot(data = couchto5k, aes(x = health)) +
  geom_density() +
  geom_boxplot(width = 1/250) +
  labs(x = "Health (range 0-100)", y = "Probability\ndensity") +
  ggtitle('Figure 4: Health')

patchwork1 <- (accountability_plot + selfmot_plot) / (happiness_plot + health_plot)
patchwork1 + plot_annotation(title = 'Marginal distribution plots:',
                             subtitle = 'psychological and physiological variables',
                             caption = 'Figures 1:4: Density plots and box plots of accountability, self motivation, happiness and health.')

#standard deviations
SDaccount <- sd(accountability)
SDself_mot <- sd(selfmot)
SDhapp <- sd(happiness)
SDhealth <- sd(health)

```

* The marginal distribution of scores for accountability is unimodal with a mean of `r mean(accountability)`. There is variation across accountability scores (SD = `r sd(accountability)`).
* The marginal distribution of self-motivation is unimodal with a mean of `r mean(selfmot)`. There is variation across self-motivation scores (SD = `r sd(selfmot)`).
* The marginal distribution of happiness is *not* unimodal with a mean of `r mean(happiness)`. There is variation in happiness scores (SD = `r sd(happiness)`).
* The marginal distribution of health scores is unimodal with a mean of `r mean(health)`. There is variation in happiness scores (SD = `r sd(health)`).

All plots appeared relatively normal in distribution form, except happiness which is part of the focus of the research question, and therefore is expected to be a composite of factors.

Barplots to represent categorical data highlighted that: 1) dropout was not uniform across weeks; 2) participation was not uniform across seasons; 3) participation was not uniform across cities.

```{r fig.show="hold", out.width="50%", results = "hide"}

#categorical variable plots
histwkst <- hist(weekstopped, main="Week stopped by frequency",
  xlab="Week stopped", ylab="Frequency")

plotseason <- plot(season, main="Season by frequency",
  xlab="Season", ylab="Frequency")

plotcity <- plot(city, main="City by frequency",
  xlab="City", ylab="Frequency")

histwkst 
plotseason 
plotcity 

```


# Question 1 

## Question 1a

```{r q1a}

#creates new column and adds it to couchto5k
couchto5k$stopgroup <- weekstopped 


#changes values to strings 
couchto5k$stopgroup[couchto5k$stopgroup == 9] <- "completed"
couchto5k$stopgroup[couchto5k$stopgroup < 5] <- "before halfway"
couchto5k$stopgroup[couchto5k$stopgroup > 4 & couchto5k$stopgroup < 9] <- "after halfway"

#create new data frame with just data needed
df1a <- data.frame(couchto5k$stopgroup)

df1a <- tibble(df1a) #creates tibble

tab1a <- df1a %>% group_by(couchto5k.stopgroup) %>% tally() #group by stopgroup

tab1a <- tab1a %>% transmute(couchto5k.stopgroup = couchto5k.stopgroup, relative = c(0.1, 0.45, 0.45), observed = n)

tab1a <- tab1a %>% mutate(expected = relative*sum(observed))

tab1a <- tab1a %>%
  mutate(
    step1 = expected - observed,
    step2 = step1^2,
    step3 = step2/expected
  )

x2 <- sum(tab1a$step3)

gof_res <- chisq.test(tab1a$observed, p = c(0.1, 0.45, 0.45))

```
An $X^2$ goodness-of-fit test was conducted to examine whether the collected data (i.e. the observed frequencies of the week in which participants stopped the programme) was consistent with the distribution in the previous nationwide survey. The proportional breakdown of expected week stopped frequencies were therefore:

* stopping before halfway: 45% stopping in Week 1, 2, 3 or 4 
* stopping after halfway: 10% stopping in Week 5, 6, 7, 8 
* completed: 45% stopping in Week 9

This gave a null hypothesis of: $H_0$ $=$ P(0.45, 0.1, 0.45) i.e. the data in our sample follows the pattern of frequencies from the previous survey. The alternative hypothesis was: $H_1$ $\neq$ P(0.45, 0.1, 0.45) i.e. the data does not follow the pattern of frequencies from the previous survey. The alpha level was set by convention to 0.05. 

The goodness-of-fit test was not significant ($X^2$ (2) = 4, p = 0.2). Therefore, the null hypothesis could not be rejected, meaning there is not sufficient evidence to infer that the present sample differs significantly from the previous nationwide survey. However, a Type II (false negative) error may have occurred whereby the null hypothesis is wrongly accepted.


## Question 1b

```{r q1b, warning=FALSE, fig.width = 4, fig.height=3}

#creates subset of couchto5k for this question
df1b <- data.frame(couchto5k$pptID, couchto5k$stopgroup, couchto5k$city) 

#observed attrition rates
testtabularform <- df1b %>% select(couchto5k.stopgroup, couchto5k.city) %>% table() 

#expected attrition rates
expected1b <- rowSums(testtabularform) %o% colSums(testtabularform) / sum(testtabularform) 

#expected attrition plot
mosexpected <- plot(as.table(expected1b), main="Expected attrition rate",
        xlab="Attrition by group",
        ylab="City",
        sub="Equal distribution of attrition rate expected")



```

```{r fig.width = 7, fig.height=4, warning=FALSE}
#observed attrition plot
mosattrition <- ggplot(data = df1b) +
  geom_mosaic(aes(x = product(couchto5k.stopgroup, couchto5k.city), fill = couchto5k.stopgroup)) +
  labs(x = "\n City", y = "")

mosattrition + plot_annotation(title = 'Observed attrition rate:',
                            caption = 'Figure 9: Visual depiction of the patterns of attrition (grouped by: before halfway, after halfway, or completed) differ by city.')

#pearson's chi-squared test
city_toi <- chisq.test(testtabularform) 
```

To examine whether the attrition rates differed by city, I conducted a $X^2$ test of independence. Before the calculation, I observed the observed and expected values for the data in mosaic plots. To see whether the difference between these values was significant, the alpha level ($\alpha$) was set to 0.05, and I formed the following hypotheses: the null hypothesis ($H_0$) was that attrition rates would be equally likely across cities; my alternative hypothesis ($H_1$) was that the attrition rates would not be equal. 

The result of the $X^2$ test of independence indicated that the relationship between city and the week in which the participant stopped the programme was not significant ($X^2$ (2) = 3, p = 0.2), therefore the null hypothesis could not be rejected. However, a Type II (false negative) error may have occurred whereby the null hypothesis is wrongly accepted.


## Question 1c


```{r q1c, fig.width = 4, fig.height=3}

#create dataframe subset for question
data1c <- couchto5k[c(8, 2)]

#create tibble form of dataframe
df1c <- data.frame(data1c)
df1c <- as_tibble(df1c)

#visualise data
plot1c <- ggplot(data = df1c, aes(x = city, 
                          y = age, 
                          fill = city)) +
  geom_boxplot(alpha = 0.3) + 
  geom_jitter(width = 0.1)+
  theme_minimal()

plot1c + plot_annotation(title = 'Age by city:',
                            caption = 'Figure 10: Visual depiction of age across cities.')

#calculate mean by city
meanbygroup1c <- df1c %>%
  group_by(city) %>%
  summarise(
    Mean = round(mean(age),2),
    SD = round(sd(age),2),
    N = n()
  )

#t-test age by city
res1c <- with(df1c, t.test(age ~ city, alternative = "greater"))

#shapiro - Edin
shapiro1cEdin <- shapiro.test(df1c$age[df1c$city=="Edinburgh"])
shapiro1cGlas <- shapiro.test(df1c$age[df1c$city=="Glasgow"])
shapiro1call <- shapiro.test(df1c$age[df1c$city])

#testing variance
var1c <- with(df1c, var.test(age ~ city))


```


```{r fig.width = 4, fig.height=3}
#plotting age to see if normal as shapiro suggests not in Edinburgh
age_plot <- 
  ggplot(data = couchto5k, aes(x = age)) +
  geom_density() +
  geom_boxplot(width = 1/250) +
  labs(x = "Age", y = "Probability\ndensity") +
  ggtitle('Figure 11: Age') 

age_plot + plot_annotation(title = 'Distribution of age:',
                            caption = 'Figure 11: Visual depiction of distribution of age values.')
```

An independent, two-tailed t-test was conducted to examine whether the mean age was consistent across cities. To see whether the difference between these values was significant, the alpha level ($\alpha$) was set to 0.05, and I formed the following hypotheses: the null hypothesis ($H_0$) was that average age would be equal across cities i.e. $\overline{x}_1 = \overline{x}_2$ ; my alternative hypothesis ($H_1$) was that average age would not be equal across cities i.e. $\overline{x}_1 \neq \overline{x}_2$. The results indicated a significant difference in age between the Edinburgh (mean = 40.1) and Glasgow (mean = 35.3) groups ($t(40) =$ 2, $p =$ 0.04. Therefore, I reject the null hypothesis and accept the indication of a difference in ages by city.

To check the assumptions behind this conclusion, I conducted a Shapiro-Wilk test of normality for each city with the null hypothesis ($H_0$) that the data was normally distributed. The results for Edinburgh (W = 1,  p-value: < 0.05) rejected the null hypothesis and indicated that the age data was not normally distributed in Edinburgh. The results for Glasgow (W = 0.9, p-value = 0.09) failed to reject the null hypothesis and indicated that the age data was normally distributed in Glasgow. To assess the difference between cities overall, I also conducted a Shapiro-Wilk test of normality on the age data for both cities combined, with the null hypothesis ($H_0$) that the data was normally distributed. The results (W = 0.5 p-value < 0.01) rejected the null hypothesis that the age data were drawn from a normally distributed population overall which, therefore, will need to be accommodated in subsequent models. To check the variance of the age by city, I conducted an F-test with the null hypothesis ($H_0$) that the variance would be equal across cities. The results (F = 0.9, p-value: 0.7) indicated that I should not reject the null hypothesis and therefore the variance in age is considered the same across groups.


# Question 2

## Question 2a

```{r q2a, fig.width = 4, fig.height=3}

#relevel spring to be baseline level
couchto5k$season <- relevel(couchto5k$season, ref = "spring")

mod2a <- lm(happiness ~ season, data = couchto5k)

graph2a <- couchto5k %>% group_by(season) %>% summarise(mean_se(happiness)) %>% ggplot(aes(x=season,y=y,
ymin=ymin,ymax=ymax)) +
geom_bar(stat="identity") +
geom_errorbar(width=.2) +
ylab("happiness")

graph2a

```

To examine how participants' happiness ratings were affected by the season in which they participated, I formed the following statistical hypotheses: ($H_0$) that happiness is not influenced by any season; ($H_1$) that happiness is influenced by at least one season. 

To test these hypotheses, I conducted a simple linear model with one response variable (happiness) and one explanatory variable (season) of the form $yi= \beta_0+ \beta_1x_i+ \epsilon_i$. The baseline level of the categorical predictor (season) was set to Spring as this was the largest group by season. The coefficients indicates the variation in happiness by season: specifically, for the intercept (spring) the mean happiness score was 47.9; the difference between the mean of the 'summer' group and the baseline group (spring) was 10.6; the difference between the mean of the 'autumn' group and the baseline group (spring) was -16; and the difference between the mean of the 'winter' group and the baseline group (spring) was -14.2. These indicate that the mean happiness rating was higher in Summer than in Spring, but lower than Spring in Autumn and Winter. This is visually depicted in Figure X below.

R-squared was computed as 0.0747, therefore 7% of the total variability in happiness is explained by the linear association with the season. To test the overall significance of the model, I conducted an F-test against the null hypothesis $H_0$ = season is not significant. Since F(3 and 121)=3.26, p<0.05, I can consider this model significant. Therefore, the model as a whole was informative about season by explaining some of the variation of happiness. However, between the levels of the categorical variable (season) only the baseline level is significant (Spring); the model indicates that the other seasons do not significantly impact happiness. Furthermore, since an R-squared of 7% is relatively low, further predictors need to be incorporated into the model.


## Question 2b

```{r q2b, fig.width = 4, fig.height=3, warning=FALSE, message=FALSE}

#plot happiness and age
happiness_age2b <- 
  ggplot(data = couchto5k, aes(x = age, y = happiness)) +
  geom_point(alpha = 0.5) +
  labs(x = "Age", y = "Happiness") + geom_smooth()
happiness_age2b + plot_annotation(title = 'Happiness values against age:',
                            caption = 'Figure 13: Visual depiction of distribution of age values against happiness.')

#correlation coefficient of happiness and age
cor2b <- cor(couchto5k$happiness, couchto5k$age)

mod2b <- lm(happiness ~ 1 + season + age, data = couchto5k)

#accounting for standard error
sigma2b <- sigma(mod2b)

#confidence intervals
conf2b <- confint(mod2b, level = 0.95)

#mod2 reverse order
mod2brev <- lm(happiness ~ 1 + age + season, data = couchto5k)


```

Firstly, I plotted happiness against age to visualise the data. The horizontal line of best fit indicates that happiness is seen to be relatively constant across ages. This is supported by a test of correlation (r = 0.0752) which only showed a slight positive correlation between happiness and age. This suggests that there is not a strong relationship between happiness and age.

I conducted a multiple regression model with one response variable (happiness) and two explanatory variables (season and age) of the form $yi= \beta_0+ \beta_1x_1+ \beta_2x_2+\epsilon_i$. The alpha level ($\alpha$) was set at 0.05; the null hypothesis ($H_0$) set to happiness not influenced by age; and the alternative hypothesis ($H_1$) set to happiness is influenced by age. Overall, the model was significant in that (F(4 and 120) = 2.85, p< 0.05).

To examine the relative influence of age and season, I examined the output coefficients. The output coefficient for age indicated that while holding the effect of season constant, age (on average) resulted in 0.324 unit increase in happiness (-0.187 and 0.834 for 95% confidence interval). The standard deviation of errors indicates that we would expect 95% of happiness scores to be within 67.8 (2 standard deviations) from the model fit. However, the p-value for age was > 0.05 therefore was not a significant predictor and was discarded from the model as it did not reduce variance beyond season alone.



## Question 2c

```{r q2c, ouput = FALSE}
#r2with age
q2cwithage <- summary(mod2b)$adj.r.sq

#r2without age
q2cwoutage <- summary(mod2a)$adj.r.sq

#anova to compare models

```

When comparing the adjusted R2 scores of the models with (`r summary(mod2b)$adj.r.sq`) and without (`r summary(mod2a)$adj.r.sq`) age as a predictor, we see a very small increase with age as a predictor. However, when computing a comparison of models (using anova()), the influence of incorporating age into the model is not significant (p > 0.05), and therefore age was not included in the baseline model.


# Question 3

## Question 3a

```{r q3a, fig.width = 4, fig.height=3, warning=FALSE, message=FALSE}

#rename baseline for clarity
baselinemod <- lm(happiness ~ 1 + season, data = couchto5k)

#overall test for normality of residuals
shapiro3a <- shapiro.test(residuals(baselinemod))

#creating new column for analysis by completion and changing to binary
couchto5k$completed <- NA
couchto5k$completed[couchto5k$week_stopped==9] <- "completed"
couchto5k$completed[couchto5k$week_stopped<9] <- "not completed"
couchto5k$completed<-ifelse(couchto5k$completed=="completed",1,0)

#correlation between completion and happiness
cor3a <- cor(couchto5k$completed, couchto5k$happiness)

#happiness by completion graph
graph3ahapp_comp <- ggplot(couchto5k, aes(x=happiness, y=completed))+
  geom_point()+
  geom_smooth(method="lm")
graph3ahapp_comp + plot_annotation(title = 'Happiness values against completion of programme:',
                            caption = 'Figure 14: Visual depiction of distribution of happiness against programme completion.')

#updated model with additional predictor of programme completion
mod3a <- lm(happiness ~ season + age + completed, data = couchto5k)

#f-test comparison of models
anova3a <- anova(mod3a, baselinemod)

#null model for anova
null3a <- lm(happiness ~ 1, data = couchto5k)

```

Having chosen my baseline model, I tested whether its residuals can be considered to be from a normal distribution, by conducting a Shapiro-Wilks test against the alternative hypothesis that the residuals were not sampled from a normal distribution. The results (W = 0.9, p-value = 2e-05) reject the null hypothesis that the residuals were sampled from a normal distribution and suggests there is systematic structure remaining that needs to be incorporated into the model with further predictors.

I built upon my baseline model (including variables season and age) by adding programme completion. The correlation coefficient between happiness and programme completion is 0.0678, indicating a slight positive correlation. Under a null hypothesis that all regression slopes for all variables are zero, and an alternative hypothesis that at least one of the coefficients is not equal to zero, the F-ratio indicates that the model is significant: F(5, 119) = 2.54, p < 0.05, accounting for 6% of the data (adjusted R2 = 0.0583)

Since at least one of the coefficients is not equal to zero, I conducted an incremental F-test to compare this new model against the baseline model and also against the null model. This allowed a test whether the models with additional predictors improve the model. The adjusted R-squared scores for each subset of the model were: null model: 0; model in 2a (season only): 0.0517 (5%); model in 2b (season + age): 0.0562 (6%); model in 3a (season + age + completion): 0.0583 (6%). Therefore, completing the program was found to not explain a significant amount of variance in happiness beyond that accounted for by season and age F(5, 119) = 2.54, p < 0.05.

```{r fig.height=5, warning=FALSE, message=FALSE}
#testing all correlations
#create dataframe subset
cortestall <- couchto5k[c(2, 3, 4, 5, 6, 11)]

#making categories numeric to do correlation test
seasoncornum <- as.numeric(season)
citycornum <- as.numeric(city)

#create tibble form of dataframe
cortestall <- data.frame(cortestall)
cortestall <- as_tibble(cortestall)

#adding numeric columns for categories
cortestall <- cortestall %>% mutate(cortestall, seasoncornum, citycornum)
corAll <- cor(cortestall)

#cor matrix
corAllplot <- corrplot(corAll, method="number", type="lower")
```
Having determined a baseline model, and additionally seen only small effects from the additional 'predictors' of happiness (age and season), I decided to build a correlation matrix to visually ascertain which variables looked promising for further analysis. The two strongest correlations were between completion and season (`r cor(cortestall$completed, cortestall$seasoncornum)`) and between age and health (`r cor(cortestall$age, cortestall$health)`). Therefore, these variables need to be analysed further for inclusion in the model.

## Question 3b

```{r q3b}

mod3b <- lm(happiness ~ 1 + season + health, data = couchto5k)

```

Overall, health was found to not explain a significant amount of variance in happiness over and above the baseline model which included only season. The results for the model as a whole were (F(5, 119) = 2.96, p < 0.05.); and the contribution of health was not significant (p-value: 0.16).


## Question 3c

```{r q3c, fig.width = 3, fig.height=3, warning=FALSE, message=FALSE}

mod3c1 <- lm(happiness ~ 1 + season + health + completed + health * completed, data = couchto5k)

#graph to illustrate interaction between health and completion
plot3c <- plot_model(mod3c1, type = "int")
plot3c

#plotting assumptions

patchwork2 <- (plot(mod3c1, 1) + (plot(mod3c1, 2) / (plot(mod3c1, 3) + (plot(mod3c1, 4)))))
#patchwork2 + plot_annotation(title = 'Assumption tests:',caption = 'Figures 16 to 19 Testing assumptions behing linear model.')

```

To assess if the further that participants got in the programme, the more their happiness would be affected by the health metric, I examined the interaction of health and completion (Figure 15). This demonstrated there was a strong interaction between health and completing the program which when added to the baseline model accounted for 22% of the variance (Adjusted R2 = 0.219), representing an improvement on the baseline model. The model with this interaction included was significant (F(6, 118) = 6.78, p < 0.01.)

I then tested the assumptions behind the linear model: 

* The residuals vs. fitted plot showed a reasonably straight line, which indicated that the mean of the residuals that are, as of yet, unexplained by the model are close to zero. 
* The QQ plot indicated that the residuals are fairly normally distributed by being close to the dotted line, with a small deviation at higher and lower levels which subsequent models will aim to explain. 
* The scale location plot indicated that the variance of the residuals remains fairly constant across fitted values. 
* The residuals vs. leverage plot highlighted three datapoints that potentially have high influence: 51, 99 and 123. Therefore the impact of these points was noted as elements for further models to consider.




## Question 3d

Happiness is significantly influenced by season. Age does not significantly influence happiness beyond a model which incorporates season alone. Completing the program did not significantly influence happiness beyond a model which incorporates season alone. However, the interaction of health and happiness did significantly impact the model, accounting for 20% of the variation.

# Question 4

```{r q4, fig.width = 5, fig.height=4}

#subset of data of only participants who completed programme.
onlyCompleted4a <- couchto5k[couchto5k$completed == "1", ]

Glas4 <- onlyCompleted4a[onlyCompleted4a$city == "Glasgow", ]
Edin4 <- onlyCompleted4a[onlyCompleted4a$city == "Edinburgh", ]

GlasSpring <- Glas4[Glas4$season == "spring",]
GlasSummer <- Glas4[Glas4$season == "summer",]
GlasAutumn <- Glas4[Glas4$season == "autumn",]
GlasWinter <- Glas4[Glas4$season == "winter",]

EdinSpring <- Edin4[Edin4$season == "spring",]
EdinSummer <- Edin4[Edin4$season == "summer",]
EdinAutumn <- Edin4[Edin4$season == "autumn",]
EdinWinter <- Edin4[Edin4$season == "winter",]

GlasSprHapp <- mean(GlasSpring[["happiness"]])
GlasSumHapp <- mean(GlasSummer[["happiness"]])
GlasAutHapp <- mean(GlasAutumn[["happiness"]])
GlasWinHapp <- mean(GlasWinter[["happiness"]])

EdinSprHapp <- mean(EdinSpring[["happiness"]])
EdinSumHapp <- mean(EdinSummer[["happiness"]])
EdinAutHapp <- mean(EdinAutumn[["happiness"]])
EdinWinHapp <- mean(EdinWinter[["happiness"]])

happAvg4 <- c(GlasSprHapp, GlasSumHapp, GlasAutHapp, GlasWinHapp, EdinSprHapp, EdinSumHapp, EdinAutHapp, EdinWinHapp)


plotq4 <- barplot(happAvg4, main = "Mean happiness by season and city", xlab = "Season/city", ylab = "Happiness", 
ylim = c(0, 100), names.arg = c("Glas.Spr", "Glas.Sum", "Glas.Aut", "Glas.Win", "Edin.Spr", "Edin.Sum", "Edin.Aut", "Edin.Win"), las=2, col = c("#b9e38d", "#b9e38d", "#b9e38d", "#b9e38d", "#a1e9f0", "#a1e9f0", "#a1e9f0", "#a1e9f0"))



```


# Question 5

## Question 5a

```{r q5a}

mod5a <- glm(completed ~ selfmot, family = binomial, data=couchto5k)

```
Completion of the programme was modelled as a binary (completed or not completed.) To examine the probability of completion, I fitted a logistic regression model to handle the binomial variable for completion and probability of dropping out of the programme. 


## Question 5b

```{r q5b}

```

## Question 5c

```{r q5c}


```










