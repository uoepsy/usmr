---
title: "USMR 2021-2022 Coursework"
author: "`r params$B156187`"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
params:
  examnumber: B156187
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)
# load any other packages that you require here:
library(tidyverse)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0
<!-- If you have run the R code above this point (you can do this now by pressing Ctrl-Shift-Alt-P, or running the chunk above) then your data will be in a dataframe called `couchto5k`. -->

In the dataset provided there are three impossible value in rows 78,110,116,38, and 42. To remove the rows with the impossible observations we can use the minus sign - inside square brackets.

```{r cleaning, include = FALSE}
# Neither output nor code from this chunk will be shown in the compiled document. 
couchto5k <- couchto5k[-c(78, 110, 116,38,42), ]

couchto5k$season[couchto5k$season=="autunm"] <- "autumn"
couchto5k$season <-as.factor(couchto5k$season)
couchto5k$city <-as.factor(couchto5k$city)

total <- count(couchto5k)
```

In the current dataset there are six numerical data and two categorical data. First, we can summarize our data with the summary() function. Second, to represent the data visually we need to use the appropriate graphs, depicted below.

```{r descriptives}
# Code will not be shown from this chunk (because we set echo = FALSE in the very first chunk)
# the output from this code will be shown. 
couchto5k %>%
  summary()
```

```{r fig.cap="Figure 1. Marginal Distribution of Age"}

ggplot(data=couchto5k, aes(x=age)) +geom_density() + labs(title= "Marginal Distribution of Happiness Ratings", 
x= "happiness", y= "probability density") + theme_classic()

```

The marginal distribution of age scores is bimodal with a mean of approximately 39.3 .There is variation in accountability scores (SD=12.4). 

```{r, echo=FALSE}
summary(couchto5k$age)
sd(couchto5k$age)
```


```{r fig.cap="Figure 2. Marginal Distribution of Accountability Scores"}

ggplot(data=couchto5k, aes(x=happiness)) +geom_density() + labs(title= "Marginal Distribution of Accountability Ratings", 
x= "accountability", y= "probability density") + theme_classic()

```

The marginal distribution of accountability scores is roughly bimodal with a mean of approximately 20 .There is variation in accountability scores (SD=5.36). 

```{r, echo=FALSE}
summary(couchto5k$accountability)
sd(couchto5k$accountability)
```


```{r fig.cap="Figure 3. Marginal Distribution of Self Motivation Scores"}

ggplot(data=couchto5k, aes(x=selfmot)) +geom_density() + labs(title= "Marginal Distribution of Self Motivation Scores", 
x= "Self Motivation", y= "probability density") + theme_classic()

```

The marginal distribution of self motivation scores is unimodal with a mean of approximately 15.1.There is variation in accountability scores (SD=2.85). 

```{r, echo=FALSE}
summary(couchto5k$selfmot)
sd(couchto5k$selfmot)
```


```{r fig.cap="Figure 4. Marginal Distribution of Happiness Ratings"}

ggplot(data=couchto5k, aes(x=happiness)) +geom_density() + labs(title= "Marginal Distribution of Happiness Ratings", 
x= "happiness", y= "probability density") + theme_classic()

```
The marginal distribution of happiness scores is bimodal with a mean of approximately 46.2.There is variation in accountability scores (SD=30.2). 

```{r, echo=FALSE}
summary(couchto5k$happiness)
sd(couchto5k$happiness)
```

```{r fig.cap="Figure 5. Marginal Distribution of Health Scores"}

ggplot(data=couchto5k, aes(x=health)) +geom_density() + labs(title= "Marginal Distribution of the Health Metric", 
x= "happiness", y= "probability density") + theme_classic()

```

The marginal distribution of health scores is unimodal with a mean of approximately 57 .There is variation in accountability scores (SD=10). 

```{r, echo=FALSE}
summary(couchto5k$health)
sd(couchto5k$health)
```

# Question 1 

## Question 1a

In an earlier nationwide survey, researchers found that 45% of participants abandoned the programme before the halfway point in week 5, and a further 10% gave up before the end of the programme. Is the data in the sample you have been given in line with data from the earlier survey? Once you have created a suitable variable to map to the information in the question, you should be able to answer this using a simple statistical test. 

In order to answer this question, it is crucial to visualize the data to get a general idea.

```{r q1a, fig.cap="Figure 6. Week Stopped"}
barplot(table(couchto5k$pptID, couchto5k$week_stopped))
```

```{r, echo=FALSE}
# Then, a categorical variable with the three attrition levels ("stopped before week 5", "stopped after week 5", and "completed") is created.To do that the mutate function is used and the weeks' numbers are matched to each attrition level. Next, the factor function is used to create the column.

# In order to create this new column, named 'stoplevel', we use the mutate function,
# then set the levels of this column and turn it into a factor.
couchto5k <- 
  couchto5k %>% 
  mutate(
    stoplevel = week_stopped
  )

couchto5k$stoplevel[couchto5k$stoplevel == 1 | couchto5k$stoplevel == 2 | couchto5k$stoplevel == 3 |couchto5k$stoplevel == 4 |couchto5k$stoplevel == 5] = "stopped before week 5"
couchto5k$stoplevel[couchto5k$stoplevel == 5 | couchto5k$stoplevel == 6 | couchto5k$stoplevel == 7 | couchto5k$stoplevel == 8] = "stopped after week 5"
couchto5k$stoplevel[couchto5k$stoplevel == 9] = "completed"

couchto5k$stoplevel <- factor(couchto5k$stoplevel, levels = c("stopped before week 5", "stopped after week 5", "completed"))
# Number of participants by attrition rates.
summary(couchto5k$stoplevel)

```

```{r}
# After having created the new column, to assess the extent to which the current sample conforms to the data of the previous survey, a chi-square goodness of fit test is performed.


# A χ2 goodness of fit test is performed to assess the extent to which our sample of participants conform to distribution of attrition proportions in the nationwide survey.


# The p-value of the test is p-value = .04, which is below the significance level alpha = .05. We can, thus, conclude that the observed data are significantly similar to the data of the earlier nationwide survey, and thus it is likely that we can reject the null hypothesis that they do not differ.

chi_nationwide <- chisq.test(table(couchto5k$stoplevel), p = c(.45,.10,.45))
chi_nationwide
```

A categorical variable was created to map 3 levels of participants: stopped before week 5, stopped after 5, and completed the program. A chi-square test was performed to assess whether attrition rates differed between the two surveys. 

All `r total` observations were included in the analysis. Effects were considered statistically significant at $\alpha=0.05$.

The hypotheses that were considered are: 

$H_0: \chi^2 = 0$. The attrition rates from the earlier survey and the data in the sample do not differ significantly.

$H_1: \chi^2 > 0$. The attrition rates from the earlier survey and the data in the sample differ significantly. 


A chi-square of independence was performed to compare the observed sample to the nation wide survey. The proportions did not differ, X2(2) = 6, p = 0.04. 

The chi-square test gave the following values $X^2$(`r chi_nationwide$parameter`)= `r chi_nationwide$statistic`, n = `r total`, *p* = `r chi_nationwide$p.value` and thus, it is likely that we can reject the null hypothesis.


```{r}

# There is another simpler way to check this. The table below shows that 43% of participants abandoned the programme before week 5 compared to 45% from the neationwide survey, 16% after week 5 compared to 10%, and 40% completed the program compared to 45%.

# Percentage of participants by attrition rates.
table(couchto5k$stoplevel)/134*100
```

## Question 1b

Using the same three categories (stopped before week 5, stopped after week 5, completed), examine whether the patterns of attrition rates differ by city.

```{r echo=FALSE}

# Whether the patterns of attrition rates differ by city can be examined by using the table() function. Based on the output 33% of participants that abandoned the program before week 5 were from Edinburgh while 10% were from Glasgow. The participants who abandoned the program after week 5 where 12% from Edinburgh while 4% from Glasgow. Finally, 30% of participants from Edinburgh completed the program, whereas only 10% of the ones from Glasgow did so too. 

# Percentages of participants by attrition rates and city.
table(couchto5k$stoplevel, couchto5k$city)/134*100
```

```{r echo=FALSE, fig.cap="Figure 6. Stop Level by City"}
# To visualize the table above, we can use the plot() function.

plot(table(couchto5k$stoplevel, couchto5k$city))
```

```{r}
# Whether the attrition rates differ by city can be examined with a chi-square of independence. 
chi_city <- chisq.test(table(couchto5k$stoplevel, couchto5k$city)) 
chi_city
```

A chi-square test of independence was performed to assess whether attrition rates differed by city. 
All `r total` observations were included in the analysis. Effects were considered statistically significant at $\alpha=0.05$.

The hypotheses that were considered are: 

$H_0: \chi^2 = 0$. The attrition rates do not differ significantly by city.

$H_1: \chi^2 > 0$. The attrition rates significantly differ by city. 


A chi-square of independence was performed to compare the observed sample to the nationwide survey. The proportions did not differ, X2(2) = 0.1, p = 1. 

The chi-square test gave the following values $X^2$(`r chi_city$parameter`)= `r chi_city$statistic`, n = `r total`, *p* = `r chi_city$p.value` and thus, failed to reject the null hypothesis. 


As shown above, the p-value of the test is $p-value=1$, which is different from the significance level alpha = 0.05 and it can, thus, be concluded that the association between attrition rates and city is not significant. The proportions did not differ by attrition rates ($X^2$ `r summary(chi_city)`)


## Question 1c

Do the average ages of participants who commenced the programme differ by city?

```{r echo=FALSE}

# As one can observe in the table below, the mean age of the participants from Edinburgh is 39 years, whereas the mean age of the participants from Glasgow is 40 years. They do not differ significantly.

group_by(couchto5k, city) %>%
  summarise(
    count = n(),
    mean = mean(age, na.rm = TRUE)
  )
```

```{r}
# The null hypothesis is that the data are normally distributed, whereas our alternative hypothesis is that the data are not normally distributed.

# From the output, the p-value of the Glasgow group is greater than the significance level alpha>.05 implying that the distribution of the data are not significantly different from the normal distribution, and thus we can assume normality. However, that is not the case with the group from Edinburgh since the p-value is close to zero.

shapedi <- shapiro.test(couchto5k$age[couchto5k$city=="Edinburgh"]) # Violation of normality
shapglas <- shapiro.test(couchto5k$age[couchto5k$city=="Glasgow"])

shapedi
shapglas
```

```{r}
# However, the data was plotted using a qqnorm and density plot where the data looked roughly normally distributed, therefore assumptions of normality were met.

plot(density(couchto5k$age))
qqnorm(couchto5k$age)


ggplot(data = couchto5k, aes(x = age, y = city)) +
  geom_boxplot()

group.var <- with(couchto5k, var.test(age ~ city)) # Check whether they vary and that they are independent samples.
group.var

```
An unpaired two-samples t-test has to be performed to compare the mean age of the two independent groups (participants from Edinburgh and participants from Glasgow). Before the conduction of the test, the independent samples t-test assumptions need to be met.

The first assumption refers to whether two samples are independent. To determine whether the two samples are independent, an F-test to compare two variances was conducted. Indeed, the true ratio of variances is equal to 1.23 and the p-value is `r group.var$p.value`, thus, we can accept the null hypothesis that the two groups vary equally. 

The second assumption refers to whether the data from each of the two groups follow a normal distribution. To test this, a Shapiro wilk test was conducted to assess whether the average ages of participants in the program differed by city.  All `r total` observations were included in the analysis. Effects were considered statistically significant at $\alpha>0.05$.

The Shapiro Wilk test indicated a violation of normality for both Edinburgh  (W= `r shapedi$statistic`, p = `r shapedi$p.value`) but not for Glasgow (W= `r shapglas$statistic`, p = `r shapglas$p.value`)

The Shapiro-Wilk normality test does not meet the assumptions of normal distribution for one of the groups (Edinburgh). However, to ensure the validity of this test the data were plotted. As is evident, the data are roughly normally distributed, and the slight variance observed in the graphs could be attributed to the small size of the data.

```{r}
t.cage <- with(couchto5k, t.test(age ~ city, alternative = "two.sided"))
t.cage
```

A Welch Two-Sample t-test was conducted to assess whether the two groups have different means. All `r total` observations were included in the analysis. The significance level considered is $alpha .05$.

$H_0: \mu Edinburgh = mu Glasgow$. The mean age of the two groups does not differ significantly by city.

$H_1: \mu Edinburgh \neq mu Glasgow$. The mean age of the two groups does differ by city.

The mean age in the two groups, Edinburgh and Glasgow, were mean=`r t.cage$estimate`. The Welch two-samples t-test showed that the difference was not statistically significant (t=`r t.cage$statistic`, p=`r t.cage$p.value`).

Since the p-value is more than the significance level $alpha .05$, we cannot reject the null hypothesis and, thus, we conclude that the difference in mean age of participants between the two cities is not statistically significant.


# Question 2

## Question 2a

Are participants’ happiness ratings affected by the season they were interviewed in? Describe the way in which season influences happiness outcomes.

```{r q2a}
# Because the season variable is categorical the lm function creates dummy variables automatically.
# Thus, the following code is not needed.

library(dplyr)
couchto5k %>%
  mutate(is_autumn = ifelse(season=='autumn',1,0),
         is_winter = ifelse(season=='winter',1,0),
         is_spring = ifelse(season=='spring',1,0),
         is_summer = ifelse(season=='summer',1,0),

  ) 

# Create model for season.

model_season <- lm(happiness ~ 1 + season, data = couchto5k)
summary(model_season)

plot(model_season)
plot(model_season, which=4)

# define cutoff check for influential data, because it's categorical maybe it looks okay?
```

Whether happiness ratings are affected by the season participants were interviewed in can be investigated with a multiple linear regression model.The null hypothesis is that season does not influence happiness ratings. The model is as printed below:

$$
\text{Happiness} = \beta_0 + \beta_1\ {autumn} + \beta_2\ {spring} + \beta_3\ {summer} + \beta_4\ {winter} +\epsilon\
$$

The p-value of the model is $p = .000274$, which is less than the significance level $alpha = .05$ and it can, thus, be concluded that the association between happiness ratings and season is significant.

As we can observe the median is close to zero, and thus implies that our model is not skewed one way or the other. Another condition of the linear regression that needs to be met is that the residuals should be symmetrically distributed and thus, the Min. and Max. values should have the same magnitude, and in this case they do.

Looking at the coefficients, a difference is notable in the average happiness scores for the season variable. Summer and spring have the highest average happiness scores, winter follows and autumn has the lowest average happiness scores.

R-squared $R^2$ `r summary(model_season)$r.squared` gives a measurement of what percentage of the variance in the outcome variable can be explained by the regression. In this case season explained 11.5% of the variance in happiness ratings ($Adjusted R^2=0.115, F(3,130)=6.79, p= 0.000274$).

The F-statistic, on the other hand, is an indicator of whether there is actually a relationship between the predictor variable and the outcome variable. The F-statistic is further from 1 and the p-value $p=.000274$ indicates that the model as a whole is statistically significant.

After building the model, it is crucial to check whether regression assumptions are met. These are: the linearity of the data, which involves that the relationship between the predictor (x) and the outcome (y) is assumed to be linear, normality of residuals, that is that the residual errors are assumed to be normally distributed, homogeneity of residuals variance which menas that the residuals are assumed to have a constant variance, and independence of residuals error terms.

The first plot (Residuals vs Fitted) shows the difference between the observed and the fitted (predicted) values. The red line needs to be approximately horizontal to zero. In the present residual plot there does not seem to be a distinctive pattern as the horizontal line is almost exactly at zero. This suggests that we can assume a linear relationship between the predictor (season) and the outcome variable (happiness).(not for categorical variables, check, we do not report this for categorical variables, assumptions were met, the values cluster around four levels, because psring and sumemr have approcxiamtely the same values they cluster around the same spot)

The second plot (Normal Q-Q) shows if residuals are normally distributed. In the present plot residuals follow a straight line well, however three observations deviate slightly.

The third plot (Scale-Location) shows whether the residuals are spread out equally along the range of predictor variable. This plot is used to check the assumption of equal variance. In this case the residuals appear randomly spread and thus since the red line is horizantal we can assume that residuals variance is homogeneous.

The fourth and last plot (Residuals vs Leverage) shows whether there are any influential cases. The previous plots showed some outliers, however, they did not reveal whether these are influential enough to determine the regression line. In this case it does not seem that there are any influential cases as we can observe merely a small red dashed line since all cases appear inside of the Cook’s distance line. 


## Question 2b

Accounting for any effects you discovered in (2a), is happiness affected by age?

```{r q2b}
# To investigate whether happiness scores are influenced by age, a linear regression model has to be created using the following R code shown under 'Call'. Looking at the p-value 

model_sage <- lm(happiness ~ season + age, data = couchto5k)
summary(model_sage)
plot(model_sage)
```

Whether happiness ratings are affected by the season participants were interviewed in as well as by age can be investigated with a multiple linear regression model. Thus, the previous model was refitted to include age. No outliers were found. 

As shown in the ouput, age improves the model as in comparison to the previous model ($AdjustedR^2= .115$) season and age explained 15.8% of the variance in happiness ratings ($Adjusted R^2=0.158, F(4,129)=7.24, p= 2.72e-05$).

Again, it is crucial to check whether regression assumptions are met in the new model as well. The model is as printed below:

$$
\text{Happiness} = \beta_0 + \beta_1\ {autumn} + \beta_2\ {spring} + \beta_3\ {summer} + \beta_4\ {winter} + \beta_4\ {age} + \epsilon\
$$

As we can observe in the ouput the median is close to zero which implies that our model is not skewed one way or the other and the residuals are symmetrically distributed since the Min. and Max. values have the same magnitude.

In the first residual plot there does not seem to be a distinctive pattern. This suggests that we can assume linear relationship between the predictor and the outcome variable.

In the second plot, residuals follow a straight line well and there are three observations deviate slightly. Thus, we can assume that the residuals are normally distributes.

In the third plot, the residuals appear randomly spread and since the red line is horizantal we can assume that residuals variance is homogeneous.

The previous plots showed some outliers, however, they did not reveal whether these are influential enough to determine the regression line. In the fourth plot, it does not seem that there are any influential cases as we can observe merely a small red dashed line since all cases appear inside of the Cook’s distance line.

## Question 2c

The models you have built above explore ‘baseline’ effects; that is, effects that are not of primary interest to the researchers but which might affect the outcome variable of happiness. For use in question 3, pick a specific baseline model and justify why you are using this.

```{r q2c}
# Our baseline model will be model_sage because both season and age seem to have a significant effect on happiness.

anova <- anova(model_season, model_sage)
anova
```

An Analysis of Variance was conducted to compare the two models. The ouput shows a Df of 1 which indicates that the complex model has one additional parameter (age), and a small $p-value= .0068$). Thus, the addittion of age to the model significantly improved fit over the first model. For this reason, the baseline model will be the model that includes both season and age as predictors because it explaines most of the variance in happiness ratings.  

The hypotheses that were considered are: 

$H_0: R^2 = 0$. Health does not significiantly explain the variance in happiness scores in addition to season and Program completion.

$H_1: R^2 > 0$. Health significantly explains the variance in happiness scores in addition to season and program completion.

# Question 3

## Question 3a

Building on your baseline model, are participants’ happiness ratings affected by whether or not they completed the programme? Describe the way in which programme completion influences happiness outcomes.

```{r q3a}
# In order to answer the question of whether participants' happiness ratings are affected by whether or not they completed the program, we first need to create a binary variable with yes/no values.

couchto5k <- couchto5k %>%
  mutate(if_completion = ifelse(week_stopped=='9',"yes","no")
  )
if_completion <- as.factor(couchto5k$week_stopped)

model3 <- lm(happiness ~ season + age + if_completion, data = couchto5k)

summary(model3)
plot(model3)
```

In order to answer the question of whether participants' happiness ratings are affected by whether or not they completed the program, a binary variable with yes/no values was computed.

Then, building on our baseline model the new variable was added and the data were plotted to evaluate whether the linear regression assumptions are met. The model is as printed below:

$$
\text{Happiness} = \beta_0 + \beta_1\ {autumn} + \beta_2\ {spring} + \beta_3\ {summer} + \beta_4\ {winter} + \beta_4\ {age} + \beta_5 {ifcompletion} +\epsilon\
$$
The hypotheses that were considered are: 

$H_0: R^2 = 0$. Program completion does not significiantly explain the variance in happiness scores in addition to season and age.

$H_1: R^2 > 0$. Program completion significantly explains the variance in happiness scores in addition to season age.

In the first residual plot there does not seem to be a distinctive pattern. This suggests that we can assume a linear relationship between the predictor and the outcome variable.

In the second plot, residuals follow a straight line well and there are three observations deviate slightly. Thus, we can assume that the residuals are normally distributes.

In the third plot, the residuals appear randomly spread and since the red line is horizantal we can assume that residuals variance is homogeneous.

The previous plots showed some outliers, however, they did not reveal whether these are influential enough to determine the regression line. In the fourth plot, it does not seem that there are any influential cases as we can observe merely a small red dashed line since all cases appear inside of the Cook’s distance line.

The new model explains 18.6% of the variance in the model ($Adjusted R^2= .186, t=2.31, p=.022$)


## Question 3b

Building on the analysis in (3a), is happiness additionally affected by the “health metric”?

```{r q3b}
model4 <- lm(happiness ~ season + age + if_completion + health, data = couchto5k)

summary(model4)
plot(model4)
```

Building on the model in 3a) we invesitgate whether happiness is additionally influenced by health. The data were plotted to evaluate whether the linear regression assumptions are met. The new model explains 19.5% of the variance in the model. The new model explains 19.5% of the variance in the model ($Adjusted R^2= .195, t=-0.61, p=.6.74e-06$) Thus, the addition of the health predictor slightly improved the model.

The new model is as shown below:

$$
\text{Happiness} = \beta_0 + \beta_1\ {autumn} + \beta_2\ {spring} + \beta_3\ {summer} + \beta_4\ {winter} + \beta_4\ {age} + \beta_5\ {ifcompletion} + \beta_6\ {health} +\epsilon\
$$

The hypotheses that were considered are: 

$H_0: R^2 = 0$. Health does not significiantly explain the variance in happiness scores in addition to season, age, and program completion.

$H_1: R^2 > 0$. Health significantly explains the variance in happiness scores in addition to season, age, and program completion.

In the first residual plot there does not seem to be a distinctive pattern. This suggests that we can assume linear relationship between the predictor and the outcome variable.

In the second plot, residuals follow a straight line well and there are three observations deviate slightly. Thus, we can assume that the residuals are normally distributes.

In the third plot, the residuals appear randomly spread and since the red line is horizantal we can assume that residuals variance is homogeneous.

The previous plots showed some outliers, however, they did not reveal whether these are influential enough to determine the regression line. In the fourth plot, it does not seem that there are any influential cases as we can observe merely a small red dashed line since all cases appear inside of the Cook’s distance line. Hence, all the assumptions were met.

## Question 3c

It’s been hypothesised that the effects of good health are amplified by the feeling of acting healthily, such that the happiness of participants who got further along the programme might be more affected by the health metric than that of those who stopped earlier. Building on the model in (3b), can you test this hypothesis?

```{r q3c}
# Fit model with interaction

model.int <- lm(happiness ~ season + age + if_completion + health + health:if_completion, data = couchto5k)

summary(model.int)

library(sjPlot)
library(sjmisc)
library(ggplot2)
plot_model(model.int, type = "pred", terms = c("health", "if_completion"), show.data = TRUE)
plot(model.int)
```

A multiple linear regression test was conducted to investigate whether happiness rates are affected more by the health metric based on how far participants went into the program. The model was fitted with the interaction between program completion and health. Then, the data were plotted and the assumptions were met.

The model is as shown below:

$$
\text{Happiness} = \beta_0 + \beta_1\ {autumn} + \beta_2\ {spring} + \beta_3\ {summer} + \beta_4\ {winter} + \beta_4\ {age} + \beta_5\ {ifcompletion} + \beta_6\ {health} + \beta_7\ {health*ifcompletion} + \epsilon\
$$
The hypotheses that were considered are: 

$H_0: R^2 = 0$. Health does not influence happiness rates for those who got further in the program.

$H_1: R^2 > 0$. Health significantly influences happiness rates for those who got further in the program.

Health does not influence happiness rates differently based on how far participants went into the program as the  interaction term between program completion and health is not significant ($p-value=.074, Adjusted R^2=.21, F(7,126)=6.04$).

## Question 3d

What can we conclude about the various causes of happiness in our data? Write a brief description of the effects in the model, such as you might find in an academic paper.

As exemplified by the models, season, age, and program completion explained 18.6% of the variance in happiness ratings, and season, age, program completion and health explained 19.5% of the variance. The fitted model that includes most predictor variables in the dataset (season, age, program completion, health, and program completion and health interaction) indicates that happiness ratings are affected by various predictors as they explain 21% of the variance. However, the interaction between program completion and health in the fitted model improved the model but was insignificant in influencing happiness ratings.


# Question 4

Create a subset of the data, including only those participants who completed the programme. Create a plot of the average happiness ratings grouped by season and city, that can be used in a presentation to the funders of the project.

```{r q4}
	completed_prog<-filter(couchto5k, week_stopped==9)
	
	completed_prog
	
	library(dplyr)

happiness <- completed_prog %>%
  group_by(city, season) %>%
  summarise_at(vars(happiness), list(meanhappiness = mean))

ggplot(data = completed_prog, aes(x=season, y= happiness, col= city)) + 
  geom_boxplot() + 
  facet_wrap(~city, scales = "free_x") + 
  theme(legend.position="none")
```


# Question 5

## Question 5a

Build a model that predicts the likelihood of dropping out (at all).

```{r q5a}
couchto5k <- 
  couchto5k %>% 
  mutate(
    dropout = ifelse(week_stopped == 9, "completed", "dropout")) %>% as_factor()

couchto5k$dropout <- factor(couchto5k$dropout, levels= c("completed", "dropout"))
summary(couchto5k$dropout)
    

model_predicted <- dropout ~ age + accountability + selfmot + health + happiness + season + city

fitted.model <- glm(dropout ~ age + accountability + selfmot + health + happiness + season + city, data = couchto5k, family = "binomial")
summary(fitted.model)


nullmodel <- glm(dropout ~ 1, family = "binomial", data = couchto5k)
summary(nullmodel)
```

To investigate the probability of dropping out a new fitted model was created accounting for all variables in the dataset (age, accountability, selfmot, health, happiness, season, city). In order to create the model, we first need to compute for the 'dropout' variable and put it into a factor to distinguish between participants who dropped out and those who completed the program. The model is printed below:

$$
\text{Dropout} = \beta_0 + \beta_1\ {age} + \beta_2\ {accountability} + \beta_3\ {selfmot} + \beta_4\ {health} + \beta_5\ {happiness} + \beta_6\ {autumn} + \beta_7\ {spring} + \beta_8\ {summer} + \beta_9\ {winter} + \beta10\ {city} +\epsilon\
$$

## Question 5b

Briefly describe the effects in your model as you would in an academic paper.

```{r q5b}
anova(fitted.model)
```

An Analysis of Deviance was conducted to compare the likelihood of the new fitted model to that of the previous model, with the previous model being the null. As can be seen in the table above, the most important predictor variable for dropping out of the program is the season in which the participants were interviewed, with health, city and age following.


## Question 5c

Draw a graph representing the probability of quitting as a function of how self motivated participants were.

```{r q5c}
drop_smot <- glm(dropout ~ selfmot, data=couchto5k, family = "binomial" )

couchto5k$probs = predict(drop_smot, newdata = couchto5k, type = "response") 
couchto5k %>%
  filter(dropout==dropout) %>%
  ggplot(., aes(x=selfmot, y=probs)) + 
  geom_line() + 
  xlab("Self Motivation") +
  ylab("Probability of Dropping Out") +
  theme_classic()
```



