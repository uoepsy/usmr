---
title: "USMR 2021-2022 Coursework"
author: "`r params$examnumber`"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: default
params:
  examnumber:B191239
---

<!-- We have provided a template below with headings/sub-headings for each question/sub-question. This is just a template. Feel free to add or delete code-chunks if desired.  -->
<!-- Beneath here is some code which will set everything up for you.  Anything that is needed to run your code should be explicitly set up below -->

```{r setup, include=FALSE}
# this line will mean that when you compile/knit your document, 
# the code will not show, but the output (e.g., plots) will!
knitr::opts_chunk$set(echo = FALSE)
# this line means all numeric output gets rounded to 3 dp
options(digits=3)

# load any other packages that you require here:
library(tidyverse)

##options
options(digits=2)
# This will read in your own personal data:
source("https://uoepsy.github.io/data/usmr_2122_data.R")
```

# Question 0
## The researchers' interests are two-fold: They are interested in the psychological factors that make people continue on the programme, and in the effects of taking the programme on health and wellbeing.
<!-- If you have run the R code above this point (you can do this now by pressing Ctrl-Shift-Alt-P, or running the chunk above) then your data will be in a dataframe called `couchto5k`. -->

## Have a look at the data. 
## -Check for impossible values and deal with these in an appropriate manner.
## - Describe the data, either in words or using suitable graphs (or a combination). 
## - Remember to detail the decisions you have made.

The data for this analysis was obtained from an NHS fitness program, `couchto5k`. The 123 participants engaged in a nine week program that commenced in each season throughout the year and began in either Edinburgh or Glasgow. The participants filled out a questionnaire after either completing or dropping the program and which included the variables to be examined in the analysis such as age in years, a psychometric measure of accountability (scored 1-7), a psychometric measure of self-motivation (scored 1-7), a multi-test health measure (0-100), a simple happiness scale (0-100), whether the program was completed or not, as well as the season and city the program commenced in. Overall, the data was complete and no missing values were detected. An impossible value was removed from the `week_stopped` column because it was greater than the number of weeks of the program and an impossible value was removed from the `selfmot` column as the value was beyond the sum of the five questions scores. Checking the levels of the categorical variable `season` revealed that there was an additional level due to the incorrect spelling of autumn. The `season` variable was converted to a factor so the incorrect spelling could be removed. One value was removed as an outlier rather than an impossible value because it is still a plausible value. This observation was removed from the `age` column after visualizing the data with a box plot. However no further points where removed at this time. The data was further visualized with diagnostic graphs. The psychological factors, including accountability, happiness, and self-motivation, were plotted with density and box plots to compare the patterns of distribution for these factors (plot 0a). Likewise, other factors such as age, health metric, and week stopped were plotted with density and box plots to compare these patterns of distribution to get an idea how how they influence overall health and well-being (plot 0b). For each variable, a Shapiro-Wilk test ws conducted to determine if the sample came from a population that is normally distributed. Only the  Shapiro-Wilk test for accountability, `(w = 1, p = .2)`, health `(w = 1, p = .3)`, and self-motivation `(w = 1, p = .05)` did not indicate violation of the assumption of normality. Seasons and city were not tested as they are categorical variables. 
```{r cleaning, include = FALSE, warning= FALSE}
# Neither output nor code from this chunk will be shown in the compiled document. 
couchto5k
summary(couchto5k)

##Impossible values 
couchto5k <- couchto5k %>% mutate(selfmot=ifelse(selfmot<5,NA,selfmot))
couchto5k <- couchto5k %>% mutate(week_stopped=ifelse(week_stopped>9.0,NA,week_stopped))

##Change characters to factors (for season and city), check levels, and remove incorrect spelling "autunm"

##change season to factor and change levels for mispelling "autunm"
couchto5k$season<- as.factor(couchto5k$season)

class(couchto5k$season)

levels(couchto5k$season)

couchto5k <- couchto5k %>% 
  mutate(
    season = (gsub("autunm","autumn",season)))

## Outliers - remove age 123 
couchto5k <- couchto5k %>% mutate(
  age=ifelse(age>100,NA,age))

```


```{r descriptives, warning=FALSE}
# Code will not be shown from this chunk (because we set echo = FALSE in the very first chunk)
# the output from this code will be shown. 

##Descriptives before outliers 
library(patchwork)
account_plot <-  
  ggplot(data=couchto5k,aes(accountability)) + geom_density() + geom_boxplot(width=.05) 

hap_plot <- ggplot(data=couchto5k,aes(happiness)) + geom_density() + geom_boxplot(width=.05) 

selfmotplot <- ggplot(data=couchto5k,aes(selfmot)) + geom_density() + geom_boxplot() 

psych_factors_plot <- account_plot + selfmotplot + hap_plot 

psych_factors_plot + plot_annotation(title = '0a. Psychological factors that influence program completion',caption = 'These plots reveal the the IQR as well as the overall pattern of distribution for each psychological factor.') 

health_plot <-ggplot(data=couchto5k,aes(health)) + geom_density() + geom_boxplot(width=.05) 

age_plot <- ggplot(data=couchto5k,aes(age)) + geom_density() +  geom_boxplot(width=.05) 

stop_plot <-ggplot(data=couchto5k,aes(week_stopped)) + geom_density() + geom_boxplot(width=.05) 

wellbeing_plot <- health_plot + age_plot + stop_plot
wellbeing_plot + plot_annotation(title= '0b. Other factors that influence overall health and wellbeing', caption = 'These plots show the IQR as well as the overall pattern of distribution for each factor that may influence health and well-being')

shapiro.test(couchto5k$age)
## not significant
shapiro.test(couchto5k$accountability)
## significant 
shapiro.test(couchto5k$health)
## significant 
shapiro.test(couchto5k$week_stopped)
## not significant 
shapiro.test(couchto5k$selfmot)
## significant 
shapiro.test(couchto5k$happiness)
## not significant 
```


# Question 1 

## Question 1a
## In an earlier nationwide survey, researchers found that 45% of participants abandoned the programme before the halfway point in week 5, and a further 10% gave up before the end of the programme. Is the data in the sample you have been given in line with data from the earlier survey? Once you have created a suitable variable to map to the information in the question, you should be able to answer this using a simple statistical test.

In order to determine whether the data in the sample is in line with the data from the survey, a chi-squared goodness of fit test was conducted. First, it was necessary to create a new column in the `couchto5k` dataset that organizes the sample data into three categories by mutating the `week_stopped` variable based on if participants abandoned the program before week five, after week five and before the program ended, or if it was completed. A table of both the expected counts of the three categories and the observed counts was created and these counts were visualized with a bar plot. A chi-squared goodness of fit test was conducted in order to compare the observed counts with the expected and determine whether the sample data was in line with the original data. The test was not significant, meaning that the sample data is not in line with the original data, `x^2(2, n = 2, p > .05). These results provide evidence to reject the null hypothesis that the sample data is in line with the original data. 

```{r q1a}
##Chi squared goodness of fit test

##Make new column that categorizes values before5, after5, and completed
week_stopped_data <- couchto5k %>%
  select(week_stopped) %>%
  mutate(week_stopped_group = case_when(
    week_stopped < 5 ~ "B5",
    week_stopped > 4 & week_stopped < 9 ~ "A5",
    week_stopped > 8 ~ "C"))

##Make a table 
table(week_stopped_data$week_stopped_group)

couchto5k$ws_stopped_col <- week_stopped_data$week_stopped_group

##View observed proportions 
prop.table(table(week_stopped_data$week_stopped_group))*100
barplot(prop.table(table(week_stopped_data$week_stopped_group))*100)

##Compare observed proportions with predicted (expected) proportions 
chisq.test(
  table(week_stopped_data$week_stopped_group),p=c(.1,.45,.45))
```

## Question 1b
## Using the same three categories (stopped before week 5, stopped after week 5, completed), examine whether the patterns of attrition rates differ by city.

Using the same three categories, a plot was created to visualize the association between the patterns of attrition and city participants commenced the program in. A chi-squared test of independence was conducted to further examine the relationship between these variables. The association between these variables was insignificant given that `x^2(2, n = 0.9, p > .05)`. The large size of the p-value provides strong evidence to accept the null hypothesis that the variables are independent meaning that patterns of attrition do not differ by city. 

```{r q1b, warning=FALSE}
plot(table(couchto5k$ws_stopped_col,couchto5k$city))

chisq.test(couchto5k$ws_stopped_col,couchto5k$city)
```

## Question 1c
## Do the average ages of participants who commenced the programme differ by city

A Welch Two Sample t-test was conducted in order to determine if the average ages of participants who commenced the program differed by city. It must first be recognized that the previously conducted Shapiro-Wilk test indicated that age violated the assumption of normality `(w = .9, p < .01)`, however given that this question concerns mean age, a Two Sample t-test is the best option for analysis. The mean age of participants who commenced in Edinburgh was `(M=41)` while the mean age of participants who began the program in Glasgow was `(M=33)`. The results of the t-test indicated that there is a significant difference in the mean ages of participants in the two cities, `(t(3,86), p = .001)`. 

```{r q1c}

shapiro.test(couchto5k$age)

t.test(couchto5k$age ~ couchto5k$city,alternative="two.sided")
```

# Question 2

## Question 2a
## Are participants’ happiness ratings affected by the season they were interviewed in? Describe the way in which season influences happiness outcomes.

To determine whether participants' happiness ratings were affected by the season they were interviewed in, it was necessary to fit a linear model in which happiness outcomes are influenced by season. Participant 66 was removed from the `age` column of the data since this value was converted to NA during data cleaning because it was an outlier (age 123). At first the model was constructed and analyzed including the value, however the NA later affected the subsequent ANOVA in Question 2c because the models were not fitted to the same size data set, so, it was necessary to remove participant 66 to analyze this model. The linear model was fitted as `happinessmodel` and was assigned to the `modified_age` data set which excludes participant 66. Three observations were excluded from the analysis - participant 2, 100, and 102 - as it was determined that these values influenced the model heavily. Removing these points was justified because all remaining observations returned below the cut-off. To make inferences about this model, a `t.test` was conducted against the null hypothesis that `season` does not influence `happiness`. Two of the seasons returned as statistically significant: `t(118) = 3.51, p < .001, two-sided` for autumn and `t(118) = 2.71, p < .001, two-sided` for spring. The larger t-statistics for autumn and spring lead to small p-values. This means that there is strong evidence against the null hypothesis, therefore there is a relationship between the seasons autumn and spring and increased levels of happiness. Conversely, the smaller t-statistics for summer and winter lead to larger p-values which means that there is evidence to accept the null hypothesis that there is no relationship between summer and winter and levels of happiness. The R-squared value of the model indicates that approximately 86% of the total variability in happiness is explained by the linear association with seasons. An F-test was also conducted to determine the overall significance of the model, `F(3,118) = 4.8, p < .005` which indicates that there is evidence that seasons are an effective predictor of happiness levels and explains more of the variance than a null model when accounting for seasons. These findings were further justified by plot 2a which depicts the relationship between each season and happiness levels. 

```{r q2a}
##remove participant 66 because of the NA 
modified_age <- 
  couchto5k[-c(66),]

##Model - 
happinessmodel <- 
lm(happiness ~ 1 + season, data= modified_age)

##Cook's Distance 
plot(happinessmodel,which=4)
modified_age <- modified_age[-c(2),]
modified_age <- modified_age[-c(102),]
modified_age <- modified_age[-c(100),]

##Interpreting how season influence happiness - *** 
coef(happinessmodel)
summary(happinessmodel)

library(patchwork)
happyplot <- 
  ggplot(
    data=modified_age, aes(x=happiness,y=season, color=season)) + 
  geom_boxplot() + 
  plot_annotation(
    title = 'Plot 2a: Relationship between seasons and happiness'
  )
happyplot

##Add interprtation of box plot?

```

## Question 2b
## Accounting for any effects you discovered in (2a), is happiness affected by age?
A multiple regression model was fitted as `agemodel` and used to investigate whether seasons and age significantly predicted happiness levels. Again, the `modified_age` data set was used. There were no further values with high leverage that would greatly influence the model, so no observations were excluded. The analysis showed that the overall regression was statistically significant `F(4,144) = 4.59, p < .005`, however the only predictor that contributed significantly to the model was spring `(B = 24.19, p < .05)` while all other predictors returned as insignificant, therefore happiness is not affected by age. Additionally, only 13% of the total variability in happiness was explained by linear association of age with seasons held constant. Plot 2b further demonstrates these results. 
```{r q2b}
##model 
agemodel <- 
  lm(happiness ~ 1 + season + age, data= modified_age)

##Cook's distance 
plot(agemodel,which=4)

##Examining the coefficients 
coef(agemodel)
summary(agemodel)
sigma(agemodel)

ageplot <- 
  ggplot(
    data=modified_age, aes(x=age,y=happiness, color=season)) + 
  geom_boxplot() + 
  facet_wrap(~season) +
  plot_annotation(
    title = 'Plot 2b: Relationship between season, age, and happiness')
ageplot
```

## Question 2c
## The models you have built above explore ‘baseline’ effects; that is, effects that are not of primary interest to the researchers but which might affect the outcome variable of happiness. For use in question 3, pick a specific baseline model and justify why you are using this.

Baseline effects were investigated through creating a simple regression model `happinessmodel` and a multiple regression model `agemodel` which allowed for examining effects that were not of primary interest but that still may impact levels of happiness. A null model with no explanatory variables was fit to the data for comparison. An ANOVA was conducted to compare the null model with the `happinessmodel`. The results found that `F(3,115) = 5.32, p < 0.005` meaning that the season a participant was interviewed in explains a significant amount of variance in happiness scores. An ANOVA was also conducted to compare the null model with the `agemodel`.The results depicted that `F(4,114) = 4.59, p < 0.005` meaning age as an added predictor also explains a significant amount of variance in happiness scores when season is held constant. The two regression models were also compared together which returned insignificant results as `F(1,114) = 2.21, p > .05.` The small f-statistic produces a large p-value, therefore providing evidence to reject the null hypothesis. While keeping in mind that the residuals do not fit the assumption of normality, as exemplified by the Shapiro-Wilk test which returned `p < .005`, the `happinessmodel`was chosen as the most suitable option for the baseline model as it proved to be better for explaining happiness because it explains more of the overall variance over and above the null and age model. Since both models were statistically significant and had the same p-value, this decision was made based on looking at the diagnostic plots. Comparison of the plots showed that the mean of the residuals was closer to zero and the variance of residuals was more constant for the `happinessmodel`. Likewise, the plots indicated that the data points on the `agemodel` had higher residuals that were further away from the regression line meaning that the had more influence than the data points on the `happinessmodel`. 

```{r q2c}

happinessmodel <- 
lm(happiness ~ 1 + season, data= modified_age)

agemodel <- 
  lm(happiness ~ 1 + season + age, data= modified_age)


##Null model happiness
null_model_happiness <- 
  lm(happiness ~ 1, data = modified_age)


##model comparison vs simple linear model 
anova(null_model_happiness,happinessmodel)


##model comparison vs multiple linear model
anova(null_model_happiness, agemodel)
##Interpretation: p < .005, so there is evidence to reject the null meaning that there is a significant relationship between...

##comparison of the two models 
anova(happinessmodel,agemodel)


##visualize both models to decide which baseline to use 
plot(happinessmodel)
plot(agemodel)

shapiro.test(residuals(happinessmodel))
shapiro.test(residuals(agemodel))

```
# Question 3

## Question 3a
## Building on your baseline model, are participants’ happiness ratings affected by whether or not they completed the programme? Describe the way in which programme completion influences happiness outcomes.

Building on the selected baseline model, multiple linear regression was used to test if whether or not completion of the program influenced happiness ratings. First, the `week_stopped` data was separated into a new column which indicated whether or not participants fully completed the program. Again, the `modified_age` data was utilized. One observation, participant 35, was excluded from the final analysis as it was deemed to be highly influential to the model. The results of the regression depicted that this model only explained 17% of the variance, however the model was an overall significant predictor of happiness levels `F(4,113) = 5.68, p < .001`. Spring contributed significantly to the model, `(B = 37.8, p < .01)` as did if participants fully completed the program `(B = 16.1, p < .05)`. However, the rest of the seasons did not contribute significantly to the model. Judging from these results, there is evidence that that program completion is an effective predictor of happiness levels when holding season constant. 

```{r q3a}
completed_week <- 
  modified_age %>% 
  select(week_stopped) %>%
  mutate(completed_when = case_when(
    week_stopped < 9 ~ "NO",
    week_stopped > 8 ~ "YES"))

modified_age$completed_week <- 
  completed_week$completed_when

completemodel <-  
  lm(happiness ~ 1 + season + completed_week, data=modified_age) 
summary(completemodel)
coef(completemodel)

library(sjPlot)
##Remove for Cook's distance 
plot(completemodel,which=4)
modified_age <- modified_age[-c(35),]
```

## Question 3b
## Building on the analysis in (3a), is happiness additionally affected by the “health metric”?

Multiple linear regression was also used in this instance by building on the previous model to examine whether happiness was also affected by the "health metric". There were no observations excluded from the analysis. The regression results indicated that this model accounted for 54% of the variability meaning that this model is a relatively good predictor of happiness outcomes, `F(4,111) = 32.3, p <.001` as compared to a null model that does not account for these predictors. Again, spring contributed significantly to the model `(B = 52.15, p < .001)` along with fulling completing the program, `(B = 16.60, p < .05)` and the health metric, `(B = -.61, p < .05)`. The other seasons did not contribute significantly to the model. There is indication that health ratings are an effective predictor of happiness levels.

```{r q3b}

complete_health <- 
  lm(happiness ~ 1 + season + completed_week + health, data=modified_age) 

summary(complete_health)

plot(complete_health,which=4)

```

## Question 3c
## It’s been hypothesised that the effects of good health are amplified by the feeling of acting healthily, such that the happiness of participants who got further along the programme might be more affected by the health metric than that of those who stopped earlier. Building on the model in (3b), can you test this hypothesis?

Accounting for the results of the previous multiple linear regression model, an additional predictor was included in order to investigate if the effects of good health are bolstered by the feeling of acting healthy. For example, it is possible that participants who got further through the program might be more affected by the health metric than those who stopped early. 24% of the variance was explained by this model. There is also indication that this model was a significant predictor of happiness, `F(6,110) = 5.7, p < .001`, meaning that this model, including the measure of those who got further in the program, is a better measure of happiness levels. Spring maintained as a significant contributor to the model `(B=41.91, p < .001)` along with the health metric `(B= -0.67, p < .05)`, and the week that participants stopped was also significant `(B= 4.04, p <.05)`.However, the feeling of acting healthy did not significantly impact the model, `(B= -1.85, p > .05).

```{r q3c}

week_stopped_model <- 
lm(happiness ~ 1 + season + completed_week + health + week_stopped, data=modified_age) 

summary(week_stopped_model)

```

## Question 3d
## What can we conclude about the various causes of happiness in our data? Write a brief description of the effects in the model, such as you might find in an academic paper.

The preceding analysis focused on examining the effects that each variable has on happiness. The results from 3a indicated that when adding whether participants completed the program into the model, seasons did not have a heavy influence to the overall significance of the model. This is because only one season was statistically significant. It is possible that whether the program was completed was more of a significant predictor of happiness than the seasons. The results of the regression that included the health metric explained more of the variance in happiness outcome meaning that ratings of health are a significant predictor of happiness when the other factors are considered. When additionally accounting for the effects of good health, interestingly health ratings was no longer a significant predictor. This could be occurring because there is a difference between actually being healthy, as would be rated on the health metric, and the feelings of acting healthy which would coincided with how far a participant got in the program. The feelings of acting healthy was not a significant predictor of happiness, and therefore it may have influenced the change in significance of the health metric from the model in 2b to 2c. Overall, it can be concluded that the most steady significant effect of happiness was the season a particular did the program in and in particular, spring which maintained as significant throughout the addition of other factors. 

# Question 4
## Create a subset of the data, including only those participants who completed the programme. Create a plot of the average happiness ratings grouped by season and city, that can be used in a presentation to the funders of the project.

All participants who fully completed the program were grouped into a subset of data. Box plots were then produced using the subset data to compare average happiness ratings with the city a participant commenced the program in (Edinburgh or Glasgow). The data was also faceted by season in order to compare all of these variables. 

```{r q4}
## subset data for participants who completed 
subset_data <- 
  subset(couchto5k,week_stopped== 9)

ggplot(data=subset_data, aes(happiness,city,fill=season)) +
  geom_boxplot() +
  facet_wrap(~season) +
  scale_fill_brewer(palette = "PiYG")

```

# Question 5

## Question 5a
## Build a model that predicts the likelihood of dropping out (at all).

In order to build a model that predicts the likelihood of dropping out at all, it was necessary to create a new column of data into two categories based on whether participants fully completed the program or dropped out. Since the data in this column was coded categorically, it had to be converted to a factor string in order to be added to the GLM. A `pairs.panels()` table of the `couchto5k` data was created to get an initial overview of the data and to decide which variables significantly influence the data and should be incorporated into the GLM. After determining that `season` and `selfmot` should be further investigated, two separate GLM's were created for each variable. The `selfmot` GLM returned significant `(B= -.19, p <.001)` as did two of the seasons: spring, `(B = 3.01, p < .001)`, and autumn, `(B = -1.50, p = .05)`. Based on these results, a GLM was created which included both `selfmot` and `season` as predictors. Other factors were added in to see how the model was affected, but it was determined that this was the optimal model for predicting the likelihood of dropping out at all. The results of this model indicated that again the two seasons spring and autumn as well as self-motivation were significant predictors of the model.

```{r q5a}

##Create separate column with who dropped out "D" and who completed "C" 
drop_out <-  
  couchto5k %>%  
  select(week_stopped) %>% 
  mutate(dropped_out = case_when( 
    week_stopped < 9 ~ "D", 
    week_stopped > 8 ~ "C" 
 )) 

##Add a new column to couchto5k data set for drop out and completed 
couchto5k$dropped_out <- drop_out$dropped_out 

##Convert  
dropped_out <- couchto5k$dropped_out %>% 
  factor(c("D","C")) 

couchto5k$dropped_out <- as.factor(couchto5k$dropped_out)

library(psych)
pairs.panels(couchto5k)

selfmotglm <- 
  glm(dropped_out ~ selfmot, data= couchto5k,family="binomial")

summary(selfmotglm)
## significant

seasonglm <- 
  glm(dropped_out ~ season, data = couchto5k,family="binomial")

summary(seasonglm)
##spring and autumn significant 

selfmot_season_glm <- 
  glm(dropped_out ~ selfmot + season, data=couchto5k,family="binomial")

summary(selfmot_season_glm)

```

## Question 5b
## Briefly describe the effects in your model as you would in an academic paper.

The probability of dropping out was affected by multiple factors. Variables such as accountability, age, health, and city were not included in the model because they returned as having an insignificant relationship to the likelihood of dropping out. The self motivation variable was highly significant due to the small p-value, while two seasons, spring and autumn, were also significant. However, when the other insignificant variables were added into the model this made the significant factors return as no longer significant as well. It is reasonable that the self-motivation factor would influence likelihood of drop out because participants who are more motivated should in turn be less likely to drop out of the program. It is interesting that other psychological factors that are like self-motivation, such as accountability, were did not significantly impact the likelihood of dropping out. 

```{r q5b}
```

## Question 5c
## Draw a graph representing the probability of quitting as a function of how self motivated participants were.

The following graph represents the probability of quitting as a function of how self-motivated participants were. In order to create the graph, it was necessary to assign the output of the newly created drop out column to either a 1 or a 0 where participants who completed the program were encoded as a 0 and participants who dropped out were encoded as a 1. The graph highlights that participants are more likely to drop out the lower their self-motivation is, therefore as levels of self-motivation increase, the probability of dropping out decreases. 
```{r q5c, warning=FALSE}

couchto5k <- couchto5k %>% 
  mutate(drop = ifelse(dropped_out == "D", 1,0))

ggplot(data=couchto5k,aes(x=selfmot, y=drop, color=drop)) +
  labs(x='Self-motivation',y='Probability of dropping out') + 
  geom_jitter(size=3,width=0,height=.2,alpha=.1) + scale_y_discrete(breaks=seq(0,2,by=.2)) +
  geom_smooth(method="glm",method.args=list(family="binomial"))


```










